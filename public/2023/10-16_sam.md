## keras cv
  ```py
  from keras_cv.backend import ops
  from keras_cv.models.backbones.vit_det.vit_det_aliases import ViTDetBBackbone
  from keras_cv.models.segmentation.segment_anything.sam_mask_decoder import SAMMaskDecoder
  from keras_cv.models.segmentation.segment_anything.sam_prompt_encoder import SAMPromptEncoder
  from keras_cv.models.segmentation.segment_anything.sam_transformer import TwoWayTransformer
  from keras_cv.models.segmentation.segment_anything.sam import SegmentAnythingModel
  # from keras_cv.models.segmentation.segment_anything.sam_layers import TwoWayMultiHeadAttention

  image_encoder = ViTDetBBackbone(use_rel_pos=False)
  prompt_encoder = SAMPromptEncoder(embed_dim=256, image_embedding_size=(64, 64), input_image_size=(1024, 1024), mask_in_chans=16)
  transformer = TwoWayTransformer(depth=2, embed_dim=256, mlp_dim=2048, num_heads=8)
  mask_decoder = SAMMaskDecoder(transformer_dim=256, transformer=transformer, num_multimask_outputs=3, iou_head_depth=3, iou_head_hidden_dim=256)
  # model = SegmentAnythingModel(backbone=image_encoder, prompt_encoder=prompt_encoder, mask_decoder=mask_decoder)

  def get_prompts(batch_size, prompts="all"):
      rng = np.random.default_rng(0)
      prompts_dict = {}

      if "all" in prompts or "points" in prompts:
          prompts_dict["points"] = ops.convert_to_tensor(rng.integers(0, 1023, (batch_size, 10, 2)), dtype="float32")
          prompts_dict["labels"] = ops.convert_to_tensor(1 * (rng.random((B, 10)) > 0.5), dtype="int32")
      if "all" in prompts or "boxes" in prompts:
          x1y1 = rng.integers(0, 1022, (batch_size, 2))
          x2y2 = rng.integers(x1y1, 1023, (batch_size, 2))
          box = np.stack([x1y1, x2y2], axis=1)
          prompts_dict["boxes"] = ops.convert_to_tensor(box[:, None, ...], dtype="float32")
      if "all" in prompts or "masks" in prompts:
          prompts_dict["masks"] = ops.convert_to_tensor(1.0 * (rng.random((B, 1, 256, 256, 1)) > 0.5), dtype="float32")
      return prompts_dict

  # We use box-only prompting for this test.
  features = image_encoder(np.ones((1, 1024, 1024, 3)))
  outputs_ex = prompt_encoder(get_prompts(1, "boxes"))
  outputs_ex = mask_decoder(
      {
          "image_embeddings": features,
          "image_pe": outputs_ex["dense_positional_embeddings"],
          "sparse_prompt_embeddings": outputs_ex["sparse_embeddings"],
          "dense_prompt_embeddings": outputs_ex["dense_embeddings"],
      },
  )
  masks_ex, iou_pred_ex = outputs_ex["masks"], outputs_ex["iou_pred"]
  ```
## MobileSAM
  ```py
  sys.path.append('../pytorch-image-models/')

  import torch
  from torch.nn import functional as F
  from mobile_sam.modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, Sam, TwoWayTransformer, TinyViT
  from mobile_sam.automatic_mask_generator import SamAutomaticMaskGenerator
  from mobile_sam.predictor import SamPredictor

  device = "cuda" if torch.cuda.is_available() else "cpu"
  checkpoint = "./weights/mobile_sam.pt"
  prompt_embed_dim = 256
  image_size = 1024
  vit_patch_size = 16
  image_embedding_size = image_size // vit_patch_size
  mobile_sam = Sam(
      image_encoder=TinyViT(img_size=1024, in_chans=3, num_classes=1000,
          embed_dims=[64, 128, 160, 320],
          depths=[2, 2, 6, 2],
          num_heads=[2, 4, 5, 10],
          window_sizes=[7, 7, 14, 7],
          mlp_ratio=4.,
          drop_rate=0.,
          drop_path_rate=0.0,
          use_checkpoint=False,
          mbconv_expand_ratio=4.0,
          local_conv_size=3,
          layer_lr_decay=0.8
      ),
      prompt_encoder=PromptEncoder(
          embed_dim=prompt_embed_dim,
          image_embedding_size=(image_embedding_size, image_embedding_size),
          input_image_size=(image_size, image_size),
          mask_in_chans=16,
      ),
      mask_decoder=MaskDecoder(
          num_multimask_outputs=3,
          transformer=TwoWayTransformer(depth=2, embedding_dim=prompt_embed_dim, mlp_dim=2048, num_heads=8),
          transformer_dim=prompt_embed_dim,
          iou_head_depth=3,
          iou_head_hidden_dim=256,
      ),
      pixel_mean=[123.675, 116.28, 103.53],
      pixel_std=[58.395, 57.12, 57.375],
  )

  mobile_sam.eval()
  mobile_sam.load_state_dict(torch.load(checkpoint))
  mobile_sam.to(device=device)
  mobile_sam.eval()
  image = plt.imread("app/assets/picture6.jpg")

  # Using prompts
  predictor = SamPredictor(mobile_sam)
  predictor.set_image(image)
  input_point = np.array([[400, 400]])
  input_label = np.array([1])
  masks, _, _ = predictor.predict(input_point, input_label)

  # or generate masks for an entire image:
  mask_generator = SamAutomaticMaskGenerator(mobile_sam)
  masks = mask_generator.generate(image)

  def show_anns(anns):
      if len(anns) == 0:
          return
      sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
      ax = plt.gca()
      ax.set_autoscale_on(False)

      img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
      img[:,:,3] = 0
      for ann in sorted_anns:
          m = ann['segmentation']
          color_mask = np.concatenate([np.random.random(3), [0.35]])
          img[m] = color_mask
      ax.imshow(img)

  fig = plt.figure(figsize=(20,20))
  plt.imshow(image)
  show_anns(masks)
  plt.axis('off')
  plt.show()

  fig.savefig('aa.jpg')
  ```
***

# MobileSAM Reload weights
## TinyViT
  ```py
  sys.path.append('../learning/sam/')
  from tiny_vit_sam import TinyViT
  import torch
  ss = torch.load("../learning/sam/tinysam.pth", map_location=torch.device('cpu'))
  tt = {kk[len('image_encoder.'):]: vv for kk, vv in ss.items() if kk.startswith('image_encoder.') and kk.split('.')[1] not in ["head", "norm_head"]}

  image_encoder = TinyViT(img_size=1024)
  image_encoder.load_state_dict(tt, strict=False)
  _ = image_encoder.eval()

  import kecam
  from keras_cv_attention_models.segment_anything import image_encoders
  mm = image_encoders.ImageEncoder_TinyViT_5M(pretrained=None)
  tail_align_dict = {"attn_attn_pos": -2, "mlp_ln": -2, "mlp_Dense_0": -2, "mlp_Dense_1": -2}
  additional_transfer = {kecam.attention_layers.MultiHeadPositionalEmbedding: lambda ww: [ww[0].T]}
  kecam.download_and_load.keras_reload_from_torch_model(
      tt,
      mm,
      tail_align_dict=tail_align_dict,
      additional_transfer=additional_transfer,
      save_name="{}_{}_imagenet.h5".format(mm.name, mm.input_shape[1]),
      do_predict=False,
      do_convert=True,
  )

  inputs = kecam.backend.numpy_image_resize(kecam.test_images.cat(), [1024, 1024])[None] / 128 - 1
  keras_out = mm(inputs).numpy()
  torch_out = image_encoder(torch.from_numpy(inputs.transpose([0, 3, 1, 2]))).detach()
  print(np.allclose(torch_out.permute([0, 2, 3, 1]).detach().cpu(), keras_out, atol=1e-5))
  # True
  print(f"{torch_out.min() = }, {torch_out.max() = }")
  # torch_out.min() = tensor(-0.6095), torch_out.max() = tensor(0.5755)
  print(f"{keras_out.min() = }, {keras_out.max() = }")
  # keras_out.min() = -0.60954326, keras_out.max() = 0.57550085
  ```
## MaskDecoder
  ```py
  sys.path.append('../learning/sam/')
  from mask_decoder import MaskDecoder
  from transformer import TwoWayTransformer
  import torch

  torch_mask_decoder = MaskDecoder(transformer=TwoWayTransformer())
  ss = torch.load("../learning/sam/tinysam.pth", map_location=torch.device('cpu'))
  torch_mask_decoder.load_state_dict({kk[len('mask_decoder.'):]: vv for kk, vv in ss.items() if kk.startswith('mask_decoder.')})
  _ = torch_mask_decoder.eval()

  import kecam
  from keras_cv_attention_models.segment_anything import mask_decoder
  mm = mask_decoder.MaskDecoder()

  full_name_align_dict = {
      "attn_0_cross_tokens_query": -2, "attn_0_cross_tokens_key": -2, "attn_0_cross_tokens_value": -3,
      "attn_0_cross_tokens_output": -4, "attn_0_cross_tokens_ln": -5,
      # "up_2_conv_transpose": -4,
      "up_1_ln": -4, "up_2_conv_transpose": -8,
      "masks_1_dense_2": -3, "masks_1_dense_3": -7,
      "masks_2_dense_2": -2, "masks_2_dense_3": -5,
      "masks_3_dense_2": -1, "masks_3_dense_3": -3, "masks_4_dense_3": -1,
  }

  tt = torch_mask_decoder.state_dict()
  tt["mask_tokens.weight"] = torch.concat([tt["iou_token.weight"], tt["mask_tokens.weight"]], axis=0)[None]
  _ = tt.pop("iou_token.weight")
  kecam.download_and_load.keras_reload_from_torch_model(tt, mm, full_name_align_dict=full_name_align_dict, do_convert=True, do_predict=False)

  image_embeddings = np.random.uniform(size=[1, 64, 64, 256]).astype("float32")
  image_pe = np.random.normal(size=[1, 64, 64, 256]).astype("float32")
  sparse_prompt_embeddings = np.random.uniform(size=[1, 2, 256]).astype("float32") * 400
  dense_prompt_embeddings = np.random.normal(size=[1, 64, 64, 256]).astype("float32")
  low_res_masks, iou_predictions = torch_mask_decoder.predict_masks(
      torch.from_numpy(image_embeddings).permute([0, 3, 1, 2]),
      torch.from_numpy(image_pe).permute([0, 3, 1, 2]),
      torch.from_numpy(sparse_prompt_embeddings),
      torch.from_numpy(dense_prompt_embeddings).permute([0, 3, 1, 2]),
  )

  masks, iou_pred = mm([image_embeddings + dense_prompt_embeddings, sparse_prompt_embeddings, image_pe])
  print(f"{low_res_masks.shape = }, {iou_predictions.shape = }")
  # low_res_masks.shape = torch.Size([1, 4, 256, 256]), iou_predictions.shape = torch.Size([1, 4])
  print(f"{masks.shape = }, {iou_pred.shape = }")
  # masks.shape = TensorShape([1, 256, 256, 4]), iou_pred.shape = TensorShape([1, 4])
  print(np.allclose(iou_predictions.detach(), iou_pred))
  # True
  print(np.allclose(low_res_masks.detach(), masks, atol=1e-3))
  # True
  ```
  **TinySAM `output_hypernetworks_mlps.0.layers` almost being all zeros**
  ```py
  import torch
  ss = torch.load("../learning/sam/tinysam.pth", map_location=torch.device('cpu'))

  ww = ss["mask_decoder.output_hypernetworks_mlps.0.layers.0.weight"]
  print(ww[torch.where(ww.abs() > 1e-6)])
  # tensor([-0.1436, -0.0390,  0.3668,  0.2065,  0.1118, -0.0201,  0.1688])
  ww = ss["mask_decoder.output_hypernetworks_mlps.0.layers.1.weight"]
  print(ww[torch.where(ww.abs() > 1e-6)])
  # tensor([0.1090, 0.0203, 0.8415, 0.0125, 0.2405, 0.1774])
  ww = ss["mask_decoder.output_hypernetworks_mlps.0.layers.2.weight"]
  print(ww[torch.where(ww.abs() > 1e-6)])
  # tensor([])
  ```
## PromptEncoder
  ```py
  sys.path.append('../learning/sam/')
  import torch
  from prompt_encoder import PromptEncoder
  prompt_encoder = PromptEncoder()
  ss = torch.load("../learning/sam/tinysam.pth", map_location=torch.device('cpu'))
  tt = {kk[len('prompt_encoder.'):]: vv for kk, vv in ss.items() if kk.startswith('prompt_encoder.')}
  prompt_encoder.load_state_dict(tt)
  _ = prompt_encoder.mask_downscaling.eval()

  """ MaskEncoder """
  import kecam
  from keras_cv_attention_models.segment_anything.prompt_encoder import PromptEncoder
  mm = PromptEncoder.MaskEncoder()
  kecam.download_and_load.keras_reload_from_torch_model(prompt_encoder.mask_downscaling.state_dict(), mm)

  inputs = np.random.uniform(size=[1, 32, 32, 1]).astype("float32")
  keras_out = mm(inputs)
  torch_out = prompt_encoder.mask_downscaling(torch.from_numpy(inputs).permute([0, 3, 1, 2])).permute([0, 2, 3, 1])
  print(np.allclose(torch_out.detach(), keras_out, atol=1e-5))
  # True

  """ EmptyMask """
  mm = PromptEncoder.EmptyMask()
  kecam.download_and_load.keras_reload_from_torch_model(prompt_encoder.no_mask_embed.state_dict(), mm, do_predict=False)

  """ PositionEmbeddingRandom """
  mm = PromptEncoder.PositionEmbeddingRandom()
  kecam.download_and_load.keras_reload_from_torch_model(prompt_encoder.pe_layer.state_dict(), mm, do_predict=False)

  """ BboxesEncoder """
  mm = PromptEncoder.BboxesEncoder()
  ww = {'ww': torch.stack([prompt_encoder.point_embeddings[2].weight, prompt_encoder.point_embeddings[3].weight], axis=1).detach()}
  kecam.download_and_load.keras_reload_from_torch_model(ww, mm, do_predict=False)

  """ PointsEncoder """
  mm = PromptEncoder.PointsEncoder()
  ww = {'ww': torch.concat([prompt_encoder.not_a_point_embed.weight, prompt_encoder.point_embeddings[0].weight, prompt_encoder.point_embeddings[1].weight], axis=0).detach()}
  kecam.download_and_load.keras_reload_from_torch_model(ww, mm, do_predict=False)
  ```
## Predict
***

# EfficientViTL0 SAM Reload weights
## EfficientViTL0
  ```py
  sys.path.append("../pytorch-image-models/")
  sys.path.append('../efficientvit/')
  import torch
  import torch
  from efficientvit.sam_model_zoo import create_sam_model
  tt = create_sam_model('l0', weight_url='EfficientViT-L0-SAM.pt')
  _ = tt.eval()

  import kecam
  from keras_cv_attention_models.segment_anything import image_encoders
  mm = image_encoders.ImageEncoder_EfficientViT_L0()

  full_name_align_dict = {"features_3_conv": -1, "features_2_conv": -2, "features_3_bn": -3, "features_2_bn": -2}
  kecam.download_and_load.keras_reload_from_torch_model(tt.image_encoder, mm, full_name_align_dict=full_name_align_dict)

  inputs = kecam.backend.numpy_image_resize(kecam.test_images.cat(), [1024, 1024])[None] / 128 - 1
  keras_out = mm(inputs)
  torch_out = tt.image_encoder(torch.from_numpy(inputs.transpose([0, 3, 1, 2]))).detach()
  print(np.allclose(torch_out.permute([0, 2, 3, 1]).detach().cpu(), keras_out, atol=2e-1))
  # True
  print(f"{torch_out.min() = }, {torch_out.max() = }")
  # torch_out.min() = tensor(-0.6095), torch_out.max() = tensor(0.5755)

  aa = keras.models.Model(mm.inputs, mm.get_layer('stack4_block2_mlp_output').output)(inputs)
  bb = tt.image_encoder.bakbone.forward_features(torch.from_numpy(inputs.transpose([0, 3, 1, 2]).astype('float32')))
  np.allclose(bb.reshape(aa.shape.as_list()).detach().cpu(), aa, atol=1e-3)
  ```
  ```py
  os.environ["KECAM_BACKEND"] = "torch"
  import kecam
  from keras_cv_attention_models.segment_anything import image_encoders
  mm = image_encoders.ImageEncoder_EfficientViT_L0()
  mm.load_weights('image_encoder_imagenet.h5')

  sys.path.append("../pytorch-image-models/")
  sys.path.append('../efficientvit/')
  import torch
  import torch
  from efficientvit.sam_model_zoo import create_sam_model
  tt = create_sam_model('l0', weight_url='EfficientViT-L0-SAM.pt')
  _ = tt.eval()

  inputs = kecam.backend.numpy_image_resize(kecam.test_images.cat(), [1024, 1024])[None].astype('float32') / 128 - 1
  torch_out = tt.image_encoder(torch.from_numpy(inputs.transpose([0, 3, 1, 2]))).detach()

  """ Torch """
  keras_out = mm(torch.from_numpy(inputs.transpose([0, 3, 1, 2]))).detach()
  print(np.allclose(torch_out.detach().cpu(), keras_out, atol=2e-4))
  # True

  """ TF """
  keras_out = mm(inputs)
  print(np.allclose(torch_out.permute([0, 2, 3, 1]).detach().cpu(), keras_out, atol=1e-1))  # TF bicubic resize if different from Torch
  # True
  ```
## MaskDecoder
  ```py
  sys.path.append('../efficientvit/')
  sys.path.append('../pytorch-image-models/')
  import torch
  from efficientvit.sam_model_zoo import create_sam_model
  tt = create_sam_model('l0', weight_url='EfficientViT-L0-SAM.pt')
  _ = tt.eval()

  ss = {"mask_tokens.weight": torch.concat([tt.mask_decoder.state_dict()["iou_token.weight"], tt.mask_decoder.state_dict()["mask_tokens.weight"]], axis=0)[None]}
  ss.update({kk: vv for kk, vv in tt.mask_decoder.state_dict().items() if kk not in ["mask_tokens.weight", "iou_token.weight"]})

  import kecam
  from keras_cv_attention_models.segment_anything import mask_decoder
  mm = mask_decoder.MaskDecoder(pretrained=None)

  full_name_align_dict = {
      "attn_0_cross_tokens_query": -2, "attn_0_cross_tokens_key": -2, "attn_0_cross_tokens_value": -3,
      "attn_0_cross_tokens_output": -4, "attn_0_cross_tokens_ln": -9, "attn_1_cross_tokens_ln": -4,
      "up_1_ln": -4, "up_2_conv_transpose": -8, "masks_top_dense_2": -3, "masks_top_dense_3": -6,
      "masks_left_dense_2": -2, "masks_left_dense_3": -4,
      "masks_bottom_dense_2": -1, "masks_bottom_dense_3": -2,
  }

  kecam.download_and_load.keras_reload_from_torch_model(ss, mm, full_name_align_dict=full_name_align_dict, do_convert=True, do_predict=False)

  image_embeddings = np.random.uniform(size=[1, 64, 64, 256]).astype("float32")
  image_pe = np.random.normal(size=[1, 64, 64, 256]).astype("float32")
  sparse_prompt_embeddings = np.random.uniform(size=[1, 2, 256]).astype("float32") * 400
  dense_prompt_embeddings = np.random.normal(size=[1, 64, 64, 256]).astype("float32")
  low_res_masks, iou_predictions = tt.mask_decoder.predict_masks(
      torch.from_numpy(image_embeddings).permute([0, 3, 1, 2]),
      torch.from_numpy(image_pe).permute([0, 3, 1, 2]),
      torch.from_numpy(sparse_prompt_embeddings),
      torch.from_numpy(dense_prompt_embeddings).permute([0, 3, 1, 2]),
  )

  masks, iou_pred = mm([image_embeddings + dense_prompt_embeddings, sparse_prompt_embeddings, image_pe])
  print(f"{low_res_masks.shape = }, {iou_predictions.shape = }")
  # low_res_masks.shape = torch.Size([1, 4, 256, 256]), iou_predictions.shape = torch.Size([1, 4])
  print(f"{masks.shape = }, {iou_pred.shape = }")
  # masks.shape = TensorShape([1, 256, 256, 4]), iou_pred.shape = TensorShape([1, 4])
  print(np.allclose(iou_predictions.detach(), iou_pred))
  # True
  print(np.allclose(low_res_masks.detach().permute([0, 2, 3, 1]), masks, atol=1e-3))
  # True
  ```
## PromptEncoder
  ```py
  sys.path.append('../efficientvit/')
  sys.path.append('../pytorch-image-models/')
  import torch
  from efficientvit.sam_model_zoo import create_sam_model
  tt = create_sam_model('l0', weight_url='EfficientViT-L0-SAM.pt')
  _ = tt.eval()

  prompt_encoder = tt.prompt_encoder

  """ MaskEncoder """
  import kecam
  from keras_cv_attention_models.segment_anything import sam
  mm = sam.MaskEncoder()
  kecam.download_and_load.keras_reload_from_torch_model(prompt_encoder.mask_downscaling.state_dict(), mm)

  inputs = np.random.uniform(size=[1, 32, 32, 1]).astype("float32")
  keras_out = mm(inputs)
  torch_out = prompt_encoder.mask_downscaling(torch.from_numpy(inputs).permute([0, 3, 1, 2])).permute([0, 2, 3, 1])
  print(np.allclose(torch_out.detach(), keras_out, atol=1e-5))
  # True

  """ EmptyMask """
  mm = sam.EmptyMask()
  kecam.download_and_load.keras_reload_from_torch_model(prompt_encoder.no_mask_embed.state_dict(), mm, do_predict=False)

  """ PositionEmbeddingRandom """
  mm = sam.PositionEmbeddingRandom()
  kecam.download_and_load.keras_reload_from_torch_model(prompt_encoder.pe_layer.state_dict(), mm, do_predict=False)

  """ BboxesEncoder """
  mm = sam.BboxesEncoder()
  ww = {'ww': torch.stack([prompt_encoder.point_embeddings[2].weight, prompt_encoder.point_embeddings[3].weight], axis=1).detach()}
  kecam.download_and_load.keras_reload_from_torch_model(ww, mm, do_predict=False)

  """ PointsEncoder """
  mm = sam.PointsEncoder()
  ww = {'ww': torch.concat([prompt_encoder.not_a_point_embed.weight, prompt_encoder.point_embeddings[0].weight, prompt_encoder.point_embeddings[1].weight], axis=0).detach()}
  kecam.download_and_load.keras_reload_from_torch_model(ww, mm, do_predict=False)
  ```
## Precidt
  ```py
  sys.path.append("../pytorch-image-models/")
  sys.path.append('../efficientvit/')
  import torch
  import torch
  from efficientvit.sam_model_zoo import create_sam_model
  from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor
  tt = create_sam_model('l0', weight_url='EfficientViT-L0-SAM.pt')
  _ = tt.eval()
  efficientvit_sam_predictor = EfficientViTSamPredictor(tt)

  from keras_cv_attention_models import test_images
  from keras_cv_attention_models.segment_anything import sam
  point_coords, point_labels, box = np.array([(400, 400)]), np.array([1]), np.array([256, 256, 512, 512])

  image = test_images.dog_cat()
  efficientvit_sam_predictor.set_image(image)
  masks, _, _ = efficientvit_sam_predictor.predict(point_coords=point_coords, point_labels=point_labels, box=box, multimask_output=True)
  # plt.imsave('aa.jpg', masks.transpose([1, 2, 0]).astype('uint8') * 255)
  sam.SAM.show(image, masks, points=point_coords, labels=point_labels, save_path='cc.jpg')

  """ Raw efficientvit one """
  from demo_sam_model import draw_scatter, draw_binary_mask, cat_images
  plots = [draw_scatter(draw_binary_mask(
          image, binary_mask, (0, 0, 255)), point_coords, color=["g" if l == 1 else "r" for l in point_labels], s=10, ew=0.25, tmp_name="foo.png"
      ) for binary_mask in masks]
  plots = cat_images(plots, axis=1)
  plt.imsave('foo.png', plots)
  ```
  **KECAM one**
  ```py
  from keras_cv_attention_models import test_images
  from keras_cv_attention_models.segment_anything import sam
  mm = sam.SAM(image_encoder='efficientvit_l0',pretrained="efficientvit_l0")
  image = test_images.dog_cat()
  points, labels = np.array([[400, 400]]), np.array([1])
  masks, iou_predictions, low_res_masks = mm(image, points, labels)
  fig = mm.show(image, masks, iou_predictions, points=points, labels=labels, save_path='aa.jpg')
  ```
***

# Rename Maks decoder
```py
from keras_cv_attention_models import segment_anything, model_surgery, test_images, models
mm = segment_anything.mask_decoder.MaskDecoder(pretrained=None)
mm.load_weights('/home/leondgarse/.keras/models/mobile_sam_5m_mask_decoder_sam.h5', by_name=True)
mm.save('mobile_sam_5m_mask_decoder_sam.h5')

from keras_cv_attention_models import segment_anything, model_surgery, test_images, models
mm = segment_anything.mask_decoder.MaskDecoder(pretrained=None)
mm.load_weights('/home/leondgarse/.keras/models/efficientvit_sam_l0_mask_decoder_sam.h5', by_name=True)
mm.save('efficientvit_sam_l0_mask_decoder_sam.h5')
```
```py
from keras_cv_attention_models import segment_anything, model_surgery, test_images, models
mm = segment_anything.mask_decoder.MaskDecoder(pretrained=None)
mm.load_weights('mobile_sam_5m_mask_decoder_sam.h5')
mm.save('mobile_sam_5m_mask_decoder_sam.h5')

from keras_cv_attention_models import segment_anything, model_surgery, test_images, models
mm = segment_anything.mask_decoder.MaskDecoder(pretrained=None)
mm.load_weights('efficientvit_sam_l0_mask_decoder_sam.h5')
mm.save('efficientvit_sam_l0_mask_decoder_sam.h5')
```
```py
from keras_cv_attention_models import segment_anything, model_surgery, test_images, models

ccs = ["PointsEncoder", "BboxesEncoder", "MaskEncoder", "EmptyMask", "PositionEmbeddingRandom"]
for cc in ccs:
    print(cc)
    aa = getattr(segment_anything.sam, cc)()
    bb = getattr(segment_anything.sam, cc)(name=aa.name.replace("mobile_sam_5m", "efficientvit_sam_l0"))
    print([np.allclose(ii, jj) for ii, jj in zip(aa.get_weights(), bb.get_weights())])
```
```py
from keras_cv_attention_models import segment_anything, model_surgery, test_images, models
mm = segment_anything.sam.PointsEncoder()
mm.save(mm.name + "_sam.h5")
```
