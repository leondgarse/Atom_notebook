## keras cv
  ```py
  from keras_cv.backend import ops
  from keras_cv.models.backbones.vit_det.vit_det_aliases import ViTDetBBackbone
  from keras_cv.models.segmentation.segment_anything.sam_mask_decoder import SAMMaskDecoder
  from keras_cv.models.segmentation.segment_anything.sam_prompt_encoder import SAMPromptEncoder
  from keras_cv.models.segmentation.segment_anything.sam_transformer import TwoWayTransformer
  from keras_cv.models.segmentation.segment_anything.sam import SegmentAnythingModel
  # from keras_cv.models.segmentation.segment_anything.sam_layers import TwoWayMultiHeadAttention

  image_encoder = ViTDetBBackbone(use_rel_pos=False)
  prompt_encoder = SAMPromptEncoder(embed_dim=256, image_embedding_size=(64, 64), input_image_size=(1024, 1024), mask_in_chans=16)
  transformer = TwoWayTransformer(depth=2, embed_dim=256, mlp_dim=2048, num_heads=8)
  mask_decoder = SAMMaskDecoder(transformer_dim=256, transformer=transformer, num_multimask_outputs=3, iou_head_depth=3, iou_head_hidden_dim=256)
  # model = SegmentAnythingModel(backbone=image_encoder, prompt_encoder=prompt_encoder, mask_decoder=mask_decoder)

  def get_prompts(batch_size, prompts="all"):
      rng = np.random.default_rng(0)
      prompts_dict = {}

      if "all" in prompts or "points" in prompts:
          prompts_dict["points"] = ops.convert_to_tensor(rng.integers(0, 1023, (batch_size, 10, 2)), dtype="float32")
          prompts_dict["labels"] = ops.convert_to_tensor(1 * (rng.random((B, 10)) > 0.5), dtype="int32")
      if "all" in prompts or "boxes" in prompts:
          x1y1 = rng.integers(0, 1022, (batch_size, 2))
          x2y2 = rng.integers(x1y1, 1023, (batch_size, 2))
          box = np.stack([x1y1, x2y2], axis=1)
          prompts_dict["boxes"] = ops.convert_to_tensor(box[:, None, ...], dtype="float32")
      if "all" in prompts or "masks" in prompts:
          prompts_dict["masks"] = ops.convert_to_tensor(1.0 * (rng.random((B, 1, 256, 256, 1)) > 0.5), dtype="float32")
      return prompts_dict

  # We use box-only prompting for this test.
  features = image_encoder(np.ones((1, 1024, 1024, 3)))
  outputs_ex = prompt_encoder(get_prompts(1, "boxes"))
  outputs_ex = mask_decoder(
      {
          "image_embeddings": features,
          "image_pe": outputs_ex["dense_positional_embeddings"],
          "sparse_prompt_embeddings": outputs_ex["sparse_embeddings"],
          "dense_prompt_embeddings": outputs_ex["dense_embeddings"],
      },
  )
  masks_ex, iou_pred_ex = outputs_ex["masks"], outputs_ex["iou_pred"]
  ```
## MobileSAM
  ```py
  sys.path.append('../pytorch-image-models/')

  import torch
  from torch.nn import functional as F
  from mobile_sam.modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, Sam, TwoWayTransformer, TinyViT
  from mobile_sam.automatic_mask_generator import SamAutomaticMaskGenerator
  from mobile_sam.predictor import SamPredictor

  device = "cuda" if torch.cuda.is_available() else "cpu"
  checkpoint = "./weights/mobile_sam.pt"
  prompt_embed_dim = 256
  image_size = 1024
  vit_patch_size = 16
  image_embedding_size = image_size // vit_patch_size
  mobile_sam = Sam(
      image_encoder=TinyViT(img_size=1024, in_chans=3, num_classes=1000,
          embed_dims=[64, 128, 160, 320],
          depths=[2, 2, 6, 2],
          num_heads=[2, 4, 5, 10],
          window_sizes=[7, 7, 14, 7],
          mlp_ratio=4.,
          drop_rate=0.,
          drop_path_rate=0.0,
          use_checkpoint=False,
          mbconv_expand_ratio=4.0,
          local_conv_size=3,
          layer_lr_decay=0.8
      ),
      prompt_encoder=PromptEncoder(
          embed_dim=prompt_embed_dim,
          image_embedding_size=(image_embedding_size, image_embedding_size),
          input_image_size=(image_size, image_size),
          mask_in_chans=16,
      ),
      mask_decoder=MaskDecoder(
          num_multimask_outputs=3,
          transformer=TwoWayTransformer(depth=2, embedding_dim=prompt_embed_dim, mlp_dim=2048, num_heads=8),
          transformer_dim=prompt_embed_dim,
          iou_head_depth=3,
          iou_head_hidden_dim=256,
      ),
      pixel_mean=[123.675, 116.28, 103.53],
      pixel_std=[58.395, 57.12, 57.375],
  )

  mobile_sam.eval()
  mobile_sam.load_state_dict(torch.load(checkpoint))
  mobile_sam.to(device=device)
  mobile_sam.eval()
  image = plt.imread("app/assets/picture6.jpg")

  # Using prompts
  predictor = SamPredictor(mobile_sam)
  predictor.set_image(image)
  input_point = np.array([[400, 400]])
  input_label = np.array([1])
  masks, _, _ = predictor.predict(input_point, input_label)

  # or generate masks for an entire image:
  mask_generator = SamAutomaticMaskGenerator(mobile_sam)
  masks = mask_generator.generate(image)

  def show_anns(anns):
      if len(anns) == 0:
          return
      sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
      ax = plt.gca()
      ax.set_autoscale_on(False)

      img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
      img[:,:,3] = 0
      for ann in sorted_anns:
          m = ann['segmentation']
          color_mask = np.concatenate([np.random.random(3), [0.35]])
          img[m] = color_mask
      ax.imshow(img)

  fig = plt.figure(figsize=(20,20))
  plt.imshow(image)
  show_anns(masks)
  plt.axis('off')
  plt.show()

  fig.savefig('aa.jpg')
  ```
