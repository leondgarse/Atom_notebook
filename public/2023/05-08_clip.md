- [Github taki0112/CLIP-Tensorflow](https://github.com/taki0112/CLIP-Tensorflow)
- [Github lucidrains/x-clip](https://github.com/lucidrains/x-clip)
- [Multilingual CLIP with Huggingface + PyTorch Lightning](https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html)

# CLIP tensorflow
```py
import clip
text_transformer = clip.TextTransformer(dim=512, num_tokens=10000, max_seq_len=256, depth=6, heads=8, dim_head=64, rotary_pos_emb=False)
text = tf.random.uniform([4, 256], minval=0, maxval=10000, dtype=tf.int32)
print(text_transformer(text, text != 0).shape)
# TensorShape([4, 257, 512])

visual_transformer = clip.VisionTransformer(dim=512, image_size=256, patch_size=32, depth=6, heads=8, dim_head=64)
images = tf.random.normal([4, 256, 256, 3])
print(visual_transformer(images).shape)
# (4, 65, 512)

text_pad_id = 0
text_mask = text != text_pad_id
enc_text = text_transformer(text, text_mask)
enc_image = visual_transformer(images)

# early return of encodings, if needed (for DALL-E2)
# return enc_text, enc_image

to_text_latent = keras.layers.Dense(units=512, use_bias=False)
to_visual_latent = keras.layers.Dense(units=512, use_bias=False)

# project to latents
# depending on whether to do fine-grained CLIP or not, select either all tokens, or CLS tokens only
text_latents = to_text_latent(enc_text[:, 0])
image_latents = to_visual_latent(enc_image[:, 0])

def clip_loss(text_latents, image_latents, temperature) :
    # normalized features
    image_latents = image_latents / tf.norm(tensor=image_latents, ord="euclidean", axis=-1, keepdims=True)
    text_latents = text_latents / tf.norm(tensor=text_latents, ord="euclidean", axis=-1, keepdims=True)

    # cosine similarity as logits
    temperature = tf.math.exp(temperature)
    logits_per_text = tf.matmul(text_latents, image_latents, transpose_b=True) * temperature
    logits_per_image = tf.transpose(logits_per_text)
    similarity = logits_per_text

    caption_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(similarity.shape[0]), similarity, from_logits=True))
    image_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(similarity.shape[1]), tf.transpose(similarity), from_logits=True))
    return (caption_loss + image_loss) / 2.0

# calculate loss
# cl_loss = lucidrains_loss(text_latents, image_latents, self.temperature)
temperature = tf.Variable(tf.constant(1.0, dtype=tf.float32))
cl_loss = clip_loss(text_latents, image_latents, temperature)
```
```py
# https://github.com/lucidrains/x-clip
def lucidrains_loss(text_latents, image_latents, temperature):
    # equal to clip_loss
    num_batch_texts = num_batch_images = 1
    text_latents, image_latents = map(l2norm, (text_latents, image_latents))

    # get temperature
    temp = tf.exp(temperature)

    # split out multiview dimension for text and images
    text_latents = rearrange(text_latents, '(m b) ... -> m b ...', m=num_batch_texts)
    image_latents = rearrange(image_latents, '(m b) ... -> m b ...', m=num_batch_images)

    # calculate loss
    text_to_image = einsum('m t d, n i d -> m n t i', text_latents, image_latents) * temp
    image_to_text = rearrange(text_to_image, '... t i -> ... i t')

    text_to_image = rearrange(text_to_image, 'm n ... -> (m n) ...')
    image_to_text = rearrange(image_to_text, 'm n ... -> (m n) ...')

    # exponentiate
    text_to_image_exp, image_to_text_exp = map(tf.exp, (text_to_image, image_to_text))

    # numerators
    text_to_image_pos, image_to_text_pos = map(matrix_diag, (text_to_image_exp, image_to_text_exp))

    # denominator
    text_to_image_denom, image_to_text_denom = map(lambda t: tf.reduce_sum(t, axis=-1),
                                                   (text_to_image_exp, image_to_text_exp))

    # loss
    text_to_image_loss = tf.reduce_mean(-log(text_to_image_pos / text_to_image_denom), axis=-1)
    image_to_text_loss = tf.reduce_mean(-log(image_to_text_pos / image_to_text_denom), axis=-1)

    # calculate CL loss
    cl_loss = (text_to_image_loss + image_to_text_loss) / 2

    return cl_loss
```
