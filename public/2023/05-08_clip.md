- [Github taki0112/CLIP-Tensorflow](https://github.com/taki0112/CLIP-Tensorflow)
- [Github lucidrains/x-clip](https://github.com/lucidrains/x-clip)
- [Multilingual CLIP with Huggingface + PyTorch Lightning](https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html)

# CLIP tensorflow
  ```py
  import clip
  text_transformer = clip.TextTransformer(dim=512, num_tokens=10000, max_seq_len=256, depth=6, heads=8, dim_head=64, rotary_pos_emb=False)
  text = tf.random.uniform([4, 256], minval=0, maxval=10000, dtype=tf.int32)
  print(text_transformer(text, text != 0).shape)
  # TensorShape([4, 257, 512])

  visual_transformer = clip.VisionTransformer(dim=512, image_size=256, patch_size=32, depth=6, heads=8, dim_head=64)
  images = tf.random.normal([4, 256, 256, 3])
  print(visual_transformer(images).shape)
  # (4, 65, 512)

  text_pad_id = 0
  text_mask = text != text_pad_id
  enc_text = text_transformer(text, text_mask)
  enc_image = visual_transformer(images)

  # early return of encodings, if needed (for DALL-E2)
  # return enc_text, enc_image

  to_text_latent = keras.layers.Dense(units=512, use_bias=False)
  to_visual_latent = keras.layers.Dense(units=512, use_bias=False)

  # project to latents
  # depending on whether to do fine-grained CLIP or not, select either all tokens, or CLS tokens only
  text_latents = to_text_latent(enc_text[:, 0])
  image_latents = to_visual_latent(enc_image[:, 0])

  def clip_loss(y_true, y_pred) :
      # normalized features
      half_split = y_pred.shape[-1] // 2
      text_latents, image_latents = y_pred[:, :half_split], y_pred[:, half_split:]
      image_latents = image_latents / tf.norm(tensor=image_latents, ord="euclidean", axis=-1, keepdims=True)
      text_latents = text_latents / tf.norm(tensor=text_latents, ord="euclidean", axis=-1, keepdims=True)

      # cosine similarity as logits
      logits_per_text = tf.matmul(text_latents, image_latents, transpose_b=True)
      logits_per_image = tf.transpose(logits_per_text)
      similarity = logits_per_text

      caption_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[0]), similarity, from_logits=True))
      image_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[1]), tf.transpose(similarity), from_logits=True))
      return (caption_loss + image_loss) / 2.0

  # calculate loss
  # cl_loss = lucidrains_loss(text_latents, image_latents, self.temperature)
  # temperature = tf.Variable(tf.constant(1.0, dtype=tf.float32))
  cl_loss = clip_loss(None, [text_latents, image_latents])
  ```
  ```py
  # https://github.com/lucidrains/x-clip
  def lucidrains_loss(text_latents, image_latents, temperature):
      # equal to clip_loss
      num_batch_texts = num_batch_images = 1
      text_latents, image_latents = map(l2norm, (text_latents, image_latents))

      # get temperature
      temp = tf.exp(temperature)

      # split out multiview dimension for text and images
      text_latents = rearrange(text_latents, '(m b) ... -> m b ...', m=num_batch_texts)
      image_latents = rearrange(image_latents, '(m b) ... -> m b ...', m=num_batch_images)

      # calculate loss
      text_to_image = einsum('m t d, n i d -> m n t i', text_latents, image_latents) * temp
      image_to_text = rearrange(text_to_image, '... t i -> ... i t')

      text_to_image = rearrange(text_to_image, 'm n ... -> (m n) ...')
      image_to_text = rearrange(image_to_text, 'm n ... -> (m n) ...')

      # exponentiate
      text_to_image_exp, image_to_text_exp = map(tf.exp, (text_to_image, image_to_text))

      # numerators
      text_to_image_pos, image_to_text_pos = map(matrix_diag, (text_to_image_exp, image_to_text_exp))

      # denominator
      text_to_image_denom, image_to_text_denom = map(lambda t: tf.reduce_sum(t, axis=-1),
                                                     (text_to_image_exp, image_to_text_exp))

      # loss
      text_to_image_loss = tf.reduce_mean(-log(text_to_image_pos / text_to_image_denom), axis=-1)
      image_to_text_loss = tf.reduce_mean(-log(image_to_text_pos / image_to_text_denom), axis=-1)

      # calculate CL loss
      cl_loss = (text_to_image_loss + image_to_text_loss) / 2

      return cl_loss
  ```
## Multi inputs outputs model train test
  ```py
  from keras_cv_attention_models.backend import layers, models, functional

  inputs_1 = layers.Input([32])
  inputs_2 = layers.Input([32])
  mm = models.Model([inputs_1, inputs_2], functional.concat([inputs_1 + inputs_2, inputs_1 * inputs_2], axis=-1))
  print(f"{mm.input_shape = }, {mm.output_shape = }")

  xx_1 = tf.random.uniform([256, 224, 224, 3])
  xx_2 = tf.random.uniform([256, 32], 0, 1024, dtype='int64')
  dd = tf.data.Dataset.from_tensor_slices((xx_1, xx_2))
  dd = dd.map(lambda xx_1, xx_2: ((xx_1, xx_2), 0)).batch(16)

  def multi_loss(y_true, y_pred):
      # tf.print(y_true.shape, y_pred.shape)
      return y_pred[:, :32] - y_pred[:, 32:]

  mm.compile(loss=multi_loss)
  mm.fit(dd)
  ```
## CLIP model class
  ```py
  import math
  from keras_cv_attention_models.backend import layers, models, functional
  from keras_cv_attention_models.attention_layers import ExpLogitScale
  from keras_cv_attention_models import fastervit, gpt2

  def clip_loss(y_true, y_pred) :
      # normalized features
      half_split = y_pred.shape[-1] // 2
      text_latents, image_latents = y_pred[:, :half_split], y_pred[:, half_split:]
      image_latents = image_latents / tf.norm(tensor=image_latents, ord="euclidean", axis=-1, keepdims=True)
      text_latents = text_latents / tf.norm(tensor=text_latents, ord="euclidean", axis=-1, keepdims=True)

      # cosine similarity as logits
      logits_per_text = tf.matmul(text_latents, image_latents, transpose_b=True)
      logits_per_image = tf.transpose(logits_per_text)
      similarity = logits_per_text

      caption_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[0]), similarity, from_logits=True))
      image_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[1]), tf.transpose(similarity), from_logits=True))
      return (caption_loss + image_loss) / 2.0


  def convert_to_clip_model(image_model, text_model):
      image_input, image_output = image_model.inputs[0], image_model.outputs[-1]
      text_input, text_output = text_model.inputs[0], text_model.outputs[-1]

      # image_output = layers.Dense(latents_dim, use_bias=False, name="image_latents")(image_output)
      eol_index = functional.argmax(text_input, axis=-1)
      text_output = functional.gather_nd(text_output, functional.expand_dims(eol_index, axis=-1), batch_dims=1)
      text_output = layers.Dense(image_output.shape[-1], use_bias=False, name="text_latents")(text_output)
      text_output = ExpLogitScale(axis=None, init_value=math.log(1 / 0.07), name="temperature")(text_output)
      return models.Model([image_input, text_input], functional.concat([image_output, text_output], axis=-1))

  image_model = fastervit.FasterViT0(num_classes=512, classifier_activation=None)
  text_model = gpt2.GPT2_Base(include_top=False)
  text_model.trainable = False
  print(f"{image_model.output_shape = }, {text_model.output_shape = }")
  # image_model.output_shape = (None, 512), text_model.output_shape = (None, 1024, 768)

  mm = convert_to_clip_model(image_model, text_model)
  print(f"{mm.input_shape = }, {mm.output_shape = }")
  # mm.input_shape = [(None, 224, 224, 3), (None, None)], mm.output_shape = [(None, 512), (None, 512)]

  xx_1 = tf.random.uniform([256, 224, 224, 3])
  xx_2 = tf.random.uniform([256, 77], 0, 1024, dtype='int64')
  dd = tf.data.Dataset.from_tensor_slices((xx_1, xx_2)).map(lambda image, text: ((image, text), 0)).batch(64)
  mm.compile(loss=clip_loss)
  mm.fit(dd)
  ```
***

# COCO tiny caption
## coco caption Datasets
  ```py
  {'captions': {'id': array([529376, 529715, 531782, 531980, 534542]),
    'text': array([b'A toilet and sink in a tiled bathroom',
           b'A unisex bathroom decorated with a vintage theme. ',
           b'A white toilet sitting next to a bidet toilet.',
           b'A bathroom with a toilet, sink, and other bathroom items in it. ',
           b'A bathroom with gold circle patterns containing a toilet, sink towel rack and shelving.'],
          dtype=object)},
   'image': array([[...]], dtype=uint8),
   'image/filename': b'COCO_train2014_000000357057.jpg',
   'image/id': 357057,
   'objects': {'area': array([24057, 12005,  1817,  6639,  6771]),
    'bbox': array([[0.55177826, 0.16220312, 1.        , 0.4548125 ],
           [0.72771966, 0.81904685, 0.98838913, 1.        ],
           [0.        , 0.55546874, 0.11621339, 0.67389065],
           [0.838954  , 0.43260938, 0.9870084 , 0.6518125 ],
           [0.838954  , 0.43842188, 0.9870084 , 0.6537656 ]], dtype=float32),
    'id': array([1092901, 1131489, 1675957, 1981899, 2133571]),
    'is_crowd': array([False, False, False, False, False]),
    'label': array([61, 71, 78, 71, 61])}}
  ```
## COCO tiny caption json
  ```py
  import json

  """ Load info from captions_train2017.json """
  with open('datasets/annotations/captions_train2017.json') as ff:
      aa = json.load(ff)
  image_dict = {ii['id']: ii['file_name'] for ii in aa['images']}
  gg = {}
  for ii in aa['annotations']:
      gg.setdefault(image_dict[ii['image_id']], []).append(ii['caption'])

  """ Match captions with coco_tiny """
  train = []
  for ii in os.listdir('datasets/coco_tiny/train2017/images/'):
      train.extend([{"image": os.path.join("train2017/images", ii), "caption": jj} for jj in gg[ii]])
  test = []
  for ii in os.listdir('datasets/coco_tiny/val2017/images/'):
      test.extend([{"image": os.path.join("val2017/images", ii), "caption": jj} for jj in gg[ii]])
  print(f"{len(train) = }, {len(test) = }")
  # len(train) = 16941, len(test) = 1888

  """ Save json """
  dd = {"train": train, "test": test, "info": {"base_path": "datasets/coco_tiny"}}
  with open('datasets/coco_tiny/coco_tiny_captions.json', 'w') as ff:
      json.dump(dd, ff, indent=2)
  ```
## Train test
```py
from keras_cv_attention_models.clip import simple_tokenizer
from keras_cv_attention_models.imagenet import data

caption_tokenizer = simple_tokenizer.SimpleTokenizer('datasets/bpe_simple_vocab_16e6.txt.gz')
dd = data.init_dataset(data_name='datasets/coco_tiny/coco_tiny_captions.json', caption_tokenizer=caption_tokenizer)[0]

# Show
(aa, bb), cc = dd.as_numpy_iterator().next()
cc = [caption_tokenizer(ii) for ii in bb]
ax = data.show_batch_sample((aa, cc))
ax.get_figure().savefig('aa.png')

# Train
...
mm.fit(dd)
```
