- [Github taki0112/CLIP-Tensorflow](https://github.com/taki0112/CLIP-Tensorflow)
- [Github lucidrains/x-clip](https://github.com/lucidrains/x-clip)
- [Multilingual CLIP with Huggingface + PyTorch Lightning](https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html)
- [kecam_caption_test.ipynb](https://colab.research.google.com/drive/1VaOOE4Q2rD_pV4k3YymY1glqtlNjoikT?usp=sharing)
***

# OpenClip
  ```sh
  python -m training.main \
      --save-frequency 1 \
      --zeroshot-frequency 1 \
      --report-to tensorboard \
      --train-data="/path/to/train_data.csv"  \
      --val-data="/path/to/validation_data.csv"  \
      --csv-img-key filepath \
      --csv-caption-key title \
      --imagenet-val=/path/to/imagenet/root/val/ \
      --warmup 10000 \
      --batch-size=128 \
      --lr=1e-3 \
      --wd=0.1 \
      --epochs=30 \
      --workers=8 \
      --model RN50
  ```
  ```py
  """data"""
  import torch
  import pandas as pd
  from PIL import Image
  from torch.utils.data import Dataset, DataLoader
  from torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor
  from keras_cv_attention_models import clip

  class CsvDataset(Dataset):
      def __init__(self, input_filename, tokenizer, image_size=224, sep="\t"):
          df = pd.read_csv(input_filename, header=None, sep=sep, names=['image', 'caption'])
          self.images = df['image'].tolist()
          self.captions = df['caption'].tolist()

          self.mean, self.std = (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)
          self.transforms = Compose([
              RandomResizedCrop(image_size, scale=(0.9, 1.0), interpolation=InterpolationMode.BICUBIC),
              lambda image: image.convert('RGB'),
              ToTensor(),
              Normalize(mean=self.mean, std=self.std),
          ])
          self.tokenizer = tokenizer

      def tokenize(self, texts, context_length: int = 77):
          if isinstance(texts, str):
              texts = [texts]
          all_tokens = [[self.tokenizer.sot_token] + self.tokenizer.encode(text) + [self.tokenizer.eot_token] for text in texts]
          result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)

          for i, tokens in enumerate(all_tokens):
              if len(tokens) > context_length:
                  tokens = tokens[:context_length]  # Truncate
                  tokens[-1] = eot_token
              result[i, :len(tokens)] = torch.tensor(tokens)
          return result

      def __len__(self):
          return len(self.captions)

      def __getitem__(self, idx):
          images = self.transforms(Image.open(str(self.images[idx])))
          texts = self.tokenize([str(self.captions[idx])])[0]
          return images, texts

  caption_tokenizer = clip.SimpleTokenizer()
  dataset = CsvDataset('datasets/coco_dog_cat/captions.tsv', tokenizer=caption_tokenizer)
  num_samples = len(dataset)

  batch_size, num_workers = 4, 8
  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, sampler=None, drop_last=True)
  dataloader.num_samples = num_samples
  dataloader.num_batches = len(dataloader)
  print(">>>> Data:", [ii.shape for ii in next(iter(dataloader))])

  """model"""
  import torch
  from torch import nn
  import torch.nn.functional as F

  from keras_cv_attention_models import clip, gpt2, beit, backend

  class CLIP(nn.Module):
      def __init__(self, image_model, text_model):
          super().__init__()
          self.image_model, self.text_model = image_model, text_model
          self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

      def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):
          # lock image tower as per LiT - https://arxiv.org/abs/2111.07991
          self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)

      @torch.jit.ignore
      def set_grad_checkpointing(self, enable=True):
          self.visual.set_grad_checkpointing(enable)
          self.transformer.grad_checkpointing = enable

      def forward(self, image, text):
          image_features = F.normalize(self.image_model(image), dim=-1)
          text_features = F.normalize(self.text_model(text), dim=-1)
          return image_features, text_features, self.logit_scale.exp()

  text_model = gpt2.GPT2_Base(include_top=False)
  text_inputs = text_model.inputs[0]
  text_outputs = text_model.outputs[0]
  text_outputs = clip.models.text_model_index_header(text_inputs, text_outputs, 512)
  text_model = backend.models.Model(text_inputs, text_outputs)
  # text_model(torch.ones([1, 77], dtype=torch.long)).shape
  image_model = beit.ViT(num_classes=512, classifier_activation=None)
  model = CLIP(image_model, text_model)
  # model.to(device=device, dtype=dtype)
  # print({ii:jj.shape for ii , jj in model.named_parameters()})

  """optimizer"""
  from torch import optim

  lr, wd, beta1, beta2, eps = 1e-3, 0.2, 0.9, 0.98, 1.0e-6
  named_parameters = list(model.named_parameters())
  exclude = lambda n, p: p.ndim < 2 or "bn" in n or "ln" in n or "bias" in n or 'logit_scale' in n or "class_tokens" in n
  params = [
      {"params": [p for n, p in named_parameters if exclude(n, p) and p.requires_grad], "weight_decay": 0.},
      {"params": [p for n, p in named_parameters if not exclude(n, p) and p.requires_grad], "weight_decay": wd}
  ]
  optimizer = optim.AdamW(params, lr=lr, betas=(beta1, beta2), eps=eps)

  """lr"""
  def cosine_lr(optimizer, base_lr, warmup_length, steps):
      def _lr_adjuster(step):
          if step < warmup_length:
              lr = base_lr * (step + 1) / warmup_length
          else:
              e = step - warmup_length
              es = steps - warmup_length
              lr = 0.5 * (1 + np.cos(np.pi * e / es)) * base_lr
          for param_group in optimizer.param_groups:
              param_group["lr"] = lr
          return lr
      return _lr_adjuster

  wd, warmup, accum_freq, epochs = 0.2, 10000, 1, 30
  total_steps = (dataloader.num_batches // accum_freq) * epochs
  scheduler = cosine_lr(optimizer, lr, warmup, total_steps)

  """loss"""
  def clip_loss(image_features, text_features, logit_scale):
      logits_per_image = logit_scale * image_features @ text_features.T
      logits_per_text = logit_scale * text_features @ image_features.T

      labels = torch.arange(logits_per_image.shape[0], device=image_features.device, dtype=torch.long)
      return (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2

  """train"""
  import math
  from tqdm import tqdm
  from contextlib import nullcontext

  device = torch.device("cpu")
  device_type = device.type
  if device_type == "cpu":
      scaler = torch.cuda.amp.GradScaler(enabled=False)
      global_context = nullcontext()
      input_dtype = torch.float32
  else:
      scaler = torch.cuda.amp.GradScaler(enabled=True)
      global_context = torch.amp.autocast(device_type=device_type, dtype=torch.float16)
      input_dtype = torch.float16

  grad_clip_norm = 10.0
  start_epoch = 0
  bar_format = "{n_fmt}/{total_fmt} [{bar:30}] - ETA: {elapsed}<{remaining} {rate_fmt}{postfix}{desc}"
  for epoch in range(start_epoch, epochs):
      model.train()
      process_bar = tqdm(enumerate(dataloader), total=dataloader.num_batches, bar_format=bar_format, ascii=".>>=")
      for id, batch in process_bar:
          step = dataloader.num_batches * epoch + id
          scheduler(step)

          images, texts = batch
          images = images.to(device=device, dtype=input_dtype, non_blocking=True)
          texts = texts.to(device=device, dtype=torch.long, non_blocking=True)
          optimizer.zero_grad()

          with global_context:
              image_out, text_out, logit_scale = model(images, texts)
              losses = clip_loss(image_out, text_out, logit_scale)
          scaler.scale(losses).backward()

          if grad_clip_norm > 0:
              scaler.unscale_(optimizer)
              torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm, norm_type=2.0)
          scaler.step(optimizer)
          scaler.update()

          with torch.no_grad():
              model.logit_scale.clamp_(0, math.log(100))  # clamp to 4.6052 = ln(100)
          process_bar.desc = " - loss: {:.4f}".format(losses)
          process_bar.refresh()
      print()
  ```
***
# CLIP tensorflow
  ```py
  import clip
  text_transformer = clip.TextTransformer(dim=512, num_tokens=10000, max_seq_len=256, depth=6, heads=8, dim_head=64, rotary_pos_emb=False)
  text = tf.random.uniform([4, 256], minval=0, maxval=10000, dtype=tf.int32)
  print(text_transformer(text, text != 0).shape)
  # TensorShape([4, 257, 512])

  visual_transformer = clip.VisionTransformer(dim=512, image_size=256, patch_size=32, depth=6, heads=8, dim_head=64)
  images = tf.random.normal([4, 256, 256, 3])
  print(visual_transformer(images).shape)
  # (4, 65, 512)

  text_pad_id = 0
  text_mask = text != text_pad_id
  enc_text = text_transformer(text, text_mask)
  enc_image = visual_transformer(images)

  # early return of encodings, if needed (for DALL-E2)
  # return enc_text, enc_image

  to_text_latent = keras.layers.Dense(units=512, use_bias=False)
  to_visual_latent = keras.layers.Dense(units=512, use_bias=False)

  # project to latents
  # depending on whether to do fine-grained CLIP or not, select either all tokens, or CLS tokens only
  text_latents = to_text_latent(enc_text[:, 0])
  image_latents = to_visual_latent(enc_image[:, 0])

  def clip_loss(y_true, y_pred) :
      # normalized features
      half_split = y_pred.shape[-1] // 2
      text_latents, image_latents = y_pred[:, :half_split], y_pred[:, half_split:]
      image_latents = image_latents / tf.norm(tensor=image_latents, ord="euclidean", axis=-1, keepdims=True)
      text_latents = text_latents / tf.norm(tensor=text_latents, ord="euclidean", axis=-1, keepdims=True)

      # cosine similarity as logits
      logits_per_text = tf.matmul(text_latents, image_latents, transpose_b=True)
      logits_per_image = tf.transpose(logits_per_text)
      similarity = logits_per_text

      caption_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[0]), similarity, from_logits=True))
      image_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[1]), tf.transpose(similarity), from_logits=True))
      return (caption_loss + image_loss) / 2.0

  # calculate loss
  # cl_loss = lucidrains_loss(text_latents, image_latents, self.temperature)
  # temperature = tf.Variable(tf.constant(1.0, dtype=tf.float32))
  cl_loss = clip_loss(None, [text_latents, image_latents])
  ```
  ```py
  # https://github.com/lucidrains/x-clip
  def lucidrains_loss(text_latents, image_latents, temperature):
      # equal to clip_loss
      num_batch_texts = num_batch_images = 1
      text_latents, image_latents = map(l2norm, (text_latents, image_latents))

      # get temperature
      temp = tf.exp(temperature)

      # split out multiview dimension for text and images
      text_latents = rearrange(text_latents, '(m b) ... -> m b ...', m=num_batch_texts)
      image_latents = rearrange(image_latents, '(m b) ... -> m b ...', m=num_batch_images)

      # calculate loss
      text_to_image = einsum('m t d, n i d -> m n t i', text_latents, image_latents) * temp
      image_to_text = rearrange(text_to_image, '... t i -> ... i t')

      text_to_image = rearrange(text_to_image, 'm n ... -> (m n) ...')
      image_to_text = rearrange(image_to_text, 'm n ... -> (m n) ...')

      # exponentiate
      text_to_image_exp, image_to_text_exp = map(tf.exp, (text_to_image, image_to_text))

      # numerators
      text_to_image_pos, image_to_text_pos = map(matrix_diag, (text_to_image_exp, image_to_text_exp))

      # denominator
      text_to_image_denom, image_to_text_denom = map(lambda t: tf.reduce_sum(t, axis=-1),
                                                     (text_to_image_exp, image_to_text_exp))

      # loss
      text_to_image_loss = tf.reduce_mean(-log(text_to_image_pos / text_to_image_denom), axis=-1)
      image_to_text_loss = tf.reduce_mean(-log(image_to_text_pos / image_to_text_denom), axis=-1)

      # calculate CL loss
      cl_loss = (text_to_image_loss + image_to_text_loss) / 2

      return cl_loss
  ```
## Multi inputs outputs model train test
  ```py
  from keras_cv_attention_models.backend import layers, models, functional

  inputs_1 = layers.Input([32])
  inputs_2 = layers.Input([32])
  mm = models.Model([inputs_1, inputs_2], functional.concat([inputs_1 + inputs_2, inputs_1 * inputs_2], axis=-1))
  print(f"{mm.input_shape = }, {mm.output_shape = }")

  xx_1 = tf.random.uniform([256, 224, 224, 3])
  xx_2 = tf.random.uniform([256, 32], 0, 1024, dtype='int64')
  dd = tf.data.Dataset.from_tensor_slices((xx_1, xx_2))
  dd = dd.map(lambda xx_1, xx_2: ((xx_1, xx_2), 0)).batch(16)

  def multi_loss(y_true, y_pred):
      # tf.print(y_true.shape, y_pred.shape)
      return y_pred[:, :32] - y_pred[:, 32:]

  mm.compile(loss=multi_loss)
  mm.fit(dd)
  ```
## CLIP model class
  ```py
  import math
  from keras_cv_attention_models.backend import layers, models, functional
  from keras_cv_attention_models.attention_layers import ExpLogitScale
  from keras_cv_attention_models import fastervit, gpt2

  def clip_loss(y_true, y_pred) :
      # normalized features
      half_split = y_pred.shape[-1] // 2
      text_latents, image_latents = y_pred[:, :half_split], y_pred[:, half_split:]
      image_latents = image_latents / tf.norm(tensor=image_latents, ord="euclidean", axis=-1, keepdims=True)
      text_latents = text_latents / tf.norm(tensor=text_latents, ord="euclidean", axis=-1, keepdims=True)

      # cosine similarity as logits
      logits_per_text = tf.matmul(text_latents, image_latents, transpose_b=True)
      logits_per_image = tf.transpose(logits_per_text)
      similarity = logits_per_text

      caption_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[0]), similarity, from_logits=True))
      image_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(tf.range(tf.shape(similarity)[1]), tf.transpose(similarity), from_logits=True))
      return (caption_loss + image_loss) / 2.0


  def convert_to_clip_model(image_model, text_model):
      image_input, image_output = image_model.inputs[0], image_model.outputs[-1]
      text_input, text_output = text_model.inputs[0], text_model.outputs[-1]

      # image_output = layers.Dense(latents_dim, use_bias=False, name="image_latents")(image_output)
      eol_index = functional.argmax(text_input, axis=-1)
      text_output = functional.gather_nd(text_output, functional.expand_dims(eol_index, axis=-1), batch_dims=1)
      text_output = layers.Dense(image_output.shape[-1], use_bias=False, name="text_latents")(text_output)
      text_output = ExpLogitScale(axis=None, init_value=math.log(1 / 0.07), name="temperature")(text_output)
      return models.Model([image_input, text_input], functional.concat([image_output, text_output], axis=-1))

  image_model = fastervit.FasterViT0(num_classes=512, classifier_activation=None)
  text_model = gpt2.GPT2_Base(include_top=False)
  text_model.trainable = False
  print(f"{image_model.output_shape = }, {text_model.output_shape = }")
  # image_model.output_shape = (None, 512), text_model.output_shape = (None, 1024, 768)

  mm = convert_to_clip_model(image_model, text_model)
  print(f"{mm.input_shape = }, {mm.output_shape = }")
  # mm.input_shape = [(None, 224, 224, 3), (None, None)], mm.output_shape = [(None, 512), (None, 512)]

  xx_1 = tf.random.uniform([256, 224, 224, 3])
  xx_2 = tf.random.uniform([256, 77], 0, 1024, dtype='int64')
  dd = tf.data.Dataset.from_tensor_slices((xx_1, xx_2)).map(lambda image, text: ((image, text), 0)).batch(64)
  mm.compile(loss=clip_loss)
  mm.fit(dd)
  ```
***

# COCO tiny caption
## coco caption Datasets
  ```py
  {'captions': {'id': array([529376, 529715, 531782, 531980, 534542]),
    'text': array([b'A toilet and sink in a tiled bathroom',
           b'A unisex bathroom decorated with a vintage theme. ',
           b'A white toilet sitting next to a bidet toilet.',
           b'A bathroom with a toilet, sink, and other bathroom items in it. ',
           b'A bathroom with gold circle patterns containing a toilet, sink towel rack and shelving.'],
          dtype=object)},
   'image': array([[...]], dtype=uint8),
   'image/filename': b'COCO_train2014_000000357057.jpg',
   'image/id': 357057,
   'objects': {'area': array([24057, 12005,  1817,  6639,  6771]),
    'bbox': array([[0.55177826, 0.16220312, 1.        , 0.4548125 ],
           [0.72771966, 0.81904685, 0.98838913, 1.        ],
           [0.        , 0.55546874, 0.11621339, 0.67389065],
           [0.838954  , 0.43260938, 0.9870084 , 0.6518125 ],
           [0.838954  , 0.43842188, 0.9870084 , 0.6537656 ]], dtype=float32),
    'id': array([1092901, 1131489, 1675957, 1981899, 2133571]),
    'is_crowd': array([False, False, False, False, False]),
    'label': array([61, 71, 78, 71, 61])}}
  ```
## COCO tiny caption json
  ```py
  import json

  """ Load info from captions_train2017.json """
  with open('datasets/annotations/captions_train2017.json') as ff:
      aa = json.load(ff)
  image_dict = {ii['id']: ii['file_name'] for ii in aa['images']}
  gg = {}
  for ii in aa['annotations']:
      gg.setdefault(image_dict[ii['image_id']], []).append(ii['caption'])

  """ Match captions with coco_dog_cat """
  train = []
  for ii in os.listdir('datasets/coco_dog_cat/train2017/images/'):
      train.extend([{"image": os.path.join("train2017/images", ii), "caption": jj} for jj in gg[ii]])

  """ Load info from captions_val2017.json """
  with open('datasets/annotations/captions_val2017.json') as ff:
      aa = json.load(ff)
  image_dict = {ii['id']: ii['file_name'] for ii in aa['images']}
  gg = {}
  for ii in aa['annotations']:
      gg.setdefault(image_dict[ii['image_id']], []).append(ii['caption'])

  test = []
  for ii in os.listdir('datasets/coco_dog_cat/val2017/images/'):
      test.extend([{"image": os.path.join("val2017/images", ii), "caption": jj} for jj in gg[ii]])

  print(f"{len(train) = }, {len(test) = }")
  # len(train) = 41475, len(test) = 1745

  """ Save json """
  dd = {"train": train, "test": test, "info": {"base_path": "datasets/coco_dog_cat"}}
  with open('datasets/coco_dog_cat/captions.json', 'w') as ff:
      json.dump(dd, ff, indent=2)
  ```
## Train test
  ```py
  from keras_cv_attention_models.clip import tokenizer
  from keras_cv_attention_models.imagenet import data

  caption_tokenizer = tokenizer.Tokenizer('datasets/bpe_simple_vocab_16e6.txt.gz')
  dd = data.init_dataset(data_name='datasets/coco_tiny/coco_tiny_captions.json', caption_tokenizer=caption_tokenizer)[0]

  # Show
  (aa, bb), cc = dd.as_numpy_iterator().next()
  cc = [caption_tokenizer(ii) for ii in bb]
  ax = data.show_batch_sample((aa, cc))
  ax.get_figure().savefig('aa.png')

  # Train
  ...
  mm.fit(dd)
  ```
  ```py
  from keras_cv_attention_models import fastervit, gpt2, clip

  caption_tokenizer = clip.TikToken('gpt2')
  dd = clip.init_dataset(data_name='datasets/coco_dog_cat/captions.json', caption_tokenizer=caption_tokenizer)[0]

  image_model = fastervit.FasterViT0(num_classes=512, classifier_activation=None)
  text_model = gpt2.GPT2_Base(include_top=False)
  text_model.trainable = False
  mm = clip.convert_to_clip_model(image_model, text_model)
  print(f"{image_model.output_shape = }, {text_model.output_shape = }, {mm.input_shape = }, {mm.output_shape = }")

  optimizer = keras.optimizers.Adam(weight_decay=0.2)
  mm.compile(loss=clip.clip_loss, optimizer=optimizer)
  mm.fit(dd, epochs=3)
  ```
  **Predict**
  ```py
  from keras_cv_attention_models import test_images
  labels = np.stack([caption_tokenizer("a cat"), caption_tokenizer("a dog"), caption_tokenizer("a diagram")])
  imm = test_images.cat()
  image_features = image_model(image_model.preprocess_input(imm))
  text_features = text_model(labels)
  image_features /= tf.norm(image_features, axis=-1, keepdims=True)
  text_features /= tf.norm(text_features, axis=-1, keepdims=True)
  text_probs = tf.nn.softmax(100 * image_features @ tf.transpose(text_features))
  print(f"{text_probs = }")
  ```
## Train test using train_func
  ```py
  import tensorflow as tf

  from keras_cv_attention_models import fastervit, beit, gpt2, clip
  from keras_cv_attention_models.imagenet import train_func

  train_func.init_global_strategy(enable_float16=True)

  # caption_tokenizer = clip.TikToken('gpt2')
  caption_tokenizer = clip.GPT2Tokenizer()
  train_dataset, test_dataset = clip.init_dataset(data_name='datasets/coco_dog_cat/captions.json', batch_size=32, caption_tokenizer=caption_tokenizer)[:2]

  # image_model = fastervit.FasterViT0(num_classes=512, classifier_activation=None)
  image_model = beit.BeitBasePatch16(num_classes=512, classifier_activation=None)
  text_model = gpt2.GPT2_Base(include_top=False)
  text_model.trainable = False
  model, image_model, text_model = clip.convert_to_clip_model(image_model, text_model, caption_tokenizer=caption_tokenizer)
  print(f"{image_model.output_shape = }, {text_model.output_shape = }, {model.input_shape = }, {model.output_shape = }")

  lr_base, lr_decay_steps, warmup_steps, cooldown_steps, weight_decay = 1e-3, 18, 2, 2, 0.2
  lr_scheduler, lr_total_epochs = train_func.init_lr_scheduler(lr_base, lr_decay_steps, warmup_steps=warmup_steps, cooldown_steps=cooldown_steps)
  optimizer = tf.optimizers.Adam(learning_rate=lr_base, weight_decay=weight_decay)

  model.compile(loss=clip.clip_loss, optimizer=optimizer, metrics=["acc"])
  initial_epoch = 0
  basic_save_name = "clip_test"
  latest_save, hist = train_func.train(
      model, lr_total_epochs, train_dataset, test_dataset, initial_epoch, lr_scheduler, basic_save_name, logs=None
  )
  ```
  **Precition**
  ```py
  from keras_cv_attention_models import plot_func

  data_path = "datasets/coco_dog_cat/train2017/images"
  images = [plt.imread(os.path.join(data_path, ii)) for ii in os.listdir(data_path)[:10]]
  aa = np.concatenate([image_model.preprocess_input(ii) for ii in images], axis=0)
  sim = model.run_prediction(aa, ['cat', 'dog', 'person', 'compuer'])
  ax = plot_func.show_images_texts_similarity(images, model.run_prediction.text_labels, sim)
  ax.get_figure().savefig("aa.png")
  ```
  ```py
  from keras_cv_attention_models import plot_func, test_images

  images = np.stack([test_images.cat(), test_images.dog(), test_images.dog_cat()])
  sim = model.run_prediction(image_model.preprocess_input(images), ['cat', 'dog', 'person', 'compuer'])
  ax = plot_func.show_images_texts_similarity(images, model.run_prediction.text_labels, sim)
  ax.get_figure().savefig("aa.png")
  ```
  **Show on skimages**
  ```py
  descriptions = {
      "page": "a page of text about segmentation",
      "chelsea": "a facial photo of a tabby cat",
      "astronaut": "a portrait of an astronaut with the American flag",
      "rocket": "a rocket standing on a launchpad",
      "motorcycle_right": "a red motorcycle standing in a garage",
      "camera": "a person looking at a camera on a tripod",
      "horse": "a black-and-white silhouette of a horse",
      "coffee": "a cup of coffee on a saucer"
  }

  import os, skimage
  from PIL import Image
  from keras_cv_attention_models import plot_func

  images, texts = [], []
  for filename in os.listdir(skimage.data_dir):
      if not (filename.endswith(".png") or filename.endswith(".jpg")):
          continue
      name = os.path.splitext(filename)[0]
      if name not in descriptions:
          continue
      image = Image.open(os.path.join(skimage.data_dir, filename)).convert("RGB")
      images.append(np.array(image))
      texts.append(descriptions[name])
  aa = np.concatenate([image_model.preprocess_input(ii) for ii in images], axis=0)
  sim = model.run_prediction(aa, texts)
  ax = plot_func.show_images_texts_similarity(images, texts, sim)
  ax.get_figure().savefig("aa.png")
  ```
***

# Single tower
## BeiT dynamic
  ```py
  from keras_cv_attention_models import clip, beit, attention_layers, aotnet
  from keras_cv_attention_models.backend import layers, models, functional
  image_model = beit.BeitBasePatch16()
  body_start_layer, body_end_layer = image_model.get_layer("block0_attn_ln"), image_model.get_layer("block11_mlp_output")
  body_model = models.Model(body_start_layer.input, body_end_layer.output)

  def foo(xx):
      if isinstance(xx, beit.MultiHeadRelativePositionalEmbedding):
          aa = beit.MultiHeadRelativePositionalEmbedding.from_config(xx.get_config())
          aa.build(xx.input_shape)
          return aa
      else:
          return xx
  bb = keras.models.clone_model(body_model, keras.layers.Input([None, 768]), clone_function=foo)
  bb(tf.ones([1, 66, 768]))
  bb(tf.ones([1, 78, 768]))
  ```
***

# TF and Torch optimizer
  ```py
  import torch
  aa = torch.nn.Linear(2, 2, bias=False)
  aa.weight.data = torch.ones([2, 2])
  aa = aa.train()
  out = aa(torch.ones([1, 2]))
  loss = out.mean()
  loss.backward()
  pp = torch.optim.Adam(aa.parameters(), lr=0.1, eps=1e-08, weight_decay=0)
  pp.step()
  aa.weight.data.numpy(), pp.state_dict()['state'][0]['exp_avg'].data.numpy(), pp.state_dict()['state'][0]['exp_avg_sq'].data.numpy()
  # (array([[0.90000004, 0.90000004],
  #         [0.90000004, 0.90000004]], dtype=float32),
  #  array([[0.05, 0.05],
  #         [0.05, 0.05]], dtype=float32),
  #  array([[0.00025, 0.00025],
  #         [0.00025, 0.00025]], dtype=float32))


  aa = keras.layers.Dense(2, use_bias=False, kernel_initializer='ones')
  aa.build([None, 2])
  inputs = tf.ones([1, 2])
  with tf.GradientTape() as tape:
      out = aa(inputs)
      loss = tf.reduce_mean(out)
  gradients = tape.gradient(loss, aa.trainable_variables)
  pp = keras.optimizers.Adam(learning_rate=0.1, epsilon=1e-08, weight_decay=0)
  pp.apply_gradients(zip(gradients, aa.trainable_variables))
  aa.weights[0].numpy(), pp.variables[1].numpy(), pp.variables[2].numpy()
  # (array([[0.90000075, 0.90000075],
  #         [0.90000075, 0.90000075]], dtype=float32),
  #  array([[0.05, 0.05],
  #         [0.05, 0.05]], dtype=float32),
  #  array([[0.00025, 0.00025],
  #         [0.00025, 0.00025]], dtype=float32))
  ```
  ```py
  import torch
  aa = torch.nn.Linear(2, 2, bias=False)
  aa.weight.data = torch.ones([2, 2])
  aa = aa.train()
  out = aa(torch.ones([1, 2]))
  loss = out.mean()
  loss.backward()
  pp = torch.optim.AdamW(aa.parameters(), lr=0.1, eps=1e-08, weight_decay=0.2)
  pp.step()
  aa.weight.data.numpy(), pp.state_dict()['state'][0]['exp_avg'].data.numpy(), pp.state_dict()['state'][0]['exp_avg_sq'].data.numpy()
  # (array([[0.88000005, 0.88000005],
  #         [0.88000005, 0.88000005]], dtype=float32),
  #  array([[0.05, 0.05],
  #         [0.05, 0.05]], dtype=float32),
  #  array([[0.00025, 0.00025],
  #         [0.00025, 0.00025]], dtype=float32))

  aa = keras.layers.Dense(2, use_bias=False, kernel_initializer='ones')
  aa.build([None, 2])
  inputs = tf.ones([1, 2])
  with tf.GradientTape() as tape:
      out = aa(inputs)
      loss = tf.reduce_mean(out)
  gradients = tape.gradient(loss, aa.trainable_variables)
  pp = keras.optimizers.Adam(learning_rate=0.1, epsilon=1e-08, weight_decay=0.2)
  pp.apply_gradients(zip(gradients, aa.trainable_variables))
  aa.weights[0].numpy(), pp.variables[1].numpy(), pp.variables[2].numpy()
  # (array([[0.88000077, 0.88000077],
  #         [0.88000077, 0.88000077]], dtype=float32),
  #  array([[0.05, 0.05],
  #         [0.05, 0.05]], dtype=float32),
  #  array([[0.00025, 0.00025],
  #         [0.00025, 0.00025]], dtype=float32))
  ```
  ```py
  image_signature = tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)
  text_signature = tf.TensorSpec(shape=(None, 77), dtype=tf.int64)
  label_signature = tf.TensorSpec(shape=(None), dtype=tf.int64)
  gen = lambda : (((images.permute([0, 2, 3, 1]).numpy(), texts.numpy()), labels.numpy()) for (images, texts), labels in iter(aa))
  dd = tf.data.Dataset.from_generator(gen, output_signature=((image_signature, text_signature), label_signature))
  ```
