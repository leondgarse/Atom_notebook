## NanoGPT
## Prepare
  - [Github karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
  - [Github jaymody/picoGPT](https://github.com/jaymody/picoGPT)
  ```sh
  pip instal tiktoken
  ```
## Model definition
  ```py
  import torch
  from torch import nn
  from torch.nn import functional as F

  class CausalSelfAttention(nn.Module):
      def __init__(self, block_size=1024, n_embd=768, n_head=12, bias=True, dropout=0.0):
          super().__init__()
          self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=bias)
          self.attn_out = nn.Linear(n_embd, n_embd, bias=bias)
          self.attn_dropout = nn.Dropout(dropout)
          self.output_dropout = nn.Dropout(dropout)
          causal_mask = (1 - np.tri(block_size).astype("float32")[None, None]) * -1e10
          self.register_buffer("causal_mask", torch.from_numpy(causal_mask), persistent=False)
          self.n_head, self.block_size, self.n_embd = n_head, block_size, n_embd

      def forward(self, inputs):
          batch, blocks, channels = inputs.size()
          key_dim = channels // self.n_head
          qq_scale = 1.0 / (float(key_dim) ** 0.5)

          # efficient attention using Flash Attention CUDA kernels
          # torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)
          qq, kk, vv = self.qkv(inputs).split(self.n_embd, dim=-1)
          qq = qq.view(batch, blocks, self.n_head, key_dim).transpose(1, 2)
          kk = kk.view(batch, blocks, self.n_head, key_dim).permute([0, 2, 3, 1])
          vv = vv.view(batch, blocks, self.n_head, key_dim).transpose(1, 2)

          # print(f"{qq.shape = }, {kk.shape = }, {vv.shape = }, {self.causal_mask.shape = }")
          attn = (qq @ kk) * qq_scale + self.causal_mask[:, :, :blocks, :blocks]
          attn = F.softmax(attn, dim=-1)
          attn = self.attn_dropout(attn)

          out = attn @ vv
          out = out.transpose(1, 2).contiguous().view(batch, blocks, channels)
          out = self.attn_out(out)
          out = self.output_dropout(out)
          return out


  class AttnMlpBlock(nn.Module):
      def __init__(self, block_size=1024, n_embd=768, n_head=12, bias=True, dropout=0.0):
          super().__init__()
          self.attn_ln = nn.LayerNorm(n_embd)  # bias=bias
          self.attn = CausalSelfAttention(block_size, n_embd, n_head, bias, dropout)
          self.mlp_ln = nn.LayerNorm(n_embd)  # bias=bias
          self.mlp = nn.Sequential(
              nn.Linear(n_embd, 4 * n_embd, bias=bias),
              nn.GELU(approximate="tanh"),
              nn.Linear(4 * n_embd, n_embd, bias=bias),
              nn.Dropout(dropout),
          )

      def forward(self, inputs):
          attn_out = inputs + self.attn(self.attn_ln(inputs))
          mlp_out = attn_out + self.mlp(self.mlp_ln(attn_out))
          return mlp_out


  class GPT2(nn.Module):
      def __init__(self, n_layer=12, vocab_size=50304, block_size=1024, n_embd=768, n_head=12, bias=True, dropout=0.0):
          super().__init__()
          self.wte = nn.Embedding(vocab_size, n_embd)  # Encoder
          self.wpe = nn.Embedding(block_size, n_embd)  # Encoder

          self.drop = nn.Dropout(dropout)
          blocks = [AttnMlpBlock(block_size, n_embd, n_head, bias, dropout) for _ in range(n_layer)]
          self.blocks = nn.Sequential(*blocks)
          self.ln_f = nn.LayerNorm(n_embd)  # bias=bias

          self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
          self.wte.weight = self.lm_head.weight # may not necessary, https://paperswithcode.com/method/weight-tying
          self.block_size = block_size

      def forward(self, idx, targets=None):
          batch, blocks = idx.size()
          pos_idx = torch.arange(0, blocks, dtype=torch.long, device=idx.device).unsqueeze(0)

          tok_emb = self.wte(idx)
          pos_emb = self.wpe(pos_idx)
          out = self.drop(tok_emb + pos_emb)
          for block in self.blocks:
              out = block(out)
          out = self.ln_f(out)

          if targets is not None:
              # if we are given some desired targets also calculate the loss
              logits = self.lm_head(out)
              loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
          else:
              # inference-time mini-optimization: only forward the lm_head on the very last position
              logits = self.lm_head(out[:, [-1], :]) # note: using list [-1] to preserve the time dim
              loss = None

          return logits, loss

      @torch.no_grad()
      def generate(self, idx, max_new_tokens=40, temperature=1.0, top_k=None):
          """
          Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
          the sequence max_new_tokens times, feeding the predictions back into the model each time.
          Most likely you'll want to make sure to be in model.eval() mode of operation for this.
          """
          for _ in range(max_new_tokens):
              # if the sequence context is growing too long we must crop it at block_size
              idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]
              # forward the model to get the logits for the index in the sequence
              logits, _ = self(idx_cond)
              # pluck the logits at the final step and scale by desired temperature
              logits = logits[:, -1, :] / temperature
              # optionally crop the logits to only the top k options
              if top_k is not None:
                  v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                  logits[logits < v[:, [-1]]] = -float('Inf')
              # apply softmax to convert logits to (normalized) probabilities
              probs = F.softmax(logits, dim=-1)
              # sample from the distribution
              idx_next = torch.multinomial(probs, num_samples=1)
              # append sampled index to the running sequence and continue
              idx = torch.cat((idx, idx_next), dim=1)

          return idx
  ```
## Model load statedict from huggingface
  ```py
  config_args = {
      'gpt2': dict(n_layer=12, n_head=12, n_embd=768, vocab_size=50257),  # 124M params
      'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024, vocab_size=50257), # 350M params
      'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280, vocab_size=50257), # 774M params
      'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600, vocab_size=50257), # 1558M params
  }

  def weight_name_map(weight_name):
      weight_name = weight_name.replace("transformer.h.", "blocks.")
      weight_name = weight_name.replace("transformer.", "")
      weight_name = weight_name.replace(".ln_1.", ".attn_ln.")
      weight_name = weight_name.replace(".attn.c_attn.", ".attn.qkv.")
      weight_name = weight_name.replace(".attn.c_proj.", ".attn.attn_out.")
      weight_name = weight_name.replace(".ln_2.", ".mlp_ln.")
      weight_name = weight_name.replace(".mlp.c_fc.", ".mlp.0.")
      weight_name = weight_name.replace(".mlp.c_proj.", ".mlp.2.")
      return weight_name

  def convert_gpt2_state_dict(state_dict):
      need_transpose_sufixes = ["attn.c_attn.weight", "attn.c_proj.weight", "mlp.c_fc.weight", "mlp.c_proj.weight"]
      need_transpose = lambda weight_name: any([weight_name.endswith(ii) for ii in need_transpose_sufixes])
      exclude_sufixes = [".attn.masked_bias", ".attn.bias"]
      exclude = lambda weight_name: any([weight_name.endswith(ii) for ii in exclude_sufixes])
      result = {weight_name_map(kk): vv.T if need_transpose(kk) else vv for kk, vv in state_dict.items() if not exclude(kk)}
      return result

  model_type = "gpt2"
  model = GPT2(**config_args[model_type])
  # print({kk: vv.shape for kk, vv in model.state_dict().items()})

  from transformers import GPT2LMHeadModel

  source_state_dict = GPT2LMHeadModel.from_pretrained(model_type).state_dict()
  target_state_dict = convert_gpt2_state_dict(model.state_dict())
  model.load_state_dict(target_state_dict)
  ```
## Model evaluation
  ```py
  import tiktoken
  enc = tiktoken.get_encoding('gpt2')

  start = "hello world"
  start_ids = enc.encode(start)
  inputs = (torch.tensor(start_ids, dtype=torch.long)[None])

  # run generation
  num_samples = 10 # number of samples to draw
  max_new_tokens = 500 # number of tokens generated in each sample
  temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions
  top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability

  for k in range(num_samples):
      out = model.generate(inputs, max_new_tokens, temperature=temperature, top_k=top_k)
      print(enc.decode(out[0].tolist()))
      print('---------------')
  ```
## Shakespeare datasets
  ```py
  import os
  import requests
  import tiktoken
  import numpy as np

  # download the tiny shakespeare dataset
  data_dir = "data"
  if not os.path.exists(data_dir):
      os.makedirs(data_dir, exist_ok=True)

  input_file_path = os.path.join(data_dir, 'input.txt')
  if not os.path.exists(input_file_path):
      data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
      with open(input_file_path, 'w') as ff:
          ff.write(requests.get(data_url).text)

  with open(input_file_path, 'r') as ff:
      data = ff.read()
  n = len(data)
  train_data = data[:int(n*0.9)]
  val_data = data[int(n*0.9):]

  # encode with tiktoken gpt2 bpe
  enc = tiktoken.get_encoding("gpt2")
  train_ids = enc.encode_ordinary(train_data)
  val_ids = enc.encode_ordinary(val_data)
  print(f"train has {len(train_ids):,} tokens")
  print(f"val has {len(val_ids):,} tokens")

  # export to bin files
  train_ids = np.array(train_ids, dtype=np.uint16)
  val_ids = np.array(val_ids, dtype=np.uint16)
  train_ids.tofile(os.path.join(data_dir, 'train.bin'))
  val_ids.tofile(os.path.join(data_dir, 'val.bin'))
  ```
## Optimizer
  ```py
  import inspect

  def configure_optimizers(model, weight_decay=0.1, learning_rate=6e-4, betas=(0.9, 0.95), device_type="cuda"):
      """
      This long function is unfortunately doing something very simple and is being very defensive:
      We are separating out all parameters of the model into two buckets: those that will experience
      weight decay for regularization and those that won't (biases, and layernorm/embedding weights).
      We are then returning the PyTorch optimizer object.
      """

      # separate out all parameters to those that will and won't experience regularizing weight decay
      decay = set()
      no_decay = set()
      whitelist_weight_modules = (torch.nn.Linear, )
      blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
      for mn, m in model.named_modules():
          for pn, p in m.named_parameters():
              fpn = '%s.%s' % (mn, pn) if mn else pn # full param name
              # random note: because named_modules and named_parameters are recursive
              # we will see the same tensors p many many times. but doing it this way
              # allows us to know which parent module any tensor p belongs to...
              if pn.endswith('bias'):
                  # all biases will not be decayed
                  no_decay.add(fpn)
              elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):
                  # weights of whitelist modules will be weight decayed
                  decay.add(fpn)
              elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):
                  # weights of blacklist modules will NOT be weight decayed
                  no_decay.add(fpn)

      # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they
      # will appear in the no_decay and decay sets respectively after the above.
      # In addition, because named_parameters() doesn't return duplicates, it
      # will only return the first occurence, key'd by 'transformer.wte.weight', below.
      # so let's manually remove 'lm_head.weight' from decay set. This will include
      # this tensor into optimization via transformer.wte.weight only, and not decayed.
      decay.remove('lm_head.weight')

      # validate that we considered every parameter
      param_dict = {pn: p for pn, p in model.named_parameters()}
      inter_params = decay & no_decay
      union_params = decay | no_decay
      assert len(inter_params) == 0, "parameters %s made it into both decay/no_decay sets!" % (str(inter_params), )
      assert len(param_dict.keys() - union_params) == 0, "parameters %s were not separated into either decay/no_decay set!" \
                                                  % (str(param_dict.keys() - union_params), )

      # create the pytorch optimizer object
      optim_groups = [
          {"params": [param_dict[pn] for pn in sorted(list(decay))], "weight_decay": weight_decay},
          {"params": [param_dict[pn] for pn in sorted(list(no_decay))], "weight_decay": 0.0},
      ]
      # new PyTorch nightly has a new 'fused' option for AdamW that is much faster
      use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)
      print(f"using fused AdamW: {use_fused}")
      extra_args = dict(fused=True) if use_fused else dict()
      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)

      return optimizer
  ```
## Model Train
  ```py
  import os
  import math
  import time
  import torch
  import numpy as np
  from contextlib import nullcontext

  class Datasets:
      def __init__(self, data_dir, split='train', block_size=1024, batch_size=12, device="cpu"):
          data_file = 'train.bin' if split == 'train' else 'val.bin'
          self.data = np.memmap(os.path.join(data_dir, data_file), dtype=np.uint16, mode='r')
          self.block_size, self.batch_size, self.device = block_size, batch_size, device
          self.device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast

      def get_random_batch(self):
          idx = torch.randint(len(self.data) - self.block_size, (self.batch_size,))
          xx = torch.stack([torch.from_numpy((self.data[i:i+self.block_size]).astype(np.int64)) for i in idx])
          yy = torch.stack([torch.from_numpy((self.data[i+1:i+1+self.block_size]).astype(np.int64)) for i in idx])
          if self.device_type == 'cuda':
              xx, yy = xx.pin_memory().to(self.device, non_blocking=True), yy.pin_memory().to(self.device, non_blocking=True)
          else:
              xx, yy = xx.to(device), yy.to(self.device)
          return xx, yy

  # helps estimate an arbitrarily accurate loss over either split using many batches
  def estimate_loss(model, dataset, context, eval_iters=200):
      with torch.no_grad():
          model.eval()
          losses = torch.zeros(eval_iters)
          for iter in range(eval_iters):
              xx, yy = dataset.get_random_batch()
              with context:
                  logits, loss = model(xx, yy)
              losses[iter] = loss.item()
          model.train()
      return losses.mean()

  # learning rate decay scheduler (cosine with warmup)
  def cosine_with_warmup_lr(it, learning_rate=6e-4, warmup_iters=2000, lr_decay_iters=600000, min_lr=6e-5):
      if it < warmup_iters:  # linear warmup for warmup_iters steps
          return learning_rate * it / warmup_iters
      if it > lr_decay_iters:  # it > lr_decay_iters, return min learning rate
          return min_lr
      # in between, use cosine decay down to min learning rate
      decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
      assert 0 <= decay_ratio <= 1
      coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1
      return min_lr + coeff * (learning_rate - min_lr)


  def train(
      model,
      optimizer,
      train_data,
      val_data,
      max_iters=60000,
      eval_interval=2000,
      gradient_accumulation_steps=5,
      log_interval=1,
      out_dir="checkpoints",
      device="cuda:0"
  ):
      device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast
      # initialize a GradScaler. If enabled=False scaler is a no-op
      scaler = torch.cuda.amp.GradScaler(enabled=(device_type=='cuda'))
      context = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=torch.float16)

      iter_num = 0
      best_val_loss = 1e9
      train_x, train_y = train_data.get_random_batch()
      while iter_num < max_iters:
          t0 = time.time()
          lr = cosine_with_warmup_lr(iter_num, lr_decay_iters=max_iters)
          for param_group in optimizer.param_groups:
              param_group['lr'] = lr

          # evaluate the loss on train/val sets and write checkpoints
          if iter_num > 0 and iter_num % eval_interval == 0:
              train_loss = estimate_loss(model, train_data, context=context)
              val_loss = estimate_loss(model, val_data, context=context)
              print(f"step {iter_num}: train loss {train_loss:.4f}, val loss {val_loss:.4f}")
              if val_loss < best_val_loss:
                  best_val_loss = val_loss
                  checkpoint = {'model': model.state_dict(), 'optimizer': optimizer.state_dict()}
                  print(f"saving checkpoint to {out_dir}")
                  torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))

          # forward backward update, with optional gradient accumulation to simulate larger batch size
          # and using the GradScaler if data type is float16
          for _ in range(gradient_accumulation_steps):
              with context:
                  logits, loss = model(train_x, train_y)
              # immediately async prefetch next batch while model is doing the forward pass on the GPU
              train_x, train_y = train_data.get_random_batch()
              # backward pass, with gradient scaling if training in fp16
              scaler.scale(loss).backward()
          scaler.unscale_(optimizer)
          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the gradient
          scaler.step(optimizer)  # step the optimizer and scaler if training in fp16
          scaler.update()
          optimizer.zero_grad(set_to_none=True)  # flush the gradients as soon as we can, no need for this memory anymore

          if iter_num % log_interval == 0:
              lossf = loss.item() # loss as float. note: this is a CPU-GPU sync point
              dt = time.time() - t0
              print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms")
          iter_num += 1

  device = "cuda:0"
  device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast
  model = GPT2(n_layer=6, vocab_size=50304)  # defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
  model.to(device)
  optimizer = configure_optimizers(model, weight_decay=0.1, learning_rate=6e-4, betas=(0.9, 0.95), device_type=device_type)
  # model = torch.compile(model) # requires PyTorch 2.0

  # poor man's data loader
  data_dir, block_size, batch_size = "data", 1024, 12
  train_data = Datasets(data_dir, split="train", block_size=block_size, batch_size=batch_size, device=device)
  val_data = Datasets(data_dir, split="val", block_size=block_size, batch_size=batch_size, device=device)

  train(
      model=model,
      optimizer=optimizer,
      train_data=train_data,
      val_data=val_data,
      max_iters=600000,
      eval_interval=2000,
      gradient_accumulation_steps=5,
      log_interval=1,
      out_dir="checkpoints",
      device=device
  )
  ```
***
