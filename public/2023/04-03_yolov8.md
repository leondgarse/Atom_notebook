## YOLOV8
  ```py
  sys.path.append('../ultralytics/')
  import torch
  # from ultralytics import YOLO
  # model = YOLO('yolov8n.pt')  # load an official model
  tt = torch.load('yolov8n.pt')
  _ = tt['model'].eval()
  ss = tt['model'].state_dict()

  from keras_cv_attention_models.yolov8 import yolov8
  mm = yolov8.YOLOV8_N(pretrained=None)

  headers = [
      'head_1_left_1_conv', 'head_1_left_1_bn', 'head_1_left_2_conv', 'head_1_left_2_bn', 'head_1_left_3_conv',
      'head_2_left_1_conv', 'head_2_left_1_bn', 'head_2_left_2_conv', 'head_2_left_2_bn', 'head_2_left_3_conv',
      'head_3_left_1_conv', 'head_3_left_1_bn', 'head_3_left_2_conv', 'head_3_left_2_bn', 'head_3_left_3_conv',
      'head_1_right_1_conv', 'head_1_right_1_bn', 'head_1_right_2_conv', 'head_1_right_2_bn', 'head_1_right_3_conv',
      'head_2_right_1_conv', 'head_2_right_1_bn', 'head_2_right_2_conv', 'head_2_right_2_bn', 'head_2_right_3_conv',
      'head_3_right_1_conv', 'head_3_right_1_bn', 'head_3_right_2_conv', 'head_3_right_2_bn', 'head_3_right_3_conv',
  ]
  specific_match_func = lambda tt: tt[:- len(headers)] + headers

  tail_align_dict = {"output_conv": "pre_0_1_conv", "output_bn": "pre_0_1_bn"}

  import kecam
  # ss = {}
  kecam.download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      tail_align_dict=tail_align_dict,
      specific_match_func=specific_match_func,
      save_name=mm.name + "_coco.h5",
      do_predict=False,
      do_convert=True,
  )
  ```
## Predict
```py
if source is None:
    source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'
    LOGGER.warning(f"WARNING ⚠️ 'source' is missing. Using 'source={source}'.")
is_cli = (sys.argv[0].endswith('yolo') or sys.argv[0].endswith('ultralytics')) and \
         ('predict' in sys.argv or 'mode=predict' in sys.argv)

overrides = self.overrides.copy()
overrides['conf'] = 0.25
overrides.update(kwargs)  # prefer kwargs
overrides['mode'] = kwargs.get('mode', 'predict')
assert overrides['mode'] in ['track', 'predict']
overrides['save'] = kwargs.get('save', False)  # not save files by default
if not self.predictor:
    self.task = overrides.get('task') or self.task
    self.predictor = TASK_MAP[self.task][3](overrides=overrides)
    self.predictor.setup_model(model=self.model, verbose=is_cli)
else:  # only update args if predictor is already setup
    self.predictor.args = get_cfg(self.predictor.args, overrides)
return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)
```
```py
sys.path.append('../ultralytics/')
import torch
from ultralytics.yolo.utils import ops
from keras_cv_attention_models.test_images import dog_cat
from skimage.transform import resize

tt = torch.load('yolov8n.pt')['model']
# tt['model'].model[-1].stride
_ = tt.eval()
_ = tt.float()

imm = resize(dog_cat(), [640, 640])
preds_torch, torch_out = tt(torch.from_numpy(imm[None]).permute([0, 3, 1, 2]).float())

conf, iou, agnostic_nms, max_det, classes = 0.25, 0.5, False, 300, 80
ops.non_max_suppression(preds_torch, conf, iou, agnostic=agnostic_nms, max_det=max_det, classes=classes)

from keras_cv_attention_models.yolov8 import yolov8
from keras_cv_attention_models.test_images import dog_cat
mm = yolov8.YOLOV8_N(pretrained='yolov8_n_coco.h5')

imm = mm.preprocess_input(dog_cat())
preds = mm(imm).numpy()

# DFL return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)
# preds = np.split(preds, [6400, 8000], axis=1)
# pred = preds[0].reshape([1, 80, 80, -1])
pred = preds.copy()
bbox, cls = np.split(pred, [pred.shape[-1] - 80], axis=-1)
bbox = bbox.reshape([1, bbox.shape[1], 4, -1])
dfl_out = (tf.nn.softmax(bbox, axis=-1).numpy() * np.arange(bbox.shape[-1])).sum(-1)

def make_anchors(input_shape=[640, 640], strides=[8, 16, 32], grid_cell_offset=0.5):
    """Generate anchors from features."""
    anchor_points, stride_tensors = [], []
    hh, ww = input_shape[:2]
    for i, stride in enumerate(strides):
        sx = np.arange(ww // stride) + grid_cell_offset  # shift x
        sy = np.arange(hh // stride) + grid_cell_offset  # shift y
        sy, sx = np.meshgrid(sy, sx, indexing='ij')
        anchor_point = np.stack((sx, sy), -1).reshape(-1, 2)
        stride_tensor = np.zeros([anchor_point.shape[0], 1]) + stride
        anchor_points.append(anchor_point)
        stride_tensors.append(stride_tensor)
    return np.concatenate(anchor_points), np.concatenate(stride_tensors)

def dist2bbox(distance, anchor_points, xywh=True, axis=-1):
    """Transform distance(ltrb) to box(xywh or xyxy)."""
    lt, rb = np.split(distance, 2, axis=axis)
    x1y1 = anchor_points - lt
    x2y2 = anchor_points + rb
    if xywh:
        c_xy = (x1y1 + x2y2) / 2
        wh = x2y2 - x1y1
        return np.concatenate((c_xy, wh), axis=axis)  # xywh bbox
    return np.concatenate((x1y1, x2y2), axis=axis)  # xyxy bbox

anchors, strides = make_anchors()
dbox = dist2bbox(dfl_out, anchors) * strides
out = np.concatenate([dbox, tf.nn.sigmoid(cls).numpy()], axis=-1)

from ultralytics.yolo.utils import ops
conf, iou, agnostic_nms, max_det, classes = 0.25, 0.5, False, 300, 80
preds_torch = torch.from_numpy(preds).permute([0, 2, 1])
ops.non_max_suppression(preds_torch, conf, iou, agnostic=agnostic_nms, max_det=max_det, classes=classes)
```
