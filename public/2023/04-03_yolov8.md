## YOLOV8
  ```py
  sys.path.append('../ultralytics/')
  import torch
  # from ultralytics import YOLO
  # model = YOLO('yolov8n.pt')  # load an official model
  tt = torch.load('yolov8n.pt')
  _ = tt['model'].eval()
  ss = tt['model'].state_dict()

  from keras_cv_attention_models.yolov8 import yolov8
  mm = yolov8.YOLOV8_N(pretrained=None)

  headers = [
      'head_1_reg_1_conv', 'head_1_reg_1_bn', 'head_1_reg_2_conv', 'head_1_reg_2_bn', 'head_1_reg_3_conv',
      'head_2_reg_1_conv', 'head_2_reg_1_bn', 'head_2_reg_2_conv', 'head_2_reg_2_bn', 'head_2_reg_3_conv',
      'head_3_reg_1_conv', 'head_3_reg_1_bn', 'head_3_reg_2_conv', 'head_3_reg_2_bn', 'head_3_reg_3_conv',
      'head_1_cls_1_conv', 'head_1_cls_1_bn', 'head_1_cls_2_conv', 'head_1_cls_2_bn', 'head_1_cls_3_conv',
      'head_2_cls_1_conv', 'head_2_cls_1_bn', 'head_2_cls_2_conv', 'head_2_cls_2_bn', 'head_2_cls_3_conv',
      'head_3_cls_1_conv', 'head_3_cls_1_bn', 'head_3_cls_2_conv', 'head_3_cls_2_bn', 'head_3_cls_3_conv',
  ]
  specific_match_func = lambda tt: tt[:- len(headers)] + headers

  tail_align_dict = {"output_conv": "pre_0_1_conv", "output_bn": "pre_0_1_bn"}

  import kecam
  # ss = {}
  kecam.download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      tail_align_dict=tail_align_dict,
      specific_match_func=specific_match_func,
      save_name=mm.name + "_coco.h5",
      do_predict=False,
      do_convert=True,
  )
  ```
  **Convert bboxes output `[left, top, right, bottom]` -> `top, left, bottom, right`**
  ```py
  from keras_cv_attention_models.yolov8 import yolov8
  mm = yolov8.YOLOV8_N(pretrained="yolov8_n_coco.h5")
  heads = [ii.name for ii in mm.layers if ii.name.startswith('head')]
  for ii in range(1, 100):
      layer_name = 'head_{}_reg_3_conv'.format(ii)
      print(f">>>> {layer_name = }")
      if layer_name not in heads:
          break
      conv_layer = mm.get_layer(layer_name)
      new_ww = []
      for ww in conv_layer.get_weights():
          ww = np.reshape(ww, [*ww.shape[:-1], 4, 16])[..., [1, 0, 3, 2], :]
          ww = np.reshape(ww, [*ww.shape[:-2], -1])
          new_ww.append(ww)
      conv_layer.set_weights(new_ww)

  mm.save(mm.name + "_coco.h5")


  from keras_cv_attention_models import test_images, coco
  imm = test_images.dog_cat()
  preds = mm(mm.preprocess_input(imm))
  bboxes, labels, confidences = mm.decode_predictions(preds)[0]
  print(f"{bboxes = }, {labels = }, {confidences = }")
  coco.show_image_with_bboxes(imm, bboxes, labels, confidences, num_classes=80)
  ```
## Predict
```py
if source is None:
    source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'
    LOGGER.warning(f"WARNING ⚠️ 'source' is missing. Using 'source={source}'.")
is_cli = (sys.argv[0].endswith('yolo') or sys.argv[0].endswith('ultralytics')) and \
         ('predict' in sys.argv or 'mode=predict' in sys.argv)

overrides = self.overrides.copy()
overrides['conf'] = 0.25
overrides.update(kwargs)  # prefer kwargs
overrides['mode'] = kwargs.get('mode', 'predict')
assert overrides['mode'] in ['track', 'predict']
overrides['save'] = kwargs.get('save', False)  # not save files by default
if not self.predictor:
    self.task = overrides.get('task') or self.task
    self.predictor = TASK_MAP[self.task][3](overrides=overrides)
    self.predictor.setup_model(model=self.model, verbose=is_cli)
else:  # only update args if predictor is already setup
    self.predictor.args = get_cfg(self.predictor.args, overrides)
return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)
```
```py
sys.path.append('../ultralytics/')
import torch
from ultralytics.yolo.utils import ops
from keras_cv_attention_models.test_images import dog_cat
from skimage.transform import resize

tt = torch.load('yolov8n.pt')['model']
# tt['model'].model[-1].stride
_ = tt.eval()
_ = tt.float()

imm = resize(dog_cat(), [640, 640])
preds_torch, torch_out = tt(torch.from_numpy(imm[None]).permute([0, 3, 1, 2]).float())
print(ops.non_max_suppression(preds_torch, conf_thres=0.5, iou_thres=0.45))
# [3.9073e+02, 6.9824e+01, 6.3999e+02, 6.3612e+02, 8.4271e-01, 1.5000e+01]
# [1.0403e+02, 1.6174e-02, 4.1447e+02, 6.3392e+02, 6.4019e-01, 1.6000e+01]

from keras_cv_attention_models.yolov8 import yolov8
from keras_cv_attention_models.test_images import dog_cat
mm = yolov8.YOLOV8_N(pretrained='yolov8_n_coco.h5')

imm = mm.preprocess_input(dog_cat())
preds = mm(imm).numpy()

# torch_out_1 = torch_out[0].permute([0, 2, 3, 1]).reshape([1, -1, 144]).detach().numpy()
# print(f"{np.allclose(torch_out_1, preds[:, :torch_out_1.shape[1]], atol=0.15) = }")
# np.allclose(torch_out_1, preds[:, :print(f"{np.allclose(torch_out_1, preds[:, :torch_out_1.shape[1]], atol=0.15) = }").shape[1]], atol=0.15) = True

# DFL return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)
bbox, cls = np.split(preds, [preds.shape[-1] - 80], axis=-1)
bbox = bbox.reshape([1, bbox.shape[1], 4, -1])
dfl_out = (tf.nn.softmax(bbox, axis=-1).numpy() * np.arange(bbox.shape[-1])).sum(-1)

def make_anchors(input_shape=[640, 640], strides=[8, 16, 32], grid_cell_offset=0.5):
    """Generate anchors from features."""
    anchor_points, stride_tensors = [], []
    hh, ww = input_shape[:2]
    for i, stride in enumerate(strides):
        sx = np.arange(ww // stride) + grid_cell_offset  # shift x
        sy = np.arange(hh // stride) + grid_cell_offset  # shift y
        sy, sx = np.meshgrid(sy, sx, indexing='ij')
        anchor_point = np.stack((sx, sy), -1).reshape(-1, 2)
        stride_tensor = np.zeros([anchor_point.shape[0], 1]) + stride
        anchor_points.append(anchor_point)
        stride_tensors.append(stride_tensor)
    return np.concatenate(anchor_points), np.concatenate(stride_tensors)

def dist2bbox(distance, anchor_points, xywh=True, axis=-1):
    """Transform distance(ltrb) to box(xywh or xyxy)."""
    lt, rb = np.split(distance, 2, axis=axis)
    c_xy = (rb - lt) / 2 + anchor_points
    wh = rb + lt
    return np.concatenate((c_xy, wh), axis=axis)  # xywh bbox

anchors, strides = make_anchors()
dbox = dist2bbox(dfl_out, anchors) * strides
left_top, right_bottom = np.split(dfl_out, [2], axis=-1)
((right_bottom - left_top) / 2 + anchors) * strides
(right_bottom + left_top) * strides
out = np.concatenate([dbox, tf.nn.sigmoid(cls).numpy()], axis=-1)

sys.path.append('../ultralytics/')
import torch
from ultralytics.yolo.utils import ops
preds_torch = torch.from_numpy(out).permute([0, 2, 1])
print(ops.non_max_suppression(preds_torch, conf_thres=0.5, iou_thres=0.45))
# [3.9072e+02, 6.9849e+01, 6.3999e+02, 6.3612e+02, 8.4250e-01, 1.5000e+01]
# [1.0403e+02, 9.5776e-03, 4.1447e+02, 6.3393e+02, 6.4016e-01, 1.6000e+01]
```
```py
from keras_cv_attention_models.yolov8 import yolov8
from keras_cv_attention_models.test_images import dog_cat
mm = yolov8.YOLOV8_N(pretrained='yolov8_n_coco.h5')

imm = mm.preprocess_input(dog_cat())
preds = mm(imm).numpy()
from keras_cv_attention_models.coco import anchors_func

anchors = anchors_func.get_anchor_free_anchors(input_shape=mm.input_shape[1:-1], grid_zero_start=False)
dd = anchors_func.yolov8_decode_bboxes(preds[0], anchors).numpy()
rr = tf.image.non_max_suppression(dd[:, :4], dd[:, 4:].max(-1), score_threshold=0.3, max_output_size=15, iou_threshold=0.5)
dd_nms = tf.gather(dd, rr).numpy()
bboxes, labels, scores = dd_nms[:, :4], dd_nms[:, 4:].argmax(-1), dd_nms[:, 4:].max(-1)
print(f"{bboxes = }, {labels = }, {scores = }")

from keras_cv_attention_models.coco import data
data.show_image_with_bboxes(dog_cat(), bboxes, labels, scores)
```
