## YOLOV8
  ```py
  sys.path.append('../ultralytics/')
  import torch
  # from ultralytics import YOLO
  # model = YOLO('yolov8n.pt')  # load an official model
  tt = torch.load('yolov8x6.pt')
  _ = tt['model'].eval()
  ss = tt['model'].state_dict()

  from keras_cv_attention_models.yolov8 import yolov8
  mm = yolov8.YOLOV8_X6(pretrained=None)

  headers = [
      'head_1_reg_1_conv', 'head_1_reg_1_bn', 'head_1_reg_2_conv', 'head_1_reg_2_bn', 'head_1_reg_3_conv',
      'head_2_reg_1_conv', 'head_2_reg_1_bn', 'head_2_reg_2_conv', 'head_2_reg_2_bn', 'head_2_reg_3_conv',
      'head_3_reg_1_conv', 'head_3_reg_1_bn', 'head_3_reg_2_conv', 'head_3_reg_2_bn', 'head_3_reg_3_conv',
      'head_4_reg_1_conv', 'head_4_reg_1_bn', 'head_4_reg_2_conv', 'head_4_reg_2_bn', 'head_4_reg_3_conv',  # For X6
      'head_1_cls_1_conv', 'head_1_cls_1_bn', 'head_1_cls_2_conv', 'head_1_cls_2_bn', 'head_1_cls_3_conv',
      'head_2_cls_1_conv', 'head_2_cls_1_bn', 'head_2_cls_2_conv', 'head_2_cls_2_bn', 'head_2_cls_3_conv',
      'head_3_cls_1_conv', 'head_3_cls_1_bn', 'head_3_cls_2_conv', 'head_3_cls_2_bn', 'head_3_cls_3_conv',
      'head_4_cls_1_conv', 'head_4_cls_1_bn', 'head_4_cls_2_conv', 'head_4_cls_2_bn', 'head_4_cls_3_conv',  # For X6
  ]
  specific_match_func = lambda tt: tt[:- len(headers)] + headers

  tail_align_dict = {"output_conv": "pre_0_1_conv", "output_bn": "pre_0_1_bn"}

  import kecam
  # ss = {}
  kecam.download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      tail_align_dict=tail_align_dict,
      specific_match_func=specific_match_func,
      save_name=mm.name + "_coco.h5",
      do_predict=False,
      do_convert=True,
  )
  ```
  **Convert bboxes output `[left, top, right, bottom]` -> `top, left, bottom, right`**
  ```py
  from keras_cv_attention_models.yolov8 import yolov8
  mm = yolov8.YOLOV8_X6(pretrained="yolov8_x6_coco.h5")
  heads = [ii.name for ii in mm.layers if ii.name.startswith('head')]
  for ii in range(1, 100):
      layer_name = 'head_{}_reg_3_conv'.format(ii)
      print(f">>>> {layer_name = }")
      if layer_name not in heads:
          break
      conv_layer = mm.get_layer(layer_name)
      new_ww = []
      for ww in conv_layer.get_weights():
          ww = np.reshape(ww, [*ww.shape[:-1], 4, 16])[..., [1, 0, 3, 2], :]
          ww = np.reshape(ww, [*ww.shape[:-2], -1])
          new_ww.append(ww)
      conv_layer.set_weights(new_ww)

  mm.save(mm.name + "_coco.h5")


  from keras_cv_attention_models import test_images, coco
  imm = test_images.dog_cat()
  preds = mm(mm.preprocess_input(imm))
  bboxes, labels, confidences = mm.decode_predictions(preds)[0]
  print(f"{bboxes = }, {labels = }, {confidences = }")
  coco.show_image_with_bboxes(imm, bboxes, labels, confidences, num_classes=80)
  ```
## YOLOV8 Classification
  ```py
  sys.path.append('../ultralytics/')
  import torch
  # from ultralytics import YOLO
  # model = YOLO('yolov8n.pt')  # load an official model
  tt = torch.load('yolov8n-cls.pt')
  _ = tt['model'].eval()
  ss = tt['model'].float()

  from keras_cv_attention_models.yolov8 import yolov8
  mm = yolov8.YOLOV8_N_CLS(pretrained=None)

  tail_align_dict = {"output_conv": "pre_0_1_conv", "output_bn": "pre_0_1_bn"}

  from keras_cv_attention_models import download_and_load
  # ss = {}
  download_and_load.keras_reload_from_torch_model(ss, mm, tail_align_dict=tail_align_dict, do_convert=True)
  ```
## Predict
  ```py
  sys.path.append('../ultralytics/')
  import torch
  from ultralytics.yolo.utils import ops
  from keras_cv_attention_models.test_images import dog_cat
  from skimage.transform import resize

  tt = torch.load('yolov8n.pt')['model']
  # tt['model'].model[-1].stride
  _ = tt.eval()
  _ = tt.float()

  imm = resize(dog_cat(), [640, 640])
  preds_torch, torch_out = tt(torch.from_numpy(imm[None]).permute([0, 3, 1, 2]).float())
  print(ops.non_max_suppression(preds_torch, conf_thres=0.5, iou_thres=0.45))
  # [3.9073e+02, 6.9824e+01, 6.3999e+02, 6.3612e+02, 8.4271e-01, 1.5000e+01]
  # [1.0403e+02, 1.6174e-02, 4.1447e+02, 6.3392e+02, 6.4019e-01, 1.6000e+01]

  from keras_cv_attention_models.yolov8 import yolov8
  from keras_cv_attention_models.test_images import dog_cat
  mm = yolov8.YOLOV8_N(pretrained='yolov8_n_coco.h5', classifier_activation=None)
  preds = mm(imm[None]).numpy()

  # torch_out_1 = torch_out[0].permute([0, 2, 3, 1]).reshape([1, -1, 144]).detach().numpy()
  # print(f"{np.allclose(torch_out_1, preds[:, :torch_out_1.shape[1]], atol=0.15) = }")
  # np.allclose(torch_out_1, preds[:, :print(f"{np.allclose(torch_out_1, preds[:, :torch_out_1.shape[1]], atol=0.15) = }").shape[1]], atol=0.15) = True

  # DFL return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)
  bbox, cls = np.split(preds, [preds.shape[-1] - 80], axis=-1)
  bbox = bbox.reshape([1, bbox.shape[1], 4, -1])
  dfl_out = (tf.nn.softmax(bbox, axis=-1).numpy() * np.arange(bbox.shape[-1])).sum(-1)

  def make_anchors(input_shape=[640, 640], strides=[8, 16, 32], grid_cell_offset=0.5):
      """Generate anchors from features."""
      anchor_points, stride_tensors = [], []
      hh, ww = input_shape[:2]
      for i, stride in enumerate(strides):
          sx = np.arange(ww // stride) + grid_cell_offset  # shift x
          sy = np.arange(hh // stride) + grid_cell_offset  # shift y
          sy, sx = np.meshgrid(sy, sx, indexing='ij')
          anchor_point = np.stack((sx, sy), -1).reshape(-1, 2)
          stride_tensor = np.zeros([anchor_point.shape[0], 1]) + stride
          anchor_points.append(anchor_point)
          stride_tensors.append(stride_tensor)
      return np.concatenate(anchor_points), np.concatenate(stride_tensors)

  def dist2bbox(distance, anchor_points, xywh=True, axis=-1):
      """Transform distance(ltrb) to box(xywh or xyxy)."""
      lt, rb = np.split(distance, 2, axis=axis)
      c_xy = (rb - lt) / 2 + anchor_points
      wh = rb + lt
      return np.concatenate((c_xy, wh), axis=axis)  # xywh bbox

  anchors, strides = make_anchors()
  dbox = dist2bbox(dfl_out, anchors) * strides
  left_top, right_bottom = np.split(dfl_out, [2], axis=-1)
  ((right_bottom - left_top) / 2 + anchors) * strides
  (right_bottom + left_top) * strides
  out = np.concatenate([dbox, tf.nn.sigmoid(cls).numpy()], axis=-1)

  sys.path.append('../ultralytics/')
  import torch
  from ultralytics.yolo.utils import ops
  preds_torch = torch.from_numpy(out).permute([0, 2, 1])
  print(ops.non_max_suppression(preds_torch, conf_thres=0.5, iou_thres=0.45))
  # [3.9072e+02, 6.9849e+01, 6.3999e+02, 6.3612e+02, 8.4250e-01, 1.5000e+01]
  # [1.0403e+02, 9.5776e-03, 4.1447e+02, 6.3393e+02, 6.4016e-01, 1.6000e+01]
  ```
  ```py
  from keras_cv_attention_models.yolov8 import yolov8
  from keras_cv_attention_models.test_images import dog_cat
  mm = yolov8.YOLOV8_N(pretrained='yolov8_n_coco.h5')

  imm = mm.preprocess_input(dog_cat())
  preds = mm(imm).numpy()
  from keras_cv_attention_models.coco import anchors_func

  anchors = anchors_func.get_anchor_free_anchors(input_shape=mm.input_shape[1:-1], grid_zero_start=False)
  dd = anchors_func.yolov8_decode_bboxes(preds[0], anchors).numpy()
  rr = tf.image.non_max_suppression(dd[:, :4], dd[:, 4:].max(-1), score_threshold=0.3, max_output_size=15, iou_threshold=0.5)
  dd_nms = tf.gather(dd, rr).numpy()
  bboxes, labels, scores = dd_nms[:, :4], dd_nms[:, 4:].argmax(-1), dd_nms[:, 4:].max(-1)
  print(f"{bboxes = }, {labels = }, {scores = }")

  from keras_cv_attention_models.coco import data
  data.show_image_with_bboxes(dog_cat(), bboxes, labels, scores)
  ```
***
# Data
  ```py
  from ultralytics.yolo.data import build_dataloader
  from ultralytics.yolo.data.utils import check_det_dataset
  from ultralytics.yolo.cfg import get_cfg
  from ultralytics.yolo.utils import DEFAULT_CFG

  dataset_path = "coco.yaml"
  data = check_det_dataset(dataset_path)
  cfg = get_cfg(DEFAULT_CFG)
  cfg.data = dataset_path
  build_dataloader(cfg, 16, img_path=dataset_path, stride=32, mode="val", names=data['names'])[0]


  ```
***
# Train
  ```py
  from ultralytics import YOLO
  yolo = YOLO('./ultralytics/models/v8/yolov8n.yaml')
  yolo.train(data='ultralytics/datasets/coco.yaml', epochs=100)
  ```
  ```py
  from ultralytics import yolo

  overrides = {"model": '../ultralytics/ultralytics/models/v8/yolov8n.yaml', "data": "coco128.yaml"}
  trainer = yolo.v8.detect.DetectionTrainer(overrides=overrides)
  trainer._setup_train(rank=-1, world_size=1)
  ```
  - `ultralytics/yolo/data/build.py`
  ```py
  import torch
  from torch.utils.data import DataLoader
  from ultralytics.yolo.cfg import get_cfg
  from ultralytics.yolo.utils import DEFAULT_CFG
  from ultralytics.yolo.data.utils import check_det_dataset
  from ultralytics.yolo.data.dataset import YOLODataset

  def to_data_loader(data, cfg, mode="train", batch_size=16):
      if mode == "train":
          augment, pad, shuffle, rect = True, 0, True, False
      else:
          augment, pad, shuffle, rect = False, 0.5, False, True
      dataset = YOLODataset(
          img_path=data['train'] if mode == "train" else data['val'],
          imgsz=640,
          batch_size=batch_size,
          augment=augment,  # augmentation
          hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
          rect=rect,  # rectangular batches
          cache=None,
          single_cls=False,
          stride=32,
          pad=pad,
          names=data['names']
          # classes=cfg.classes,
      )

      generator = torch.Generator()
      generator.manual_seed(6148914691236517205 - 1)
      collate_fn = getattr(dataset, 'collate_fn', None)
      data_loader = DataLoader(
          dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=16, sampler=None, pin_memory=True, collate_fn=collate_fn, generator=generator
      )
      return data_loader

  dataset_path = "../ultralytics/ultralytics/datasets/coco.yaml"
  cfg = get_cfg(DEFAULT_CFG)
  cfg.data = dataset_path
  data = check_det_dataset(dataset_path)
  train_loader, val_loader = to_data_loader(data, cfg), to_data_loader(data, cfg, "val")

  for aa in train_loader:
      break
  plt.imshow(aa['img'][0].permute(1, 2, 0).numpy())
  ```
  **Loss**
  ```py
  import torch
  from ultralytics.yolo.utils.tal import TaskAlignedAssigner, dist2bbox, make_anchors
  from ultralytics.yolo.utils.loss import BboxLoss
  from ultralytics.yolo.utils.ops import xywh2xyxy
  from torch import nn

  class Loss:
      def __init__(self, device="cpu", stride=[8, 16, 32], nc=80, reg_max=16, box_weight=7.5, cls_weight=0.5, dfl_weight=1.5, min_memory=False):
          # self.device = next(model.parameters()).device  # get model device
          # h = model.args  # hyperparameters
          # m = model.model[-1]  # Detect() module
          self.bce = nn.BCEWithLogitsLoss(reduction='none')
          self.stride, self.nc, self.reg_max, self.device = stride, nc, reg_max, device
          self.box_weight, self.cls_weight, self.dfl_weight, self.min_memory = box_weight, cls_weight, dfl_weight, min_memory

          self.no = nc + reg_max * 4
          self.use_dfl = reg_max > 1
          roll_out_thr = min_memory if min_memory > 1 else 64 if min_memory else 0  # 64 is default

          self.assigner = TaskAlignedAssigner(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0, roll_out_thr=roll_out_thr)
          self.bbox_loss = BboxLoss(reg_max - 1, use_dfl=self.use_dfl).to(self.device)
          self.proj = torch.arange(reg_max, dtype=torch.float, device=self.device)

      def preprocess(self, targets, batch_size, scale_tensor):
          if targets.shape[0] == 0:
              out = torch.zeros(batch_size, 0, 5, device=self.device)
          else:
              i = targets[:, 0]  # image index
              _, counts = i.unique(return_counts=True)
              out = torch.zeros(batch_size, counts.max(), 5, device=self.device)
              for j in range(batch_size):
                  matches = i == j
                  n = matches.sum()
                  if n:
                      out[j, :n] = targets[matches, 1:]
              out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))
          return out

      def bbox_decode(self, anchor_points, pred_dist):
          if self.use_dfl:
              b, a, c = pred_dist.shape  # batch, anchors, channels
              pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))
              # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))
              # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)
          return dist2bbox(pred_dist, anchor_points, xywh=False)

      def __call__(self, preds, batch):
          loss = torch.zeros(3, device=self.device)  # box, cls, dfl
          feats = preds[1] if isinstance(preds, tuple) else preds
          pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(
              (self.reg_max * 4, self.nc), 1)

          pred_scores = pred_scores.permute(0, 2, 1).contiguous()
          pred_distri = pred_distri.permute(0, 2, 1).contiguous()

          dtype = pred_scores.dtype
          batch_size = pred_scores.shape[0]
          imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)
          anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)

          # targets
          targets = torch.cat((batch['batch_idx'].view(-1, 1), batch['cls'].view(-1, 1), batch['bboxes']), 1)
          targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])
          gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy
          mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)

          # pboxes
          pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)

          _, target_bboxes, target_scores, fg_mask, _ = self.assigner(
              pred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),
              anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)

          target_bboxes /= stride_tensor
          target_scores_sum = max(target_scores.sum(), 1)

          # cls loss
          # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way
          loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE

          # bbox loss
          if fg_mask.sum():
              loss[0], loss[2] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,
                                                target_scores_sum, fg_mask)

          loss[0] *= self.box_weight  # box gain
          loss[1] *= self.cls_weight  # cls gain
          loss[2] *= self.dfl_weight  # dfl gain

          return loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)
  ```
  `ultralytics/yolo/engine/validator.py` it will fuse bn if use directly on model
  ```py
  from ultralytics.yolo import v8
  from ultralytics.yolo.utils.ops import Profile
  from ultralytics.yolo.data.utils import check_det_dataset

  validator = v8.detect.DetectionValidator(val_loader, save_dir=Path("./test"), args=copy.copy(cfg))
  # def eval(validator, model):
  validator.data = check_det_dataset(validator.args.data)
  validator.device = next(model.parameters()).device
  dt = Profile(), Profile(), Profile()
  n_batches = len(validator.dataloader)
  desc = validator.get_desc()

  bar = tqdm(validator.dataloader, desc, n_batches, bar_format='{l_bar}{bar:10}{r_bar}')
  validator.init_metrics(model)
  validator.jdict = []  # empty before each val
  for batch_i, batch in enumerate(bar):
      validator.run_callbacks('on_val_batch_start')
      validator.batch_i = batch_i
      # preprocess
      with dt[0]:
          batch = validator.preprocess(batch)

      # inference
      with dt[1]:
          preds = model(batch['img'])

      # postprocess
      with dt[2]:
          preds = validator.postprocess(preds)

      validator.update_metrics(preds, batch)
      if validator.args.plots and batch_i < 3:
          validator.plot_val_samples(batch, batch_i)
          validator.plot_predictions(batch, preds, batch_i)

      validator.run_callbacks('on_val_batch_end')
  stats = validator.get_stats()
  validator.check_stats(stats)
  validator.print_results()
  # validator.speed = dict(zip(validator.speed.keys(), (x.t / len(validator.dataloader.dataset) * 1E3 for x in dt)))
  validator.finalize_metrics()
  validator.run_callbacks('on_val_end')
  stats = validator.eval_json(stats)  # update stats
  return stats
  ```
  `ultralytics/yolo/v8/detect/train.py`
  ```py
  import copy
  import torch
  from tqdm import tqdm
  from pathlib import Path
  from torch import nn
  from torch.cuda import amp
  from torch.optim import lr_scheduler

  from ultralytics import YOLO
  # from ultralytics.yolo import v8
  # from ultralytics.yolo.utils.torch_utils import ModelEMA

  def build_optimizer(model, lr=0.01, momentum=0.937, decay=5e-4):
      g = [], [], []  # optimizer parameter groups
      bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()
      for v in model.modules():
          if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)
              g[2].append(v.bias)
          if isinstance(v, bn):  # weight (no decay)
              g[1].append(v.weight)
          elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)
              g[0].append(v.weight)

      optimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)
      optimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay
      optimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)
      return optimizer

  epochs = 100
  batch_size = 16
  warmup_epochs = 3
  close_mosaic = 10
  use_amp = True

  yolo = YOLO('../ultralytics/ultralytics/models/v8/yolov8n.yaml')
  model = yolo.model.cuda()
  _ = model.train()
  device = next(model.parameters()).device  # get model device
  compute_loss = Loss(device=device)
  accumulate = max(round(64 / batch_size), 1)
  optimizer = build_optimizer(model)
  # lf = lambda x: (x * (1 - 0.01) / warmup_epochs + 0.01) if x < warmup_epochs else ((1 - x / epochs) * (1.0 - 0.01) + 0.01)  # linear
  lf = lambda x: (1 - x / epochs) * (1.0 - 0.01) + 0.01  # linear
  scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)
  scaler = amp.GradScaler(enabled=use_amp)
  # validator = v8.detect.DetectionValidator(val_loader, save_dir=Path("./test"), args=copy.copy(cfg))
  # ema = ModelEMA(model)

  nb = len(train_loader)
  nbs = 64
  nw = max(round(warmup_epochs * nb), 100)
  warmup_bias_lr = 0.1
  momentum = 0.937
  warmup_momentum = 0.8
  last_opt_step = -1
  for epoch in range(0, epochs):
      # self.run_callbacks('on_train_epoch_start')
      model.train()
      # Update attributes (optional)
      if epoch == (epochs - close_mosaic):
          print('Closing dataloader mosaic')
          if hasattr(train_loader.dataset, 'mosaic'):
              train_loader.dataset.mosaic = False
          if hasattr(train_loader.dataset, 'close_mosaic'):
              train_loader.dataset.close_mosaic(hyp=cfg)

      tloss = None
      optimizer.zero_grad()
      loss_names = ["box_loss", "cls_loss", "dfl_loss"]
      print(('\n' + '%11s' * (3 + len(loss_names))) % ('Epoch', *loss_names, 'Instances', 'Size'))
      pbar = tqdm(enumerate(train_loader), total=nb, bar_format='{l_bar}{bar:10}{r_bar}')
      for i, batch in pbar:
          # self.run_callbacks('on_train_batch_start')
          ni = i + nb * epoch
          if ni <= nw:
              xi = [0, nw]  # x interp
              accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())
              for j, x in enumerate(optimizer.param_groups):
                  # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                  x['lr'] = np.interp(
                      ni, xi, [warmup_bias_lr if j == 0 else 0.0, x['initial_lr'] * lf(epoch)])
                  if 'momentum' in x:
                      x['momentum'] = np.interp(ni, xi, [warmup_momentum, momentum])

          # Forward
          with torch.cuda.amp.autocast(use_amp):
              preds = model(batch['img'].to(device, non_blocking=True).float() / 255)
              loss, loss_items = compute_loss(preds, batch)
              tloss = (tloss * i + loss_items) / (i + 1) if tloss is not None else loss_items

          # Backward
          scaler.scale(loss).backward()

          # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
          if ni - last_opt_step >= accumulate:
              # optimizer_step(model, optimizer, scaler)
              scaler.unscale_(optimizer)  # unscale gradients
              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
              scaler.step(optimizer)
              scaler.update()
              optimizer.zero_grad()
              # ema.update(model)
              last_opt_step = ni

          loss_len = tloss.shape[0] if len(tloss.size()) else 1
          losses = tloss if loss_len > 1 else torch.unsqueeze(tloss, 0)
          pbar.set_description(
              ('%11s' * 1 + '%11.4g' * (2 + loss_len)) %
              (f'{epoch + 1}/{epochs}', *losses, batch['cls'].shape[0], batch['img'].shape[-1]))
      scheduler.step()
      model.eval()
      validator(model=model)
      # yolo.val(data=dataset_path)
      torch.cuda.empty_cache()
  ```
***

# To yolov8 detector
  ```py
  import torch
  from torch import nn

  def dist2bbox(distance, anchor_points, xywh=True, dim=-1):
      """Transform distance(ltrb) to box(xywh or xyxy)."""
      lt, rb = distance.chunk(2, dim)
      x1y1 = anchor_points - lt
      x2y2 = anchor_points + rb
      if xywh:
          c_xy = (x1y1 + x2y2) / 2
          wh = x2y2 - x1y1
          return torch.cat((c_xy, wh), dim)  # xywh bbox
      return torch.cat((x1y1, x2y2), dim)  # xyxy bbox


  class DFL(nn.Module):
      # Integral module of Distribution Focal Loss (DFL)
      # Proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391
      def __init__(self, c1=16):
          super().__init__()
          self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)
          x = torch.arange(c1, dtype=torch.float)
          self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))
          self.c1 = c1

      def forward(self, x):
          b, c, a = x.shape  # batch, channels, anchors
          return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)
          # return self.conv(x.view(b, self.c1, 4, a).softmax(1)).view(b, 4, a)


  class Detect(nn.Module):
      def __init__(self, model, reg_max=16, num_pyramid_levels=3, export=False):  # detection layer
          super().__init__()
          self.model, self.reg_max, self.num_pyramid_levels, self.export = model, reg_max, num_pyramid_levels, export
          self.build(model.output_shape)

      def build(self, input_shape):
          self.num_classes = input_shape[-1] - self.reg_max * 4
          pyramid_len = input_shape[1] // sum([4 ** ii for ii in range(self.num_pyramid_levels)])

          self.pyramid_lens = [pyramid_len * (4 ** ii) for ii in range(self.num_pyramid_levels)][::-1]
          self.pyramid_lens_sqrt = [int(math.sqrt(ii)) for ii in self.pyramid_lens]
          self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()

          self.device = next(self.model.parameters()).device
          self.anchors, self.strides = self.make_anchors()
          # self.anchors, self.strides = anchors.transpose(0, 1), strides.transpose(0, 1)

      def make_anchors(self, grid_cell_offset=0.5):
          """Generate anchors from features."""
          anchor_points, stride_tensor = [], []
          strides = [2 ** ii for ii in range(3, 3 + self.num_pyramid_levels)]
          for block, stride in zip(self.pyramid_lens_sqrt, strides):
              sx = torch.arange(end=block, device=self.device, dtype=torch.float32) + grid_cell_offset  # shift x
              sy = torch.arange(end=block, device=self.device, dtype=torch.float32) + grid_cell_offset  # shift y
              sy, sx = torch.meshgrid(sy, sx, indexing='ij')
              anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))
              stride_tensor.append(torch.full((block * block, 1), stride, dtype=torch.float32, device=self.device))
          return torch.cat(anchor_points).transpose(0, 1), torch.cat(stride_tensor).transpose(0, 1)

      def forward(self, inputs):
          out = self.model(inputs)
          out = out.permute([0, 2, 1])

          train_out = torch.split(out, self.pyramid_lens, dim=-1)
          train_out = [ii.view([-1, ii.shape[1], shape, shape]) for ii, shape in zip(train_out, self.pyramid_lens_sqrt)]

          if self.training:
              return train_out
          else:
              box, cls = torch.split(out, (self.reg_max * 4, self.num_classes), dim=1)
              box = box.reshape([-1, 4, 16, box.shape[-1]])[:, [1, 0, 3, 2]].reshape([-1, 64, box.shape[-1]])
              dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides
              val_out = torch.cat((dbox, cls.sigmoid()), 1)
              return val_out if self.export else (val_out, train_out)
  ```
  ```py
  from ultralytics import YOLO
  from keras_cv_attention_models.yolov8 import torch_wrapper, yolov8

  yolo = YOLO('yolov8n.yaml')
  mm = yolov8.YOLOV8_N(pretrained=None, classifier_activation=None)
  _ = mm.train()
  tt = torch_wrapper.Detect(mm)
  tt.yaml = yolo.model.yaml
  yolo.model = tt
  yolo.train(data='coco128.yaml', epochs=100)
  ```
***

# Plot log
  ```py
  def pass_log(log_file, start_from=1):
      with open(log_file) as ff:
          aa = [ii.strip() for ii in ff.readlines() if '/100' in ii]
      with open(log_file) as ff:
          val_map50_95 = [float(ii.strip().split(' ')[-1]) for ii in ff.readlines() if 'all' in ii]

      box_loss, cls_loss, dfl_loss = [], [], []
      for line in aa:
          items = [ii for ii in line.split(' ')  if len(ii.strip()) != 0]
          box_loss.append(float(items[start_from]))
          cls_loss.append(float(items[start_from + 1]))
          dfl_loss.append(float(items[start_from + 2]))

      print(f"{len(box_loss) = }, {len(cls_loss) = }, {len(dfl_loss) = }, {len(val_map50_95) = }")
      # len(box_loss) = 100, len(cls_loss) = 100, len(dfl_loss) = 100, len(val_map50_95) = 100
      return box_loss, cls_loss, dfl_loss, val_map50_95[:len(dfl_loss)]

  from keras_cv_attention_models.imagenet import eval_func

  fig, axes = plt.subplots(1, 4, figsize=(20, 5))
  files_and_starts = {
      "yolov8n_ultralytics.log": {"label_prefix": "[ultralytics yolov8n] ", "start_from": 2},
      "yolov8n_kecam_ema_e100.log": {"label_prefix": "[kecam yolov8n] ", "va": "top"},
      "yolov8_effv2b0_ema_e100.log": {"label_prefix": "[kecam effiv2b0 + yolov8] "},
  }
  for file, kwargs in files_and_starts.items():
      box_loss, cls_loss, dfl_loss, val_map50_95 = pass_log(file, start_from=kwargs.get("start_from", 1))

      label_prefix = kwargs.get("label_prefix", "")
      va = kwargs.get("va", "bottom")
      eval_func.plot_and_peak_scatter(axes[0], box_loss, peak_method=np.argmin, label=label_prefix + "box_loss", va=va)
      eval_func.plot_and_peak_scatter(axes[1], cls_loss, peak_method=np.argmin, label=label_prefix + "cls_loss", va=va)
      eval_func.plot_and_peak_scatter(axes[2], dfl_loss, peak_method=np.argmin, label=label_prefix + "dfl_loss", va=va)
      eval_func.plot_and_peak_scatter(axes[3], val_map50_95, peak_method=np.argmax, label=label_prefix + "val_map50_95", va=va)
      # axes[0].plot(box_loss, label=label_prefix + "box_loss")
      # axes[1].plot(cls_loss, label=label_prefix + "cls_loss")
      # axes[2].plot(dfl_loss, label=label_prefix + "dfl_loss")
      # axes[3].plot(val_map50_95, label=label_prefix + "val_map50_95")

  for ax in axes:
      ax.legend()
      ax.grid(True)
  plt.tight_layout()
  ```
***

# YOLO NAS
## Analysing source code
  - `super_gradients/training/models/detection_models/yolo_nas/yolo_stages.py`
  ```py
  from super_gradients.training import models
  from super_gradients.modules import QARepVGGBlock
  from super_gradients.training.models.arch_params_factory import get_arch_params
  get_arch_params("yolo_nas_s_arch_params")

  {'backbone': {'NStageBackbone': {'stem': {'YoloNASStem': {'out_channels': 48}},
     'stages': [
      {'YoloNASStage': {'out_channels': 96, 'num_blocks': 2, 'activation_type': 'relu', 'hidden_channels': 32, 'concat_intermediates': False}},
      {'YoloNASStage': {'out_channels': 192, 'num_blocks': 3, 'activation_type': 'relu', 'hidden_channels': 64, 'concat_intermediates': False}},
      {'YoloNASStage': {'out_channels': 384, 'num_blocks': 5, 'activation_type': 'relu', 'hidden_channels': 96, 'concat_intermediates': False}},
      {'YoloNASStage': {'out_channels': 768, 'num_blocks': 2, 'activation_type': 'relu', 'hidden_channels': 192, 'concat_intermediates': False}}],
     'context_module': {'SPP': {'output_channels': 768, 'activation_type': 'relu', 'k': [5, 9, 13]}},
     'out_layers': ['stage1', 'stage2', 'stage3', 'context_module']}},
   'neck': {'YoloNASPANNeckWithC2': {
     'neck1': {'YoloNASUpStage': {'out_channels': 192, 'num_blocks': 2, 'hidden_channels': 64, 'width_mult': 1, 'depth_mult': 1, 'activation_type': 'relu', 'reduce_channels': True}},
     'neck2': {'YoloNASUpStage': {'out_channels': 96, 'num_blocks': 2, 'hidden_channels': 48, 'width_mult': 1, 'depth_mult': 1, 'activation_type': 'relu', 'reduce_channels': True}},
     'neck3': {'YoloNASDownStage': {'out_channels': 192, 'num_blocks': 2, 'hidden_channels': 64, 'activation_type': 'relu', 'width_mult': 1, 'depth_mult': 1}},
     'neck4': {'YoloNASDownStage': {'out_channels': 384, 'num_blocks': 2, 'hidden_channels': 64, 'activation_type': 'relu', 'width_mult': 1, 'depth_mult': 1}}}},
   'heads': {'NDFLHeads': {'num_classes': 80, 'reg_max': 16, 'heads_list': [
      {'YoloNASDFLHead': {'inter_channels': 128, 'width_mult': 0.5, 'first_conv_group_size': 0, 'stride': 8}},
      {'YoloNASDFLHead': {'inter_channels': 256, 'width_mult': 0.5, 'first_conv_group_size': 0, 'stride': 16}},
      {'YoloNASDFLHead': {'inter_channels': 512, 'width_mult': 0.5, 'first_conv_group_size': 0, 'stride': 32}}]}},
   'bn_eps': 0.001,
   'bn_momentum': 0.03,
   'inplace_act': True}
  ```
  **QARepVGGBlock** `from super_gradients.modules import QARepVGGBlock, Conv`
  ```py

  def forward(self, inputs):
      if self.fully_fused:
          return self.se(self.nonlinearity(self.rbr_reparam(inputs)))

      if self.partially_fused:
          return self.se(self.nonlinearity(self.post_bn(self.rbr_reparam(inputs))))

      if self.identity is None:
          id_out = 0.0
      else:
          id_out = self.identity(inputs)

      x_3x3 = self.branch_3x3(inputs)
      x_1x1 = self.alpha * self.branch_1x1(inputs)

      branches = x_3x3 + x_1x1 + id_out

      out = self.nonlinearity(self.post_bn(branches))
      se = self.se(out)

      return se

  def YoloNASCSPLayer_forward(self, x: Tensor) -> Tensor:
      x_1 = self.conv1(x)
      x_1 = self.bottlenecks(x_1)
      x_2 = self.conv2(x)
      x = torch.cat((*x_1, x_2), dim=1)
      return self.conv3(x)

  def YoloNASPANNeckWithC2_forward(self, inputs: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tuple[Tensor, Tensor, Tensor]:
      c2, c3, c4, c5 = inputs

      x_n1_inter, x = self.neck1([c5, c4, c3])
      x_n2_inter, p3 = self.neck2([x, c3, c2])
      p4 = self.neck3([p3, x_n2_inter])
      p5 = self.neck4([p4, x_n1_inter])

      return p3, p4, p5

  def YoloNASUpStage_forward(self, inputs):
      x, skip_x1, skip_x2 = inputs
      skip_x1, skip_x2 = self.reduce_skip1(skip_x1), self.reduce_skip2(skip_x2)
      skip_x = [skip_x1, self.downsample(skip_x2)]
      x_inter = self.conv(x)
      x = self.upsample(x_inter)
      x = torch.cat([x, *skip_x], 1)
      x = self.reduce_after_concat(x)
      x = self.blocks(x)
      return x_inter, x

  def YoloNASDownStage_forward(self, inputs):
      x, skip_x = inputs
      x = self.conv(x)
      x = torch.cat([x, skip_x], 1)
      x = self.blocks(x)
      return x

  ```
## Convert
  ```py
  from super_gradients.training import models
  from super_gradients import modules
  tt = models.get("yolo_nas_l", pretrained_weights='coco')
  _ = tt.eval()
  for name, module in tt.named_modules():
      if isinstance(module, modules.QARepVGGBlock):
          print("partial_fusion:", name)
          module.partial_fusion()
  ss = {kk: vv for kk, vv in tt.state_dict().items() if not ('branch_3x3' in kk or 'branch_1x1' in kk)}

  from keras_cv_attention_models.yolov8 import yolo_nas
  mm = yolo_nas.YOLO_NAS_L()

  from keras_cv_attention_models import download_and_load
  from keras_cv_attention_models import attention_layers

  tail_align_dict = {"middle_bn": -1, "short_0_alpha": -4}
  tail_align_dict.update({"short_{}_alpha".format(ii): "pre_{}_1_conv".format(ii) for ii in range(1, 10)})

  full_name_align_dict = {"stack4_spp_fast_output_bn": -1, "pafpn_p3p4p5_output_bn": -1}
  parallel_stacks, num_blocks = [1, 2, 3, 4], [2, 3, 5, 2]
  for id, num_block in zip(parallel_stacks, num_blocks):
      full_name_align_dict.update({"stack{}_c2f_pre_{}_2_bn".format(id, num_block-1): -1})

  headers = [
      'pafpn_out1_conv', 'pafpn_out1_bn', 'head_1_cls_1_conv', 'head_1_cls_1_bn',
      'head_1_reg_1_conv', 'head_1_reg_1_bn', 'head_1_cls_3_conv', 'head_1_reg_3_conv',
      'pafpn_out2_conv', 'pafpn_out2_bn', 'head_2_cls_1_conv', 'head_2_cls_1_bn',
      'head_2_reg_1_conv', 'head_2_reg_1_bn', 'head_2_cls_3_conv', 'head_2_reg_3_conv',
      'pafpn_out3_conv', 'pafpn_out3_bn', 'head_3_cls_1_conv', 'head_3_cls_1_bn',
      'head_3_reg_1_conv', 'head_3_reg_1_bn', 'head_3_cls_3_conv', 'head_3_reg_3_conv',
  ]

  specific_match_func = lambda tt: tt[:- len(headers)] + headers
  additional_transfer = {attention_layers.ZeroInitGain: lambda ww: [ww[0][0]]}
  download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      tail_align_dict=tail_align_dict,
      full_name_align_dict=full_name_align_dict,
      specific_match_func=specific_match_func,
      additional_transfer=additional_transfer,
      do_predict=False,
      do_convert=True,
  )
  ```
  **Convert bboxes output `[left, top, right, bottom]` -> `top, left, bottom, right`**
  ```py
  from keras_cv_attention_models.yolov8 import yolo_nas
  mm = yolo_nas.YOLO_NAS_L(pretrained="yolo_nas_l_imagenet.h5")
  heads = [ii.name for ii in mm.layers if ii.name.startswith('head')]
  for ii in range(1, 100):
      layer_name = 'head_{}_reg_3_conv'.format(ii)
      print(f">>>> {layer_name = }")
      if layer_name not in heads:
          break
      conv_layer = mm.get_layer(layer_name)
      new_ww = []
      for ww in conv_layer.get_weights():
          ww = np.reshape(ww, [*ww.shape[:-1], 4, 17])[..., [1, 0, 3, 2], :]
          ww = np.reshape(ww, [*ww.shape[:-2], -1])
          new_ww.append(ww)
      conv_layer.set_weights(new_ww)

  """ Concat short + deep weights -> pre """
  bb = keras.models.load_model('yolo_nas_l_imagenet.h5')
  for ii in mm.layers:
      if not ii.name.endswith('pre_conv') or ii.name.endswith('spp_fast_pre_conv'):
          continue
      prefix = ii.name[:-len('pre_conv')]
      print(f">>>> {prefix = }")
      short_conv_ww = bb.get_layer(prefix + 'short_conv').get_weights()[0]
      deep_conv_ww = bb.get_layer(prefix + 'deep_conv').get_weights()[0]
      ii.set_weights([np.concatenate([deep_conv_ww, short_conv_ww], axis=-1)])

      short_bn_wws = bb.get_layer(prefix + 'short_bn').get_weights()
      deep_bn_wws = bb.get_layer(prefix + 'deep_bn').get_weights()
      bn_layer = mm.get_layer(prefix + 'pre_bn')
      bn_layer.set_weights([np.concatenate([ii, jj], axis=-1) for ii, jj in zip(deep_bn_wws, short_bn_wws)])

  """ [parallel_mode=True] Convert c2f output for deep short concat order """
  parallel_stacks = [1, 2, 3, 4]
  out_names = ["stack{}_c2f_output_conv".format(id) for id in parallel_stacks]
  for name in out_names:
      layer = mm.get_layer(name)
      pre_layer = mm.get_layer(layer.inbound_nodes[0].inbound_layers.name)
      print(f"{name = }, {len(pre_layer.input_shape) = }")
      ww = layer.get_weights()[0]
      # [deep_1, deep_2, short] -> [short, deep_1, deep_2]
      wws = np.split(ww, len(pre_layer.input_shape), axis=2)
      layer.set_weights([np.concatenate(wws[-1:] + wws[:-1], axis=2)])
  pre_names = ["stack{}_c2f_pre_conv".format(id) for id in parallel_stacks]
  pre_names += ["stack{}_c2f_pre_bn".format(id) for id in parallel_stacks]
  for name in pre_names:
      layer = mm.get_layer(name)
      # [deep, short] -> [short, deep]
      wws = [np.split(ww, 2, axis=-1) for ww in layer.get_weights()]
      layer.set_weights([np.concatenate([ww[1], ww[0]], axis=-1) for ww in wws])

  mm.save(mm.name + "_coco.h5")

  from keras_cv_attention_models import test_images, coco
  imm = test_images.dog_cat()
  preds = mm(mm.preprocess_input(imm))
  bboxes, labels, confidences = mm.decode_predictions(preds)[0]
  print(f"{bboxes = }, {labels = }, {confidences = }")
  coco.show_image_with_bboxes(imm, bboxes, labels, confidences, num_classes=80)
  ```
