```py
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

model_path = "LinkSoul/Chinese-Llama-2-7b"

tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
instruction = "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. " \
"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. " \
"Please ensure that your responses are socially unbiased and positive in nature. " \
"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. " \
"If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n\n{} [/INST]"

prompt = instruction.format("用英文回答，什么是夫妻肺片？")
hf_output = tokenizer(prompt, return_tensors='pt').input_ids
tokenizer.decode(hf_output[0])

from keras_cv_attention_models import clip
tt = clip.SentencePieceTokenizer()
kecam_out = np.array(tt.encode(prompt, add_sot=True))
np.allclose(kecam_out, hf_output.detach())
tt.decode(kecam_out)
```
- **`transformers/generation/streamers.py` put**
```py
self._is_chinese_char(ord(test[-1]))
```
- Reloading LLaMA2 weights fitting transformers
```py
import kecam
mm = kecam.models.LLaMA2_42M()
for ii in mm.layers:
    if ii.name.endswith("q_proj") or ii.name.endswith("k_proj"):
        print(ii.get_weights()[0].shape)
        ww = ii.get_weights()[0]
        ww = ww.reshape([ww.shape[0], 8, -1, 2]).transpose([0, 1, 3, 2]).reshape(ww.shape)
        ii.set_weights([ww])
mm.run_prediction('As evening fell, a maiden stood at the edge of a wood. In her hands,')
mm.save(mm.name + "_tiny_stories.h5")
```
