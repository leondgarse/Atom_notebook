```py
import numpy as np
from keras_cv_attention_models.backend import layers, models, functional, image_data_format, register_keras_serializable
from keras_cv_attention_models.beit.beit import Beit, keras_model_load_weights_from_pytorch_model


@register_keras_serializable(package="kecam/eva")
class PositionalEncodingFourierRot(layers.Layer):
    def __init__(self, temperature=1e4, ref_feature_shape=16, **kwargs):
        super().__init__(**kwargs)
        self.temperature, self.ref_feature_shape = float(temperature), ref_feature_shape

    def build(self, input_shape):
        _, height, width, channels = input_shape  # ex: height, width, filters = 12, 27, 32
        hh, ww = np.arange(height, dtype="float32"), np.arange(width, dtype="float32")
        if self.ref_feature_shape is not None and self.ref_feature_shape > 0:
            # eva's scheme for resizing rope embeddings (ref shape = pretrain)
            hh = hh / height * self.ref_feature_shape
            ww = ww / height * self.ref_feature_shape

        pos_fileters = channels // 4
        grid = np.stack(np.meshgrid(hh, ww, indexing='ij'), axis=-1)
        dim_t = self.temperature ** (np.arange(pos_fileters, dtype="float32") / pos_fileters)  # (filters,)
        grid = np.expand_dims(grid, -1) / dim_t
        grid = np.reshape(grid, [height, width, -1])
        pos_sin, pos_cos = np.sin(grid), np.cos(grid)
        pos_sin, pos_cos = np.repeat(pos_sin, 2, axis=-1), np.repeat(pos_cos, 2, axis=-1)

        if hasattr(self, "register_buffer"):  # PyTorch
            self.register_buffer("pos_sin", functional.convert_to_tensor(pos_sin, dtype="float32"), persistent=False)
            self.register_buffer("pos_cos", functional.convert_to_tensor(pos_cos, dtype="float32"), persistent=False)
        else:
            self.pos_sin = functional.convert_to_tensor(pos_sin, dtype="float32")
            self.pos_cos = functional.convert_to_tensor(pos_cos, dtype="float32")
        self.height, self.width, self.channels = height, width, channels
        super().build(input_shape)

    def call(self, inputs, **kwargs):
        # def rot(x): return torch.stack([-x[..., 1::2], x[..., ::2]], -1).reshape(x.shape)
        left, right = functional.split(functional.reshape(inputs, [-1, self.height, self.width, self.channels // 2, 2]), 2, axis=-1)
        rot = functional.reshape(functional.concat([-right, left], axis=-1), (-1, self.height, self.width, self.channels))
        # def apply_rot_embed_cat(x: torch.Tensor, emb): return x * cos_emb + rot(x) * sin_emb
        return inputs * self.pos_cos + rot * self.pos_sin

    def get_config(self):
        base_config = super().get_config()
        base_config.update({"ref_feature_shape": self.ref_feature_shape, "temperature": self.temperature})
        return base_config


def EvaLargePatch14(input_shape=(196, 196, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
    patch_size = kwargs.pop("patch_size", 14)
    embed_dim = 1024
    depth = 24
    num_heads = 16
    gamma_init_value = 0
    use_abs_pos_emb = True
    attn_qkv_bias = True
    force_reload_mismatch = patch_size != 14  # If patch_size not 14, force reload pos_emb and stem_conv weights
    return Beit(**locals(), model_name="eva_large_patch14", **kwargs)


def EvaGiantPatch14(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
    patch_size = kwargs.pop("patch_size", 14)
    mlp_ratio = 6144 / 1408
    embed_dim = 1408
    depth = 40
    num_heads = 16
    gamma_init_value = 0
    use_abs_pos_emb = True
    force_reload_mismatch = patch_size != 14  # If patch_size not 14, force reload pos_emb and stem_conv weights
    return Beit(**locals(), model_name="eva_giant_patch14", **kwargs)


def eva02_small_patch14_224(pretrained=False, **kwargs):
    model_kwargs = dict(
        img_size=224,
        patch_size=14,
        embed_dim=384,
        depth=12,
        num_heads=6,
        mlp_ratio=4 * 2 / 3,
        swiglu_mlp=True,
        use_rot_pos_emb=True,
        ref_feat_shape=(16, 16),  # 224/14
    )
    model = _create_eva("eva02_small_patch14_224", pretrained=pretrained, **dict(model_kwargs, **kwargs))
    return model


class SwiGLU:
    """SwiGLU
    NOTE: GluMLP above can implement SwiGLU, but this impl has split fc1 and
    better matches some other common impl which makes mapping checkpoints simpler.
    """

    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer="SiLU", norm_layer="LayerNorm", bias=True, drop=0.0):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        bias = to_2tuple(bias)
        drop_probs = to_2tuple(drop)

        self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])
        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])
        self.drop2 = nn.Dropout(drop_probs[1])

        self.drop = nn.Dropout(drop)

    def init_weights(self):
        # override init of fc1 w/ gate portion set to weight near zero, bias=1
        nn.init.ones_(self.fc1a.bias)
        nn.init.normal_(self.fc1a.weight, std=1e-6)

    def forward(self, x):
        x_gate = self.fc1_g(x)
        x = self.fc1_x(x)
        x = self.act(x_gate) * x
        x = self.drop1(x)
        x = self.norm(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x
```
