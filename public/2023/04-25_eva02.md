```py
sys.path.append('../pytorch-image-models/')
import torch
from timm.layers import RotaryEmbeddingCat, apply_rot_embed_cat

input_shape = [1, 3, 256, 64]
inputs = np.arange(np.prod(input_shape)).reshape(input_shape).astype("float32") / np.prod(input_shape)

embed_dim, num_heads, grid_size, ref_feat_shape = 192, 3, (16, 16), (16, 16)
rope = RotaryEmbeddingCat(embed_dim // num_heads, in_pixels=False, feat_shape=grid_size, ref_feat_shape=ref_feat_shape)
rot_pos_embed = rope.get_embed()
torch_out = apply_rot_embed_cat(torch.from_numpy(inputs), rot_pos_embed)

from keras_cv_attention_models.beit.beit import PositionalEncodingFourierRot
aa = PositionalEncodingFourierRot(with_cls_token=False)
tf_out = aa(inputs)

print(f"{np.allclose(torch_out.detach(), tf_out, atol=1e-6) = }")
# np.allclose(torch_out.detach(), tf_out, atol=1e-6) = True
```
```py
import numpy as np
from keras_cv_attention_models.backend import layers, models, functional, image_data_format, register_keras_serializable
from keras_cv_attention_models.beit.beit import Beit, keras_model_load_weights_from_pytorch_model


class PositionalEncodingFourierRot(keras.layers.Layer):
    def __init__(self, with_cls_token=True, attn_height=-1, temperature=1e4, ref_feature_shape=16, **kwargs):
        super().__init__(**kwargs)
        self.with_cls_token, self.attn_height = with_cls_token, attn_height
        self.temperature, self.ref_feature_shape = float(temperature), ref_feature_shape
        self.cls_token_len = 1 if with_cls_token else 0

    def build(self, input_shape):
        # input (with_cls_token=True): `[batch, num_heads, attn_blocks, attn_blocks]`. where `attn_blocks = attn_height * attn_width + class_token`
        # input (with_cls_token=False): `[batch, num_heads, attn_blocks, attn_blocks]`. where `attn_blocks = attn_height * attn_width`
        # print(input_shape)
        if self.attn_height == -1:
            height = width = int(float(input_shape[2] - self.cls_token_len) ** 0.5)  # hh == ww, e.g. 14
        else:
            height = self.attn_height
            width = int(float(input_shape[2] - self.cls_token_len) / height)
        channels = input_shape[-1]

        hh, ww = np.arange(height, dtype="float32"), np.arange(width, dtype="float32")
        if self.ref_feature_shape is not None and self.ref_feature_shape > 0:
            # eva's scheme for resizing rope embeddings (ref shape = pretrain)
            hh = hh / height * self.ref_feature_shape
            ww = ww / height * self.ref_feature_shape

        pos_fileters = channels // 4
        grid = np.stack(np.meshgrid(hh, ww, indexing="ij"), axis=-1)
        dim_t = self.temperature ** (np.arange(pos_fileters, dtype="float32") / pos_fileters)  # (filters,)
        grid = np.expand_dims(grid, -1) / dim_t
        grid = np.reshape(grid, [height, width, -1])
        pos_sin, pos_cos = np.sin(grid), np.cos(grid)
        pos_sin, pos_cos = np.repeat(pos_sin, 2, axis=-1), np.repeat(pos_cos, 2, axis=-1)
        print(f"{pos_sin.shape = }, {pos_cos.shape = }, {height = }, {width = }, {channels = }")
        pos_sin, pos_cos = np.reshape(pos_sin, [height * width, channels]), np.reshape(pos_cos, [height * width, channels])

        if hasattr(self, "register_buffer"):  # PyTorch
            self.register_buffer("pos_sin", functional.convert_to_tensor(pos_sin, dtype="float32"), persistent=False)
            self.register_buffer("pos_cos", functional.convert_to_tensor(pos_cos, dtype="float32"), persistent=False)
        else:
            self.pos_sin = functional.convert_to_tensor(pos_sin, dtype="float32")
            self.pos_cos = functional.convert_to_tensor(pos_cos, dtype="float32")
        self.blocks, self.channels = height * width, channels
        super().build(input_shape)

    def call(self, inputs, **kwargs):
        # def rot(x): return torch.stack([-x[..., 1::2], x[..., ::2]], -1).reshape(x.shape)
        left, right = functional.split(functional.reshape(inputs, [-1, self.blocks, self.channels // 2, 2]), 2, axis=-1)
        rot = functional.reshape(functional.concat([-right, left], axis=-1), (-1, self.blocks, self.channels))
        # def apply_rot_embed_cat(x: torch.Tensor, emb): return x * cos_emb + rot(x) * sin_emb
        return inputs * self.pos_cos + rot * self.pos_sin

    def get_config(self):
        base_config = super().get_config()
        base_config.update({"ref_feature_shape": self.ref_feature_shape, "temperature": self.temperature})
        return base_config


def EvaLargePatch14(input_shape=(196, 196, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
    patch_size = kwargs.pop("patch_size", 14)
    embed_dim = 1024
    depth = 24
    num_heads = 16
    gamma_init_value = 0
    use_abs_pos_emb = True
    attn_qkv_bias = True
    force_reload_mismatch = patch_size != 14  # If patch_size not 14, force reload pos_emb and stem_conv weights
    return Beit(**locals(), model_name="eva_large_patch14", **kwargs)


def EvaGiantPatch14(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
    patch_size = kwargs.pop("patch_size", 14)
    mlp_ratio = 6144 / 1408
    embed_dim = 1408
    depth = 40
    num_heads = 16
    gamma_init_value = 0
    use_abs_pos_emb = True
    force_reload_mismatch = patch_size != 14  # If patch_size not 14, force reload pos_emb and stem_conv weights
    return Beit(**locals(), model_name="eva_giant_patch14", **kwargs)


def eva02_small_patch14_224(pretrained=False, **kwargs):
    model_kwargs = dict(
        img_size=224,
        patch_size=14,
        embed_dim=384,
        depth=12,
        num_heads=6,
        mlp_ratio=4 * 2 / 3,
        swiglu_mlp=True,
        use_rot_pos_emb=True,
        ref_feat_shape=(16, 16),  # 224/14
    )
    model = _create_eva("eva02_small_patch14_224", pretrained=pretrained, **dict(model_kwargs, **kwargs))
    return model


class SwiGLU:
    """SwiGLU
    NOTE: GluMLP above can implement SwiGLU, but this impl has split fc1 and
    better matches some other common impl which makes mapping checkpoints simpler.
    """

    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer="SiLU", norm_layer="LayerNorm", bias=True, drop=0.0):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        bias = to_2tuple(bias)
        drop_probs = to_2tuple(drop)

        self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])
        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])
        self.drop2 = nn.Dropout(drop_probs[1])

        self.drop = nn.Dropout(drop)

    def init_weights(self):
        # override init of fc1 w/ gate portion set to weight near zero, bias=1
        nn.init.ones_(self.fc1a.bias)
        nn.init.normal_(self.fc1a.weight, std=1e-6)

    def forward(self, x):
        x_gate = self.fc1_g(x)
        x = self.fc1_x(x)
        x = self.act(x_gate) * x
        x = self.drop1(x)
        x = self.norm(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x
```
