# ___MSI5004 AI Governance and Ethics___
***

# Lectures
## Week1
  * [12:07–12:10] **Course learning objectives (two sub-topics)**
    As part of the learning objectives, there are two sub-topics. The first is to learn the main ethical issues arising from the use of AI. We will explore different contexts where AI is used today, and we will draw on existing literature—for example, autonomous vehicles, autonomous weapons, algorithmic pricing, and similar cases. The idea is: once you understand the typical ethical issues that show up in these settings, you can transfer the same lessons to new AI applications you may work on in future.
    这门课的学习目标分两块。第一块是：了解 AI 使用会带来哪些主要伦理问题。我们会看 AI 现在用在哪些场景里，并结合已有研究来讨论，比如自动驾驶、自动武器、算法定价等。核心意思是：你先把常见伦理问题“看懂、看熟”，以后遇到新的 AI 应用，也能把这些经验迁移过去。
  * [12:08–12:10] **Course learning objectives (AI governance principles)**
    The second sub-topic is the core principles of AI governance. If you have looked at international guidelines on AI governance, you will notice that they often repeat a set of principles—transparency, privacy, and so on. In this course, we will examine what these principles actually mean, and how they work in real governance contexts (not just as slogans on a document).
    第二块是：AI 治理（AI governance）的核心原则。你如果看过一些国际指南，会发现经常出现一组“共同原则”，比如透明性、隐私等。这门课会进一步问：这些词到底是什么意思？在真实治理场景里要怎么用，而不是只把它们当口号背下来。
  * [12:09–12:10] **Course positioning (law school, but not legal training)**
    Although this course is taught in the law school, it is not primarily intended to be a legal training course. The goal is not to train you to become lawyers who handle AI legal issues. It is more of a non-legal, interdisciplinary course. That said, we may touch on regulation a little (for example, the EU AI Act), but the plan is not to go deep into detailed legal rules.
    虽然这门课在法学院开，但它不是“把你训练成 AI 律师”的课。重点不是教你处理具体法律条文，而是用更偏非法律、跨学科的方式看 AI 治理与伦理。当然监管会提到一些（比如欧盟 AI Act），但不会做很细的法条深挖。
  * [12:10–12:11] **Workload and weekly readings**
    The workload follows the standard pattern for a 4-credit course: 3 hours of seminar time and about 9.5 hours of preparatory work per week. The prep work is mainly readings assigned for each seminar. Typically, the reading list is around 5–6 papers. Some readings are marked optional (extra). Sometimes I will give pinpoint citations (for example, “read pages X–Y”), so you only need to focus on those specific parts.
    工作量按 4 学分课的标准来：每周 3 小时课堂（seminar）+ 大约 9.5 小时课前准备。课前准备主要是阅读材料。一般每次大概 5–6 篇文章。有些会标 optional（额外阅读）。有些会给 pinpoint citation（指定页码/段落），这种就只读指定部分即可。
  * [12:11–12:13] **Assessment and exam format (open-book)**
    Assessment is straightforward. Class participation is 25% (attendance plus contributions). Contributions and interaction are encouraged, but you do not need to force yourself if you feel uncomfortable. In a large class, it also would not work if everyone talked a lot—so even one or two contributions across the semester can be fine. The final exam is 75%, with one question in two parts: Part A on AI ethics, and Part B on AI governance. The exam will be open-book.
    考核很直接：课堂参与 25%（出勤 + 课堂发言/参与）。鼓励互动，但如果你不舒服，不需要硬逼自己。大班课也不可能人人都很“能说”，所以一两次有质量的参与就可以。期末考试 75%，一道题分两部分：A 问 AI 伦理，B 问 AI 治理。考试是开卷。
  * [12:13–12:16] **Why define AI here (common baseline, not technical deep dive)**
    Let’s begin the actual lesson. You have probably heard a lot about AI already, especially if you took AI-related courses last semester. Those courses are better placed to cover technical details. Here, we will be brief on technicalities. But we still need a working definition of AI, because we need a shared understanding of what we mean when we discuss “AI governance” and “AI ethics” in this course.
    进入正课。你们大概率已经听过很多 AI 内容了，尤其上学期如果修过相关课，技术细节那边会讲得更专业。我们这里不会深挖技术，但我们必须先把“这门课里 AI 指什么”说清楚，因为后面讨论治理与伦理，需要大家有同一个基本口径。
  * [12:14–12:19] **Definition of AI (OED starting point → automation of cognitive tasks)**
    A common starting point is the Oxford English Dictionary: AI as the exhibition or simulation of intelligent behaviour, or something that simulates human intelligence. But the key question is: what does “intelligence” mean in practice? For this course, a helpful framing is: “artificial intelligence” is (often) a marketing term for the automation of cognitive tasks. Think of how “vintage” is a nicer way to say “old,” or “cozy” can be a nicer way to say “small.” Similarly, “AI” can make “automating mental work” sound more exciting, which can attract investment and help sell products.
    一个常见起点是牛津词典：AI 是对智能行为的展示或模拟，或者说模拟人类智能。但关键在于：落到现实里，“智能”到底指什么？在这门课里，一个更好用的说法是：很多时候，“人工智能”是一种营销式表达，本质是在讲“把认知任务自动化”。就像 “vintage” 往往是把 “old” 说得更好听，“cozy” 往往是把 “small” 说得更好听；“AI”也会把“自动化脑力劳动”包装得更吸引人，从而更容易拉投资、卖产品。
  * [12:17–12:20] **What “automation” means (and why it creates ethical tension)**
    By “automation,” we mean doing something without human intervention. The technical and ethical tension often comes from the same place: you are deliberately removing humans from the loop. For example, autonomous weapons are controversial because you are allowing a weapon to function without a human directly holding it and pulling the trigger. When you remove humans, you also remove human judgment, context, and accountability at the moment of action—this is one reason ethical problems become sharper.
    “自动化”的意思就是：尽量不需要人来干预。很多技术难点和伦理风险其实都来自同一点：你在刻意把人从流程里拿掉。比如自动武器之所以争议大，是因为它让武器可以在没有人直接扣扳机的情况下行动。人被拿掉以后，现场判断、语境理解、以及“当下谁负责”都会变弱，所以伦理问题会更尖锐。
  * [12:19–12:20] **Cognitive tasks AI tries to automate (with examples)**
    The “cognitive tasks” here are things humans normally do with their minds: classification (e.g., seeing whether an image is a cat or a dog), recognition (e.g., recognizing a face), prediction (e.g., stock market prediction), and decision-making (e.g., a bank using an algorithm to decide whether to give someone a loan). More recently, generative AI automates tasks like writing, drawing, and producing other content. This is not an exhaustive list, but it gives you a practical map of what we mean by “AI” in this course.
    这里的“认知任务”指人类通常用大脑完成的事情：分类（比如看图判断是猫还是狗）、识别（比如人脸识别）、预测（比如预测股市走势）、决策（比如银行用算法决定要不要放贷）。近几年生成式 AI 还把写作、绘画等内容生产也变成了可自动化的任务。这不是完整清单，但足够帮助我们在这门课里统一理解：我们说的 AI 大概落在哪些能力上。
  * [12:20–12:24] **AI as automation: physical vs cognitive (and hybrids)**
    To put this in a broader context, think of automation as having two overlapping circles: physical automation and cognitive automation. Historically, when people said “automation,” they often meant physical automation: steam engines automating movement, conveyor belts, factory arms, etc. AI focuses more on cognitive automation: text generators, search engines, diagnostic tools used by doctors. But there is no clean split between mind and body, so there is no clean split between physical and cognitive automation either. Some systems combine both: robots are a classic example. Autonomous vehicles and autonomous weapons also combine cognitive decisions with physical action. Even face recognition can become a hybrid when it triggers a physical outcome (e.g., a gate opens after recognizing your face).
    放到更大的自动化框架里，可以想象两个相交的圈：物理自动化和认知自动化。过去说“自动化”，更多是物理层面的：蒸汽机自动化运动、传送带、机械臂等。AI 更偏认知层面：文本生成器、搜索引擎、医生用的辅助诊断工具等。但“身心”本来就不是完全分开的，所以物理自动化和认知自动化也会混在一起：机器人是典型混合体；自动驾驶、自动武器也是“会判断 + 会行动”的组合。甚至人脸识别如果联动了实体动作（识别后开门/放行），也算混合类型。
  * [12:25–12:29] **Reading focus: anthropomorphism (Placani 2024)**
    Next, we look at an article discussing anthropomorphism—our tendency to attribute human qualities to things that are not human. This tendency is not new: humans often project human-like intention, emotion, or “inner life” onto objects and animals. The key point for AI is that anthropomorphism is not just saying “this looks human,” but treating the system as if it has a mind, motivations, or feelings.
    接下来进入阅读：拟人化（anthropomorphism）。人很容易把“人的特质”投射到非人事物上，这不是新现象。关键在 AI 场景里，拟人化不只是“它看起来像人”，而是你在心理上把它当成“有内心、有动机、有情绪的东西”来对待。
  * [12:27–12:33] **ELIZA example: why humans anthropomorphize chat systems**
    An early AI example is ELIZA, often described as one of the first successful chatbots. Technically it was very simple: it took your input and applied basic transformations, often repeating your words back in a therapist-like question (e.g., “My boyfriend made me come here.” → “Your boyfriend made you come here?”). Despite the simplicity, the creator observed that people began to treat ELIZA as somewhat human. Today, with much more capable chatbots, it is even easier for users to form human-like expectations or even relationships with chat systems.
    一个早期例子是 ELIZA，经常被认为是最早的“成功聊天机器人”之一。技术上它非常简单：把你的输入做很基础的改写，经常像心理咨询师一样把你的话反问回来（比如“我男朋友逼我来”→“你男朋友逼你来？”）。但即便这么简单，人们还是会把它当成“有点像人”。到了今天聊天机器人能力更强，更容易让人产生“像真人”的期待，甚至有人会对聊天系统形成类似关系的依赖。
  * [12:29–12:37] **Why anthropomorphism happens (language, marketing, naming)**
    Anthropomorphism appears frequently in AI talk. One reason may be language: the term “artificial intelligence” itself uses “intelligence,” which is a human trait, so it already invites human-like interpretation. Another reason is how systems are presented: assistants are given names, voices, or personalities. Everyday examples include referring to AI as “he/she” rather than “it,” or using human mental-state words like “hallucination.” Even naming a chatbot with a human name can nudge people to treat it like a person.
    拟人化在 AI 讨论里很常见，原因之一是语言本身：我们说“人工智能”，里面的“智能”本来就是人类特质，天然会引导人往“像人”那边想。另一个原因是产品呈现方式：给系统起名字、配声音、设人格。日常例子包括：把 AI 说成“他/她”而不是“它”，用“幻觉（hallucination）”这种人类词描述模型输出错误，或者直接给聊天机器人起人名——这些都会推动用户把它当成“一个人”。
  * [12:37–12:52] **Why anthropomorphism can be ethically harmful (four distortions)**
    The article links anthropomorphism to distorted moral thinking in at least four ways. (1) Moral character judgments: we start to talk as if the AI has intentions or a moral personality (“it is honest,” “it is evil,” “it is trying to help me”). (2) Moral status judgments: if we treat AI as human, we may start thinking it deserves rights, or that we have obligations to keep it “alive,” even when that makes little sense. (3) Responsibility judgments: when something goes wrong (e.g., an autonomous vehicle crash), people may blame “the AI” instead of the humans and organizations who built, deployed, and monitored it. (4) Trust judgments: we may trust AI like a person, even though human trust usually relies on the other party having an inner life and being accountable for betrayal or failure.
    文章把拟人化和“道德判断被带偏”联系起来，至少有四种扭曲：（1）道德品格判断：我们开始像评价人一样评价 AI（比如说它“善良/邪恶/有意图”）。（2）道德地位判断：把 AI 当人后，可能会觉得它“该有权利”，甚至觉得“不能关掉它”，这在很多情况下并不成立。（3）责任归因：出事时（比如自动驾驶事故）可能怪“AI”，而不是怪背后设计、部署、监管的个人与组织。（4）信任判断：把 AI 当人来信任，但人际信任通常依赖对方有“内在心智”以及“背叛/失败可以被追责”的结构，而 AI 并不具备这些条件。
  * [12:47–12:53] **Practical takeaway: keep accountability with humans; reduce anthropomorphic cues**
    A practical approach is: keep accountability with humans and deploying organizations. If you use a text generator and you communicate that output to others, you own it—you cannot say “the chatbot told me” to dodge responsibility. Another practical step is to reduce anthropomorphic cues: avoid naming systems like people, avoid describing systems as if they have feelings, and be careful about interface design that encourages users to “chat with a person” rather than “query a tool.”
    实践层面可以抓两点：第一，把责任牢牢放在人和部署组织身上。比如你用文本生成器写出一段话，再发给别人，那这段话就是你负责，不能用“是机器人说的”来甩锅。第二，尽量减少拟人化暗示：不要用人名给系统命名，不要把系统描述得像有情绪；界面设计也要谨慎——如果设计成“像在跟人聊天”，用户就更容易产生拟人化和过度依赖。
  * [13:08–13:13] **Reading focus: existential risk vs present harms (Richards 2023)**
    The next reading shifts to priorities in ethical AI: should attention focus on existential threats (AI could pose a risk to humanity if not properly controlled or “aligned”), or should it focus on current, concrete harms caused by AI today? The article argues that the existential-risk framing is often hypothetical and far away, and it can distract attention from real harms happening now—such as military uses of AI, misinformation, and other immediate impacts.
    下一篇阅读讨论 AI 伦理的“优先级”：我们应该把注意力放在存在性威胁（比如 AI 失控、对人类整体构成风险，常说的 alignment 问题），还是放在当下已经发生的现实伤害上？文章的观点是：很多存在性风险的说法很偏假设、偏遥远，容易把资源和注意力从“现在真实发生的伤害”上带走，比如军事用途、虚假信息等。
  * [13:12–13:14] **Class discussion: why some actors emphasize existential risk**
    A discussion point raised is a more cynical explanation: emphasizing existential risk can contribute to hype (it suggests AI may become super-intelligent, which attracts investment), and it may also redirect attention away from near-term issues that companies would rather not focus on. The lecture notes this as a possible social/political dynamic around AI narratives.
    课堂里还提到一种更“犬儒”的解释：强调存在性风险可能会制造更大声量（暗示 AI 会超级强，从而更好吸引投资），也可能把公众注意力从企业不想面对的短期问题上移开。这是一种围绕 AI 叙事的社会/政治层面的动力。
  * [13:14–13:18] **Extinction lens: why “more intelligent” rarely explains extinction**
    The article uses an extinction lens: historically, species extinction does not usually happen simply because a new species is “more intelligent.” Extinction commonly happens through (1) competition for resources, (2) predation (hunting), or (3) environmental change. Importantly, intelligence alone is not a typical driver of extinction in nature, and humans are a special case in that humans have caused many extinctions.
    文章用“物种灭绝”来做类比：历史上，物种灭绝通常不是因为“某个物种更聪明”。更常见的灭绝路径是：（1）资源竞争，（2）捕食/猎杀，（3）环境改变。单靠“更聪明”并不是自然界常见的灭绝原因；人类是例外，因为人类确实造成了很多物种灭绝。
  * [13:15–13:18] **Applying the extinction lens to AI scenarios (checkpoints matter)**
    Applying this to AI, the article argues that human extinction via AI is unlikely unless humans intentionally create extremely dangerous configurations. For example: “competition for resources” would require society to fully automate and expand AI infrastructure to an extreme level; “predation” becomes plausible only if humans hand over highly destructive systems (e.g., fully automated weapons) without safeguards; “environmental change” depends largely on human choices about energy use and deployment scale. Along the way, there are many checkpoints where humans can decide to stop or change course.
    套到 AI 上，文章认为“AI 导致人类灭绝”很不可能，除非人类自己把系统配置到非常危险的程度。比如“资源竞争”要发生，得是社会把自动化推到极端、基础设施无限扩张；“捕食/猎杀”只有在人类把高杀伤系统（比如完全自动化武器）交给 AI 且缺乏防护时才更像回事；“环境改变”也主要取决于人类怎么选择能源消耗与部署规模。更重要的是，中间有很多“检查点”，人类可以随时刹车或改方向。
  * [13:19–13:21] **“AI Pascal’s Wager” idea (why unlikely risks can dominate attention)**
    The lecture connects this to a structure like Pascal’s Wager: even if a catastrophic outcome is extremely unlikely, people argue that the consequence is so terrible that we should devote major resources to preventing it. The critique here is: the “very terrible but very unlikely” framing does not automatically justify shifting attention away from more probable, present harms.
    课堂把这种论证方式类比为“帕斯卡赌注”：即便灾难发生概率很低，但后果太可怕，所以就该投入大量资源去防它。这里的批评是：光靠“后果很可怕”并不能自动证明要把注意力从更高概率、正在发生的现实伤害上移走。
  * [13:21–13:22] **Course focus: AI ethics (practical harms) more than AI safety (speculative extinction)**
    The takeaway for this course framing is: we will spend more time on AI ethics issues and governance principles that address real deployments and real harms, rather than focusing primarily on speculative AI safety/extinction scenarios.
    这门课的取向是：更多讨论现实部署里真实发生的伦理问题与治理原则，而不是把重心放在高度推测的“AI 灭绝风险/AI safety”情景上。
  * [13:22–13:33] **Ethics basics: three ethical frameworks (Bonde 2013 slide)**
    The lecture introduces three broad families of ethical theory as practical reasoning tools.
    (1) Consequentialism: judge actions by outcomes. Utilitarianism is a key example: maximize overall happiness/well-being and minimize suffering. This is often used when decisions affect many people, by weighing total benefits against total harms.
    (2) Deontology (duty-based, non-consequentialist): judge actions by whether they follow duties or rules, not by outcomes. Associated with Immanuel Kant and “categorical imperatives.” A simple example is “do not steal”—a deontologist treats this as a rule you should follow even if stealing could have good consequences.
    (3) Virtue ethics (agent-centered): focus on what kind of person you should be; ask what a virtuous person would do. Associated with Aristotle; it emphasizes character over isolated acts.
    These frameworks often reach similar answers, but sometimes diverge. A classic contrast: stealing medicine to save a child—deontology says “don’t steal,” consequentialism may say “the benefit (saving a life) justifies it.”
    课堂用三大类伦理理论作为“做道德推理的工具”：
    （1）结果主义：按结果评判。功利主义是代表：尽量增加总体幸福/福祉，减少痛苦。适合分析影响很多人的决策，通过“总体利弊”来衡量。
    （2）义务论/道义论：按规则与义务评判，不看结果。常联系到康德的思路。简单例子是“不要偷窃”：即使偷了可能带来好结果，义务论也可能坚持不该做。
    （3）德性伦理：关注“我应该成为什么样的人”，问“有德性的人会怎么做”。常联系到亚里士多德，更强调品格而非单次行为。
    三种框架很多时候会得出相似结论，但也可能冲突。经典对比是“偷药救孩子”：义务论倾向说不该偷，结果主义可能说救命收益更大所以可以。
  * [13:34–13:38] **Exercise setup: Crisis Text Line data sharing (Levine 2022 slide)**
    The class does a short exercise based on a real-world scenario. Crisis Text Line is a mental health support texting service. It planned to share large volumes of conversation text data with Loris.ai, a commercial entity building AI tools to help customer service agents interact with customers. The motivation was: these conversations are highly emotional and persuasion-heavy, so they could be useful training data. The plan triggered controversy, and the service later decided not to proceed.
    课堂做了一个小练习，基于一个真实案例：Crisis Text Line 是心理健康支持短信服务。它曾计划把大量对话文本数据共享给 Loris.ai（商业公司），后者想用 AI 帮客服更好与客户沟通。理由是：这些对话情绪浓度高、带劝导性质，可能很适合作训练数据。但这事引发巨大争议，最后他们也决定不做了。
  * [13:38–14:02] **Applying duty-based ethics to the case (confidentiality and duty of care)**
    From a duty-based perspective, a central claim is that Crisis Text Line owes a duty of confidentiality (and arguably a duty of care) to users who share extremely sensitive information. Even if sharing could produce some benefits, the duty does not automatically disappear. The discussion also notes that duties can have exceptions, so one can ask: are there any relevant exceptions here that justify breaking confidentiality? The initial direction is that such exceptions are hard to defend in this context.
    用义务论来分析，一个核心观点是：Crisis Text Line 对求助者有保密义务（也可以说有照护义务），因为对方提供的是高度敏感的信息。就算共享可能带来某些好处，也不等于可以突破保密。讨论里也提到：义务有时会有例外，所以可以追问——这里有没有足够强的例外理由？课堂倾向认为：在这个情境里很难为“破坏保密”找到合理例外。
  * [14:03–14:07] **Applying virtue ethics to the case (trustworthiness; exploiting vulnerability)**
    From a virtue-ethics perspective, the focus is the character of the organization. A “trustworthy” and “good” helper should not share vulnerable people’s private conversations with others, especially with a commercial entity for a different purpose. The discussion also highlights two common difficulties with virtue ethics: (1) people can disagree on which virtues should dominate, depending on culture or religion; (2) even if we agree “trustworthiness” is a virtue, it can be hard to convert that into precise action rules in complicated cases.
    用德性伦理来分析，重点变成：这个组织想成为“什么样的机构”。如果它要成为可信、正直、真正帮助脆弱群体的机构，那就不该把求助者的私密对话转手给别人，尤其还是给商业公司做别的用途，因为这会显得在“利用他人的脆弱处境”。课堂也指出德性伦理的两个难点：（1）不同文化/宗教背景下，大家对“哪些美德最重要”可能不一致；（2）即使同意“可信”是美德，在复杂案例里怎么落到具体行动规则，有时会比较模糊。
  * [14:07–14:08] **Why ethical frameworks help (articulating your moral reasons)**
    The point of the exercise is not that you must apply every theory every time. In real life, people rarely do that. The value is: sometimes you feel something is morally problematic but cannot clearly explain why. These frameworks help you articulate and structure your reasons, which is especially useful in assessments and policy discussions.
    练习的重点不是要求你每次都把三套理论全用一遍——现实中没人这么做。它的价值在于：有时你觉得“这事不对劲”，但说不清为什么；这些框架能帮你把理由组织出来、讲清楚，这在写作、考试和政策讨论里都很有用。
  * [14:16–14:21] **AI governance principles: OECD vs ASEAN (no single agreed list)**
    The lecture notes there is a lot of governance material from international organizations. Two examples are OECD and ASEAN guidelines. They share some common principles (e.g., transparency; fairness), but they also differ in what they emphasize (e.g., ASEAN includes “human-centricity” explicitly). Even when the same principle word appears, its interpretation can differ across documents. So there is no universally agreed list. The course approach is to identify commonly recurring principles (transparency, explainability, robustness, accountability, etc.), clarify what they mean, and practice applying them.
    AI 治理材料很多，国际组织也各有各的指南。课堂对比了 OECD 和 ASEAN 两套：有些原则重合（比如透明、公平），也有一些强调点不同（比如 ASEAN 会更明确提“以人为本/human-centricity”）。而且就算同一个词出现（比如 transparency），不同文件里的解释也可能不一样。所以目前没有“全球统一的一份原则清单”。课程的做法是：抓住高频共同原则（透明、可解释、稳健、问责等），先把含义讲清楚，再练习如何应用到实际场景。
  * [14:21–14:25] **Example application: employee monitoring + bonus adjustment system**
    Example scenario: you are asked to launch an internal AI system that evaluates employees’ performance based on their use of company AI tools, and adjusts bonuses accordingly. Which governance principles matter?
    Robustness/reliability: measurements must be correct and stable, because jobs and pay are affected.
    Privacy: what data is collected, from which devices, and whether personal activity is being monitored.
    The scenario is used to show how “principles” become concrete system requirements.
    例子是：公司要上线内部 AI 系统，根据员工使用公司 AI 工具的情况来评估表现，并据此调整奖金。哪些治理原则会冒出来？
    稳健性/可靠性：统计要准、系统要稳，因为影响工资奖金和评价。
    隐私：采集哪些数据、从哪些设备采集，会不会监控到个人活动。
    这个例子是为了说明：治理“原则”不是空话，最后要变成具体的系统要求与上线规则。
  * [14:25–14:29] **Soft law vs hard law (and why soft law still matters)**
    The lecture distinguishes soft law and hard law in AI governance. Soft law refers to non-binding frameworks or guidelines (e.g., Singapore’s Model AI Governance Framework, including versions for generative AI and general AI). It is easier to implement and revise quickly. Hard law refers to binding legal rules with penalties (e.g., the EU AI Act; China is also developing more binding rules in some areas). A key trend is that soft-law principles can be incorporated into hard-law rules over time. So organizations have a strategic reason to follow soft-law guidance early, because it may become legally relevant later.
    AI 治理里常讲“软法 vs 硬法”。软法是非强制的框架/指南，比如新加坡的 Model AI Governance Framework（既有生成式 AI 版本，也有通用 AI 版本）。软法好处是推得快、改得也快。硬法是有法律约束力的规则，违反会有处罚，比如欧盟的 EU AI Act；中国也在一些方向上走向更强约束的规则。一个重要趋势是：软法原则可能逐步被写进硬法里。所以企业即使现在不“被罚”，也有理由早点按软法做，因为未来它可能会变成硬性的合规要求。
  * [14:29] **Referenced course readings (from Seminar 1 slides)**
    Placani (2024); Richards (2023); Bonde (2013); Levine (2022); OECD (2025); ASEAN (2024); Pasqua (2025).

    Seminar 1 的主要阅读包括：Placani (2024)、Richards (2023)、Bonde (2013)、Levine (2022)、OECD (2025)、ASEAN (2024)、Pasqua (2025)。
## Week2
  - [12:02 - 12:03] **Course Administration & Reading List Updates**

    To begin, I have decided to adjust the angle of the course slightly. I realized that simply assigning the readings might be overwhelming given their length and complexity. Therefore, I have updated your reading list to include "guiding questions." These questions are designed to focus your study efforts on the most critical arguments rather than getting lost in the density of the texts. This should make the material more manageable and help you extract the key takeaways more effectively.

    首先，我决定稍微调整一下课程的切入角度。我意识到仅仅布置阅读材料可能会因为篇幅和难度让大家感到不知所措。因此，我在阅读清单中增加了“引导性问题”。设计这些问题的目的是为了让大家的学习重点集中在最关键的论点上，避免迷失在晦涩的文本中。这应该能让学习材料更易于掌握，帮助大家更有效地提取核心观点。

  - [12:03 - 12:07] **Popular Conceptions vs. Reality of Autonomous Weapons**

    Today we are covering two main topics: the ethics of autonomous weapons and autonomous vehicles. Let's start with autonomous weapons. When we use this term, the popular conception usually stems from science fiction—what we might call "killer robots." You might think of the Battle Droids from *Star Wars*, the T-800 from *Terminator*, or the ED-209 from *RoboCop*. These are the images that typically come to mind, but none of these actually exist yet; they remain in the realm of fiction.

    However, we do have existing weapons with varying degrees of autonomy. A classic, albeit crude, example is the **anti-personnel landmine**, which is unfortunately still in use today. It functions autonomously by triggering upon contact. Moving to more modern examples, consider the **AIM-9X Sidewinder** missile—think of the movie *Top Gun*. Once a pilot fires this missile, it uses infrared technology to track the heat signature of an enemy jet autonomously; the pilot effectively "fires and forgets." Another example is the **Phalanx CIWS (Close-In Weapon System)**. This is a ship-mounted automated gun used by the US Navy. It detects incoming projectiles or missiles and automatically fires a "wall of bullets" to intercept them. In this system, there is no human physically aiming the gun; the software detects the threat and engages it automatically to protect the ship.

    今天我们要讨论两个主题：自主武器的伦理问题和自动驾驶汽车。我们先从自主武器讲起。当我们提到这个词时，大众的普遍印象通常来自科幻小说，也就是所谓的“杀手机器人”。大家可能会想到《星球大战》里的战斗机器人、《终结者》里的T-800，或者是《机械战警》里的ED-209。这些是人们脑海中浮现的形象，但它们目前并不存在，仍属于科幻范畴。

    然而，我们确实拥有具备不同程度自主能力的现役武器。一个经典虽然简陋的例子是**反步兵地雷**，不幸的是它至今仍被广泛使用。它通过接触触发，在某种意义上是自主运作的。至于更现代的例子，可以看看**AIM-9X 响尾蛇导弹**——想想电影《壮志凌云》。一旦飞行员发射了这种导弹，它就会利用红外技术自动追踪敌机的热源；飞行员实际上是“发射后不管”的。另一个例子是**密集阵近程防御系统（Phalanx CIWS）**。这是美国海军舰艇上装备的一种自动火炮系统。它能探测来袭的炮弹或导弹，并自动发射“弹幕”进行拦截。在这个系统中，并没有人类在操作瞄准，软件会自动检测威胁并进行攻击以保护舰船。

  - [12:07 - 12:12] **Defining "Autonomous Weapons": The Automation of Cognition**

    How do we strictly define an "autonomous weapon"? Recalling our first lesson, we defined Artificial Intelligence (AI) as the automation of cognitive tasks. Following that logic, an autonomous weapon is a weapon system that **automates the cognitive tasks relating to its use**. Using a weapon involves both physical tasks (like loading or pulling a trigger) and cognitive tasks. The defining feature here is the automation of the "thinking" part.

    So, what are these cognitive tasks?

    1. **Context Understanding:** As a student mentioned, one task is understanding the context—specifically, is this person a threat?
    2. **Target Discrimination:** A soldier must distinguish between valid targets (enemy combatants) and protected persons (civilians, wounded soldiers). This is the act of targeting: aiming the weapon at the right object.
    3. **Engagement Decision:** The final cognitive step is the decision to use force—the actual command to "fire."

    In a traditional setting, a human makes the call to shoot. In an autonomous system, algorithms process sensor data to detect a target, identify it as an enemy, and decide to engage, potentially without human intervention. This shift from human judgment to algorithmic processing is the core ethical issue we are examining.

    我们该如何严格定义“自主武器”？回顾第一节课，我们将人工智能（AI）定义为认知任务的自动化。顺着这个逻辑，自主武器就是**自动化了与其使用相关的认知任务**的武器系统。使用武器既涉及物理任务（如装填或扣动扳机），也涉及认知任务。这里的定义特征在于“思考”部分的自动化。

    那么，这些认知任务具体指什么？

    1. **理解语境：** 正如一位同学所提到的，任务之一是理解语境——具体来说，这个人是威胁吗？
    2. **目标识别：** 士兵必须区分合法目标（敌方战斗人员）和受保护人员（平民、伤员）。这就是瞄准行为：将武器对准正确的目标。
    3. **交战决策：** 最后的认知步骤是使用武力的决定——即下达“开火”的指令。

    在传统场景中，由人类决定是否射击。而在自主系统中，算法处理传感器数据来探测目标，将其识别为敌人，并决定进行攻击，这一过程可能完全无需人类干预。这种从人类判断到算法处理的转变正是我们审视的核心伦理问题。

  - [12:13 - 12:16] **Incentives for Development: Arms Races and Force Protection**

    Why are nations incentivized to develop these weapons? There are several driving forces:

    - **Game Theory & The Arms Race:** As discussed, if your adversary develops autonomous weapons, you become vulnerable. To avoid a strategic disadvantage, you are compelled to develop them as well. This creates a classic "security dilemma" or arms race dynamic where everyone builds them simply because everyone else might.
    - **Force Protection:** Autonomous weapons reduce the risk to your own personnel. As a student noted, wars are politically unpopular when you send your own citizens to die. Robots allow a nation to project power without the emotional and political cost of human casualties on their side.
    - **Efficiency and Speed:** Machines can process information and react much faster than humans. In modern warfare, speed is critical. A system that can identify and engage a target in milliseconds offers a massive tactical advantage over a human-operated system.

    Despite the ethical concerns, these incentives create a powerful push for development. We must recognize that regardless of the moral arguments, the geopolitical reality is that major powers are actively pursuing these technologies.

    各国为何有动力去研发这些武器？这背后有几个驱动力：

    - **博弈论与军备竞赛：** 正如我们在讨论中提到的，如果你的对手研发了自主武器，你就会变得脆弱。为了避免处于战略劣势，你也不得不研发。这就造成了典型的“安全困境”或军备竞赛，大家都在造，仅仅因为别人可能也在造。
    - **兵力保护：** 自主武器降低了己方人员的风险。正如一位同学指出的，当你把自己的公民送上战场牺牲时，战争在政治上是不受欢迎的。机器人允许国家投射力量，而无需承担己方人员伤亡带来的情感和政治代价。
    - **效率与速度：** 机器处理信息和反应的速度远超人类。在现代战争中，速度至关重要。一个能在毫秒内识别并攻击目标的系统，相比人工操作系统具有巨大的战术优势。

    尽管存在伦理担忧，这些诱因构成了强大的研发推动力。我们必须认识到，无论道德论证如何，地缘政治的现实是主要大国都在积极追求这些技术。

  - [12:16 - 12:17] **Distinction: Automatic vs. Autonomous Weapons**

    It is crucial to distinguish between "autonomous" and "automatic" weapons, as people often conflate them.

    - **Automatic Weapons** automate **physical tasks**. For example, an automatic rifle uses the energy of a fired cartridge to physically extract the spent shell and chamber a new round. This is a mechanical automation of a physical process.
    - **Autonomous Weapons** automate **cognitive tasks**. This concerns the decision-making process: *Who* do I shoot? *When* do I shoot?

    In this course, we are not concerned with mechanical automation (like the rifle's loading mechanism). We are focusing entirely on the automation of the cognitive aspects—the "brain" of the weapon—and the ethical problems that arise when we delegate lethal decisions to software.

    区分“自主”武器和“自动”武器至关重要，因为人们经常混淆这两个概念。

    - **自动武器（Automatic Weapons）** 自动化的是**物理任务**。例如，自动步枪利用发射子弹的能量来机械地抛出弹壳并装填新弹。这是物理过程的机械自动化。
    - **自主武器（Autonomous Weapons）** 自动化的是**认知任务**。这涉及决策过程：我要射击*谁*？*何时*射击？

    在本课程中，我们不关注机械自动化（比如步枪的装填机制）。我们完全聚焦于认知层面的自动化——即武器的“大脑”——以及当我们把致死决策权交给软件时所引发的伦理问题。

  - [12:17 - 12:20] **Legal Landscape: The Lack of International Consensus**

    Currently, there is no definitive international consensus on the legality of autonomous weapons. While the United Nations has held discussions (specifically under the Convention on Certain Conventional Weapons), no binding ban or specific regulation currently exists for Lethal Autonomous Weapons Systems (LAWS).

    - **The Exception (Landmines):** There is consensus regarding anti-personnel landmines. The **Ottawa Treaty** prohibits their use, stockpiling, and production, and most countries have signed it. This is arguably a ban on a primitive form of autonomous weapon.
    - **Proliferation:** Despite this, development is accelerating. Major powers like China and the US are investing heavily, but it's not just them. Smaller states and even non-state actors are gaining access to these technologies, as seen in recent conflicts where drones act with increasing levels of autonomy. This proliferation suggests that without regulation, these weapons will become a common feature of future warfare.

    目前，关于自主武器的合法性，国际上尚未达成明确共识。虽然联合国已经举行了相关讨论（特别是在《特定常规武器公约》框架下），但目前针对致命自主武器系统（LAWS）尚无具有约束力的禁令或具体法规。

    - **例外（地雷）：** 关于反步兵地雷是存在共识的。**《渥太华条约》**禁止使用、储存和生产地雷，大多数国家都签署了该条约。这可以被视为对一种原始形式的自主武器的禁令。
    - **扩散：** 尽管如此，研发进程正在加速。中国和美国等大国投入巨大，但不仅限于它们。较小的国家甚至非国家行为体也开始获得这些技术，正如我们在近期冲突中看到的，无人机正表现出越来越高的自主性。这种扩散表明，如果缺乏监管，这些武器将成为未来战争的常见特征。

  - [12:20 - 12:27] **Ethical Arguments: The Responsibility Gap and Unpredictability**

    Let's examine the ethical arguments, focusing on the reading by Peter Asaro (2020), who argues that autonomous weapons are morally wrong.

    **1. Unpredictability:** A major ethical concern is the inherent unpredictability of AI. Unlike a rifle where the bullet goes where you aim it, an autonomous system given a general command (e.g., "clear this sector") makes its own decisions on how to execute that task.

    - **Interaction Effects:** As a student pointed out, we face a "black box" problem. We don't fully know how the neural networks will behave in novel situations. Furthermore, when algorithms from opposing sides interact on the battlefield, the outcome is mathematically impossible to predict perfectly.
    - **Differentiation Failure:** There is a high risk that the system will fail to distinguish between a soldier and a civilian, or between a surrender signal and a threat. Who is responsible when this differentiation goes wrong and a war crime is committed?

    **2. The Responsibility Gap:** This leads to Asaro's core argument. If an autonomous weapon commits a war crime (e.g., bombing a hospital), who is morally and legally responsible?

    - **The Commander?** They didn't pull the trigger or specifically order that specific attack; they just deployed the system.
    - **The Programmer?** They wrote the code years ago and couldn't predict this specific battlefield scenario.
    - **The Machine?** You cannot put a robot in jail or hold it morally culpable.

    This creates a "Responsibility Gap"—a situation where a terrible crime occurs, but no human can be justly held accountable. Asaro argues this gap makes the use of such weapons immoral because it erodes the legal and moral framework of warfare.

    我们来审视一下伦理论点，重点关注彼得·阿萨罗（Peter Asaro, 2020）的文章，他认为自主武器在道德上是错误的。

    **1. 不可预测性：** 一个主要的伦理担忧是人工智能固有的不可预测性。步枪的子弹会飞向你瞄准的地方，但自主系统在接收到一个概括性指令（例如“清理该区域”）后，会自行决定如何执行任务。

    - **交互效应：** 正如一位同学指出的，我们要面对“黑箱”问题。我们并不完全清楚神经网络在新奇情境下会如何表现。此外，当敌对双方的算法在战场上互动时，其结果在数学上是不可能完美预测的。
    - **区分失败：** 系统很有可能无法区分士兵和平民，或者无法区分投降信号和威胁。当这种区分出错导致战争罪行时，谁该负责？

    **2. 责任鸿沟（Responsibility Gap）：** 这引出了阿萨罗的核心论点。如果自主武器犯下战争罪行（例如轰炸医院），谁在道德和法律上负责？

    - **指挥官？** 他们没有扣动扳机，也没有具体下令进行那次特定的攻击；他们只是部署了系统。
    - **程序员？** 他们是几年前写的代码，无法预测这个特定的战场场景。
    - **机器？** 你不能把机器人关进监狱，也不能让它承担道德罪责。

    这就造成了“责任鸿沟”——即发生可怕罪行却没有任何人能被公正地追责。阿萨罗认为，这种鸿沟使得使用此类武器在道德上是错误的，因为它侵蚀了战争的法律和道德框架。

  - [12:27 - 12:35] **Debating Responsibility: Risk vs. Control**

    How do we resolve this responsibility gap? We discussed two main approaches in class:

    - **The Risk Argument (Strict Liability):** One view is that if you deploy a dangerous system, you accept the risk. Therefore, the commander is responsible for *any* damage the machine causes, regardless of whether it was a "glitch." If you release a tiger into a village, you are responsible for who it eats, even if you can't control the tiger. This approach effectively imposes strict liability on the user.
    - **The Control Argument (Meaningful Human Control):** The counter-argument is that moral responsibility requires control. If the weapon is truly autonomous—meaning the human had no input on the specific targeting decision—can we fairly blame the human? This is why many ethicists and the UN advocate for **"Meaningful Human Control" (MHC)**. The argument is that for a weapon to be ethical, a human must retain a significant level of agency in the loop to validate targets and intervention. Without this control, responsibility cannot land anywhere, which brings us back to the immorality of the weapon.

    我们该如何解决这个责任鸿沟？我们在课堂上讨论了两种主要方法：

    - **风险论点（严格责任）：** 一种观点是，如果你部署了一个危险系统，你就承担了风险。因此，指挥官要对机器造成的*任何*损害负责，不管那是不是“故障”。就像你把老虎放进村庄，你就得对它吃人负责，即使你控制不了老虎。这种方法实际上是对使用者施加了严格责任。
    - **控制论点（有意义的人类控制）：** 反方论点是，道德责任需要控制权。如果武器是真正自主的——意味着人类对具体的瞄准决策没有投入——我们能公平地指责人类吗？这就是为什么许多伦理学家和联合国倡导**“有意义的人类控制”（Meaningful Human Control, MHC）**。其论点是，为了使武器符合伦理，人类必须在回路中保留相当程度的代理权，以验证目标和进行干预。如果没有这种控制，责任就无处着落，这又把我们带回了武器本身的不道德性。

  - [12:35 - 12:50] **Human Dignity and the Threshold of War**

    Beyond responsibility, there is a deontological argument regarding **Human Dignity**. Asaro argues that it is fundamentally dehumanizing to be killed by a machine. Killing in war is only "legal" because it is a decision made by a moral agent acting under necessity. An algorithm has no concept of the value of human life; it treats a human target merely as a data point to be processed. Therefore, delegating the decision to kill to a machine violates the dignity of the victim.

    Finally, we touched on the broader societal implications:

    - **Lowering the Threshold of War:** If using autonomous weapons carries no risk of casualties for the attacker (0% casualty rate for your own side), leaders might be more tempted to solve political disputes with force rather than diplomacy. This effectively "gamifies" war.
    - **The "Skynet" Risk:** While some might call it science fiction, there is a non-zero probability of escalation where AI systems interact in ways that spiral out of control—potentially overriding human objectives entirely. While I personally think the "Terminator/Skynet" scenario is low probability, the risk of rapid, unintended escalation (flash wars) is real.

    We will pause here and resume with the counter-arguments after the break.

    除了责任问题，还有一个关于**人类尊严**的义务论论点。阿萨罗认为，被机器杀死在根本上是去人性化的。战争中的杀戮之所以“合法”，是因为这是道德主体在必要性下做出的决定。算法对人类生命的价值没有任何概念；它仅仅将人类目标视为待处理的数据点。因此，将杀戮决定权交给机器侵犯了受害者的尊严。

    最后，我们触及了更广泛的社会影响：

    - **降低战争门槛：** 如果使用自主武器对攻击者来说没有伤亡风险（己方零伤亡），领导人可能会更倾向于用武力而不是外交手段来解决政治争端。这实际上将战争“游戏化”了。
    - **“天网”风险：** 虽然有人称之为科幻小说，但确实存在非零的升级概率，即AI系统的互动方式失控——可能完全覆盖人类的目标。虽然我个人认为“终结者/天网”情景的概率很低，但快速、意外的冲突升级（闪电战）的风险是真实的。

    我们先讲到这里，休息之后再继续讨论反方观点。

  - [13:01 - 13:04] **Counter-Arguments: The Ethics of Non-Lethal Engagement**

    Let's consider the counter-arguments regarding the morality of autonomous weapons. Earlier, we argued that machines lack empathy, which is a negative trait. However, one could argue that this lack of emotion is actually a benefit. Human soldiers are affected by stress, fear, and anger, which can lead to rash decisions or war crimes. An autonomous system is immune to these psychological pressures. It will not act out of revenge or panic.

    Furthermore, there is an interesting argument from your readings (referencing Haiden) about the potential for non-lethal engagement. Consider a human soldier: if they are threatened, they are likely to shoot to kill because they fear for their own life—they want to go home safely. They cannot take the risk of using non-lethal force (like rubber bullets) if the enemy has a real gun. However, a robot is "disposable." It has no self-preservation instinct. Therefore, we can imagine a future where autonomous units are programmed to take risks that humans cannot, such as getting close enough to disarm a combatant or using non-lethal weapons, potentially resulting in fewer overall deaths in conflict.

    让我们来思考关于自主武器道德性的反方论点。早些时候，我们认为机器缺乏同理心，这是一个负面特征。然而，有人可能会争辩说，缺乏情感实际上是一种优势。人类士兵会受到压力、恐惧和愤怒的影响，这可能导致鲁莽的决定或战争罪行。而自主系统对这些心理压力免疫，它不会出于复仇或恐慌而采取行动。

    此外，你们的阅读材料中（参考Haiden）提出了一个关于非致命交战潜力的有趣论点。试想一名人类士兵：如果受到威胁，他们很可能会为了自保而开枪射杀——因为他们想活着回家。如果敌人持有真枪，他们无法冒险使用非致命武力（如橡胶子弹）。然而，机器人是“可消耗的”。它没有自我保护的本能。因此，我们可以想象在未来，自主作战单位被编程去承担人类无法承担的风险，例如靠近解除战斗人员的武装或使用非致命武器，这最终可能会减少冲突中的总死亡人数。

  - [13:04 - 13:07] **Algorithmic Bias and the Dignity Argument**

    A student raised a crucial point about the developers of these weapons. Since algorithms are written by humans, they inevitably inherit the biases and discrimination of their creators. We see this moral issue clearly in other AI domains, such as predictive policing or medical AI, where racial or ethnic bias is a documented problem. While we haven't yet seen definitive examples of "unfair targeting" in autonomous weapons specifically, it is a mathematically probable risk that these systems could disproportionately target specific demographics based on biased training data.

    However, returning to the core ethical debate, the strongest argument against these weapons often comes back to **Human Dignity**. As cited in the article by Asaro, the argument is that being killed by a machine denies you your fundamental human status. It is a "common sense" moral intuition: even in the brutality of war, there is a belief that the decision to end a life must be made by a moral agent who understands the gravity of that act. A machine treats you as an object to be processed, not a human to be engaged, and for many ethicists, that violation of dignity is sufficient grounds to ban them.

    一位同学提出了关于这些武器开发者的关键一点。由于算法是由人类编写的，它们不可避免地会继承创造者的偏见和歧视。我们在其他人工智能领域（如预测性警务或医疗人工智能）中清楚地看到了这一道德问题，种族或民族偏见在这些领域已是有据可查的问题。虽然我们尚未在自主武器中看到“不公平目标锁定”的确切案例，但在数学上存在极大的风险，即这些系统可能基于有偏差的训练数据，不成比例地针对特定人群。

    然而，回到核心的伦理辩论，反对这些武器的最强有力论据往往回归到**人类尊严**。正如阿萨罗（Asaro）的文章所引用的，该论点认为被机器杀死剥夺了你作为人的基本地位。这是一种“常识性”的道德直觉：即使在残酷的战争中，人们也认为结束生命的决定必须由一个理解该行为严重性的道德主体做出。机器把你当作一个待处理的物体，而不是一个需要去交战的人类，对于许多伦理学家来说，这种对尊严的侵犯足以成为禁止它们的理由。

  - [13:07 - 13:13] **Introduction to Autonomous Vehicles and SAE Levels**

    Let's move on to a technology you are more likely to encounter in daily life: **Autonomous Vehicles (AVs)**. Broadly speaking, AVs are not new; autopilots for planes and ships have existed for decades. However, our focus here is on self-driving cars.

    To define what counts as an AV, we use the industry standard **SAE J3016**, which defines 6 levels of driving automation (Level 0 to Level 5):

    - **Levels 0-2 (Driver Support):** The human is still driving. Level 0 involves simple warnings or momentary assistance (like emergency braking). Level 1 is simple assistance like lane centering *or* cruise control. Level 2 (like current Tesla Autopilot) handles steering and acceleration, but the driver must keep their eyes on the road and be ready to take over instantly.
    - **Levels 3-5 (Automated Driving):** The system drives. Level 3 is a "traffic jam chauffeur"—you can take your eyes off the road, but must intervene if the car requests it. Level 4 is "High Automation," like a driverless taxi in a geofenced area (no steering wheel needed). Level 5 is "Full Automation," where the car can drive anywhere a human can, under any conditions.

    我们接着讨论一项大家在日常生活中更可能接触到的技术：**自动驾驶汽车（AVs）**。广义上讲，自动驾驶并不新鲜；飞机和船舶的自动驾驶仪已经存在几十年了。然而，我们这里的重点是自动驾驶汽车。

    为了定义什么是自动驾驶汽车，我们使用行业标准 **SAE J3016**，它将驾驶自动化定义为6个等级（L0到L5）：

    - **L0-L2（驾驶员辅助）：** 人类仍在驾驶。L0涉及简单的警告或瞬间辅助（如紧急制动）。L1是简单的辅助，如车道居中*或*巡航控制。L2（像目前的特斯拉Autopilot）控制转向和加速，但驾驶员必须注视道路并准备随时接管。
    - **L3-L5（自动驾驶）：** 系统负责驾驶。L3是“交通拥堵代驾”——你可以把目光移开道路，但如果汽车发出请求，必须进行干预。L4是“高度自动化”，比如地理围栏区域内的无人驾驶出租车（不需要方向盘）。L5是“完全自动化”，汽车可以在任何人类能驾驶的地方、任何条件下行驶。

  - [13:13 - 13:16] **Regulation and Liability in Singapore and Beyond**

    In terms of regulation, Singapore currently operates under the **Road Traffic (Autonomous Motor Vehicles) Rules 2017**. The key takeaway from Section 4 is that you cannot undertake a trial or use an autonomous vehicle on a public road without specific authorization. While you can own a Tesla, using its "Full Self-Driving" (beta) features hands-free is generally restricted. We are currently in a phase of small-scale trials, such as autonomous shuttles in Punggol.

    The pressing ethical and legal issue right now is **liability**. Who pays when an AV crashes?

    - **Case 1:** A Florida jury recently ordered Tesla to pay **$313 million** regarding a fatal Autopilot crash.

    - **Case 2:** A Waymo vehicle in Denver parked itself in a bike lane. The company argued it wasn't the software's fault but rather a human driver who had previously positioned it.

      This highlights the complexity: responsibility is often shifted between the manufacturer, the software, and the human operator.

    在监管方面，新加坡目前执行的是**2017年《道路交通（自动驾驶机动车）规则》**。第4节的关键点是，未经特别授权，不得在公共道路上进行试验或使用自动驾驶汽车。虽然你可以拥有一辆特斯拉，但通常限制使用其“完全自动驾驶”（测试版）的脱手功能。我们目前正处于小规模试验阶段，例如榜鹅（Punggol）的自动穿梭巴士。

    目前紧迫的伦理和法律问题是**责任**。当自动驾驶汽车发生事故时，谁来赔偿？

    - **案例1：** 佛罗里达州陪审团最近因一起致命的自动驾驶事故，判决特斯拉赔偿**3.13亿美元**。

    - **案例2：** 丹佛的一辆Waymo汽车停在了自行车道上。该公司辩称这不是软件的错，而是之前人类驾驶员停放位置的问题。

      这凸显了复杂性：责任往往在制造商、软件和人类操作员之间推来推去。

  - [13:16 - 13:30] **The Moral Responsibility Framework**

    So, who *should* be morally responsible? Let's break down the potential parties:

    1. **The Manufacturer/Developer:** They built the algorithm.
    2. **The Operator:** For example, a taxi company deploying a fleet.
    3. **The Regulator:** The government that certified the car as safe.
    4. **The Passenger/Driver:** The person inside.

    As a student noted, if you are asleep in a Level 4/5 car, you have **zero control**, so morally, you shouldn't be liable. However, liability is a spectrum.

    - If it's a Level 2 car, the driver is responsible for supervision.
    - If the driver failed to maintain the car (e.g., muddy sensors), they might be liable.
    - If the government failed to maintain road infrastructure (e.g., broken traffic lights causing the AI to fail), the state might be liable.
    - There is also the **"Strict Liability"** argument raised by a student (and discussed in the Hevelke reading): The person who puts the car on the road creates a risk for their own benefit/convenience, and therefore should bear the cost of any accidents, regardless of fault.

    那么，谁*应该*承担道德责任？让我们分析一下潜在的当事方：

    1. **制造商/开发者：** 他们构建了算法。
    2. **运营商：** 例如部署车队的出租车公司。
    3. **监管者：** 认证汽车安全的政府。
    4. **乘客/驾驶员：** 车里的人。

    正如一位同学指出的，如果你在L4/L5级汽车里睡觉，你拥有**零控制权**，所以在道德上你不应该负责。然而，责任是一个通过谱系。

    - 如果是L2级汽车，驾驶员负责监督。
    - 如果驾驶员未能维护汽车（例如传感器被泥土覆盖），他们可能要负责。
    - 如果政府未能维护道路基础设施（例如交通灯故障导致AI失效），国家可能要负责。
    - 还有一位同学提出的（在Hevelke的阅读材料中讨论过的）**“严格责任”**论点：将汽车开上路的人为了自己的利益/便利创造了风险，因此无论是否有过错，都应承担任何事故的成本。

  - [13:30 - 13:39] **Collective Responsibility and the Legal-Moral Divergence**

    Following the reading by **Hevelke (2015)**, there is a proposal for **Collective Responsibility**. Since autonomous cars generally reduce accidents but introduce new types of risks, perhaps all users should contribute to a mandatory insurance pool. When a crash happens, the compensation comes from this pool rather than blaming an individual.

    This leads to a critical question: *Should legal responsibility match moral responsibility?*

    Morally, we might say the manufacturer is responsible for a coding error. However, from a **policy perspective**, if we make manufacturers 100% legally liable for every accident, they will go bankrupt or refuse to release the technology. Since AVs could save thousands of lives overall, we might design laws that limit manufacturer liability to encourage innovation. This is a clear example where legal expediency might diverge from strict moral culpability.

    根据**Hevelke (2015)\**的阅读材料，有一个关于\**集体责任**的提议。由于自动驾驶汽车总体上减少了事故，但也引入了新型风险，也许所有用户都应该向一个强制保险池供款。当事故发生时，赔偿金来自这个资金池，而不是指责个人。

    这引出了一个关键问题：*法律责任是否应该与道德责任相匹配？*

    在道德上，我们可能会说制造商应对代码错误负责。然而，从**政策角度**来看，如果我们让制造商对每一起事故承担100%的法律责任，他们将会破产或拒绝发布该技术。由于自动驾驶汽车总体上可以挽救成千上万的生命，我们可能会制定限制制造商责任的法律以鼓励创新。这是一个法律权宜之计可能与严格的道德罪责相背离的明显例子。

  - [13:39 - 13:51] **Crash Algorithms: The Trolley Problem**

    Now we confront the "Crash Algorithms" (referencing **Ungern-Sternberg**). If a crash is unavoidable, what should the car prioritize?

    1. **Minimize Harm:** Ideally, the car should choose to hit an inanimate object (like a wall) rather than a person, or choose a path that causes injury rather than death.
    2. **The Trolley Problem:** What if the choice is between hitting a wall (killing the 5 passengers) or swerving to hit a pedestrian (killing 1 person)?
       - **Utilitarianism:** Save the 5. It acts for the "greater good."
       - **Deontology:** Swerving is an intentional act of killing an innocent bystander, which is murder. You cannot trade one life for another.

    This is an unsolved philosophical problem. In fact, in jurisdictions like Germany, it is arguably **illegal** to program a car to sacrifice one life to save others, as human lives are considered incommensurable (cannot be measured against each other).

    现在我们面临“碰撞算法”的问题（参考**Ungern-Sternberg**）。如果碰撞不可避免，汽车应该优先考虑什么？

    1. **最小化伤害：** 理想情况下，汽车应该选择撞击无生命的物体（如墙壁）而不是人，或者选择造成受伤而不是死亡的路径。
    2. **电车难题：** 如果选择是在撞墙（导致5名乘客死亡）和转向撞击行人（导致1人死亡）之间呢？
       - **功利主义：** 救那5个人。这是为了“更伟大的利益”。
       - **义务论：** 转向是故意杀害无辜旁观者的行为，这就是谋杀。你不能用一条生命去换取另一条生命。

    这是一个未解的哲学问题。事实上，在像德国这样的司法管辖区，编写程序让汽车牺牲一条生命来拯救其他人可能在**法律上是禁止的**，因为人类生命被认为是不可通约的（不能相互衡量）。

  - [13:52 - 13:59] **Selection of Victims: The "Clean Hands" Argument**

    The final and most difficult scenario: If the car must kill either Person A or Person B (e.g., both are pedestrians), how does it choose?

    - **Equality Argument:** All human lives are equal. The car should not choose based on age, profession, or social value. Random choice might be the only ethical option.
    - **"Clean Hands" Argument:** A student suggested that if Person A is walking on the sidewalk and Person B is jaywalking (breaking the rules), the car should save Person A. This aligns with the intuition that we should protect those who are following the law.

    While mathematically we might want the car to calculate "path of least physical damage," embedding moral judgments about *who* is worth saving is incredibly dangerous and controversial.

    最后也是最困难的场景：如果汽车必须在杀死A和B之间做出选择（例如两人都是行人），它该如何选择？

    - **平等论点：** 所有人的生命都是平等的。汽车不应根据年龄、职业或社会价值进行选择。随机选择可能是唯一符合伦理的选项。
    - **“清白之手”（Clean Hands）论点：** 一位同学建议，如果A走在人行道上，而B在乱穿马路（违反规则），汽车应该救A。这符合我们应该保护守法者的直觉。

    虽然在数学上我们可能希望汽车计算“物理伤害最小的路径”，但植入关于*谁*值得拯救的道德判断是极其危险和充满争议的。

  - [14:07 - 14:13] **Ethics of Crash Algorithms: The Problem of Free Choice**

    We now face the question raised by Von Ungern-Sternberg: Should a driver or manufacturer be free to decide the "crash algorithm" of their vehicle? Imagine a scenario where a manufacturer offers you a range of ethical settings—a "Self-Preservation Mode" (protects the driver at all costs) versus an "Altruistic Mode" (minimizes total harm, even if it sacrifices the driver).



    There are several strong arguments against allowing this free choice:

    - **The Market Failure Argument:** If left unregulated, rational self-interest dictates that almost everyone would choose "Self-Preservation Mode." Manufacturers that fail to offer a "Protect Driver" option would simply be wiped out of the market because no one wants to buy a car that might decide to kill them. This leads to a "race to the bottom" in ethical standards.
    - **The Public Trust Argument:** If pedestrians know that every autonomous vehicle on the road is programmed to prioritize the driver's life over theirs, public trust in the technology would collapse. You would be terrified to walk on the sidewalk knowing that a car would swerve into you to save its owner from a minor injury.
    - **The Responsibility Argument:** Conversely, if we *do* allow choice, it changes the liability equation. If a driver explicitly selects "Self-Preservation Mode," they are making a premeditated decision to prioritize their life over others. In that case, they—not the manufacturer—should perhaps be held fully morally and legally accountable for the resulting deaths.

    However, there is a counter-argument: Today, human drivers *do* have this choice, albeit implicitly. In a split-second emergency, a human driver reacts instinctively, often to save themselves. If we currently allow humans this autonomy, why is it unethical to allow them to pre-program the same instinct into their car?

    我们现在面临冯·翁恩-斯特恩伯格（Von Ungern-Sternberg） 提出的问题：驾驶员或制造商是否有权决定车辆的“碰撞算法”？想象一下，如果制造商为你提供一系列伦理设置选项——“自我保护模式”（不惜一切代价保护驾驶员）与“利他模式”（最小化总伤害，即使牺牲驾驶员）。



    反对允许这种自由选择有几个强有力的论据：

    - **市场失灵论点：** 如果缺乏监管，理性的利己主义决定了几乎每个人都会选择“自我保护模式”。那些不提供“保护驾驶员”选项的制造商将直接被市场淘汰，因为没人愿意买一辆可能会决定杀死自己的车 。这将导致伦理标准的“逐底竞争”。
    - **公众信任论点：** 如果行人知道路上的每一辆自动驾驶汽车都被编程为优先考虑驾驶员的生命而不是他们的生命，公众对该技术的信任将会崩塌。当你走在人行道上，知道一辆车为了让车主免受轻伤可能会转向撞向你，你会感到恐惧。
    - **责任论点：** 反过来说，如果我们*确实*允许选择，这会改变责任的平衡。如果驾驶员明确选择了“自我保护模式”，他们就是在做一个预谋的决定，即把自己的生命置于他人之上。在这种情况下，导致死亡的完全道德和法律责任或许应由他们——而非制造商——来承担。

    然而，也存在反方论点：今天，人类驾驶员*确实*拥有这种选择权，尽管是隐性的。在瞬间的紧急情况下，人类驾驶员会本能地做出反应，往往是为了自保。如果我们目前允许人类拥有这种自主权，为什么允许他们将同样的本能预编程到汽车中就是不道德的呢？

  - [14:14 - 14:17] **Societal Implications: External Control**

    Moving beyond crash ethics, we must consider the broader societal implications of autonomous vehicles (AVs), as discussed in the reading by Hansson. The first major implication is **External Control**. Once cars are networked and automated, it opens the door for third parties to control where your vehicle goes.

    - **Positive Use Case:** A traffic regulator could centrally reroute all autonomous cars to instantly clear a path for an ambulance or fire engine. This increases overall safety.
    - **Ethical Concern:** This removes your individual freedom. You are no longer the captain of your ship; you are a packet in a data stream.
    - **Corruption Risk:** Hansson raises a fascinating concern about commercial influence. What if a manufacturer makes a deal with a fast-food chain? Your car might subtly alter its route to drive you past a specific McDonald's or Starbucks to increase the likelihood of you stopping. This introduces commercial bias into physical navigation.

    除了碰撞伦理，我们必须考虑自动驾驶汽车（AVs）更广泛的社会影响，正如汉森（Hansson）的阅读材料中所讨论的那样 。第一个主要影响是**外部控制（External Control）** 。一旦汽车实现联网和自动化，就为第三方控制你的车辆去向打开了大门。

    - **正面用例：** 交通监管部门可以集中重新规划所有自动驾驶汽车的路线，瞬间为救护车或消防车腾出通道。这提高了整体安全性。
    - **伦理担忧：** 这剥夺了你的个人自由。你不再是你这艘“船”的船长；你只是数据流中的一个数据包。
    - **腐败风险：** 汉森提出了一个关于商业影响的有趣担忧。如果制造商与快餐连锁店达成协议会怎样？你的汽车可能会巧妙地改变路线，带你经过特定的麦当劳或星巴克，以增加你停车消费的可能性。这将商业偏见引入了物理导航中。

  - [14:17 - 14:19] **Societal Implications: Privacy and Surveillance**

    The second implication is **Privacy**. While GPS tracking exists today on our phones, widespread AV adoption takes this to a new level. If every vehicle is constantly broadcasting its location, speed, and destination to a central server to coordinate traffic, we create a pervasive surveillance network.



    Companies could use this data not just to train their driving models, but to profile users. They would know exactly when you leave home, where you work, where you shop, and who you visit. If this data is leaked or sold, it constitutes a massive violation of user privacy. We must ask: Is the convenience of self-driving worth the loss of anonymity in our physical movements?

    第二个影响是**隐私（Privacy）** 。虽然如今我们的手机上已存在GPS定位，但自动驾驶汽车的广泛采用将这一问题提升到了新的高度。如果每辆车都在不断向中央服务器广播其位置、速度和目的地以协调交通，我们就建立了一个无孔不入的监控网络。



    公司不仅可以利用这些数据训练驾驶模型，还可以对用户进行画像。他们会确切知道你何时离家、在哪里工作、在哪里购物以及拜访谁。如果这些数据被泄露或出售，将构成对用户隐私的巨大侵犯。我们必须追问：自动驾驶的便利性是否值得以丧失我们在物理移动中的匿名性为代价？

  - [14:19 - 14:24] **Societal Implications: Employment, Inequality, and Crime**

    Third, we must look at the socio-economic impacts:

    - **Job Displacement:** The widespread adoption of AVs threatens the livelihoods of millions of professional drivers—taxi drivers, truck drivers, and bus drivers. We face a future of structural unemployment for this demographic.
    - **Inequality and the "Gig Economy":** Conversely, capital owners stand to gain. If I own a Tesla, I can send it out as a "robotaxi" to earn money while I sleep. This exacerbates inequality: those with capital (the car owners) get richer, while those who rely on labor (the taxi drivers) lose their income.
    - **Facilitation of Crime:** While AVs prevent drunk driving, they might facilitate new types of crime. Criminals could use empty AVs to transport illicit goods (drugs, weapons) without risking capture themselves.
    - **Loss of Human Agency:** Finally, for some, driving is a passion. A ban on human driving to achieve perfect safety would rob hobbyists of their pleasure. From a utilitarian standpoint, this loss of joy is a negative consequence that must be weighed against safety gains.

    第三，我们必须审视社会经济影响 ：

    - **就业置换：** 自动驾驶汽车的广泛采用威胁着数百万职业司机的生计——出租车司机、卡车司机和公交司机。这一人群将面临结构性失业的未来。
    - **不平等与“零工经济”：** 相反，资本所有者将获益。如果我拥有一辆特斯拉，我可以把它作为“机器人出租车”派出去，在我睡觉时赚钱。这加剧了不平等：拥有资本的人（车主）变得更富，而依赖劳动的人（出租车司机）失去收入。
    - **助长犯罪：** 虽然自动驾驶汽车杜绝了酒驾，但它们可能助长新型犯罪。罪犯可以利用空载的自动驾驶汽车运输非法货物（毒品、武器），而无需自己承担被捕的风险。
    - **人类能动性的丧失：** 最后，对一些人来说，驾驶是一种激情。为了实现完美安全而禁止人类驾驶，将剥夺爱好者的乐趣。从功利主义角度来看，这种乐趣的丧失是一种必须与安全收益相权衡的负面后果。

  - [14:24 - 14:28] **Societal Implications: Environmental Impact & Conclusion**

    Fourth, consider the **Environmental Impact**.

    - **The Optimistic View:** AVs drive more efficiently than humans (smoother acceleration, less braking), which reduces fuel consumption and emissions.
    - **The "Rebound Effect" (Jevons Paradox):** However, if AVs make traveling cheap, easy, and convenient, people might abandon public transport (buses, trains) in favor of private autonomous pods. This could lead to *more* cars on the road, increased congestion, and higher overall energy consumption.

    **Summary & Closing:**

    To recap, today we covered:

    1. **Autonomous Weapons:** The debate on morality, the responsibility gap, and human dignity.
    2. **Autonomous Vehicles:** The levels of automation (SAE), the question of moral vs. legal liability in crashes, the ethics of crash algorithms (Trolley Problem), and the broader social implications (Privacy, Jobs, Environment).

    For your next steps, please refer to the **guiding questions** I have uploaded to the reading list. The readings themselves (especially the legal ones) can be dense, so use the questions to focus your study on the key moral arguments rather than getting bogged down in technical details.

    第四，考虑**环境影响**。

    - **乐观观点：** 自动驾驶汽车比人类驾驶更高效（加速更平稳，制动更少），这能降低燃料消耗和排放。
    - **“回弹效应”（杰文斯悖论）：** 然而，如果自动驾驶汽车让出行变得廉价、轻松且便捷，人们可能会放弃公共交通（公交、火车），转而选择私人自动驾驶舱。这可能导致路上的汽车*更多*，加剧拥堵，并提高整体能源消耗。

    **总结与结束语：**

    回顾一下，今天我们涵盖了：

    1. **自主武器：** 关于道德性、责任鸿沟和人类尊严的辩论。
    2. **自动驾驶汽车：** 自动化分级（SAE）、碰撞中的道德与法律责任问题、碰撞算法的伦理（电车难题），以及更广泛的社会影响（隐私、就业、环境）。

    关于接下来的安排，请参考我上传到阅读清单中的**引导性问题**。阅读材料本身（尤其是法律类的）可能很晦涩，所以请利用这些问题将学习重点集中在关键的道德论证上，而不要深陷于技术细节中。
***

# Reading
## Week2
  Corporate Foresight Benchmarking Report by Rene Rohrbeck
  1. What is the purpose of the benchmarking report?
  2. List down the challenges outlined by the authors in establishing the impact of
  corporate foresight?
  3. How did the authors address each one of those challenges?
  4. How many items were there in measuring Corporate Foresight Need and Corporate
  Foresight Maturity? What are they (no need to list each one of those, just list the
  dimensions)?
  5. How did they measure performance?
  6. Why should you give importance to aspects in Questions 2-5?
  7. On the basis of corporate foresight need and maturity, how did they categorize the
  firms based on their corporate foresighting profile?
  8. What are the three core skills of vigilant firms? What are they? Can you highlight
  examples of activities related to the three core skills?
  9. Why firms find it difficult to engage in corporate foresighting activities?
  10. How to address the challenges that prevent firms from engaging in foresighting
  activities?
***
