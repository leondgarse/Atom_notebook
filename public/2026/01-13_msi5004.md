# ___MSI5004 AI Governance and Ethics___
***

## INFO
  - **Course Description** The development and deployment of artificial intelligence (“AI”) systems can have ethical implications. Developers and deployers of AI systems are therefore expected to exercise good governance over their AI systems, to ensure that ethical norms are not violated. This course will cover basic concepts of AI governance, and address key ethical issues in relation to AI.

  - **Assessment components**
    | CA Component                                 | % Weightage | Remarks                                               |
    | -------------------------------------------- | ----------- | ----------------------------------------------------- |
    | Class Participation                          | 25%         |                                                       |
    | Essays                                       | 0%          |                                                       |
    | Project/Group Project                        | 0%          | E.g. Individual - 20% (Week 4); Group - 30% (Week 11) |
    | Quizzes/Tests                                | 0%          | E,g, Test 1 - 10% (Week 3); Test 2 - 20% (Week 10)    |
    | Laboratory Tests                             | 0%          |                                                       |
    | Mid-term Test                                | 0%          |                                                       |
    | Others 1 (if applicable & describe in notes) | 0%          |                                                       |
    | Others 2 (if applicable & describe in notes) | 0%          |                                                       |
    | Others 3 (if applicable & describe in notes) | 0%          |                                                       |
    | Final Exam                                   | 75%         | Open-book in-person exam                              |

  ```
  Dear students,

  I would like to be a bit clearer about (1) how to learn AI ethics in this course and (2) how your learning relates to the exam.

  Going forward, I will include guiding questions in the reading lists. Please use these questions to guide your study of the topic. Your primary goal should be to use these questions to acquire a good personal understanding of the topic.

  Your personal understanding, based on these guiding questions, will be required to answer the exam essay question. Here's an example of a (simplified) essay question: "The use of AI often leads to unfair outcomes. Discuss." (Obviously, the actual exam question will be a bit more sophisticated than this, but you get the idea).

  The readings should not be treated as textbook, but as useful resources to help you to answer the guiding questions. You should not feel the need to digest the readings wholesale – many of them are written by philosophers and lawyers for other philosophers and lawyers to read, and so are often not very accessible to the layperson. Nevertheless, I recommend them because they are the highest quality material that I can find. Just extract what you can from them, learn from our discussions in class, and also feel free to look outside the readings to deepen your own understanding.

  I hope that this makes things more straightforward for you, and assuages any concerns about the exam.

  Regards,

  Benjamin
  ```
***

# Summary and Tips
## Week 1 Summary Introduction to AI Ethics & Governance 人工智能伦理与治理导论
  - **Defining AI as Cognitive Automation**

    The course defines Artificial Intelligence not as a mystical "intelligence" but as the **automation of cognitive tasks** (e.g., classification, recognition, prediction, decision-making). The professor emphasized that "AI" is often a marketing term used to hype up software. A key distinction was drawn between **physical automation** (like steam engines or factory arms) and **cognitive automation** (like text generators or diagnostic tools), though they can overlap in robotics (e.g., autonomous vehicles).

    **将人工智能定义为认知自动化**

    本课程将人工智能定义为**认知任务的自动化**（如分类、识别、预测、决策），而非某种神秘的“智能”。教授强调，“AI”常被用作一种营销术语来炒作软件。课程区分了**物理自动化**（如蒸汽机或工厂机械臂）和**认知自动化**（如文本生成器或诊断工具），尽管它们在机器人领域（如自动驾驶汽车）可能存在重叠。

  - **The Trap of Anthropomorphism (Placani, 2024)**

    A major portion of the seminar focused on the fallacy of **anthropomorphism**—attributing human qualities (intentions, feelings, consciousness) to non-human entities. This is dangerous because it leads to **hype** and distorts moral judgment in four ways:

    1. **Moral Character:** Assuming AI has virtues (e.g., "friendly," "honest").
    2. **Moral Status:** Wrongly believing AI deserves rights or moral consideration.
    3. **Responsibility:** Blaming the AI for errors instead of the developers or users.
    4. **Trust:** Trusting AI based on non-existent emotional bonds rather than reliability.

    **拟人化的陷阱 (Placani, 2024)**

    研讨会的一大部分重点讨论了**拟人化**的谬误——将人类特质（意图、情感、意识）归因于非人类实体。这是危险的，因为它会导致**炒作**并以四种方式扭曲道德判断：

    1. **道德品质**：假设 AI 具有美德（如“友好的”、“诚实的”）。
    2. **道德地位**：错误地认为 AI 应享有权利 or 道德考量。
    3. **责任**：将错误归咎于 AI，而不是开发者或用户。
    4. **信任**：基于不存在的情感纽带而非可靠性来信任 AI。

  - **Existential Risk vs. Immediate Harms (Richards, 2023)**

    The class discussed the "illusion" of existential risk. While sci-fi scenarios (e.g., *The Terminator*) suggest AI might wipe out humanity, the lecture argued this is statistically unlikely based on historical biological extinctions. Focusing on these long-term, hypothetical "Superintelligence" risks is a form of **Pascal's Wager** that distracts us from addressing immediate, real-world harms like algorithmic bias, military misuse, and disinformation.

    **存在性风险与直接危害 (Richards, 2023)**

    课堂讨论了存在性风险的“幻觉”。虽然科幻场景（如《终结者》）暗示 AI 可能会毁灭人类，但讲座指出，根据历史上的生物灭绝情况，这在统计上是不太可能的。关注这些长期的、假设性的“超级智能”风险是一种**帕斯卡赌注**，它分散了我们要解决算法偏见、军事滥用和虚假信息等现实世界直接危害的注意力。

  - **Three Ethical Frameworks**

    To analyze ethical dilemmas (like the *Crisis Text Line* case), the course introduced three primary frameworks:

    1. **Consequentialism (Utilitarianism):** Focuses on the *outcome* (maximizing overall happiness/good).
    2. **Deontology (Duty-based):** Focuses on following moral *rules* and obligations (e.g., "never steal," "duty of care"), regardless of the outcome.
    3. **Virtue Ethics:** Focuses on the *character* of the agent (e.g., "what would a trustworthy person do?").

    **三种伦理框架**

    为了分析伦理困境（如 *Crisis Text Line* 案例），课程介绍了三种主要框架：

    1. **后果主义（功利主义）**：关注*结果*（最大化整体幸福/善）。
    2. **义务论（基于责任）**：关注遵循道德*规则*和义务（例如，“从不偷窃”，“注意义务”），而不管结果如何。
    3. **美德伦理学**：关注代理人的*品质*（例如，“一个值得信赖的人会怎么做？”）。

  - **AI Governance: Principles & Soft vs. Hard Law**

  Governance relies on common principles found in guidelines like OECD and ASEAN (e.g., **Transparency, Robustness, Accountability, Privacy**). Currently, most AI governance is **Soft Law** (non-binding guidelines), which is flexible and easy to implement. However, there is a global trend moving toward **Hard Law** (enforceable legal rules with penalties), exemplified by the **EU AI Act**.

  **AI 治理：原则与软法 vs. 硬法**

  治理依赖于 OECD 和 ASEAN 等准则中的共同原则（如**透明度、稳健性、问责制、隐私**）。目前，大多数 AI 治理属于**软法**（非约束性准则），灵活且易于实施。然而，全球趋势正朝着**硬法**（具有惩罚措施的可执行法律规则）发展，**欧盟《人工智能法案》**就是一个典型的例子。
## Week 1 Learning Tips 学习建议
  - **Watch Your Language (Avoid Anthropomorphism)**

    - **Tip:** When writing essays or speaking in class, actively avoid using human pronouns ("he/she") for AI. Use "it" or "the model." Instead of saying "The AI *thinks*," say "The AI *processes*," "The AI *calculates*," or "The AI *outputs*."
    - **Why:** This demonstrates that you understand the Placani reading and prevents you from falling into the "responsibility trap" (blaming the tool instead of the user).

    **注意你的语言（避免拟人化）**

    - **建议**：在写论文或课堂发言时，积极避免对 AI 使用人类代词（“他/她”）。使用“它”或“模型”。不要说“AI *思考*”，而要说“AI *处理*”、“AI *计算*”或“AI *输出*”。
    - **原因**：这表明你理解了 Placani 的文章，并防止你陷入“责任陷阱”（责怪工具而不是用户）。

  - **Practice "Switching Hats" with Ethical Frameworks**

    - **Tip:** When you see an AI news headline (e.g., "Company X uses AI to monitor employees"), force yourself to argue for it using one framework and against it using another.
      - *Utilitarian:* "It increases efficiency (Good)."
      - *Deontologist:* "It violates the duty of privacy (Bad)."
    - **Why:** The final exam will likely require you to analyze a hypothetical scenario using these specific lenses.

    **练习用伦理框架“换位思考”**

    - **建议**：当你看到 AI 新闻标题时（例如，“X 公司使用 AI 监控员工”），强迫自己用一种框架支持它，再用另一种框架反对它。
      - *功利主义*：“它提高了效率（好）。”
      - *义务论*：“它违反了隐私义务（坏）。”
    - **原因**：期末考试很可能要求你使用这些特定的视角来分析假设场景。

  - **Distinguish Between "Safety" and "Ethics"**

  - **Tip:** Remember the distinction made in class. If a reading talks about "extinction," "superintelligence," or "alignment," it is likely **AI Safety** (long-term, hypothetical). If it talks about "bias," "privacy," "copyright," or "labor displacement," it is **AI Ethics** (immediate, practical).
  - **Why:** This course focuses on the latter. Don't waste time debating sci-fi scenarios unless explicitly asked; focus on the "mundane" but real harms.

  **区分“安全”与“伦理”**

  - **建议**：记住课堂上所做的区分。如果阅读材料谈论“灭绝”、“超级智能”或“对齐”，这通常是 **AI 安全**（长期、假设性的）。如果它谈论“偏见”、“隐私”、“版权”或“劳动力替代”，这是 **AI 伦理**（直接、实际的）。
  - **原因**：本课程侧重于后者。除非明确要求，否则不要浪费时间辩论科幻场景；关注那些“平凡”但真实的危害。
## Week 3 Summary
  - **Ethics of Algorithmic Wage Suppression**

    The lecture examined the "RealWorld" case study, where a "WageFinder" algorithm recommended the lowest possible salary for job postings by aggregating data from current employment contracts. This scenario represents a **Hub-and-Spoke** anti-competitive model, where the algorithm (hub) coordinates the behavior of individual employers (spokes). Ethically, this creates extreme information asymmetry—employers gain a holistic view of market wages while job seekers remain uninformed. This practice can lead to "wage-fixing," artificially suppressing income levels even when labor demand is high.

    本节通过 “RealWorld” 招聘平台和 “WageFinder” 算法的案例，探讨了算法压低工资的伦理问题。该场景代表了**轴辐式 (Hub-and-Spoke)** 反竞争模型，其中算法作为中心轴，协调各雇主（辐条）的行为。从伦理上看，这制造了严重的信息不对称——雇主掌握了市场薪资的全局视图，而求职者则处于劣势。这种做法可能导致 “工资操纵”，在劳动力需求旺盛的情况下人为压低收入水平。

  - **The Three Degrees of Price Discrimination**

    Personalized pricing is the digital pursuit of **First-degree (Perfect) price discrimination**, where each consumer is charged their exact "reservation price" (the maximum they are willing to pay). For price discrimination to be viable, a seller needs market power, the ability to prevent arbitrage (reselling), and the data to segment the market. AI significantly enhances this capability; for example, an investigation into **Instacart** found grocery prices varying by up to 23% between customers, while **Delta Air Lines** has explored AI to predict the maximum price a traveler will accept.

    个性化定价是数字时代对**一级（完全）价格歧视**的追求，即向每位消费者收取其准确的 “保留价格”（愿意支付的最高金额）。价格歧视要实现，卖方需要具备市场力量、防止套利（转售）的能力以及细分市场的数据。AI 显著增强了这种能力；例如，对 **Instacart** 的调查发现，不同客户间的杂货价差最高可达 23%，而**达美航空**也探索过利用 AI 预测旅客能接受的最高票价。

  - **Transparency, Fairness, and the "Desperation" Factor**

    A key ethical distinction was made between **dynamic pricing** (transparent, supply-demand driven fluctuations like Grab’s surge pricing) and **personalized pricing** (hidden, data-driven fluctuations). The latter is often viewed as unfair or deceptive because it targets individual vulnerabilities. The most severe ethical concerns arise in "emergency" services, such as dental or medical care, where an algorithm might detect a buyer's desperation to charge a premium. Unlike "altruistic" discrimination (e.g., student discounts), personalized pricing is often perceived as an aggressive "squeeze" on consumer surplus.

    课程区分了**动态定价**（基于供需的透明波动，如 Grab 加价）与**个性化定价**（基于数据的隐蔽波动）之间的伦理差异。后者通常被视为不公平或具有欺骗性，因为它针对的是个人的特定弱点。最严重的伦理问题出现在 “紧急” 服务（如牙科或医疗）中，算法可能会利用买家的急迫心理收取溢价。与 “利他性” 歧视（如学生折扣）不同，个性化定价通常被视为对消费者剩余的激进 “榨取”。

  - **Economic Growth vs. Surveillance and Inefficiency**

    The discussion concluded with the tension between market efficiency and privacy. Proponents argue that personalized pricing contributes to economic growth by enabling sales to individuals who wouldn't buy at a fixed high price. However, critics argue it incentivizes a "surveillance state" where firms waste resources on tracking purchase history instead of improving product quality. The **Zomana** hypothetical illustrated that transparency about the *existence* of an algorithm does not resolve the ethical dilemma if the *logic* of the "black box" remains hidden.

    讨论以市场效率与隐私之间的张力告终。支持者认为，个性化定价通过向原本买不起的人提供低价来促进经济增长。然而，批评者认为这激励了 “监控状态”，企业将资源浪费在跟踪购买记录上，而不是提高产品质量。**Zomana** 的假设案例说明，即使告知用户算法的存在，如果 “黑箱” 的逻辑依然隐蔽，伦理困境仍未解决。
## Week 3 Tips
  1. **Master the Competition Act 2004 (掌握 2004 年竞争法):** Be ready to apply **Section 34** (Anti-competitive agreements), **Section 47** (Abuse of dominance), and **Section 54** (Anti-competitive mergers) to tech scenarios like the Grab-Uber merger.

     准备好将第 34 条（反竞争协议）、第 47 条（滥用主导地位）和第 54 条（反竞争合并）应用到 Grab-Uber 合并等科技场景中。

  2. **Differentiate Pricing Models (区分定价模型):** In exam questions, clearly distinguish between **Dynamic Pricing** (market-based) and **Personalized Pricing** (individual-based). One is generally accepted, while the other is ethically contentious.

     在考试中，要清晰区分**动态定价**（基于市场）和**个性化定价**（基于个人）。前者通常被接受，而后者在伦理上具有争议。

  3. **The "Hub-and-Spoke" Concept (理解 “轴辐” 模型):** This is a critical concept for AI ethics. Understand how a single software provider can facilitate collusion among competitors without them ever meeting.

     这是 AI 伦理中的核心概念。理解单一软件供应商如何在竞争对手互不谋面的情况下促进其串通。

  4. **Practice Balanced Argumentation (练习平衡论证):** When discussing ethics, always present both sides: the **Economic Utility** (efficiency, growth, redistribution) versus the **Social/Moral Harm** (unfairness, deception, privacy loss, exploitation of desperation).

     讨论伦理时，务必呈现双面观点：**经济效用**（效率、增长、再分配）与**社会/道德伤害**（不公、欺骗、隐私丧失、利用绝望心理）。

  -
## Week 4 Summary Emerging Technologies & Ethics
  - **Deepfakes: The Ethics of Synthetic Reality**

    We established that deepfakes differ from traditional editing (like Photoshop) because they use AI to generate completely synthetic media, often without the creator needing significant skill. The ethical discussion focused on the "Pervert's Dilemma": Is it unethical to create non-consensual deepfake pornography if it is never shared? While a strict Consequentialist view might struggle to find direct harm, we argued through **Virtue Ethics** (it degrades the creator's character) and **Deontology** (it violates the subject's human dignity/categorical imperative). regarding distribution, we identified three conditions for permissibility: **Consent**, **Declaration** (labeling), and **Non-malicious intent**. A critical concept introduced was the **"Liar's Dividend"**: the societal harm where the mere existence of deepfakes allows bad actors to dismiss real, incriminating evidence as fake, thereby corrupting the information ecosystem.

    我们确定了深度伪造与传统编辑（如 Photoshop）不同，因为它们使用 AI 生成完全合成的媒体，且创作者往往不需要太多技能。伦理讨论的焦点是“变态者的困境”：如果从未分享，制作非自愿的深度伪造色情内容是否不道德？虽然严格的结果论观点可能难以找到直接伤害，但我们通过**美德伦理学**（它降低了创作者的品格）和**义务论**（它侵犯了主体的人类尊严/绝对命令）进行了论证。关于传播，我们确定了允许的三个条件：**同意**、**声明**（标注）和**无恶意意图**。引入的一个关键概念是**“说谎者的红利”**：即深度伪造的存在本身允许不良行为者将真实的罪证驳斥为伪造品，从而导致社会危害并腐蚀信息生态系统。

  - **Facial Recognition: Surveillance vs. Privacy**

    The lecture distinguished between **Verification** (1-to-1 matching, e.g., unlocking a phone) and **Identification** (1-to-many scanning, e.g., police surveillance). The primary ethical concern is the erosion of the **"Right to Obscurity"**—the idea that we should be able to move anonymously in public spaces. Unlike passwords, biometric data is immutable; you cannot change your face if the data is compromised. We also discussed the **"Slippery Slope"** and **"Function Creep"**: surveillance infrastructure is often justified for extreme cases (like counter-terrorism) but is inevitably repurposed for minor offenses (like jaywalking) or commercial tracking, making it impossible for citizens to genuinely consent to being tracked.

    讲座区分了**验证**（1对1匹配，例如解锁手机）和**识别**（1对多扫描，例如警察监控）。主要的伦理担忧是**“隐姓埋名权利”**的侵蚀——即我们应该能够在公共空间匿名行动。与密码不同，生物识别数据是不可更改的；如果数据泄露，你无法改变你的脸。我们还讨论了**“滑坡效应”**和**“功能泛化”**：监控基础设施通常以极端情况（如反恐）为由是正当的，但不可避免地会被重新用于轻微犯罪（如乱穿马路）或商业追踪，使公民无法真正同意被追踪。

  - **Recommender Systems: Autonomy & Polarization**

    These systems automate content curation to manage information overload, but they introduce significant ethical risks regarding **Autonomy** and **Social Cohesion**. We explored how algorithms optimized for engagement can lead to addiction (exemplified by the EU's action against TikTok) and political polarization by creating **"Filter Bubbles"** or **"Echo Chambers"** where users only see confirming viewpoints. There is also an issue of fairness and market manipulation, where algorithms may favor "pay-for-play" content or established "superstars," creating a **"Long Tail"** problem where new or smaller creators are structurally invisible regardless of the quality of their work.

    这些系统自动策划内容以管理信息过载，但它们在**自主权**和**社会凝聚力**方面引入了重大的伦理风险。我们探讨了针对参与度优化的算法如何导致成瘾（以欧盟针对 TikTok 的行动为例）和政治极化，通过创造**“过滤气泡”**或**“回声室”**使用户只能看到确认性的观点。还存在公平性和市场操纵的问题，算法可能偏向“付费播放”内容或已成名的“超级巨星”，造成**“长尾”**问题，即无论作品质量如何，新的或较小的创作者在结构上都是不可见的。
## Week 4 Learning Tips
  - **Apply the Frameworks to Specific Scenarios**

    Don't just memorize the definitions of the technologies. The key to doing well in this module is to apply the three ethical frameworks (Consequentialism, Deontology, Virtue Ethics) to specific case studies.

    - *Example:* For private deepfake creation, a Consequentialist might say "no harm, no foul," but a Virtue Ethicist would argue "it cultivates a vicious character." Being able to switch between these perspectives is crucial for your assignments.

    **将框架应用于特定场景**

    不要只死记硬背技术的定义。学好本模块的关键是将三种伦理框架（结果论、义务论、美德伦理学）应用于具体的案例研究。

    - *例子：* 对于私人制作深度伪造，结果论者可能会说“没有伤害就没有犯规”，但美德伦理学家会争辩说“它助长了恶毒的品格”。能够在这些视角之间切换对你的作业至关重要。

  - **Master the Key Vocabulary**

    This week introduced several high-value academic terms that you should use in your writing. Terms like **"Function Creep"** (for surveillance), **"Liar's Dividend"** (for deepfakes), **"Right to Obscurity"** (for privacy), and **"Filter Bubble"** (for recommenders) carry a lot of theoretical weight. Using them correctly shows you understand the deeper mechanisms of these technologies, not just the surface-level news.

    **掌握关键通过词汇**

    本周介绍了几个高价值的学术术语，你应该在写作中使用它们。像**“功能泛化”**（用于监控）、**“说谎者的红利”**（用于深度伪造）、**“隐姓埋名的权利”**（用于隐私）和**“过滤气泡”**（用于推荐系统）这样的术语承载着很重的理论分量。正确使用它们表明你理解这些技术的深层机制，而不仅仅是表面的新闻。

  - **Distinguish Between Legal and Ethical**

    A recurring theme this week was that what is *legal* (e.g., privately making a deepfake of a deceased person, or consent terms buried in a T&C agreement) is not always *ethical*. When analyzing a problem, explicitly separate the legal analysis (e.g., POHA, GDPR) from the moral analysis. The professor specifically challenged the class to look beyond "consent forms" to see if genuine autonomy is respected.

    **区分法律与伦理**

    本周反复出现的一个主题是，*合法的*（例如，私下制作死者的深度伪造，或埋藏在条款与条件协议中的同意条款）并不总是*符合伦理的*。在分析问题时，要明确将法律分析（如 POHA, GDPR）与道德分析分开。教授特别要求班级超越“同意书”，去审视真正的自主权是否得到了尊重。
***

# Lectures
## Week 1
  - [12:07 - 12:10] **Course Overview: Learning Objectives and Scope**

    Okay, as part of this learning objective, we have two subtopics. The first one is to learn about the main ethical issues arising from the use of AI. For this part of the course, we will explore the different contexts in which AI is currently being used. We will look at the existing literature regarding things like autonomous vehicles, autonomous weapons, algorithmic pricing, and so on. We’ll be exploring all the ethical issues that arise in those situations so that in the future, if you are involved in such usages of AI or even a novel usage, you can sort of take those lessons and apply them to what you are going to do next time.

    The second subtopic is about the core principles of AI governance. If you have looked at some of the international guidelines when it comes to AI governance, you will no doubt have noticed that they have a certain set of common principles, like transparency, privacy, and so on. So, part of this course will also be exploring how these principles work in the context of AI governance.

    I want to clarify that this course, although it is taught in the Law School, is not primarily intended to be a legal course. The name of the course is "AI Governance and Ethics." I am not really trying to train you to be lawyers who have to deal with legal issues in AI. This is more of a non-legal sort of course. That being said, I may explore regulation in a little bit of detail. I haven't quite decided how much I'm going to include yet, but if I do include it, I won't go into a lot of legalistic detail. One of the things I'm thinking of doing is introducing you to the EU AI Act. I may include it, but I need to see whether it fits the schedule.

    好的，作为本课程教学目标的一部分，我们有两个子主题。第一个主题是了解因使用人工智能而产生的主要伦理问题。在这部分课程中，我们将探讨目前正在使用人工智能的不同情境。我们将研究已经发表的文献，通过自动驾驶汽车、自主武器、算法定价等案例，深入探讨这些情境中出现的所有伦理问题。这样，如果在未来你们参与到此类人工智能的应用，甚至是全新的应用中时，你们可以汲取这些经验教训，并将其应用到你们接下来的工作中。

    第二个子主题是关于人工智能治理的核心原则。如果你看过一些关于人工智能治理的国际准则，你无疑会注意到它们都有一套特定的共同原则，比如透明度、隐私等。因此，本课程的一部分内容也将探讨这些原则如何在人工智能治理的背景下运作。

    我想澄清一点，这门课虽然是在法学院开设的，但并不是一门主要的法律课程。课程名称是“人工智能治理与伦理”。我并不是要将你们培养成处理人工智能法律问题的律师。这更多是一门非法律类的课程。话虽如此，我可能会稍微详细地探讨一下监管问题。我还没完全决定要包含多少内容，但如果我真的包含它，我也不会深入讨论过多的法律细节。我正在考虑做的一件事是向大家介绍《欧盟人工智能法案》（EU AI Act）。我可能会把它加进来，但这得看课程安排是否合适。

  - [12:10 - 12:13] **Course Logistics: Workload and Assessment**

    Okay, so those are the learning objectives. In terms of the course workload, this is the standard workload for a four-credit course, which is three hours of seminar—which we are doing now—and approximately nine and a half hours of preparatory work. In terms of the prep work, I will usually assign you a reading list for each seminar. That reading list will typically contain what I expect to take about nine and a half hours of reading. Typically, I think it should be about five papers, maybe six. Some of the readings I include are marked "optional," and those optional readings are kind of extra, so you can read them or not read them. Note that for some of the readings, I will include pinpoint citations, meaning I will say "read this at pages 11 to 50." If I include that pinpoint citation, then you can just read those specific pages or specific paragraphs.

    Then, in terms of the assessment for this course, it is quite straightforward. 25% of the score will come from what we call class participation. This means your attendance of each lesson and your contributions in each lesson. I want to assure you that while contributions and interactions in class are encouraged, you don't have to force yourself if you feel uncomfortable. Attendance will form a part of this. However, if you want more than just a passing grade for class participation, then try to contribute a little bit. In a big class like this, I also don't expect everyone to be very talkative because it wouldn't work logically. So, one or two contributions are sufficient.

    The second part of the course assessment will be the final exam. The final exam will be 75% of your score. It will comprise one question with two parts. Part A will be an essay question on AI ethics, and Part B will be a hypothetical question on AI governance. Okay, so that's all I have for the course introduction. Does anyone have any questions?

    (Student asks: So the exam would be open book or closed book? I should have included that...)

    So, the exam will be open book. Any other questions?

    好了，以上就是教学目标。关于课程工作量，这是四学分课程的标准工作量，即3小时的研讨会——也就是我们现在正在进行的——以及大约9.5小时的准备工作。在准备工作方面，我通常会为每次研讨会分配一份阅读清单。这份阅读清单通常包含我预计需要大约9.5小时阅读的内容。通常情况下，我认为应该是5篇论文，也许是6篇。我包含的一些阅读材料会被标记为“可选”，这些可选阅读材料属于补充性质，你可以读也可以不读。请注意，对于某些阅读材料，我会包含具体的引用页码（pinpoint citations），也就是说我会注明“阅读第11到50页”。如果我包含了具体的页码引用，那么你只需要阅读那些特定的页面或段落。

    然后，关于本课程的考核，非常直观。25%的分数将来自课堂参与。这意味着你每节课的出勤率以及你在课堂上的贡献。我想向你们保证，虽然我们鼓励课堂上的贡献和互动，但如果你感到不舒服，你不必强迫自己。出勤率将构成这部分分数的一部分。但是，如果你想要在课堂参与方面获得不仅仅是及格的分数，那就试着做一点贡献。在这样一个大班级里，我也并不期望每个人都非常健谈，因为从逻辑上讲这也是行不通的。所以，一两次发言就足够了。

    课程考核的第二部分将是期末考试。期末考试将占你总分的75%。它将包含一道大题，分为两个部分。A部分将是关于人工智能伦理的论文题，B部分将是关于人工智能治理的假设性问题。好了，这就是我对课程介绍的所有内容。大家有什么问题吗？

    （学生提问：考试是开卷还是闭卷？我应该把这点写进去的...）

    考试将是开卷的。还有其他问题吗？

  - [12:13 - 12:20] **Defining AI: Marketing Terms and Cognitive Automation**

    Let's get started on the actual lesson. I'm sure you've heard a lot about AI right now. In the courses that you took in the previous semester, you will have heard quite a lot about what AI is and what types of AI there are. I think you guys are more qualified than I am to talk about the technical details. So for today, I'm just going to be quite brief about what we are going to focus on when we talk about AI.

    In terms of how we define AI, this is necessary because we need to have a common understanding of what we are talking about in this course. I suppose most people will begin with the Oxford English Dictionary, which gives us a fairly good starting point. If you look at the definition, it says AI is essentially the exhibition or simulation of intelligent behavior, or something that simulates human intelligence. But for this course, I want to focus on defining it as the "automation of cognitive tasks." When we talk about Artificial Intelligence, what we're really referring to are attempts to automate tasks that are done with the human mind.

    Let me break this down a little bit. I use the word "marketing term" because I think AI is a way of dressing up something. For example, when someone uses the word "vintage," that is a marketing term for "old." When someone uses the term "cozy" in the context of an apartment, it's kind of a marketing term for "it's a small apartment." In the same way, "Artificial Intelligence" has been used as a kind of marketing term that makes this automation of cognitive tasks sound exciting, sound more interesting. And of course, there are good reasons why people use marketing terms—they want to attract investments, they may want to pump up their products.

    As for "automation," what we mean by automation is simply to do something without human intervention. The reason why AI tends to be problematic from a technical perspective is precisely because we are trying to do without humans; we are trying to do something in an automatic fashion. So, for example, autonomous weapons are problematic precisely because you are basically allowing a weapon to function without a human holding it and pulling the trigger.

    The last aspect of this definition is a "cognitive task." We're focusing here not on all types of automation, but specifically on the automation of cognitive tasks. You can see on the slide a list of them. For example, Classification: we use our minds to look at something and say, "Is that a doll? Is that a cat?" The second cognitive task is Recognition: using your mind and eyes to see someone's face and recognize them. Prediction: trying to figure out what might happen in the future, like using AI for stock market prediction. Decision-making: we see this in the context of banks where they use algorithms to try and automatically figure out whether we should give a loan to a particular person. And finally, and most recently, we have seen Generative AI, which has allowed us to automate cognitive tasks like writing, drawing, animating, and so on.

    让我们开始正式上课。我相信大家现在已经听过很多关于人工智能的讨论了。在你们上学期修的课程中，你们应该已经听过不少关于什么是AI以及AI有哪些类型的介绍。我想在技术细节方面，你们比我更有资格谈论。所以今天，我只是简要谈谈我们在讨论人工智能时将关注的重点。

    关于我们如何定义人工智能，这是必要的，因为我们需要对本课程所讨论的内容有一个共同的理解。我想大多数人会从《牛津英语词典》开始，这给了我们一个相当不错的起点。如果你看那个定义，它说人工智能本质上是智能行为的展示或模拟，或者是模拟人类智能的东西。但对于本课程，我想把重点放在将其定义为“认知任务的自动化”。当我们谈论人工智能时，我们要指的其实是试图自动化那些由人类大脑完成的任务。

    让我稍微分解一下。我使用“营销术语”这个词，是因为我认为AI是一种包装事物的方式。例如，当有人使用“复古”（vintage）这个词时，这是“旧”（old）的营销术语。当有人在描述公寓时使用“温馨”（cozy）这个词时，这其实是“小公寓”的营销术语。同样地，“人工智能”也被用作一种营销术语，让这种认知任务的自动化听起来令人兴奋，听起来更有趣。当然，人们使用营销术语是有充分理由的——他们想要吸引投资，他们可能想要推销他们的产品。

    至于“自动化”，我们要指的是在没有人类干预的情况下做某事。从技术角度来看，人工智能之所以容易出现问题，正是因为我们试图在没有人类的情况下做事；我们试图以自动化的方式做事。例如，自主武器之所以有问题，正是因为你基本上是在允许一种武器在没有人类握持和扣动扳机的情况下发挥作用。

    这个定义的最后一个方面是“认知任务”。我们在这里关注的不是所有类型的自动化，而是专门针对认知任务的自动化。你们可以在幻灯片上看到这些任务的列表。例如，分类（Classification）：我们用大脑看着某样东西说，“那是个洋娃娃吗？那是只猫吗？”第二个认知任务是识别（Recognition）：用你的大脑和眼睛看某人的脸并认出他们。预测（Prediction）：试图弄清楚未来可能会发生什么，比如利用人工智能进行股市预测。决策（Decision-making）：我们在银行的情境中看到了这一点，他们使用算法来试图自动弄清楚我们是否应该向特定的人发放贷款。最后，也是最近，我们看到了生成式人工智能（Generative AI），它使我们能够自动化诸如写作、绘画、动画制作等认知任务。

  - [12:20 - 12:24] **Distinguishing Physical vs. Cognitive Automation**

    I want to frame this in the broader context of automation. I created this rough diagram where the red circle represents physical automation and the blue circle represents cognitive automation. I think before AI was a thing, when we thought of automation, we were primarily thinking of physical automation—things like the steam engine which automates movement, a conveyor belt, factory arms, and so on. These are all physical forms of automation where the machine automates physical tasks. AI, in contrast, focuses on cognitive tasks. So we see tools like automatic text generators, Google search engine, diagnostic tools used by doctors, and so on.

    But of course, there is not a clear division between the mind and the body, and likewise, there is not a clear division between physical automation and cognitive automation. There are some things which combine the two. As a first question, can anyone think of examples that fall into this intersection?

    (Student suggests: Robots?)

    Yeah, so robots would be your classic example of a combination of physical and cognitive automation. To the extent that you have an autonomous android which can make decisions and move by itself and decide how to fold your clothes, you have this form of physical plus cognitive automation. Autonomous vehicles and autonomous weapons would also be examples of situations where you combine physical and cognitive automation. Any other possible examples you can think of?

    (Student suggests: Face recognition, maybe?)

    I would say that face recognition mostly belongs in this part of cognitive automation. The recognition of who you are is mostly a cognitive function. But I suppose if you combine that with some sort of physical function—for example, if after your face is scanned, the machine does something like open a door or provide you with a drink—then this would be an example of the combination of different types of automation. This is a pretty simple exercise to demonstrate that while we are looking primarily at cognitive automation, sometimes AI blends into physical forms of automation.

    我想把这个问题放在更广泛的自动化背景下。我制作了这个粗略的图表，红色圆圈代表物理自动化，蓝色圆圈代表认知自动化。我认为在人工智能出现之前，当我们想到自动化时，我们要想的是物理自动化——比如自动产生动力的蒸汽机、传送带、工厂机械臂等等。这些都是物理形式的自动化，机器自动化了物理任务。相比之下，人工智能专注于认知任务。所以我们看到像自动文本生成器、谷歌搜索引擎、医生使用的诊断工具等工具。

    但当然，身心之间并没有明确的界限，同样，物理自动化和认知自动化之间也没有明确的界限。有些东西结合了这两者。作为一个问题，有人能想到属于这个交叉领域的例子吗？

    （学生建议：机器人？）

    是的，机器人就是物理自动化和认知自动化结合的经典例子。在某种程度上，如果你有一个自主的仿生人（Android），它可以做决定并自主移动，决定如何折叠你的衣服，你就拥有了这种物理加认知自动化的形式。自动驾驶汽车和自主武器也是结合了物理和认知自动化的例子。大家还能想到其他可能的例子吗？

    （学生建议：人脸识别？）

    我会说人脸识别主要属于认知自动化这一部分。识别你是谁主要是一种认知功能。但我认为，如果你将其与某种物理功能结合起来——例如，在扫描你的脸之后，机器会做一些事情，比如自动开门或给你提供一杯饮料——那么这将是不同类型自动化结合的一个例子。这是一个非常简单的练习，目的是为了说明虽然我们主要关注认知自动化，但有时人工智能会融入物理形式的自动化中。

  - [12:24 - 12:30] **Anthropomorphism: Hype and Fallacy (The Placani Article)**

    So that's all I will say in relation to how we define AI. I would like to look at a couple of academic articles now that go into certain aspects of how we think about AI. We'll begin with the article by Placani (2024). You may or may not have already read this article, but let me just run through the main points that we can take away from this. This article discusses what is known as "Anthropomorphism," or our tendency to attribute humanity to things that are not human. The article looks at the trend of anthropomorphizing AI and why we shouldn't do it—why people shouldn't think of AI as human.

    The author begins with a good observation, which is that anthropomorphism is not something new. We have always liked to treat non-human objects or non-human animals as if they have human characteristics; it's just part of how we function as humans. The philosopher David Hume noted that we have a "universal tendency" to conceive all things like ourselves. And if we are not careful with our thinking, we tend to ascribe things like malice or goodwill—which are very human things—to non-human objects.

    In the context of AI, one of the early examples of anthropomorphism was with this particular chatbot called ELIZA, which I think is considered the first chatbot that was successful. If you have not heard of ELIZA, it was a very early attempt to make an automatic chatbot. But of course, ELIZA is far from what things like ChatGPT and Gemini are doing today. ELIZA used a very, very simple algorithm. Essentially, what ELIZA would do was take your prompt and then run a very simple transformation on the prompt and spit out a response like a psychotherapist. So, for example, if you typed in "Eliza, my boyfriend made me come here," ELIZA would take most of your words and convert them into a question, repeating it back to you. Eliza might say something like, "Your boyfriend made you come here?" You can see there's not a lot of change happening; it's a very simple transformation. But what the creator of ELIZA observed was that even with this very simple chatbot, people started to think of ELIZA as somewhat human, treating it like a person. And I think we can see today how some people have begun to behave with things like ChatGPT, even having relationships with chatbots.

    So what does anthropomorphism do? What is the effect? First of all, the author defines what anthropomorphism is, which is the ascription of human qualities like intention, motivation, feelings, and behaviors onto things that are not human. And in particular, it’s not just perceiving something as looking like a human, but actually having an inner life—the mind of a human.

    The author also observes that this is a fallacy. It appears quite often when we talk about AI. The author supposes that part of the reason why—and this goes back to what I said about marketing terms—is because we use the word "Artificial Intelligence" to describe these machines. The use of the word "intelligent" is already something which assumes that the thing is somewhat human, because intelligence is something that humans have. So there is perhaps one explanation—maybe not a full explanation, but one of the reasons—why we may today have a habit of anthropomorphizing AI.

    好了，关于如何定义人工智能我就说这么多。我现在想看几篇学术文章，这些文章深入探讨了我们思考人工智能的某些方面。我们将从 Placani (2024) 的文章开始。你们可能读过也可能没读过这篇文章，但我先以此为主线，梳理一下我们可以从中提取的要点。这篇文章讨论了所谓的“拟人化”（Anthropomorphism），即我们将人性归因于非人类事物的倾向。这篇文章探讨了将人工智能拟人化的趋势，以及为什么我们不应该这样做——为什么人们不应该把人工智能当成人来看待。

    作者首先提出了一个很好的观察，即拟人化并不是什么新鲜事。我们总是喜欢把非人类的物体或非人类的动物当作具有人类特征来对待；这只是作为人类运作方式的一部分。哲学家大卫·休谟（David Hume）指出，我们有一种“普遍倾向”，即把所有事物都想象成和我们自己一样。如果我们不仔细思考，我们往往会将“恶意”或“善意”——这些非常人性化的东西——归因于非人类物体。

    在人工智能的背景下，拟人化的早期例子之一是一个名为 ELIZA 的聊天机器人，我认为它被认为是第一个成功的聊天机器人。如果你没听说过 ELIZA，那是尝试制造自动聊天机器人的非常早期的尝试。当然，ELIZA 与今天的 ChatGPT 和 Gemini 相去甚远。ELIZA 使用了一个非常非常简单的算法。本质上，ELIZA 会获取你的提示（prompt），然后对提示进行非常简单的转换，并像心理治疗师一样吐出回应。例如，如果你输入“Eliza，我男朋友让我来这里的”，ELIZA 会提取你的大部分词汇并将它们转换为问题，重复给你听。Eliza 可能会说类似“你的男朋友让你来这里的？”你可以看到并没有发生太多的变化；这是一个非常简单的转换。但 ELIZA 的创造者观察到，即使是面对这个非常简单的聊天机器人，人们也开始认为 ELIZA 有点像人，把它当人一样对待。我想我们今天可以看到一些人开始如何对待像 ChatGPT 这样的事物，甚至与聊天机器人建立关系。

    那么拟人化有什么作用？有什么影响？首先，作者定义了什么是拟人化，即把意图、动机、感觉和行为等人类特质归因于非人类事物。特别是，它不仅是感知某物看起来像人，而是感知它实际上拥有内心生活——拥有人类的心智。

    作者还观察到这是一种谬误（fallacy）。当我们谈论人工智能时，这种情况经常出现。作者认为，部分原因——这回到了我之前关于营销术语的说法——是因为我们使用“人工智能”这个词来描述这些机器。使用“智能”这个词本身就已经假设了该事物在某种程度上是人类，因为智能是人类拥有的东西。这也许是一个解释——可能不是全部解释，但却是我们今天可能有将人工智能拟人化习惯的原因之一。

  - [12:30 - 12:33] **Specific Examples: From Braitenberg to AlphaZero**

    The author gives some interesting examples of how people have done this. For example, you have this cyberneticist (Valentino Braitenberg) who, when he made a robot, described it using words like "inquisitive," "friendly," and "optimistic." This would be an example of someone anthropomorphizing an artificial intelligence. More recently, you have Sophia, a robot that looks like a human, that was granted citizenship. There are also Will Smith and Sophia interactions, which are a bit strange, but this would be an example of anthropomorphism.

    More recently, you have Ilya Sutskever, who mentioned, perhaps in passing, that "it may be that today's large neural networks are slightly conscious." Here Ilya is ascribing consciousness, which is a human thing, to what is essentially a very complicated machine—a large neural network.

    And one more example: when the New York Times looked at AlphaZero, which is an AI that was designed to play a board game, how they described it was to use very human terms. They said something like, "It played like no computer ever has, intuitively and beautifully, with a romantic, attacking style. It played gambits and took risks. AlphaZero had the finesse of a virtuoso and the power of a machine." They called it "wiser." These are very human words used to describe AlphaZero.

    You can see from this why anthropomorphism can be problematic, because it contributes towards the tendency to hype AI and hype technology. When we anthropomorphize and say that this is like a human, we implicitly are suggesting that this AI can do what a human can do. It contributes to an overestimation of what the AI can actually do.

    作者给出了一些人们如何做到这一点的有趣例子。例如，有一位控制论专家（Valentino Braitenberg），他在制造机器人时，使用了“好奇”、“友好”和“乐观”等词来描述它。这就是某人将人工智能拟人化的例子。最近的例子有 Sophia，一个看起来像人的机器人，甚至被授予了公民身份。还有威尔·史密斯（Will Smith）和 Sophia 的互动，这有点奇怪，但这也是拟人化的一个例子。

    更近期的例子是 Ilya Sutskever，他可能随口提到，“今天的大型神经网络可能具有微弱的意识”。在这里，Ilya 将意识——一种人类的东西——归因于本质上是一台非常复杂的机器——一个大型神经网络。

    还有一个例子：当《纽约时报》报道 AlphaZero（一个设计用来下棋的 AI）时，他们描述它的方式使用了非常人性化的术语。他们说，“它的下法不像任何计算机，直观而优美，具有浪漫的攻击风格。它使用弃子战术（gambits）并承担风险。AlphaZero 拥有大师的技巧（finesse of a virtuoso）和机器的力量。”他们称它“更明智”。这些都是用来描述 AlphaZero 的非常人性化的词汇。

    你可以从中看出为什么拟人化可能会有问题，因为它助长了炒作 AI 和炒作技术的倾向。当我们拟人化并说这像人一样时，我们隐含地暗示这个 AI 可以做人类能做的事。它导致了对 AI 实际能力的过高估计。

  - [12:33 - 12:37] **Class Discussion: How We Anthropomorphize AI**

    Looking at all these examples, can you think of some examples that you've seen in the last few weeks, or even yourself, anthropomorphizing artificial intelligence? How do we use language?

    (Student 1: Some people, depending on the spot when they use voice actions and it's a female voice or male voice, they refer to AI as "he" or "she".)

    Yeah, exactly. So that would be one common example of anthropomorphism, where instead of using the word "it"—"it typed this to me"—some people will just use words like "he" and "she" to describe the AI chatbot.

    (Student 2: I suppose the ability of a chatbot to recognize your expressions... in AI, I guess it may lead to you believing that it is human-like. Also, when we say AI can "hallucinate." It's also a form of anthropomorphism.)

    Okay, yeah. So the words that we use to describe what AI does. For example, "hallucination." Hallucination is something that humans do, but we use that to describe how AI produces truthful or untruthful things. So those are some examples. Other examples would be, I suppose if you look at a lot of the chatbots that have been produced recently, you see people name those chatbots—Claude, Harvey. Those are almost invitations for people to treat these chatbots like humans because they're named like humans.

    回顾所有这些例子，大家能想到过去几周你们看到的，甚至你们自己将人工智能拟人化的例子吗？我们在语言使用上是怎样的？

    （学生1：有些人，当他们使用语音操作时，如果是女声或男声，他们会用“他”或“她”来指代 AI。）

    是的，没错。那是拟人化的一个常见例子，有些人不使用“它”这个词——比如“它给我打了这段字”——而是使用“他”和“她”这样的词来描述 AI 聊天机器人。

    （学生2：我想聊天机器人识别你表情的能力……可能会导致你相信它像人一样。另外，当我们说 AI 会“产生幻觉”（hallucinate）时。这也是一种拟人化。）

    好的，是的。所以我们用来描述 AI 行为的词汇。例如，“幻觉”。产生幻觉是人类会做的事情，但我们用它来描述 AI 如何产生真实或不真实的内容。这些都是例子。其他的例子，我想如果你看看最近推出的许多聊天机器人，你会发现人们给这些聊天机器人起名字——Claude，Harvey。这几乎是在邀请人们像对待人类一样对待这些聊天机器人，因为它们的名字就像人类一样。

  - [12:37 - 12:40] **The Problem with Anthropomorphism: Misplaced Trust**

    So in light of this anthropomorphism and how it contributes to hype for AI systems, what is the problem here? So what if we treat AI as like a human? Is that really ethical or is that really a social problem?

    (Student: We assume that how we work with or how we behave as human beings, we apply those to the AI as well. For example, let's say fairness or lying or social context, and we just assume, yeah, we behave the same way.)

    Yeah, I suppose one problem is that once we start to treat things like ChatGPT as almost like your personal assistant, you kind of start to assume that you can rely on it, or you can offload a lot more of your classes or your thinking to the chatbot. And that reliance may be unjustified. Because when the AI chatbot fails, it fails in a way which you would not expect a human to fail, right? That causes problems for you personally.

    I think there's something which the author also suggests in this article, which is that if we start to treat AI like humans, we assume that the AI will, for example, have basic equipment. Like we assume that AI will reciprocate any goodwill that you give to it. When you say "please" to ChatGPT, you hope that it will give you a better answer. This sort of trust might possibly be displaced, because AI is just a thing.

    鉴于这种拟人化及其如何助长人工智能系统的炒作，这里的问题是什么？如果我们把 AI 像人一样对待会怎样？这真的是一个伦理问题还是一个社会问题？

    （学生：我们假设我们要像与人类共事或行为那样，我们也把这些应用到 AI 身上。例如，公平性、撒谎或社会背景，我们就假设，是的，我们的行为方式是一样的。）

    是的，我想一个问题是，一旦我们开始把像 ChatGPT 这样的东西几乎当作你的私人助理，你就会开始假设你可以依赖它，或者你可以把更多的课程作业或思考分担给聊天机器人。这种依赖可能是不合理的。因为当 AI 聊天机器人失败时，它的失败方式是你不会期望人类失败的方式，对吧？这会给你个人带来问题。

    我认为作者在这篇文章中还暗示了一些东西，那就是如果我们开始像对待人类一样对待 AI，我们会假设 AI 具有某些基本配置。比如我们假设 AI 会回报你给予它的任何善意。当你对 ChatGPT 说“请”时，你希望它会给你一个更好的答案。这种信任可能会被错付，因为 AI 只是一个东西。

  - [12:40 - 12:47] **Four Distorting Effects on Moral Judgment**

    Let me move on to talk about how anthropomorphism influences how we think morally about the AI, and this goes into why we should avoid doing it. The author here suggests a few connections and she points out four possible distorting effects.

    First, I think you've already mentioned judgments of **Moral Character**. Meaning when we anthropomorphize AI, we ascribe a moral character to the AI. We kind of assume that the AI will behave ethically. That's one distorting effect.

    The second problem that anthropomorphism gives rise to is what we call judgments of **Moral Status**. Meaning when we start to treat AI as human, and if we seriously believe it, that may lead us to assume that AI has separate morals and being just as a human has moral standing. If we think that AI is human or a particular machine or a particular chatbot is human, then we might say, "Okay, since it's human, then it should deserve things like human rights." We may wrongly assume that we should keep it running, even though perhaps there's no use for it. Part of the problem, which the author recognizes, is that when you grant moral status to AI, then it may potentially turn them into "rights holders," which we will then have to hold duties towards. Which of course is not justified, because I start with the premise that most of us believe that only humans are proper rights holders and not other things.

    The third problem with anthropomorphism is what we call **Responsibility Judgments**. Meaning if we ascribe humanity to AI, then we may also be tempted to ascribe responsibility to the act. So if an AI tool perhaps does something, makes a wrong decision—some autonomous vehicle goes off the road and kills a few people—we may be tempted to ascribe responsibility for that terrible accident to the AI running the vehicle. When actually that is the wrong place to put responsibility. It should be the manufacturer, or the driver of the car, and so on. But because we anthropomorphize AI, we might wrongly say that, "Oh, it is the AI that is responsible."

    (Student: Yeah, I think just today or the last couple of days was a good example of Google. They have AI summaries, and they removed all health information from that just recently. Because in the past when they referred us to websites that had wrong content, it was acceptable that that's not Google's content. But now with Google summarizing it, the responsibility got carried over to Google. So they said, "Okay, that's unusual information.")

    I suppose in that case Google is actually afraid that the AI function that runs on its Google Search might cause Google to be held responsible for providing wrong health information. So that's why they removed it—because they see it could fire back on them. But I suppose that's a good thing, right? Here it is Google actually seeing itself as potentially responsible, and they're not trying to say, "Whatever the chatbot says is between you and the chatbot." If they did that, it would be a case of applying that judgment that "it's human, so we hold the AI responsible."

    And the last problem, of course, is judgments of **Trust**. Trust is something that we put on other human beings. If we see that there's a good reputation, then we will trust them with doing things for us. But when we anthropomorphize AI, then we might come to see AI as trustworthy. But AI does not have the emotional capacities necessary to be trustworthy, because we trust someone when we know that they have an inner mind and they have goodwill. And secondly, of course, it is correct to trust someone if they can actually be held responsible for breaching that trust.

    让我接着谈谈拟人化如何影响我们对 AI 的道德思考，这涉及到我们为什么要避免这样做。作者在这里提出了几个联系，并指出了四种可能的扭曲效应。

    首先，我想你们已经提到了**道德品质（Moral Character）**的判断。意味着当我们把 AI 拟人化时，我们将道德品质归因于 AI。我们要有点假设 AI 会合乎道德地行事。这是一种扭曲效应。

    拟人化引起的第二个问题是我们所谓的**道德地位（Moral Status）**的判断。意味着当我们开始把 AI 当作人来对待，如果我们真的相信这一点，那可能会导致我们假设 AI 拥有独立的道德，就像人类拥有道德地位一样。如果我们认为 AI 是人类，或者特定的机器或特定的聊天机器人是人类，那么我们可能会说，“好吧，既然它是人类，那么它应该享有像人权这样的东西。”我们可能会错误地假设我们应该让它保持运行，即使它可能没有用处。作者认识到的部分问题是，当你赋予 AI 道德地位时，它可能会把它们变成“权利持有人”，我们就必须对它们承担义务。这当然是不合理的，因为我的前提是，我们大多数人认为只有人类才是恰当的权利持有人，而不是其他东西。

    拟人化的第三个问题是我们所谓的**责任判断（Responsibility Judgments）**。意味着如果我们把人性归因于 AI，那么我们也可能会倾向于把责任归因于其行为。因此，如果一个 AI 工具做了一些事情，做出了错误的决定——比如一些自动驾驶汽车冲出道路导致几人死亡——我们可能会倾向于把那起可怕事故的责任归因于驾驶车辆的 AI。而实际上，那是错误的责任归属对象。应该是制造商，或者汽车司机等等。但因为我们将 AI 拟人化，我们可能会错误地说，“哦，是 AI 的责任。”

    （学生：是的，我想就在今天或过去几天，Google 就是一个很好的例子。他们有 AI 摘要功能，最近他们从中删除了所有的健康信息。因为过去当他们把我们引向内容错误的网站时，大家可以接受那不是 Google 的内容。但现在由 Google 进行总结，责任就转移到了 Google 身上。所以他们说，“好吧，那是异常信息。”）

    我想在这种情况下，Google 实际上是害怕在其 Google 搜索上运行的 AI 功能可能会导致 Google 因提供错误的健康信息而被追究责任。所以这就是他们删除它的原因——因为他们看到这可能会对他们造成反噬。但我想这是一件好事，对吧？这里 Google 实际上把自己视为潜在的责任人，他们并没有试图说，“聊天机器人说的任何话都是你和聊天机器人之间的事。”如果他们那样做，那就是应用了“它是人类，所以我们让 AI 负责”的判断的一个例子。

    最后一个问题，当然是**信任（Trust）**的判断。信任是我们给予其他人类的东西。如果我们看到某人有良好的声誉，那么我们会信任他们为我们做事。但是当我们把 AI 拟人化时，我们可能会开始认为 AI 是值得信任的。但 AI 并不具备值得信任所必需的情感能力，因为我们信任某人是基于我们知道他们有内心思想，他们有善意。其次，当然，如果某人实际上能因破坏信任而被追究责任，那么信任他们才是正确的。

  - [12:47 - 12:53] **Practical Solutions and Conclusion**

    Based on this discussion, what practical things do you think we can do based on what you understand from this article? If we need to kind of avoid anthropomorphizing AI, what are some practical things that we might do?

    (Student: I think what we could potentially do is with information that is generated by AI, for example through chatbots, we need to verify it?)

    Okay. Could you elaborate more on how that avoids anthropomorphism?

    (Student: Actively avoid accepting what is being produced by AI as human equivalent. Take ownership.)

    Yes. So I suppose the important thing is to take ownership of that chunk of text. I would say it is not open to you to say, "Oh actually it was the chatbot that told me this," right? Ultimately, if you are communicating something to someone else, then that text belongs to you regardless of how you produced it. Maybe you wrote it yourself or you prompted a text generator.

    Are there other things that perhaps we can do based on this article? One of the general principles of AI governance is accountability. The idea is that if you deploy an AI tool, then you should be responsible for what it turns out.

    So that's one practical thing that we can do: avoid using anthropomorphic language where we are describing or using an AI tool. So don't call your chatbot Claude would be one way to start, because that leads people to treat the thing as human. And you would even argue that the way in which organizations like OpenAI design their website almost provokes anthropomorphism. Because you can think of a different design for the website where it's just a box, you put words in, and then words come out. But it's not designed this way. ChatGPT is designed to chat with you. The whole website is designed almost like a platform for you to chat with someone else. So arguably this shouldn't be how we sort of design interactions with it.

    Any questions so far about what we just discussed?

    Okay, so on the slide, I gave a few other examples of anthropomorphism. These are fairly recent things. If you recall, when OpenAI went from GPT-4 to 4.5 to 5, there was a bit of protest because apparently the chatbot became "colder" and less personable in the way it communicated, which led some people to be very upset because they apparently formed some sort of relationship with the earlier version. So that's a great example of anthropomorphism. Some more regular examples would be Albania's AI Minister, who was apparently "pregnant" with digital assistants—the Prime Minister apparently believed that the AI tool that it was involved in developing actually had feelings and emotions. So these would be examples of anthropomorphism.

    Okay, we should maybe pause here for a bit and we can come back instead of taking a break right now? No, let's take a break.

    基于这次讨论，根据你们从这篇文章中了解到的内容，你们认为我们可以做哪些实际的事情？如果我们需要避免将 AI 拟人化，我们可以做哪些实际的事情？

    （学生：我认为我们可以做的是，对于 AI 生成的信息，例如通过聊天机器人生成的，我们需要去验证它？）

    好的。你能详细说明一下这如何避免拟人化吗？

    （学生：积极避免接受 AI 产生的东西等同于人类产生的东西。承担所有权。）

    是的。我想最重要的是对那段文字承担所有权。我会说，你不可以说“哦，其实是聊天机器人告诉我的”，对吧？归根结底，如果你在向别人传达某些信息，那么无论你是如何生成这段文字的，它都属于你。也许是你自己写的，或者是你提示文本生成器生成的。

    基于这篇文章，我们还可以做其他事情吗？人工智能治理的一般原则之一是问责制（Accountability）。这个想法是，如果你部署了一个 AI 工具，那么你应该对它的产出负责。

    所以这是我们可以做的一件实际的事情：在描述或使用 AI 工具时避免使用拟人化的语言。所以不要把你的聊天机器人叫做 Claude，这将是一个开始，因为那会导致人们把这个东西当作人来对待。你甚至可以争辩说，像 OpenAI 这样的组织设计其网站的方式几乎是在挑起拟人化。因为你可以想象网站的另一种设计，它只是一个框，你把词放进去，然后词出来。但它不是这样设计的。ChatGPT 被设计成与你聊天。整个网站的设计几乎就像一个让你与另一个人聊天的平台。所以可以说，这不应该是我们设计与它交互的方式。

    关于我们要讨论的内容，到目前为止有什么问题吗？

    好的，在幻灯片上，我给出了其他几个拟人化的例子。这些都是最近发生的事情。如果你还记得，当 OpenAI 从 GPT-4 升级到 4.5 再到 5 时，发生了一些抗议，因为显然聊天机器人在交流方式上变得更“冷漠”，更不具个性，这导致一些人非常沮丧，因为他们显然与早期版本形成了某种关系。那就是拟人化的一个极好的例子。一些更常见的例子是阿尔巴尼亚的 AI 部长，显然“怀了”数字助手——首相显然相信它参与开发的 AI 工具实际上有感觉和情绪。所以这些都是拟人化的例子。

    好的，我们也许应该在这里暂停一下，我们能不能不休息直接回来？不，我们还是休息一下吧。

  - [13:08 - 13:14] **The Illusion of AI Existential Risk (Richards 2023)**

    Then the author focused on attention. When it comes to ethical AI, development has often focused on what we call the "existential" kind of threats—that AI can pose a threat to humanity as a whole. For example, Sam Altman and various other people have warned that if AI is not properly controlled—what they call "alignment"—it may pose a risk to humanity. Other scientists have suggested that we need to watch out to make sure that AI doesn't become self-aware or acquire a sense of self-preservation, because if the AI is powerful enough, it could actually compete with us.

    Richards talks about the "illusion of AI existential risk." The essential argument of this article is that perhaps we should not focus on this kind of hypothetical, far-away risk, and instead pay more attention to the actual harms that are being caused by the improper use of AI at the present. What Richards argues is that when we focus on "rogue superintelligence," we are distracted. You know when you watch sci-fi movies that talk about artificial intelligence? They normally portray some sort of extremely powerful machine intelligence—think of *The Matrix* or *The Terminator*—that actively tries to kill human beings. But the argument here is that this is not the sort of AI that we need to deal with right now, because it distracts attention from the actual harms that are being caused, such as the military use of AI, or the use of AI for disinformation and deepfakes.

    Richards argues that although hypothetically there is some non-zero chance that some rogue AI will arise and kill everyone, that is a very, very unlikely scenario. He looks at past trends to relate what the future trends will be. Based on those past trends, it is very unlikely that this is how humanity is going to go extinct.

    (Student: I heard some people argue that some of these crisis scenarios are made up by some of the leaders of the large AI companies to get "regulatory capture"—to get favorable regulation that keeps smaller competitors out of the industry. I suppose that would be cynical but perhaps accurate?)

    Yes. This focus on what we call "AI Safety"—the existential risk—I don't think the author mentioned it because he was probably too polite, but I think there is a case for that view. There may be a self-interested reason why people like Sam Altman like to focus on existential risk. Number one, it contributes to the hype, right? Because it assumes that AI is going to be superintelligent, which brings in investor money. And the second reason, which you alluded to, is that it redirects attention away from perhaps the things that these companies don't want us to pay attention to. If we look at the long-term risk, we can avoid dealing with the short-term harms. So that would be the cynical view of why there is a temptation to focus on long-term existential risk.

    接下来，作者将重点放在了注意力上。谈到人工智能伦理时，发展的重点往往集中在我们所谓的“存在性”威胁上——即人工智能可能对整个人类构成威胁。例如，山姆·奥特曼（Sam Altman）和其他各界人士都曾警告说，如果人工智能没有得到适当的控制——他们称之为“对齐”（alignment）——它可能会对人类构成风险。其他科学家也建议我们需要小心，确保人工智能不会产生自我意识或获得自我保护的意识，因为如果人工智能足够强大，它实际上可能会与我们竞争。

    Richards 谈到了“人工智能存在风险的幻觉”。这篇文章的核心论点是，也许我们不应该关注这种假设性的、遥远的风险，而应该更多地关注目前因不当使用人工智能而造成的实际危害。Richards 认为，当我们关注“失控的超级智能”时，我们的注意力被分散了。你知道看那些关于人工智能的科幻电影时，它们通常会描绘某种极其强大的机器智能——比如《黑客帝国》或《终结者》——它们会主动试图杀死人类。但这里的论点是，这不是我们需要立即处理的那种人工智能，因为它分散了我们对正在造成的实际危害的注意力，例如人工智能的军事用途，或利用人工智能制造虚假信息和深度伪造（deepfakes）。

    Richards 认为，虽然假设上有非零的几率会出现某种失控的人工智能并杀死所有人，但这是一种非常非常不可能的情况。他通过观察过去的趋势来推断未来的趋势。基于这些过去的趋势，这不太可能是人类灭绝的方式。

    （学生：我听到有些人认为，其中一些危机场景是由大型 AI 公司的领导者编造出来的，目的是为了实现“监管俘获”（regulatory capture）——即获得有利的监管，将较小的竞争对手挡在行业之外。我想这可能是愤世嫉俗但也许是准确的看法？）

    是的。这种对所谓“AI 安全”（AI Safety）——即存在风险——的关注，我认为作者可能太客气了没在文章里提，但我认为这种观点是有道理的。像山姆·奥特曼这样的人喜欢关注存在风险，可能有其自身利益的原因。第一，它助长了炒作，对吧？因为它假设人工智能将变得超级智能，这会吸引投资者的资金。第二，正如你提到的，它将注意力从这些公司可能不希望我们关注的事情上转移开。如果我们关注长期风险，我们就可以避免处理短期的危害。所以，这就是关于为什么人们会被诱导去关注长期存在风险的一种愤世嫉俗的看法。

  - [13:14 - 13:18] **Why Superintelligence Won't Kill Us: Biological Analogies**

    Against that view, this article is basically making a case by looking at how past extinctions have happened. Essentially, Richards observes that—let's assume for the sake of argument that a superintelligence is possible—will this lead to our extinction? He looks at what is required for a species to go extinct.

    The first thing he points out is that superior intelligence is not something which has historically caused past extinctions. The only species that causes other species to go extinct because of their superior intelligence is humans. In the whole history of the natural world, extinction doesn't happen just because something is smarter. For example, in the Late Devonian period, species went extinct simply because plants spread across the world, changed the atmosphere, and caused species that were not adapted to that new atmosphere to go extinct. This is a situation where a *less* intelligent species caused the extinction of *more* intelligent beings. So basically, it's not the case that just because something is more intelligent, it will necessarily cause other less intelligent things to perish.

    Essentially, there are three main ways things go extinct. Number one is **competition for resources**: the new species out-consumes the old species. Number two is by **hunting**: a new species is the predator and hunts the old species because they are prey, to the point of devastation. Number three is simply by **altering the environment**, making it less hospitable for the old species and thereby causing the old species to go extinct.

    Looking at the way AI works, it is very unlikely that any of these three things will happen unless we really wanted it to. For example, in terms of AI potentially competing with humans for resources, this would probably only happen if we fully automated every aspect of the economy—like if we let AI take over producing data centers all over the place. Barring that, it is very unlikely that you will get human extinction because of this sort of competition for resources.

    In terms of getting hunted by AI, like the Terminators? Theoretically possible if, for example, we made nuclear weapons automated. So that would be a bit suicidal. Short of doing that, it is pretty unlikely that you will find humanity getting hunted, as there are what Richards calls "natural checkpoints" along the way where we can make decisions.

    As for changing the environment—even in *The Matrix*, the machines changed the environment—the author's argument is basically that this is entirely up to us. It's up to us how much energy we want to use, how much fossil fuel we want to burn to power all these AI items. So essentially, the extinction of humanity as a result of the development of AI, according to this article, is very unlikely.

    为了反驳那种观点，这篇文章通过观察过去的灭绝是如何发生的来进行论证。Richards 本质上观察到——让我们为了论证起见，假设超级智能是可能的——这会导致我们的灭绝吗？他研究了一个物种灭绝需要什么条件。

    他指出的第一点是，优越的智力在历史上并不是导致过去灭绝的原因。唯一因为其优越智力而导致其他物种灭绝的物种是人类。在整个自然界的历史中，灭绝不仅仅是因为某种东西更聪明而发生的。例如，在晚泥盆纪时期（Late Devonian period），物种灭绝仅仅是因为植物蔓延到全世界，改变了大气，导致那些不适应新大气的物种灭绝。这是一种*智力较低*的物种导致*智力较高*的生物灭绝的情况。所以基本上，并不是说仅仅因为某种东西更聪明，它就必然会导致其他不那么聪明的东西灭亡。

    本质上，物种灭绝主要有三种方式。第一是**资源竞争**：新物种在消耗资源上胜过旧物种。第二是**捕猎**：新物种作为捕食者捕猎旧物种，因为它们是猎物，直到将其毁灭。第三仅仅是**改变环境**，使其不适合旧物种生存，从而导致旧物种灭绝。

    看看人工智能的运作方式，除非我们要它发生，否则这三件事发生的可能性非常小。例如，就人工智能可能与人类争夺资源而言，这可能只有在我们完全自动化经济的各个方面时才会发生——比如我们让 AI 接管并在各地建造数据中心。除此之外，你很难因为这种资源竞争而导致人类灭绝。

    至于被 AI 猎杀，像《终结者》那样？理论上是可能的，例如，如果我们让核武器自动化。那将有点自杀倾向。如果不那样做，你很难发现人类被猎杀，因为这一路上有 Richards 所谓的“自然检查点”（natural checkpoints），我们可以做出决定。

    至于改变环境——即使在《黑客帝国》中，机器也改变了环境——作者的论点基本上是这完全取决于我们。这取决于我们想要使用多少能源，想要燃烧多少化石燃料来为所有这些 AI 项目供电。所以本质上，根据这篇文章，由于人工智能的发展而导致人类灭绝是非常不可能的。

  - [13:18 - 13:22] **Pascal's Wager and the Ethics vs. Safety Divide**

    So what is the problem with this sort of speculation? The problem of focusing on these long-term things that are very unlikely to happen is that it creates what the author calls an AI version of **Pascal's Wager**. If you are not familiar with Pascal's Wager, it is a philosophical thought experiment where someone who is trying to convince you to believe in God would say: "Okay, maybe you don't believe, but there is a non-zero chance that if you don't believe, you will go to Hell. And that is a very, very, very terrible thing. So if you accept that there is even a non-zero chance that you might go to Hell, then you should believe in God because that's the rational thing to do."

    This is an AI version of Pascal's Wager. The argument is that humanity's extinction is a super bad thing. So even if the risk is very, very unlikely, you should still deploy all resources to prevent it. But Richards argues that even if the risk of human extinction is terrible, it does not justify focusing resources on that type of risk instead of the more mundane but more real harms. Just like believing in God because of the wager means you might ignore more pressing realities, focusing on the extinction of humanity—which has a very small probability—distracts us.

    What this author points out is that today there is kind of an unfortunate division between two types of scholarship. On the one hand is **AI Ethics**, which is what this course is dealing with. On the other hand is **AI Safety**, which is using all these existential types of risk arguments. I want to draw this distinction to explain why this course would not be focusing on the AI Safety sort of problems. That's the main point I want to draw from this article. Any questions about it?

    那么这种推测有什么问题呢？关注这些极不可能发生的长期问题，其弊端在于它创造了作者所说的 AI 版的**帕斯卡赌注（Pascal's Wager）**。如果你不熟悉帕斯卡赌注，这是一个哲学思想实验，试图说服你相信上帝的人会说：“好吧，也许你不信，但这有非零的几率：如果你不信，你会下地狱。那是一件非常非常可怕的事情。所以，如果你接受哪怕只有非零的几率你可能会下地狱，那么你就应该相信上帝，因为那是理性的做法。”

    这就是帕斯卡赌注的 AI 版本。论点是人类灭绝是一件超级糟糕的事情。所以即使风险非常非常小，你仍然应该部署所有资源来防止它。但 Richards 认为，即使人类灭绝的风险很可怕，这也无法证明将资源集中在那类风险上，而不是关注更平凡但更真实的危害是正当的。就像因为赌注而相信上帝意味着你可能会忽视更紧迫的现实一样，关注概率极小的人类灭绝会分散我们的注意力。

    作者指出，今天在两类学术研究之间存在一种不幸的分裂。一方面是**AI 伦理（AI Ethics）**，也就是本课程要处理的内容。另一方面是**AI 安全（AI Safety）**，它使用的是所有这些存在性风险的论点。我想划清这个界限，以解释为什么本课程不会关注 AI 安全类的问题。这就是我想从这篇文章中提取的主要观点。对此有什么问题吗？

  - [13:22 - 13:34] **Introduction to Ethical Theories (Bonde 2013)**

    Okay, so that's all I have to say in relation to introducing AI. Now I would like to move on to introducing AI Ethics. The first point I should stress is that there is nothing really special about AI ethics in comparison with other types of ethics. Very much the same ethical principles apply to AI as with any other type of situation.

    What I would like to do today is to introduce you to the main ethical theories. Some of you may have already an understanding of ethical theory, but if you haven't, I want to introduce you to these main frameworks which can then be applied to AI problems. We will be referring to this particular article by Bonde and others. I like this particular webpage because I think it explains the three ethical theories in fairly simple terms.

    Ethics is essentially a set of standards for behavior that help us to decide how we should act. There are many, many ethical theories out there, but broadly speaking, you can group them into three main categories.

    The first category would be **Consequentialist theories**. These theories are focused on the report—the action's outcome. So if you apply a consequentialist theory, you would look at: "If I do this, what are the consequences?" And then you assess your behavior based on those.

    Number two, you have the **Non-consequentialist theories**. These are usually the duty-based theories, where you don't look at the consequences but instead you look at things like: "To what extent are you following certain ethical rules?"

    The third group of theories are the **Agent-centered theories**, and the main type which we will discuss later is Virtue Ethics. Here the focus is not really on the act that you are doing, but what kind of person you are. So the focus of agent-centered theories is not "Is what I'm doing correct?" but "Is this what an honest or loyal person would do?"

    Let me go into a bit more detail about each of these. In terms of Consequentialism, the most famous version is what is known as **Utilitarianism**. Here, essentially the question is: you judge the goodness or badness of something based on how much pain or pleasure it produces. So your objective, or your goal if you are a utilitarian, is to maximize happiness and minimize unhappiness. You look at the consequences of your actions. Utilitarianism tends to be useful when you are looking at something which will affect a big group of people, because then what you do is you look at how much overall benefit you are producing versus how much harm you are causing, and you try and balance out and see whether you are doing more good or more harm.

    The second group are the Non-consequentialist theories, and as I said, the main one is the **Duty-based approach** (Deontology). The main figurehead for deontological ethics is the philosopher Immanuel Kant, who came up with what he called "categorical imperatives." You don't have to go into that level of detail, but the general idea is that if you are adopting deontological ethics, you don't care about the outcome or the consequence of your actions. All you care about is: are you following the ethical rules, whatever those rules may be? So things like "You should not steal" is an ethical rule, and if you are a deontologist, you will follow that rule regardless of whether you think stealing might produce a good outcome.

    The third group is the Agent-centered theories, and the main approach taken here is the **Virtue approach**. Under this approach, the question is: what actions are consistent with a virtuous human being? So whenever you are trying to decide what you should do, the question you are asking is, "Is this what a virtuous person would do?" I suppose the most famous virtue ethicist is the Greek philosopher Aristotle. He argued that ethics should be concerned with the whole life of a person instead of the specific actions that they have.

    Let's contrast these frameworks.

    - **Consequentialist:** Thought process focuses on the **outcome** ("What kind of outcome should I try to produce?"). Aim: Produce the most good overall. Focus: Future.
    - **Deontologist:** Thought process focuses on **duties** ("What duties do I owe this person? What are my obligations?"). Aim: Perform or fulfill your duties. Focus: Present (the rules).
    - **Virtue Ethicist:** Thought process focuses on **character** ("What kind of person should I try to be?"). Aim: Develop your own virtuous character overall. Focus: The Agent.

    One thing to note is that often, whichever framework you apply, you will reach the same answer. However, sometimes the frameworks will lead to different types of answers. Let's say you have a scenario—the classic "Heinz dilemma." You know this rich person, and this rich person refuses to give you medicine. You have a sick child who really needs medicine, but you cannot afford it. Do you steal from the rich person?

    - The **Deontologist** would say: No, you should not steal. Stealing is wrong, regardless of the consequences.
    - The **Consequentialist** (Utilitarian) might say: Stealing from this rich guy is not going to hurt him much, but I get to save my child's life. The good that I produce is a lot, so go ahead and steal it.

    This is a really quick introduction to ethical theory. Going any further takes us into the philosophy of ethics, which isn't that useful for our present purposes. In terms of practical ethics, our main resource is generally our own conscience, and what we need to do is to be able to justify the decisions that we make. That's where theories come in—they help you to articulate why something is morally problematic from your point of view.

    好了，关于人工智能的介绍我就说这么多。现在我想转而介绍 AI 伦理。我首先要强调的一点是，与其他类型的伦理相比，AI 伦理并没有什么特别之处。同样的伦理原则在很大程度上适用于 AI，就像适用于任何其他类型的情况一样。

    我今天想做的是向大家介绍主要的伦理理论。你们中的一些人可能已经对伦理理论有所了解，但如果你还没有，我想向你们介绍这些主要框架，然后将其应用于 AI 问题。我们将参考 Bonde 等人的这篇文章。我喜欢这个特定的网页，因为它用相当简单的术语解释了这三种伦理理论。

    伦理本质上是一套行为标准，帮助我们决定应该如何行动。有很多很多的伦理理论，但大体上，你可以把它们分为三大类。

    第一类是**后果主义理论（Consequentialist theories）**。这些理论关注的是报告——即行动的结果。如果你应用后果主义理论，你会看：“如果我这样做，会有什么后果？”然后根据这些后果来评估你的行为。

    第二类是**非后果主义理论（Non-consequentialist theories）**。这些通常是基于义务的理论（Duty-based），你不看后果，而是看诸如：“你在多大程度上遵循了某些伦理规则？”

    第三类理论是**以代理人为中心的理论（Agent-centered theories）**，我们将讨论的主要类型是美德伦理学（Virtue Ethics）。这里的重点并不在于你正在做的行为，而在于你是什么样的人。所以以代理人为中心的理论关注的不是“我正在做的事情是否正确？”，而是“这是一个诚实或忠诚的人会做的事情吗？”

    让我更详细地谈谈每一个。关于后果主义，最著名的版本被称为**功利主义（Utilitarianism）**。在这里，本质上的问题是：你根据某件事产生了多少痛苦或快乐来判断它的好坏。所以你的目标，或者说如果你是一个功利主义者，你的目标是最大化幸福并最小化不幸福。你看待你行动的后果。功利主义在你处理会影响一大群人的事情时往往很有用，因为那时你看的是你产生了多少总体利益与你造成了多少伤害，你试图平衡并看看你是做了更多的好事还是坏事。

    第二组是非后果主义理论，正如我所说，主要的是**基于义务的方法**（义务论，Deontology）。义务论伦理学的主要代表人物是哲学家伊曼努尔·康德（Immanuel Kant），他提出了所谓的“绝对命令”（categorical imperatives）。你不必深入到那个细节层面，但总体思路是，如果你采用义务论伦理，你不关心结果或你行动的后果。你只关心：你是否遵循了伦理规则，无论那些规则是什么？所以像“你不应该偷窃”是一条伦理规则，如果你是一个义务论者，你会遵循那条规则，不管你是否认为偷窃可能会产生好的结果。

    第三组是以代理人为中心的理论，这里采取的主要方法是**美德方法**。在这种方法下，问题是：什么行为符合一个有美德的人？所以每当你试图决定你应该做什么时，你问的问题是，“这是一个有美德的人会做的事吗？”我想最著名的美德伦理学家是希腊哲学家亚里士多德（Aristotle）。他认为伦理学应该关注一个人的整体生活，而不是他们做的具体行为。

    让我们对比一下这些框架：

    - **后果主义者**：思维过程关注**结果**（“我应该试图产生什么样的结果？”）。目标：产生总体上最大的善。关注点：未来。
    - **义务论者**：思维过程关注**义务**（“我对这个人负有什么义务？我的责任是什么？”）。目标：履行或完成你的义务。关注点：现在（规则）。
    - **美德伦理学家**：思维过程关注**品质**（“我应该试图成为什么样的人？”）。目标：全面发展你自己的美德品质。关注点：代理人（行为者）。

    有一点需要注意，无论你应用哪个框架，通常你会得出相同的答案。然而，有时这些框架会导致不同类型的答案。假设你有一个场景——经典的“海因茨困境”（Heinz dilemma）。你认识这个富人，这个富人拒绝给你药。你有一个生病的孩子非常需要药，但你买不起。你会从富人那里偷东西吗？

    - **义务论者**会说：不，你不应该偷窃。偷窃是错的，不管后果如何。
    - **后果主义者**（功利主义者）可能会说：从这个富人那里偷东西不会对他造成太大伤害，但我能救我孩子的命。我产生的善很多，所以去偷吧。

    这是对伦理理论的一个非常快速的介绍。再深入下去就会带我们要进入伦理哲学的领域，这对我们要目前的目的来说并不是那么有用。在实践伦理方面，我们要的主要资源通常是我们自己的良心，我们需要做的是能够证明我们所做决定的正当性。这就是理论发挥作用的地方——它们帮助你阐明为什么从你的角度来看某件事在道德上是有问题的。

  - [13:34 - 14:08] **Group Exercise: The Crisis Text Line Case (Levine 2022)**

    With that, I'd like us to do a simple exercise. I uploaded a newspaper article which is titled the Levine 2022. That's just for your background. The main question which I would like us to explore is based on this factual scenario:

    Quite recently, there was a mental health support text service called **Crisis Text Line**. They were a voluntary service, where people in need of counseling or someone to talk to would text messages to this particular service, and they have volunteers who can intervene. What happened was that this Crisis Text Line service decided one day to share its text data—take all the conversations amassed—and share it with a sister company called **Loris.ai**. Loris.ai is a commercial entity whose objective is to use AI to help customer service support agents interact with their customers. The idea behind this was that we can use all these conversations to train an AI tool, and because all these conversations are emotional and about persuasion, maybe these conversations will be useful to help customer service agents do their job. So they wanted to share this text data with Loris.ai.

    As you can expect, this raised quite a lot of controversy. They eventually decided not to do so. But what I want us to do right now is to look at this scenario and answer the question based on one of the ethical theories. Is it ethical based on Consequentialism, Deontological ethics, or Virtue Ethics?

    I will split the class into three big groups.

    - **Group 1 (Left):** Could you look at the **Consequences**?
    - **Group 2 (Middle):** Could you approach it from the **Duty** perspective?
    - **Group 3 (Right):** Approach it from **Virtue Ethics**.

    It will take maybe about 15 minutes for this and then we come back and share.

    (Class breaks for discussion)

    **[13:56] Group Debrief**

    Okay, let's come back. Let's start with the Consequentialist group.

    (Student Group 1: It's quite difficult to put a number to these things and try to balance them all right. But I think the harms to the privacy of the users outweigh the commercial benefit.)

    Usually, that's not what people do, right? People do a rough kind of balancing exercise and then they decide if the costs are worth the benefits. And I suspect in this case, the answer is most likely "No," the harms are greater. So that's the consequences.

    Maybe we can move on to the second set, the Duty-based group.

    (Student Group 2: So there's probably a link to all the Terms and Conditions (Ts & Cs) which the users signed. We thought the service itself should have a **duty of care** to these folks who share very sensitive information. We also discussed whether it would make a difference if the sharing is with a commercial entity like Loris.ai versus a non-profit. Our initial thought is it may also not be advisable because, from the perspective of the sharer of this sensitive info, they expect confidentiality.)

    Right. So we might say that Crisis Text Line owes a **duty of confidence** or confidentiality to its clients, and this is a duty that you cannot breach. You cannot share the data, regardless of how beneficial you think the sharing might be. From a duty standpoint, you want to explain why this duty exists. The duty exists because clients of Crisis Text Line expect that their conversations will be confidential.

    One of the things which I suppose could be mentioned is that when looking at duties, all rules have exceptions. So this is what a duty ethicist may also consider. Are there any relevant exceptions to the duty of confidentiality here? I suspect the answer is no, but this is something which you might think about.

    Okay, so I guess we can move on to the last group, which is looking at it from Virtue Ethics. Would anyone like to raise your view?

    (Student Group 3: A good person... if you consult with them... I think because this was a mental health support nonprofit sharing with a commercial entity, it seems like you're commercializing sensitive or vulnerable information. From the perspective of the virtue of **trustworthiness**, you should not be sharing such conversations with others.)

    So from the perspective of being a trustworthy person, you should not be sharing such conversations.

    (Student Group 3 continues: Also from the perspective of valuing human life... actually maybe it is OK because at the end of the day, if you normalize the consequences... wait, I'm not sure how to frame this as a virtue.)

    Here you see one of the main difficulties about Virtue Ethics: there is no real agreement as to what virtues count. If you are a religious person, then you'll be informed by the teachings of your particular religion. Most religions have role models that tell you what is virtuous. On top of this, another difficulty is that it's a bit vague. Even though you may agree that "trustworthiness" is a virtue, how does that translate into action? Or if you think "valuing human life" is a virtue, how does that translate into whether I should share data?

    I hope this gives you a sense of how different ethical theories approach this question. You don't have to apply every single ethical theory when you are doing your moral reasoning, but it helps to consider what arguments can be made. If you think, "I don't like this thing, but I cannot explain why," this is where these theories can be useful—they help you to articulate why something is morally problematic.

    Okay. Any questions about this? If not, I maybe will pause here again for about 5 minutes, just a quick break, and then we will move on to the last part, which is a quick introduction to AI Governance.

    以此为基础，我想让我们做一个简单的练习。我上传了一篇标题为 Levine 2022 的报纸文章。那只是作为背景资料。我想让我们探讨的主要问题是基于这个事实场景：

    就在最近，有一个名为 **Crisis Text Line（危机短信热线）** 的心理健康支持短信服务。他们是一个志愿服务机构，需要咨询或有人交谈的人会向这个特定的服务发送短信，他们有志愿者可以介入。发生的事情是，这个危机短信热线服务有一天决定分享它的文本数据——获取积累的所有对话——并与一家名为 **Loris.ai** 的姊妹公司分享。Loris.ai 是一个商业实体，其目标是使用 AI 帮助客户服务支持代理与客户互动。这背后的想法是，我们可以使用所有这些对话来训练一个 AI 工具，因为所有这些对话都是情感化的，并且关于说服，也许这些对话将有助于客户服务代理完成他们的工作。所以他们想与 Loris.ai 分享这些文本数据。

    正如你所预料的那样，这引起了很大的争议。他们最终决定不这样做。但我现在想让我们做的是看看这个场景，并基于其中一种伦理理论来回答问题。基于后果主义、义务论伦理学或美德伦理学，这合乎道德吗？

    我将把班级分成三大组。

    - **第一组（左边）**：你们能看看**后果（Consequences）**吗？
    - **第二组（中间）**：你们能从**义务（Duty）**的角度来探讨吗？
    - **第三组（右边）**：从**美德伦理学（Virtue Ethics）**的角度探讨。

    这大概需要15分钟，然后我们回来分享。

    （班级分组讨论）

    **[13:56] 小组汇报**

    好的，我们回来吧。让我们从后果主义小组开始。

    （第一组学生：给这些事情定一个数字并试图平衡它们是很困难的。但我认为对用户隐私的伤害超过了商业利益。）

    通常人们不会定数字，对吧？人们会做一个粗略的平衡练习，然后决定成本是否值得收益。我怀疑在这种情况下，答案很可能是“不”，伤害更大。这就是后果。

    也许我们可以转到第二组，基于义务的小组。

    （第二组学生：这可能与用户签署的所有条款和条件（Ts & Cs）有关。我们认为服务本身应该对这些分享非常敏感信息的人负有**注意义务（duty of care）**。我们还讨论了如果与像 Loris.ai 这样的商业实体分享，与非营利组织分享是否有区别。我们最初的想法是这可能也是不可取的，因为从分享这些敏感信息的人的角度来看，他们期望保密。）

    对。所以我们可以说 Crisis Text Line 对其客户负有**保密义务（duty of confidence）**，这是一项你不能违反的义务。无论你认为分享可能有多大好处，你都不能分享数据。从义务的角度来看，你要解释为什么存在这种义务。这种义务之所以存在，是因为 Crisis Text Line 的客户期望他们的对话是保密的。

    我想可能值得一提的一点是，在看待义务时，所有规则都有例外。这也是义务论理学家可能会考虑的。在这里，保密义务是否有任何相关的例外情况？我怀疑答案是否定的，但这是你们可以思考的问题。

    好的，我想我们可以转到最后一组，从美德伦理学角度看的。有人想提出你们的看法吗？

    （第三组学生：一个好人……如果你咨询他们……我认为因为这是一个心理健康支持非营利组织与商业实体分享，这看起来像是你在将敏感或脆弱的信息商业化。从**值得信赖（trustworthiness）**这一美德的角度来看，你不应该与他人分享此类对话。）

    所以从做一个值得信赖的人的角度来看，你不应该分享这些对话。

    （第三组学生继续：另外从重视人类生命的角度来看……其实也许这是可以的，因为归根结底，如果你把后果正常化……等等，我不确定如何将其构建为一种美德。）

    在这里，你看到了美德伦理学的主要困难之一：对于什么美德算数并没有真正的共识。如果你是一个有宗教信仰的人，那么你会受到你特定宗教教义的影响。大多数宗教都有告诉你什么是美德的榜样。除此之外，另一个困难是它有点模糊。即使你可能同意“值得信赖”是一种美德，但这如何转化为行动？或者如果你认为“重视人类生命”是一种美德，这如何转化为我是否应该分享数据？

    我希望这能让你感觉到不同的伦理理论是如何处理这个问题的。你在进行道德推理时不必应用每一个伦理理论，但考虑可以提出哪些论点是有帮助的。如果你想，“我不喜欢这件事，但我无法解释为什么”，这就是这些理论可能有用的地方——它们帮助你阐明为什么某件事在道德上是有问题的。

    好的。对此有什么问题吗？如果没有，我可能会在这里再暂停大约5分钟，只是快速休息一下，然后我们将进入最后一部分，即对人工智能治理的快速介绍。

  - [14:08 - 14:16] **Break**

    (Class takes a 5-minute break)

    （班级休息5分钟）

  - [14:16 - 14:25] **AI Governance Principles (OECD vs. ASEAN)**

    At this point, there is actually a lot of material out there regarding AI Governance. A lot of different international organizations like ASEAN and the OECD have all come up with their own set of guidelines for AI governance. I gave you two examples in the readings, so let's just take a quick look at that.

    We'll look at the **OECD (2025)** one because I think that is the most well-regarded document. This document has been agreed by the members of the OECD (Organization for Economic Cooperation and Development). I will just focus your attention on the key component, which is the **Principles of Trustworthy AI**. In this document, what the OECD has done is to set out what it believes are some basic principles of AI.

    They also have a set of principles in the **ASEAN (2024)** document, but you will see that the principles are a bit different. So for ASEAN, you have "Transparency and Availability," which is a common principle. Then you have "Fairness and Equity," which I suppose appears in OECD also. You have "Security and Safety." You have "Human-centricity," which is a bit different from the OECD version. "Accountability and Integrity," and then "Robustness and Reliability."

    So you can see that while there is some overlap between the principles, they do differ quite substantially. And in addition, the way in which each document interprets each principle could also be different. For one document, "transparency" could be one thing, but for another document, the same word could mean something different. So I just want to highlight that there is no agreed list of principles.

    Given that there is no agreed list, the best that we can do is to try and pick up what are the common principles that you see often repeated in all these documents. So what we'll do in this course is we'll identify the common principles like transparency, explainability, robustness. We will explore what these words mean, and understanding what they mean, we can then try and apply these principles in a practical scenario.

    I gave one example here—we're not going to go through it fully today—but for example, one situation where you might want to apply these governance principles would be: Imagine that you are tasked to launch an internal AI system for your company. This AI system is meant to evaluate how employees perform based on their usage of the AI tools that the company has purchased. The company wants the employees to use AI, so they ask you to design the AI tool to track this usage, and then adjust the employees' bonuses based on how much they use AI. If you are given this task, one question which you should have in mind is: How do you do this in accordance with the standard principles of AI governance? Which principles are important?

    Let's just have a quick discussion on this. Just looking at this scenario, which AI governance principles do you think might come out?

    (Student: Yeah, I think you should have a way to govern whether the AI is actually monitored correctly. Because maybe it's an administrator kind of glitch which gives a very low score even though it actually was much higher. So this goes towards the reliability of the system.)

    (Student 2: Mine was similar—Robustness. Because I need the data to be correct.)

    Yeah. So **Robustness** and **Reliability** are related concepts. The general idea is that when you run the AI system, you hope that there are no bugs essentially, right? You hope it will always function as you expect it will function. And of course, it's important because you are dealing with people's jobs and bonuses.

    (Student 3: Also, what data is this AI system going through? That's the employee's **Privacy** if, for example, you are tracking the way they use their own personal computer.)

    Exactly. So this is an example of how the AI principles can come into play in this kind of AI deployment scenario.

    至此，实际上已经有很多关于人工智能治理的材料了。许多不同的国际组织，如东盟（ASEAN）和经合组织（OECD），都提出了自己的一套人工智能治理准则。我在阅读材料中给了你们两个例子，让我们快速看一下。

    我们将查看 **OECD (2025)** 的文件，因为我认为那是通过度最高的文件。该文件已得到 OECD（经济合作与发展组织）成员的同意。我只想把大家的注意力集中在关键部分，即**值得信赖的人工智能原则（Principles of Trustworthy AI）**。在这份文件中，OECD 所做的是列出它认为的人工智能的一些基本原则。

    在 **ASEAN (2024)** 的文件中也有一套原则，但你会发现这些原则有点不同。对于东盟，你有“透明度和可用性”，这是一个共同原则。然后你有“公平和公正”，我想这在 OECD 中也出现了。你有“安全和安保”。你有“以人为本”，这与 OECD 的版本有点不同。“问责制和诚信”，然后是“稳健性和可靠性”。

    所以你可以看到，虽然原则之间有一些重叠，但它们确实有很大的不同。此外，每份文件解释每个原则的方式也可能不同。对于一份文件，“透明度”可能是一回事，但对于另一份文件，同一个词可能意味着不同的东西。所以我只想强调，目前没有一份商定的原则清单。

    鉴于没有商定的清单，我们能做的最好的事情就是尝试找出你在所有这些文件中经常看到的共同原则。因此，我们在本课程中要做的是确定共同原则，如透明度、可解释性、稳健性。我们将探讨这些词的含义，理解了它们的含义后，我们就可以尝试将这些原则应用于实际场景中。

    我在这里给出了一个例子——我们今天不会完全过一遍——但例如，你可能想要应用这些治理原则的一种情况是：想象一下，你被指派为你的公司启动一个内部 AI 系统。该 AI 系统旨在根据员工使用公司购买的 AI 工具的情况来评估他们的表现。公司希望员工使用 AI，所以他们让你设计 AI 工具来跟踪这种使用情况，然后根据他们使用 AI 的程度来调整员工的奖金。如果你接到这个任务，你应该考虑的一个问题是：你如何按照 AI 治理的标准原则来做这件事？哪些原则是重要的？

    让我们对此进行快速讨论。仅仅看这个场景，你认为可能会出现哪些 AI 治理原则？

    （学生：是的，我认为你应该有一种方法来管理 AI 是否被正确监控。因为也许是某种管理员故障导致分数很低，即使实际上分数要高得多。所以这涉及到系统的可靠性。）

    （学生2：我的看法类似——稳健性（Robustness）。因为我需要数据是正确的。）

    是的。所以**稳健性**和**可靠性**是相关的概念。总的想法是，当你运行 AI 系统时，你希望本质上没有错误，对吧？你希望它总是像你预期的那样运行。当然，这很重要，因为你正在处理人们的工作和奖金。

    （学生3：另外，这个 AI 系统正在浏览什么数据？如果例如你正在跟踪他们使用自己个人电脑的方式，那就是员工的**隐私（Privacy）**。）

    确切地说。这就是 AI 原则如何在这种 AI 部署场景中发挥作用的一个例子。

  - [14:25 - 14:29] **Soft Law vs. Hard Law (Pasqua 2025)**

    Okay, last part of the lesson. I would like to briefly talk about the global context of AI governance. This requires us to look at what is known as **Soft Law** and **Hard Law**.

    Soft law and hard law represent two different approaches that countries have taken to govern AI. We're now looking at the government standpoint: how do countries govern the use of AI?

    In Singapore, we have this **Model Governance Framework for AI**. We have a version for Generative AI and we also have a version for AI in general. This would be an example of **Soft Law**. Soft law is "soft" because it is not legally enforceable. Even if you don't follow the rules here, there is no legal sanction.

    In contrast, some other jurisdictions have adopted what we call a **Hard Law** approach, most notably the EU. The EU has recently passed this **AI Act**, which contains legal rules that, if you violate, will attract penal action. So that's hard law. China also is beginning to implement hard law rules.

    We can take a look at the article by Pasqua (2025). I won't go through the whole thing, but the author essentially makes the point that we are beginning to see soft law—which most countries are currently still doing—sort of bridge towards hard law, in the sense that soft law principles often become incorporated into hard law rules.

    I suppose my main takeaway from this trend is that it probably makes sense for an organization to try to comply with soft law AI governance to begin with. Because even if you don't care about AI governance, at some point there is a possibility that these rules will harden. And then you really have to care about it, because if you don't, there may be legal consequences.

    But the reason why a lot of countries have started with soft law is because soft law is easy to implement and easy to change. Hard law requires the legislature to sit down and debate; it requires public consultation. On the other hand, because consequences for soft law are not so serious, governments can just push it out very quickly. And if the technology changes, the soft law can also change. Whereas hard law is more static.

    So that's what we see: countries starting with soft law first, and I suppose over time, depending on how the industry changes, we may begin to see more hard law, or countries shifting towards hard power.

    Okay, so that's the main point I want to draw from that particular article. If you're interested, you can take a closer look at it.

    好的，课程的最后一部分。我想简要谈谈人工智能治理的全球背景。这需要我们看看所谓的**软法（Soft Law）\**和\**硬法（Hard Law）**。

    软法和硬法代表了各国采取的两种不同的 AI 治理方法。我们要现在看的是政府的立场：各国如何治理 AI 的使用？

    在新加坡，我们有这个**AI 监管模型框架（Model Governance Framework for AI）**。我们有一个针对生成式 AI 的版本，也有一个针对通用 AI 的版本。这就是**软法**的一个例子。软法之所以“软”，是因为它不具有法律强制力。即使你不遵守这里的规则，也没有法律制裁。

    相比之下，其他一些司法管辖区采取了我们所谓的**硬法**方法，最著名的是欧盟。欧盟最近通过了《**人工智能法案**》（AI Act），其中包含法律规则，如果你违反，将招致刑事处罚。这就是硬法。中国也开始实施硬法规则。

    我们可以看一下 Pasqua (2025) 的文章。我不会通读全文，但作者本质上指出了这一点：我们开始看到软法——大多数国家目前仍在实行的——正在向硬法过渡，即软法原则往往会被纳入硬法规则中。

    我想我对这一趋势的主要结论是，对于一个组织来说，一开始就尝试遵守软法 AI 治理可能是有意义的。因为即使你不关心 AI 治理，在某些时候，这些规则有可能变硬。那时你就真的必须关心它了，因为如果你不关心，可能会有法律后果。

    但是，许多国家从软法开始的原因是软法易于实施且易于更改。硬法需要立法机关坐下来辩论；它需要公众咨询。另一方面，由于软法的后果不那么严重，政府可以很快将其推出。如果技术发生变化，软法也可以随之变化。而硬法则是比较静态的。

    这就是我们所看到的：各国首先从软法开始，我想随着时间的推移，根据行业的变化，我们可能会开始看到更多的硬法，或者各国向硬实力转变。

    好的，这就是我想从那篇特定的文章中提取的主要观点。如果你感兴趣，可以仔细看看。
## Week 2
  - [12:02 - 12:03] **Course Administration & Reading List Updates**

    To begin, I have decided to adjust the angle of the course slightly. I realized that simply assigning the readings might be overwhelming given their length and complexity. Therefore, I have updated your reading list to include "guiding questions." These questions are designed to focus your study efforts on the most critical arguments rather than getting lost in the density of the texts. This should make the material more manageable and help you extract the key takeaways more effectively.

    首先，我决定稍微调整一下课程的切入角度。我意识到仅仅布置阅读材料可能会因为篇幅和难度让大家感到不知所措。因此，我在阅读清单中增加了“引导性问题”。设计这些问题的目的是为了让大家的学习重点集中在最关键的论点上，避免迷失在晦涩的文本中。这应该能让学习材料更易于掌握，帮助大家更有效地提取核心观点。

  - [12:03 - 12:07] **Popular Conceptions vs. Reality of Autonomous Weapons**

    Today we are covering two main topics: the ethics of autonomous weapons and autonomous vehicles. Let's start with autonomous weapons. When we use this term, the popular conception usually stems from science fiction—what we might call "killer robots." You might think of the Battle Droids from *Star Wars*, the T-800 from *Terminator*, or the ED-209 from *RoboCop*. These are the images that typically come to mind, but none of these actually exist yet; they remain in the realm of fiction.

    However, we do have existing weapons with varying degrees of autonomy. A classic, albeit crude, example is the **anti-personnel landmine**, which is unfortunately still in use today. It functions autonomously by triggering upon contact. Moving to more modern examples, consider the **AIM-9X Sidewinder** missile—think of the movie *Top Gun*. Once a pilot fires this missile, it uses infrared technology to track the heat signature of an enemy jet autonomously; the pilot effectively "fires and forgets." Another example is the **Phalanx CIWS (Close-In Weapon System)**. This is a ship-mounted automated gun used by the US Navy. It detects incoming projectiles or missiles and automatically fires a "wall of bullets" to intercept them. In this system, there is no human physically aiming the gun; the software detects the threat and engages it automatically to protect the ship.

    今天我们要讨论两个主题：自主武器的伦理问题和自动驾驶汽车。我们先从自主武器讲起。当我们提到这个词时，大众的普遍印象通常来自科幻小说，也就是所谓的“杀手机器人”。大家可能会想到《星球大战》里的战斗机器人、《终结者》里的T-800，或者是《机械战警》里的ED-209。这些是人们脑海中浮现的形象，但它们目前并不存在，仍属于科幻范畴。

    然而，我们确实拥有具备不同程度自主能力的现役武器。一个经典虽然简陋的例子是**反步兵地雷**，不幸的是它至今仍被广泛使用。它通过接触触发，在某种意义上是自主运作的。至于更现代的例子，可以看看**AIM-9X 响尾蛇导弹**——想想电影《壮志凌云》。一旦飞行员发射了这种导弹，它就会利用红外技术自动追踪敌机的热源；飞行员实际上是“发射后不管”的。另一个例子是**密集阵近程防御系统（Phalanx CIWS）**。这是美国海军舰艇上装备的一种自动火炮系统。它能探测来袭的炮弹或导弹，并自动发射“弹幕”进行拦截。在这个系统中，并没有人类在操作瞄准，软件会自动检测威胁并进行攻击以保护舰船。

  - [12:07 - 12:12] **Defining "Autonomous Weapons": The Automation of Cognition**

    How do we strictly define an "autonomous weapon"? Recalling our first lesson, we defined Artificial Intelligence (AI) as the automation of cognitive tasks. Following that logic, an autonomous weapon is a weapon system that **automates the cognitive tasks relating to its use**. Using a weapon involves both physical tasks (like loading or pulling a trigger) and cognitive tasks. The defining feature here is the automation of the "thinking" part.

    So, what are these cognitive tasks?

    1. **Context Understanding:** As a student mentioned, one task is understanding the context—specifically, is this person a threat?
    2. **Target Discrimination:** A soldier must distinguish between valid targets (enemy combatants) and protected persons (civilians, wounded soldiers). This is the act of targeting: aiming the weapon at the right object.
    3. **Engagement Decision:** The final cognitive step is the decision to use force—the actual command to "fire."

    In a traditional setting, a human makes the call to shoot. In an autonomous system, algorithms process sensor data to detect a target, identify it as an enemy, and decide to engage, potentially without human intervention. This shift from human judgment to algorithmic processing is the core ethical issue we are examining.

    我们该如何严格定义“自主武器”？回顾第一节课，我们将人工智能（AI）定义为认知任务的自动化。顺着这个逻辑，自主武器就是**自动化了与其使用相关的认知任务**的武器系统。使用武器既涉及物理任务（如装填或扣动扳机），也涉及认知任务。这里的定义特征在于“思考”部分的自动化。

    那么，这些认知任务具体指什么？

    1. **理解语境：** 正如一位同学所提到的，任务之一是理解语境——具体来说，这个人是威胁吗？
    2. **目标识别：** 士兵必须区分合法目标（敌方战斗人员）和受保护人员（平民、伤员）。这就是瞄准行为：将武器对准正确的目标。
    3. **交战决策：** 最后的认知步骤是使用武力的决定——即下达“开火”的指令。

    在传统场景中，由人类决定是否射击。而在自主系统中，算法处理传感器数据来探测目标，将其识别为敌人，并决定进行攻击，这一过程可能完全无需人类干预。这种从人类判断到算法处理的转变正是我们审视的核心伦理问题。

  - [12:13 - 12:16] **Incentives for Development: Arms Races and Force Protection**

    Why are nations incentivized to develop these weapons? There are several driving forces:

    - **Game Theory & The Arms Race:** As discussed, if your adversary develops autonomous weapons, you become vulnerable. To avoid a strategic disadvantage, you are compelled to develop them as well. This creates a classic "security dilemma" or arms race dynamic where everyone builds them simply because everyone else might.
    - **Force Protection:** Autonomous weapons reduce the risk to your own personnel. As a student noted, wars are politically unpopular when you send your own citizens to die. Robots allow a nation to project power without the emotional and political cost of human casualties on their side.
    - **Efficiency and Speed:** Machines can process information and react much faster than humans. In modern warfare, speed is critical. A system that can identify and engage a target in milliseconds offers a massive tactical advantage over a human-operated system.

    Despite the ethical concerns, these incentives create a powerful push for development. We must recognize that regardless of the moral arguments, the geopolitical reality is that major powers are actively pursuing these technologies.

    各国为何有动力去研发这些武器？这背后有几个驱动力：

    - **博弈论与军备竞赛：** 正如我们在讨论中提到的，如果你的对手研发了自主武器，你就会变得脆弱。为了避免处于战略劣势，你也不得不研发。这就造成了典型的“安全困境”或军备竞赛，大家都在造，仅仅因为别人可能也在造。
    - **兵力保护：** 自主武器降低了己方人员的风险。正如一位同学指出的，当你把自己的公民送上战场牺牲时，战争在政治上是不受欢迎的。机器人允许国家投射力量，而无需承担己方人员伤亡带来的情感和政治代价。
    - **效率与速度：** 机器处理信息和反应的速度远超人类。在现代战争中，速度至关重要。一个能在毫秒内识别并攻击目标的系统，相比人工操作系统具有巨大的战术优势。

    尽管存在伦理担忧，这些诱因构成了强大的研发推动力。我们必须认识到，无论道德论证如何，地缘政治的现实是主要大国都在积极追求这些技术。

  - [12:16 - 12:17] **Distinction: Automatic vs. Autonomous Weapons**

    It is crucial to distinguish between "autonomous" and "automatic" weapons, as people often conflate them.

    - **Automatic Weapons** automate **physical tasks**. For example, an automatic rifle uses the energy of a fired cartridge to physically extract the spent shell and chamber a new round. This is a mechanical automation of a physical process.
    - **Autonomous Weapons** automate **cognitive tasks**. This concerns the decision-making process: *Who* do I shoot? *When* do I shoot?

    In this course, we are not concerned with mechanical automation (like the rifle's loading mechanism). We are focusing entirely on the automation of the cognitive aspects—the "brain" of the weapon—and the ethical problems that arise when we delegate lethal decisions to software.

    区分“自主”武器和“自动”武器至关重要，因为人们经常混淆这两个概念。

    - **自动武器（Automatic Weapons）** 自动化的是**物理任务**。例如，自动步枪利用发射子弹的能量来机械地抛出弹壳并装填新弹。这是物理过程的机械自动化。
    - **自主武器（Autonomous Weapons）** 自动化的是**认知任务**。这涉及决策过程：我要射击*谁*？*何时*射击？

    在本课程中，我们不关注机械自动化（比如步枪的装填机制）。我们完全聚焦于认知层面的自动化——即武器的“大脑”——以及当我们把致死决策权交给软件时所引发的伦理问题。

  - [12:17 - 12:20] **Legal Landscape: The Lack of International Consensus**

    Currently, there is no definitive international consensus on the legality of autonomous weapons. While the United Nations has held discussions (specifically under the Convention on Certain Conventional Weapons), no binding ban or specific regulation currently exists for Lethal Autonomous Weapons Systems (LAWS).

    - **The Exception (Landmines):** There is consensus regarding anti-personnel landmines. The **Ottawa Treaty** prohibits their use, stockpiling, and production, and most countries have signed it. This is arguably a ban on a primitive form of autonomous weapon.
    - **Proliferation:** Despite this, development is accelerating. Major powers like China and the US are investing heavily, but it's not just them. Smaller states and even non-state actors are gaining access to these technologies, as seen in recent conflicts where drones act with increasing levels of autonomy. This proliferation suggests that without regulation, these weapons will become a common feature of future warfare.

    目前，关于自主武器的合法性，国际上尚未达成明确共识。虽然联合国已经举行了相关讨论（特别是在《特定常规武器公约》框架下），但目前针对致命自主武器系统（LAWS）尚无具有约束力的禁令或具体法规。

    - **例外（地雷）：** 关于反步兵地雷是存在共识的。**《渥太华条约》**禁止使用、储存和生产地雷，大多数国家都签署了该条约。这可以被视为对一种原始形式的自主武器的禁令。
    - **扩散：** 尽管如此，研发进程正在加速。中国和美国等大国投入巨大，但不仅限于它们。较小的国家甚至非国家行为体也开始获得这些技术，正如我们在近期冲突中看到的，无人机正表现出越来越高的自主性。这种扩散表明，如果缺乏监管，这些武器将成为未来战争的常见特征。

  - [12:20 - 12:27] **Ethical Arguments: The Responsibility Gap and Unpredictability**

    Let's examine the ethical arguments, focusing on the reading by Peter Asaro (2020), who argues that autonomous weapons are morally wrong.

    **1. Unpredictability:** A major ethical concern is the inherent unpredictability of AI. Unlike a rifle where the bullet goes where you aim it, an autonomous system given a general command (e.g., "clear this sector") makes its own decisions on how to execute that task.

    - **Interaction Effects:** As a student pointed out, we face a "black box" problem. We don't fully know how the neural networks will behave in novel situations. Furthermore, when algorithms from opposing sides interact on the battlefield, the outcome is mathematically impossible to predict perfectly.
    - **Differentiation Failure:** There is a high risk that the system will fail to distinguish between a soldier and a civilian, or between a surrender signal and a threat. Who is responsible when this differentiation goes wrong and a war crime is committed?

    **2. The Responsibility Gap:** This leads to Asaro's core argument. If an autonomous weapon commits a war crime (e.g., bombing a hospital), who is morally and legally responsible?

    - **The Commander?** They didn't pull the trigger or specifically order that specific attack; they just deployed the system.
    - **The Programmer?** They wrote the code years ago and couldn't predict this specific battlefield scenario.
    - **The Machine?** You cannot put a robot in jail or hold it morally culpable.

    This creates a "Responsibility Gap"—a situation where a terrible crime occurs, but no human can be justly held accountable. Asaro argues this gap makes the use of such weapons immoral because it erodes the legal and moral framework of warfare.

    我们来审视一下伦理论点，重点关注彼得·阿萨罗（Peter Asaro, 2020）的文章，他认为自主武器在道德上是错误的。

    **1. 不可预测性：** 一个主要的伦理担忧是人工智能固有的不可预测性。步枪的子弹会飞向你瞄准的地方，但自主系统在接收到一个概括性指令（例如“清理该区域”）后，会自行决定如何执行任务。

    - **交互效应：** 正如一位同学指出的，我们要面对“黑箱”问题。我们并不完全清楚神经网络在新奇情境下会如何表现。此外，当敌对双方的算法在战场上互动时，其结果在数学上是不可能完美预测的。
    - **区分失败：** 系统很有可能无法区分士兵和平民，或者无法区分投降信号和威胁。当这种区分出错导致战争罪行时，谁该负责？

    **2. 责任鸿沟（Responsibility Gap）：** 这引出了阿萨罗的核心论点。如果自主武器犯下战争罪行（例如轰炸医院），谁在道德和法律上负责？

    - **指挥官？** 他们没有扣动扳机，也没有具体下令进行那次特定的攻击；他们只是部署了系统。
    - **程序员？** 他们是几年前写的代码，无法预测这个特定的战场场景。
    - **机器？** 你不能把机器人关进监狱，也不能让它承担道德罪责。

    这就造成了“责任鸿沟”——即发生可怕罪行却没有任何人能被公正地追责。阿萨罗认为，这种鸿沟使得使用此类武器在道德上是错误的，因为它侵蚀了战争的法律和道德框架。

  - [12:27 - 12:35] **Debating Responsibility: Risk vs. Control**

    How do we resolve this responsibility gap? We discussed two main approaches in class:

    - **The Risk Argument (Strict Liability):** One view is that if you deploy a dangerous system, you accept the risk. Therefore, the commander is responsible for *any* damage the machine causes, regardless of whether it was a "glitch." If you release a tiger into a village, you are responsible for who it eats, even if you can't control the tiger. This approach effectively imposes strict liability on the user.
    - **The Control Argument (Meaningful Human Control):** The counter-argument is that moral responsibility requires control. If the weapon is truly autonomous—meaning the human had no input on the specific targeting decision—can we fairly blame the human? This is why many ethicists and the UN advocate for **"Meaningful Human Control" (MHC)**. The argument is that for a weapon to be ethical, a human must retain a significant level of agency in the loop to validate targets and intervention. Without this control, responsibility cannot land anywhere, which brings us back to the immorality of the weapon.

    我们该如何解决这个责任鸿沟？我们在课堂上讨论了两种主要方法：

    - **风险论点（严格责任）：** 一种观点是，如果你部署了一个危险系统，你就承担了风险。因此，指挥官要对机器造成的*任何*损害负责，不管那是不是“故障”。就像你把老虎放进村庄，你就得对它吃人负责，即使你控制不了老虎。这种方法实际上是对使用者施加了严格责任。
    - **控制论点（有意义的人类控制）：** 反方论点是，道德责任需要控制权。如果武器是真正自主的——意味着人类对具体的瞄准决策没有投入——我们能公平地指责人类吗？这就是为什么许多伦理学家和联合国倡导**“有意义的人类控制”（Meaningful Human Control, MHC）**。其论点是，为了使武器符合伦理，人类必须在回路中保留相当程度的代理权，以验证目标和进行干预。如果没有这种控制，责任就无处着落，这又把我们带回了武器本身的不道德性。

  - [12:35 - 12:50] **Human Dignity and the Threshold of War**

    Beyond responsibility, there is a deontological argument regarding **Human Dignity**. Asaro argues that it is fundamentally dehumanizing to be killed by a machine. Killing in war is only "legal" because it is a decision made by a moral agent acting under necessity. An algorithm has no concept of the value of human life; it treats a human target merely as a data point to be processed. Therefore, delegating the decision to kill to a machine violates the dignity of the victim.

    Finally, we touched on the broader societal implications:

    - **Lowering the Threshold of War:** If using autonomous weapons carries no risk of casualties for the attacker (0% casualty rate for your own side), leaders might be more tempted to solve political disputes with force rather than diplomacy. This effectively "gamifies" war.
    - **The "Skynet" Risk:** While some might call it science fiction, there is a non-zero probability of escalation where AI systems interact in ways that spiral out of control—potentially overriding human objectives entirely. While I personally think the "Terminator/Skynet" scenario is low probability, the risk of rapid, unintended escalation (flash wars) is real.

    We will pause here and resume with the counter-arguments after the break.

    除了责任问题，还有一个关于**人类尊严**的义务论论点。阿萨罗认为，被机器杀死在根本上是去人性化的。战争中的杀戮之所以“合法”，是因为这是道德主体在必要性下做出的决定。算法对人类生命的价值没有任何概念；它仅仅将人类目标视为待处理的数据点。因此，将杀戮决定权交给机器侵犯了受害者的尊严。

    最后，我们触及了更广泛的社会影响：

    - **降低战争门槛：** 如果使用自主武器对攻击者来说没有伤亡风险（己方零伤亡），领导人可能会更倾向于用武力而不是外交手段来解决政治争端。这实际上将战争“游戏化”了。
    - **“天网”风险：** 虽然有人称之为科幻小说，但确实存在非零的升级概率，即AI系统的互动方式失控——可能完全覆盖人类的目标。虽然我个人认为“终结者/天网”情景的概率很低，但快速、意外的冲突升级（闪电战）的风险是真实的。

    我们先讲到这里，休息之后再继续讨论反方观点。

  - [13:01 - 13:04] **Counter-Arguments: The Ethics of Non-Lethal Engagement**

    Let's consider the counter-arguments regarding the morality of autonomous weapons. Earlier, we argued that machines lack empathy, which is a negative trait. However, one could argue that this lack of emotion is actually a benefit. Human soldiers are affected by stress, fear, and anger, which can lead to rash decisions or war crimes. An autonomous system is immune to these psychological pressures. It will not act out of revenge or panic.

    Furthermore, there is an interesting argument from your readings (referencing Haiden) about the potential for non-lethal engagement. Consider a human soldier: if they are threatened, they are likely to shoot to kill because they fear for their own life—they want to go home safely. They cannot take the risk of using non-lethal force (like rubber bullets) if the enemy has a real gun. However, a robot is "disposable." It has no self-preservation instinct. Therefore, we can imagine a future where autonomous units are programmed to take risks that humans cannot, such as getting close enough to disarm a combatant or using non-lethal weapons, potentially resulting in fewer overall deaths in conflict.

    让我们来思考关于自主武器道德性的反方论点。早些时候，我们认为机器缺乏同理心，这是一个负面特征。然而，有人可能会争辩说，缺乏情感实际上是一种优势。人类士兵会受到压力、恐惧和愤怒的影响，这可能导致鲁莽的决定或战争罪行。而自主系统对这些心理压力免疫，它不会出于复仇或恐慌而采取行动。

    此外，你们的阅读材料中（参考Haiden）提出了一个关于非致命交战潜力的有趣论点。试想一名人类士兵：如果受到威胁，他们很可能会为了自保而开枪射杀——因为他们想活着回家。如果敌人持有真枪，他们无法冒险使用非致命武力（如橡胶子弹）。然而，机器人是“可消耗的”。它没有自我保护的本能。因此，我们可以想象在未来，自主作战单位被编程去承担人类无法承担的风险，例如靠近解除战斗人员的武装或使用非致命武器，这最终可能会减少冲突中的总死亡人数。

  - [13:04 - 13:07] **Algorithmic Bias and the Dignity Argument**

    A student raised a crucial point about the developers of these weapons. Since algorithms are written by humans, they inevitably inherit the biases and discrimination of their creators. We see this moral issue clearly in other AI domains, such as predictive policing or medical AI, where racial or ethnic bias is a documented problem. While we haven't yet seen definitive examples of "unfair targeting" in autonomous weapons specifically, it is a mathematically probable risk that these systems could disproportionately target specific demographics based on biased training data.

    However, returning to the core ethical debate, the strongest argument against these weapons often comes back to **Human Dignity**. As cited in the article by Asaro, the argument is that being killed by a machine denies you your fundamental human status. It is a "common sense" moral intuition: even in the brutality of war, there is a belief that the decision to end a life must be made by a moral agent who understands the gravity of that act. A machine treats you as an object to be processed, not a human to be engaged, and for many ethicists, that violation of dignity is sufficient grounds to ban them.

    一位同学提出了关于这些武器开发者的关键一点。由于算法是由人类编写的，它们不可避免地会继承创造者的偏见和歧视。我们在其他人工智能领域（如预测性警务或医疗人工智能）中清楚地看到了这一道德问题，种族或民族偏见在这些领域已是有据可查的问题。虽然我们尚未在自主武器中看到“不公平目标锁定”的确切案例，但在数学上存在极大的风险，即这些系统可能基于有偏差的训练数据，不成比例地针对特定人群。

    然而，回到核心的伦理辩论，反对这些武器的最强有力论据往往回归到**人类尊严**。正如阿萨罗（Asaro）的文章所引用的，该论点认为被机器杀死剥夺了你作为人的基本地位。这是一种“常识性”的道德直觉：即使在残酷的战争中，人们也认为结束生命的决定必须由一个理解该行为严重性的道德主体做出。机器把你当作一个待处理的物体，而不是一个需要去交战的人类，对于许多伦理学家来说，这种对尊严的侵犯足以成为禁止它们的理由。

  - [13:07 - 13:13] **Introduction to Autonomous Vehicles and SAE Levels**

    Let's move on to a technology you are more likely to encounter in daily life: **Autonomous Vehicles (AVs)**. Broadly speaking, AVs are not new; autopilots for planes and ships have existed for decades. However, our focus here is on self-driving cars.

    To define what counts as an AV, we use the industry standard **SAE J3016**, which defines 6 levels of driving automation (Level 0 to Level 5):

    - **Levels 0-2 (Driver Support):** The human is still driving. Level 0 involves simple warnings or momentary assistance (like emergency braking). Level 1 is simple assistance like lane centering *or* cruise control. Level 2 (like current Tesla Autopilot) handles steering and acceleration, but the driver must keep their eyes on the road and be ready to take over instantly.
    - **Levels 3-5 (Automated Driving):** The system drives. Level 3 is a "traffic jam chauffeur"—you can take your eyes off the road, but must intervene if the car requests it. Level 4 is "High Automation," like a driverless taxi in a geofenced area (no steering wheel needed). Level 5 is "Full Automation," where the car can drive anywhere a human can, under any conditions.

    我们接着讨论一项大家在日常生活中更可能接触到的技术：**自动驾驶汽车（AVs）**。广义上讲，自动驾驶并不新鲜；飞机和船舶的自动驾驶仪已经存在几十年了。然而，我们这里的重点是自动驾驶汽车。

    为了定义什么是自动驾驶汽车，我们使用行业标准 **SAE J3016**，它将驾驶自动化定义为6个等级（L0到L5）：

    - **L0-L2（驾驶员辅助）：** 人类仍在驾驶。L0涉及简单的警告或瞬间辅助（如紧急制动）。L1是简单的辅助，如车道居中*或*巡航控制。L2（像目前的特斯拉Autopilot）控制转向和加速，但驾驶员必须注视道路并准备随时接管。
    - **L3-L5（自动驾驶）：** 系统负责驾驶。L3是“交通拥堵代驾”——你可以把目光移开道路，但如果汽车发出请求，必须进行干预。L4是“高度自动化”，比如地理围栏区域内的无人驾驶出租车（不需要方向盘）。L5是“完全自动化”，汽车可以在任何人类能驾驶的地方、任何条件下行驶。

  - [13:13 - 13:16] **Regulation and Liability in Singapore and Beyond**

    In terms of regulation, Singapore currently operates under the **Road Traffic (Autonomous Motor Vehicles) Rules 2017**. The key takeaway from Section 4 is that you cannot undertake a trial or use an autonomous vehicle on a public road without specific authorization. While you can own a Tesla, using its "Full Self-Driving" (beta) features hands-free is generally restricted. We are currently in a phase of small-scale trials, such as autonomous shuttles in Punggol.

    The pressing ethical and legal issue right now is **liability**. Who pays when an AV crashes?

    - **Case 1:** A Florida jury recently ordered Tesla to pay **$313 million** regarding a fatal Autopilot crash.

    - **Case 2:** A Waymo vehicle in Denver parked itself in a bike lane. The company argued it wasn't the software's fault but rather a human driver who had previously positioned it.

      This highlights the complexity: responsibility is often shifted between the manufacturer, the software, and the human operator.

    在监管方面，新加坡目前执行的是**2017年《道路交通（自动驾驶机动车）规则》**。第4节的关键点是，未经特别授权，不得在公共道路上进行试验或使用自动驾驶汽车。虽然你可以拥有一辆特斯拉，但通常限制使用其“完全自动驾驶”（测试版）的脱手功能。我们目前正处于小规模试验阶段，例如榜鹅（Punggol）的自动穿梭巴士。

    目前紧迫的伦理和法律问题是**责任**。当自动驾驶汽车发生事故时，谁来赔偿？

    - **案例1：** 佛罗里达州陪审团最近因一起致命的自动驾驶事故，判决特斯拉赔偿**3.13亿美元**。

    - **案例2：** 丹佛的一辆Waymo汽车停在了自行车道上。该公司辩称这不是软件的错，而是之前人类驾驶员停放位置的问题。

      这凸显了复杂性：责任往往在制造商、软件和人类操作员之间推来推去。

  - [13:16 - 13:30] **The Moral Responsibility Framework**

    So, who *should* be morally responsible? Let's break down the potential parties:

    1. **The Manufacturer/Developer:** They built the algorithm.
    2. **The Operator:** For example, a taxi company deploying a fleet.
    3. **The Regulator:** The government that certified the car as safe.
    4. **The Passenger/Driver:** The person inside.

    As a student noted, if you are asleep in a Level 4/5 car, you have **zero control**, so morally, you shouldn't be liable. However, liability is a spectrum.

    - If it's a Level 2 car, the driver is responsible for supervision.
    - If the driver failed to maintain the car (e.g., muddy sensors), they might be liable.
    - If the government failed to maintain road infrastructure (e.g., broken traffic lights causing the AI to fail), the state might be liable.
    - There is also the **"Strict Liability"** argument raised by a student (and discussed in the Hevelke reading): The person who puts the car on the road creates a risk for their own benefit/convenience, and therefore should bear the cost of any accidents, regardless of fault.

    那么，谁*应该*承担道德责任？让我们分析一下潜在的当事方：

    1. **制造商/开发者：** 他们构建了算法。
    2. **运营商：** 例如部署车队的出租车公司。
    3. **监管者：** 认证汽车安全的政府。
    4. **乘客/驾驶员：** 车里的人。

    正如一位同学指出的，如果你在L4/L5级汽车里睡觉，你拥有**零控制权**，所以在道德上你不应该负责。然而，责任是一个通过谱系。

    - 如果是L2级汽车，驾驶员负责监督。
    - 如果驾驶员未能维护汽车（例如传感器被泥土覆盖），他们可能要负责。
    - 如果政府未能维护道路基础设施（例如交通灯故障导致AI失效），国家可能要负责。
    - 还有一位同学提出的（在Hevelke的阅读材料中讨论过的）**“严格责任”**论点：将汽车开上路的人为了自己的利益/便利创造了风险，因此无论是否有过错，都应承担任何事故的成本。

  - [13:30 - 13:39] **Collective Responsibility and the Legal-Moral Divergence**

    Following the reading by **Hevelke (2015)**, there is a proposal for **Collective Responsibility**. Since autonomous cars generally reduce accidents but introduce new types of risks, perhaps all users should contribute to a mandatory insurance pool. When a crash happens, the compensation comes from this pool rather than blaming an individual.

    This leads to a critical question: *Should legal responsibility match moral responsibility?*

    Morally, we might say the manufacturer is responsible for a coding error. However, from a **policy perspective**, if we make manufacturers 100% legally liable for every accident, they will go bankrupt or refuse to release the technology. Since AVs could save thousands of lives overall, we might design laws that limit manufacturer liability to encourage innovation. This is a clear example where legal expediency might diverge from strict moral culpability.

    根据**Hevelke (2015)\**的阅读材料，有一个关于\**集体责任**的提议。由于自动驾驶汽车总体上减少了事故，但也引入了新型风险，也许所有用户都应该向一个强制保险池供款。当事故发生时，赔偿金来自这个资金池，而不是指责个人。

    这引出了一个关键问题：*法律责任是否应该与道德责任相匹配？*

    在道德上，我们可能会说制造商应对代码错误负责。然而，从**政策角度**来看，如果我们让制造商对每一起事故承担100%的法律责任，他们将会破产或拒绝发布该技术。由于自动驾驶汽车总体上可以挽救成千上万的生命，我们可能会制定限制制造商责任的法律以鼓励创新。这是一个法律权宜之计可能与严格的道德罪责相背离的明显例子。

  - [13:39 - 13:51] **Crash Algorithms: The Trolley Problem**

    Now we confront the "Crash Algorithms" (referencing **Ungern-Sternberg**). If a crash is unavoidable, what should the car prioritize?

    1. **Minimize Harm:** Ideally, the car should choose to hit an inanimate object (like a wall) rather than a person, or choose a path that causes injury rather than death.
    2. **The Trolley Problem:** What if the choice is between hitting a wall (killing the 5 passengers) or swerving to hit a pedestrian (killing 1 person)?
       - **Utilitarianism:** Save the 5. It acts for the "greater good."
       - **Deontology:** Swerving is an intentional act of killing an innocent bystander, which is murder. You cannot trade one life for another.

    This is an unsolved philosophical problem. In fact, in jurisdictions like Germany, it is arguably **illegal** to program a car to sacrifice one life to save others, as human lives are considered incommensurable (cannot be measured against each other).

    现在我们面临“碰撞算法”的问题（参考**Ungern-Sternberg**）。如果碰撞不可避免，汽车应该优先考虑什么？

    1. **最小化伤害：** 理想情况下，汽车应该选择撞击无生命的物体（如墙壁）而不是人，或者选择造成受伤而不是死亡的路径。
    2. **电车难题：** 如果选择是在撞墙（导致5名乘客死亡）和转向撞击行人（导致1人死亡）之间呢？
       - **功利主义：** 救那5个人。这是为了“更伟大的利益”。
       - **义务论：** 转向是故意杀害无辜旁观者的行为，这就是谋杀。你不能用一条生命去换取另一条生命。

    这是一个未解的哲学问题。事实上，在像德国这样的司法管辖区，编写程序让汽车牺牲一条生命来拯救其他人可能在**法律上是禁止的**，因为人类生命被认为是不可通约的（不能相互衡量）。

  - [13:52 - 13:59] **Selection of Victims: The "Clean Hands" Argument**

    The final and most difficult scenario: If the car must kill either Person A or Person B (e.g., both are pedestrians), how does it choose?

    - **Equality Argument:** All human lives are equal. The car should not choose based on age, profession, or social value. Random choice might be the only ethical option.
    - **"Clean Hands" Argument:** A student suggested that if Person A is walking on the sidewalk and Person B is jaywalking (breaking the rules), the car should save Person A. This aligns with the intuition that we should protect those who are following the law.

    While mathematically we might want the car to calculate "path of least physical damage," embedding moral judgments about *who* is worth saving is incredibly dangerous and controversial.

    最后也是最困难的场景：如果汽车必须在杀死A和B之间做出选择（例如两人都是行人），它该如何选择？

    - **平等论点：** 所有人的生命都是平等的。汽车不应根据年龄、职业或社会价值进行选择。随机选择可能是唯一符合伦理的选项。
    - **“清白之手”（Clean Hands）论点：** 一位同学建议，如果A走在人行道上，而B在乱穿马路（违反规则），汽车应该救A。这符合我们应该保护守法者的直觉。

    虽然在数学上我们可能希望汽车计算“物理伤害最小的路径”，但植入关于*谁*值得拯救的道德判断是极其危险和充满争议的。

  - [14:07 - 14:13] **Ethics of Crash Algorithms: The Problem of Free Choice**

    We now face the question raised by Von Ungern-Sternberg: Should a driver or manufacturer be free to decide the "crash algorithm" of their vehicle? Imagine a scenario where a manufacturer offers you a range of ethical settings—a "Self-Preservation Mode" (protects the driver at all costs) versus an "Altruistic Mode" (minimizes total harm, even if it sacrifices the driver).



    There are several strong arguments against allowing this free choice:

    - **The Market Failure Argument:** If left unregulated, rational self-interest dictates that almost everyone would choose "Self-Preservation Mode." Manufacturers that fail to offer a "Protect Driver" option would simply be wiped out of the market because no one wants to buy a car that might decide to kill them. This leads to a "race to the bottom" in ethical standards.
    - **The Public Trust Argument:** If pedestrians know that every autonomous vehicle on the road is programmed to prioritize the driver's life over theirs, public trust in the technology would collapse. You would be terrified to walk on the sidewalk knowing that a car would swerve into you to save its owner from a minor injury.
    - **The Responsibility Argument:** Conversely, if we *do* allow choice, it changes the liability equation. If a driver explicitly selects "Self-Preservation Mode," they are making a premeditated decision to prioritize their life over others. In that case, they—not the manufacturer—should perhaps be held fully morally and legally accountable for the resulting deaths.

    However, there is a counter-argument: Today, human drivers *do* have this choice, albeit implicitly. In a split-second emergency, a human driver reacts instinctively, often to save themselves. If we currently allow humans this autonomy, why is it unethical to allow them to pre-program the same instinct into their car?

    我们现在面临冯·翁恩-斯特恩伯格（Von Ungern-Sternberg） 提出的问题：驾驶员或制造商是否有权决定车辆的“碰撞算法”？想象一下，如果制造商为你提供一系列伦理设置选项——“自我保护模式”（不惜一切代价保护驾驶员）与“利他模式”（最小化总伤害，即使牺牲驾驶员）。



    反对允许这种自由选择有几个强有力的论据：

    - **市场失灵论点：** 如果缺乏监管，理性的利己主义决定了几乎每个人都会选择“自我保护模式”。那些不提供“保护驾驶员”选项的制造商将直接被市场淘汰，因为没人愿意买一辆可能会决定杀死自己的车 。这将导致伦理标准的“逐底竞争”。
    - **公众信任论点：** 如果行人知道路上的每一辆自动驾驶汽车都被编程为优先考虑驾驶员的生命而不是他们的生命，公众对该技术的信任将会崩塌。当你走在人行道上，知道一辆车为了让车主免受轻伤可能会转向撞向你，你会感到恐惧。
    - **责任论点：** 反过来说，如果我们*确实*允许选择，这会改变责任的平衡。如果驾驶员明确选择了“自我保护模式”，他们就是在做一个预谋的决定，即把自己的生命置于他人之上。在这种情况下，导致死亡的完全道德和法律责任或许应由他们——而非制造商——来承担。

    然而，也存在反方论点：今天，人类驾驶员*确实*拥有这种选择权，尽管是隐性的。在瞬间的紧急情况下，人类驾驶员会本能地做出反应，往往是为了自保。如果我们目前允许人类拥有这种自主权，为什么允许他们将同样的本能预编程到汽车中就是不道德的呢？

  - [14:14 - 14:17] **Societal Implications: External Control**

    Moving beyond crash ethics, we must consider the broader societal implications of autonomous vehicles (AVs), as discussed in the reading by Hansson. The first major implication is **External Control**. Once cars are networked and automated, it opens the door for third parties to control where your vehicle goes.

    - **Positive Use Case:** A traffic regulator could centrally reroute all autonomous cars to instantly clear a path for an ambulance or fire engine. This increases overall safety.
    - **Ethical Concern:** This removes your individual freedom. You are no longer the captain of your ship; you are a packet in a data stream.
    - **Corruption Risk:** Hansson raises a fascinating concern about commercial influence. What if a manufacturer makes a deal with a fast-food chain? Your car might subtly alter its route to drive you past a specific McDonald's or Starbucks to increase the likelihood of you stopping. This introduces commercial bias into physical navigation.

    除了碰撞伦理，我们必须考虑自动驾驶汽车（AVs）更广泛的社会影响，正如汉森（Hansson）的阅读材料中所讨论的那样 。第一个主要影响是**外部控制（External Control）** 。一旦汽车实现联网和自动化，就为第三方控制你的车辆去向打开了大门。

    - **正面用例：** 交通监管部门可以集中重新规划所有自动驾驶汽车的路线，瞬间为救护车或消防车腾出通道。这提高了整体安全性。
    - **伦理担忧：** 这剥夺了你的个人自由。你不再是你这艘“船”的船长；你只是数据流中的一个数据包。
    - **腐败风险：** 汉森提出了一个关于商业影响的有趣担忧。如果制造商与快餐连锁店达成协议会怎样？你的汽车可能会巧妙地改变路线，带你经过特定的麦当劳或星巴克，以增加你停车消费的可能性。这将商业偏见引入了物理导航中。

  - [14:17 - 14:19] **Societal Implications: Privacy and Surveillance**

    The second implication is **Privacy**. While GPS tracking exists today on our phones, widespread AV adoption takes this to a new level. If every vehicle is constantly broadcasting its location, speed, and destination to a central server to coordinate traffic, we create a pervasive surveillance network.



    Companies could use this data not just to train their driving models, but to profile users. They would know exactly when you leave home, where you work, where you shop, and who you visit. If this data is leaked or sold, it constitutes a massive violation of user privacy. We must ask: Is the convenience of self-driving worth the loss of anonymity in our physical movements?

    第二个影响是**隐私（Privacy）** 。虽然如今我们的手机上已存在GPS定位，但自动驾驶汽车的广泛采用将这一问题提升到了新的高度。如果每辆车都在不断向中央服务器广播其位置、速度和目的地以协调交通，我们就建立了一个无孔不入的监控网络。



    公司不仅可以利用这些数据训练驾驶模型，还可以对用户进行画像。他们会确切知道你何时离家、在哪里工作、在哪里购物以及拜访谁。如果这些数据被泄露或出售，将构成对用户隐私的巨大侵犯。我们必须追问：自动驾驶的便利性是否值得以丧失我们在物理移动中的匿名性为代价？

  - [14:19 - 14:24] **Societal Implications: Employment, Inequality, and Crime**

    Third, we must look at the socio-economic impacts:

    - **Job Displacement:** The widespread adoption of AVs threatens the livelihoods of millions of professional drivers—taxi drivers, truck drivers, and bus drivers. We face a future of structural unemployment for this demographic.
    - **Inequality and the "Gig Economy":** Conversely, capital owners stand to gain. If I own a Tesla, I can send it out as a "robotaxi" to earn money while I sleep. This exacerbates inequality: those with capital (the car owners) get richer, while those who rely on labor (the taxi drivers) lose their income.
    - **Facilitation of Crime:** While AVs prevent drunk driving, they might facilitate new types of crime. Criminals could use empty AVs to transport illicit goods (drugs, weapons) without risking capture themselves.
    - **Loss of Human Agency:** Finally, for some, driving is a passion. A ban on human driving to achieve perfect safety would rob hobbyists of their pleasure. From a utilitarian standpoint, this loss of joy is a negative consequence that must be weighed against safety gains.

    第三，我们必须审视社会经济影响 ：

    - **就业置换：** 自动驾驶汽车的广泛采用威胁着数百万职业司机的生计——出租车司机、卡车司机和公交司机。这一人群将面临结构性失业的未来。
    - **不平等与“零工经济”：** 相反，资本所有者将获益。如果我拥有一辆特斯拉，我可以把它作为“机器人出租车”派出去，在我睡觉时赚钱。这加剧了不平等：拥有资本的人（车主）变得更富，而依赖劳动的人（出租车司机）失去收入。
    - **助长犯罪：** 虽然自动驾驶汽车杜绝了酒驾，但它们可能助长新型犯罪。罪犯可以利用空载的自动驾驶汽车运输非法货物（毒品、武器），而无需自己承担被捕的风险。
    - **人类能动性的丧失：** 最后，对一些人来说，驾驶是一种激情。为了实现完美安全而禁止人类驾驶，将剥夺爱好者的乐趣。从功利主义角度来看，这种乐趣的丧失是一种必须与安全收益相权衡的负面后果。

  - [14:24 - 14:28] **Societal Implications: Environmental Impact & Conclusion**

    Fourth, consider the **Environmental Impact**.

    - **The Optimistic View:** AVs drive more efficiently than humans (smoother acceleration, less braking), which reduces fuel consumption and emissions.
    - **The "Rebound Effect" (Jevons Paradox):** However, if AVs make traveling cheap, easy, and convenient, people might abandon public transport (buses, trains) in favor of private autonomous pods. This could lead to *more* cars on the road, increased congestion, and higher overall energy consumption.

    **Summary & Closing:**

    To recap, today we covered:

    1. **Autonomous Weapons:** The debate on morality, the responsibility gap, and human dignity.
    2. **Autonomous Vehicles:** The levels of automation (SAE), the question of moral vs. legal liability in crashes, the ethics of crash algorithms (Trolley Problem), and the broader social implications (Privacy, Jobs, Environment).

    For your next steps, please refer to the **guiding questions** I have uploaded to the reading list. The readings themselves (especially the legal ones) can be dense, so use the questions to focus your study on the key moral arguments rather than getting bogged down in technical details.

    第四，考虑**环境影响**。

    - **乐观观点：** 自动驾驶汽车比人类驾驶更高效（加速更平稳，制动更少），这能降低燃料消耗和排放。
    - **“回弹效应”（杰文斯悖论）：** 然而，如果自动驾驶汽车让出行变得廉价、轻松且便捷，人们可能会放弃公共交通（公交、火车），转而选择私人自动驾驶舱。这可能导致路上的汽车*更多*，加剧拥堵，并提高整体能源消耗。

    **总结与结束语：**

    回顾一下，今天我们涵盖了：

    1. **自主武器：** 关于道德性、责任鸿沟和人类尊严的辩论。
    2. **自动驾驶汽车：** 自动化分级（SAE）、碰撞中的道德与法律责任问题、碰撞算法的伦理（电车难题），以及更广泛的社会影响（隐私、就业、环境）。

    关于接下来的安排，请参考我上传到阅读清单中的**引导性问题**。阅读材料本身（尤其是法律类的）可能很晦涩，所以请利用这些问题将学习重点集中在关键的道德论证上，而不要深陷于技术细节中。
## Week 3
  - [12:06 - 12:07] **The Social Value of Economic Competition**

    Competition is fundamentally driven by the goal of attracting customers. To do this, firms must invest in development to improve their existing products or create entirely new ones that people actually want to buy. From a consumer standpoint, this is a clear positive because it results in a marketplace filled with better, more innovative products. A third reason why economic competition is viewed so favorably in society is its effect on efficiency. Firms that are uncompetitive—meaning those that are inefficient or fail to sell products aligned with consumer desires—are forced to exit the market. This process is beneficial for society as a whole because when these firms exit, they free up critical resources such as land, manpower, capital, and raw materials. These resources are then redistributed to more efficient firms that can utilize them more effectively. While this might be difficult for the non-competitive business itself, the overall outcome for society is positive. Of course, if you are the business owner, your perspective is different. While competition is great for consumers, a business would generally prefer to be a monopoly, allowing them to set any price they want without worrying about customers choosing a rival.

    竞争从根本上说是以吸引顾客为目标的。为了实现这一点，企业必须投入研发，改进现有产品或创造人们真正想购买的新产品。从消费者的角度来看，这显然是一件好事，因为它让市场充满了更好、更具创新性的产品。经济竞争被社会视为好事的第三个原因是它对效率的影响。那些没有竞争力的企业——即那些效率低下或无法销售符合消费者需求产品的企业——将被迫退出市场。这个过程对整个社会是有益的，因为当这些企业退出时，它们释放了土地、人力、资本和原材料等关键资源。这些资源随后被重新分配给更高效的企业，从而得到更有效的利用。虽然这对那些没有竞争力的企业本身来说可能很困难，但对社会来说，总体结果是积极的。当然，如果你是企业主，你的观点就会不同。虽然竞争对消费者来说很棒，但企业通常更希望成为垄断者，这样他们就可以随意定价，而不必担心顾客选择竞争对手。

  - [12:07 - 12:09] **Introduction to Anti-Competitive Practices and Law**

    Because firms often prefer to avoid the pressures of fair competition—like lowering prices and raising quality—they may instead choose to engage in anti-competitive practices. Competition law, or antitrust law as it is known in some jurisdictions, identifies three main species of these prohibited practices. First, there are anti-competitive agreements where firms collude rather than compete. Second, if a firm is a strong or dominant player, it might "abuse" that dominance to keep rivals off the market. Third, there are anti-competitive mergers, where a firm simply buys out or merges with its competitor to eliminate the rivalry. These practices are strictly regulated. In Singapore, we are governed by the Competition Act 2004. In Europe, they have the Treaty on the Functioning of the European Union (TFEU), specifically Articles 101 and 102, and in America, there are various other laws like the Sherman Act and the Clayton Act.

    由于企业通常希望规避公平竞争的压力——比如降低价格和提高质量——它们可能会选择参与反竞争行为。竞争法（在某些司法管辖区被称为反托拉斯法）确定了这三类主要禁止的行为。首先是反竞争协议，即企业之间进行串通而非竞争。其次，如果一家企业是强大的或占据主导地位的参与者，它可能会“滥用”这种主导地位，将竞争对手排挤出市场。第三是反竞争合并，即一家企业直接收购或与竞争对手合并以消除竞争。这些行为受到严格监管。在新加坡，我们受 2004 年《竞争法》管辖。在欧洲，他们有《欧盟运行条约》（TFEU），特别是第 101 条和第 102 条；而在美国，则有《谢尔曼法》和《克莱顿法》等各种法律。

  - [12:09 - 12:13] **Deep Dive: Anti-Competitive Agreements in Singapore**

    An anti-competitive agreement is essentially any arrangement between competitors designed to reduce competition between them. A classic example is price-fixing. Using our local "economy rice" (cai fan) seller as an example: instead of lowering his price to beat a neighbor, he might go to his competitors and agree that they will all maintain a high price together so they don't have to fight. Another very common type in Singapore is bid-rigging. This typically happens in construction tenders. If a customer calls for a tender to upgrade a building, two firms might agree that in this round, Firm A will set a high price and let Firm B win with a slightly lower price, with the understanding that they will swap roles in the next tender. We also see market-sharing agreements, where two big competitors split Singapore geographically—one taking the West and the other taking the East—so they never have to compete for the same customers.

    Real-world examples are frequently in the news. We had the "chicken cartel" in Singapore where 13 chicken firms, supplying over 90% of fresh chicken products, were fined 26.9 million dollars for price-fixing and entering into non-compete deals. Then there were construction firms fined for rigging bids on community club upgrade contracts. An interesting third case involved ten financial advisory firms that collectively bullied a competitor, threatening a boycott unless that competitor stopped offering discounts or raised its prices. In Singapore, these are prohibited under Section 34 of the Competition Act.

    反竞争协议本质上是竞争对手之间旨在减少竞争的任何安排。一个经典的例子是价格操纵。以本地的“经济饭”摊贩为例：他可能不会通过降价来击败邻居，而是去找竞争对手商量，大家一起维持高价，这样就不必竞争了。在新加坡，另一种非常常见的类型是围标。这通常发生在建筑投标中。如果客户招标扩建建筑物，两家公司可能会达成协议：这一轮由 A 公司出高价，让 B 公司以略低的价格中标，条件是下一轮招标时双方互换角色。我们还看到了市场瓜分协议，两家大型竞争对手按地理位置划分新加坡——一家占领西部，另一家占领东部——这样他们就永远不必争夺相同的客户。

    现实中的例子经常出现在新闻里。新加坡曾出现过“鸡肉卡特尔”案，13 家供应超过 90% 新鲜鸡肉产品的公司因操纵价格和签订非竞争协议被罚款 2690 万新元。还有建筑公司因在民众俱乐部升级合同中围标而被罚款。第三个有趣的案例涉及十家财务咨询公司，它们集体欺凌一名竞争对手，威胁说除非该对手停止提供折扣或提高价格，否则将对其进行抵制。在新加坡，这些行为根据《竞争法》第 34 条是被禁止的。

  - [12:13 - 12:18] **Deep Dive: Abuse of Dominant Position**

    The next category is the abuse of dominance, prohibited under Section 47 of the Competition Act. This occurs when a big firm uses its massive market power to crush competitors. One major example is predatory pricing, which we see often in the tech industry. A dominant player uses its vast capital to set prices artificially low—often below cost—to eliminate all other competitors. Once the rivals are driven out of the market, the dominant firm raises prices again to recoup losses. Another tactic is exclusive dealing, where a big player tells customers, "Unless you buy only from me, I will never sell to you again," effectively locking competitors out.

    We also see tying and bundling. If you have an "essential" product that everyone needs, you might tie it to a "useless" or less popular product, forcing customers to buy both. This is a strategy to capture a second market and keep competitors out of it. Tech firms often try to justify this by arguing that they aren't actually dominant, or that the bundling is beneficial for consumers, or even that the two items are actually just one single product. For example, Apple might argue that an iPhone and its pre-installed apps are one integrated product, not a case of tying. This makes enforcement difficult for regulators, especially since cases can take three years or more to resolve. In Singapore, the ticket seller SISTIC was found to have abused its dominance by requiring event venues and promoters to use its services exclusively. Internationally, Microsoft was famously found liable for bundling Windows Media Player with the Windows operating system, though the legal landscape has evolved since then.

    下一类是滥用主导地位，根据《竞争法》第 47 条是被禁止的。这发生在大型企业利用其巨大的市场力量粉碎竞争对手时。一个主要的例子是掠夺性定价，这在科技行业经常见到。占据主导地位的参与者利用其雄厚的资金人为地将价格定得极低（通常低于成本），以消除所有其他竞争对手。一旦对手被赶出市场，主导企业就会再次提价以弥补损失。另一种策略是独家交易，大企业告诉客户：“除非你只向我购买，否则我再也不会卖给你任何东西”，这实际上将竞争对手挡在了门外。

    我们还看到了捆绑销售。如果你有一个每个人都需要的“必需”产品，你可能会将它与一个“无用”或不那么受欢迎的产品捆绑在一起，强迫客户购买两者。这是一种占领第二个市场并将竞争对手排除在外的策略。科技公司经常试图为此辩护，辩称自己实际上并不占主导地位，或者捆绑销售对消费者有利，甚至辩称这两件商品实际上只是一个单一产品。例如，苹果公司可能会辩称，iPhone 及其预装的应用程序是一个集成产品，而不是捆绑销售。这增加了监管机构的执法难度，特别是此类案件可能需要三年或更长时间才能解决。在新加坡，售票平台 SISTIC 被发现滥用主导地位，要求活动场地和推广方只能使用其服务。在国际上，微软曾因将 Windows Media Player 与 Windows 操作系统捆绑而被判有罪，不过自那以后法律环境已经发生了演变。

  - [12:18 - 12:19] **Anti-Competitive Mergers and the Grab-Uber Case**

    The final category involves anti-competitive mergers, governed by Section 54 of the Competition Act. While you can generally merge with whomever you want, a merger is prohibited if it results in a "substantial lessening of competition." If the two biggest players in a three-player market merge, competition effectively vanishes. In the tech sector, we also see "killer acquisitions" where big players swallow up innovative startups specifically to kill them or integrate the technology to prevent others from using it.

    In Singapore, the most famous recent case is the Grab-Uber merger. Grab decided to move forward with merging with Uber’s Southeast Asian operations, seemingly ignoring the competition regulator. Consequently, the Competition and Consumer Commission of Singapore (CCCS) found that the merger substantially lessened competition in the ride-hailing market and fined both companies.

    最后一类涉及反竞争合并，由《竞争法》第 54 条管辖。虽然你通常可以随心所欲地与任何人合并，但如果合并导致“竞争大幅减少”，则是被禁止的。如果一个只有三个参与者的市场中，最大的两个合而为一，竞争实际上就消失了。在科技领域，我们还看到了“杀手级收购”，大公司吞并创新初创公司，专门为了扼杀它们或整合其技术，以防止他人使用。

    在新加坡，最近最著名的案例是 Grab 与 Uber 的合并。Grab 决定继续合并 Uber 在东南亚的业务，似乎忽略了竞争监管机构。因此，新加坡竞争与消费者委员会（CCCS）认定该合并大幅减少了网约车市场的竞争，并对两家公司处以罚款。

  - [12:20 - 12:21] **The RealPage Case and Algorithmic Price-Fixing**

    Recently, there have been significant worries about how AI creates new opportunities for anti-competitive practices. A prime example is the RealPage case in the United States. RealPage is a platform used by landlords to manage their apartments. The platform helped landlords set their rents by aggregating data on what everyone else was charging. Landlords would provide their pricing data to the platform, and RealPage’s algorithm would then "advise" each landlord on what rent to set. While it looks like a simple advisory service on the surface, evidence suggested that once a critical mass of landlords used the same platform, rents across the market began to rise. This is what we call "algorithmic price-fixing." Instead of landlords meeting in a room to fix prices, they use a central platform and an algorithm to effectively achieve the same result.

    最近，人们非常担心人工智能如何为反竞争行为创造新的机会。一个典型的例子是美国的 RealPage 案。RealPage 是一个供房东管理公寓的平台。该平台通过汇总其他人的收费数据来帮助房东设定租金。房东向平台提供定价数据，然后 RealPage 的算法会“建议”每位房东设定多少租金。虽然表面上看只是简单的咨询服务，但有证据表明，一旦达到一定数量的房东使用同一个平台，整个市场的租金就开始上涨。这就是我们所说的“算法价格操纵”。房东不再是在房间里聚会商定价格，而是利用一个中央平台和算法来有效地达到同样的结果。

  - [12:21 - 12:28] **How AI Facilitates Cartels and Exclusionary Practices**

    We need to look at how AI specifically facilitates these illegal practices. One student suggested that firms can build algorithms to target profit optimization, which might coordinate to ensure they don't collectively give too many discounts. This hits on a key point: AI can "police" a cartel. In a traditional price-fixing agreement, there is always a temptation for one member to "cheat" by lowering their price slightly to steal customers from the others. However, if everyone uses an algorithm that constantly monitors market price levels, the AI can detect any deviation instantly and react, making the price-fixing agreement much more stable and effective.

    AI also facilitates the gathering and analysis of market data. It allows firms to set a "common price" and monitor the "cartel" once an agreement is in place. Another student mentioned that AI allows for very targeted discounts to specific segments—identifying who is a loyal customer and who needs a special offer to stay. This actually relates to "data-driven exclusionary practices." If a dominant firm has superior knowledge of the market, they can identify which customers are price-sensitive and which are not. This makes predatory pricing more efficient because you don't have to lower prices for everyone; you only lower them just enough to "flip" the customers who were thinking of switching to your rival.

    AI can also facilitate "market sharing" in a cannibalistic fashion. Essentially, AI gives firms the tools to analyze the market, set common prices, and ensure everyone sticks to the plan through constant surveillance.

    我们需要研究人工智能具体是如何促进这些非法行为的。一位学生建议，公司可以构建针对利润优化的算法，这些算法可能会协调一致，确保它们不会集体提供过多的折扣。这触及了一个关键点：AI 可以“监督”卡特尔。在传统的价格操纵协议中，成员总是有“作弊”的诱惑，即通过略微降价来从其他成员那里窃取客户。然而，如果每个人都使用持续监控市场价格水平的算法，AI 就可以立即检测到任何偏差并做出反应，从而使价格操纵协议更加稳定和有效。

    AI 还促进了市场数据的收集和分析。它允许企业在达成协议后设定“共同价格”并监控“卡特尔”。另一位学生提到，AI 允许针对特定细分群体提供非常有针对性的折扣——识别谁是忠实客户，谁需要特价优惠才能留下来。这实际上涉及“数据驱动的排他性行为”。如果一家主导企业对市场有超强的了解，他们就可以识别哪些客户对价格敏感，哪些不敏感。这使得掠夺性定价更加高效，因为你不必为所有人降价；你只需要降到足以让那些考虑转向竞争对手的客户“回心转意”即可。

    AI 还可以促进以一种自相残杀的方式进行“市场瓜分”。本质上，AI 为企业提供了分析市场、设定共同价格并通过持续监控确保每个人都执行计划的工具。

  - [12:28 - 12:33] **Three Models of Algorithmic Coordination**

    There are three potential models or mechanisms for how AI is used for price-fixing, ranging from simple to advanced.

    The first is the **Computer-Assisted Model**. This is the least advanced version. Here, the humans engage in traditional price-fixing—they sit down and agree on a price—and the computer is simply a tool used to implement and monitor that the agreed-upon prices are being sent out. The humans are clearly to blame here; the computer is just the tool.

    The second is the **Hub-and-Spoke Model**. In this scenario, all the firms (the spokes) use the same central app or third-party algorithm (the hub). Because everyone is feeding into and following the same algorithm, the software helps all these firms set a common price. This allows them to align their prices without ever having to sit down together in suspicious circumstances.

    The third and most advanced is the **Digital Eye Scenario**. Here, firms don't even need to agree to use the same algorithm. If every firm is using a sufficiently powerful AI, those algorithms can independently determine that the most optimal strategy for profit maximization is to align with competitors and set high prices. They "see" each other's actions through the "digital eye" and coordinate tacitly without any human communication.

    A student asked if this alludes to personalized and dynamic pricing based on supply and demand. If a firm like Grab or Google is simply reacting to market supply and demand using an algorithm, that is generally legal and often seen as a sign of a functioning market. The legal concern arises when the algorithm starts reacting specifically to what competitors are doing in a way that aligns prices at a high level.

    人工智能用于价格操纵有三种潜在的模型或机制，从简单到先进不等。

    第一种是**计算机辅助模型**。这是最不先进的版本。在这种情况下，人类进行传统的价格操纵——他们坐下来商定价格——而计算机只是用来执行和监控商定价格是否发布的工具。在这种情况下，人类显然是罪魁祸首；计算机只是工具。

    第二种是**轴辐模型 (Hub-and-Spoke Model)**。在这种情况下，所有的公司（辐条）都使用同一个中央应用程序或第三方算法（轴心）。因为每个人都向同一个算法提供数据并遵循它，该软件帮助所有这些公司设定一个共同价格。这使他们能够在从未在可疑情况下坐在一起的情况下达成价格一致。

    第三种也是最先进的是**“数字之眼”场景**。在这里，公司甚至不需要同意使用相同的算法。如果每家公司都使用足够强大的 AI，这些算法可以独立判断出实现利润最大化的最优策略是与竞争对手保持一致并设定高价。它们通过“数字之眼”看到彼此的行动，并在没有任何人类交流的情况下进行默契配合。

    一位学生问这是否暗示了基于供需的个性化和动态定价。如果像 Grab 或 Google 这样的公司只是利用算法对市场供需做出反应，这通常是合法的，且往往被视为市场运作良好的标志。当算法开始专门针对竞争对手的行为做出反应，并以一种在高位统一价格的方式进行时，法律问题就出现了。

  - [12:34 - 12:44] **Advanced Abuses: Self-Referencing, Selective Meeting, and Data Access**

    Regulators, particularly in the EU, have also investigated "self-referencing." A major case involved Google, where the search engine was accused of tweaking its algorithm to prefer its own "Google Shopping" results over other online retailers in the search results bar. While this might be more about platform design than AI per se, it relates to the expectation that algorithms should be fair and neutral.

    We also discussed how AI facilitates the "refusal to supply." In the digital age, data itself is an essential resource. If a company becomes dominant because they have a massive dataset that is essential for anyone else to enter the market, and they refuse to share that data with rivals at a fair price, it can be considered a refusal to supply an "essential facility."

    Another sophisticated abuse is "selective meeting" or loyalty discounting. A firm wants to give as little discount as possible to retain a customer. AI helps by determining exactly how much of a discount is needed for each individual customer to prevent them from defecting to a rival. Similarly, in "tying," an algorithm can figure out exactly which customers will still buy a product even if they are forced to buy a "useless" extra item. This makes these anti-competitive strategies far more surgical and effective.

    监管机构（尤其是欧盟）还调查了“自我优待 (self-referencing)”。一个重大案例涉及谷歌，该公司被指控调整算法，使其在搜索结果栏中优先显示自家的“谷歌购物”结果，而不是其他在线零售商。虽然这可能更多是关于平台设计而非 AI 本身，但它涉及人们对算法应公平且中立的预期。

    我们还讨论了 AI 如何促进“拒绝供应”。在数字时代，数据本身就是一种至关重要的资源。如果一家公司因为拥有对任何其他人进入市场都至关重要的海量数据集而占据主导地位，并且拒绝以公平的价格与竞争对手共享这些数据，这可以被视为拒绝供应“关键设施”。

    另一种复杂的滥用行为是“选择性应战 (selective meeting)”或忠诚折扣。企业希望提供尽可能少的折扣来留住客户。AI 可以通过确定每个客户到底需要多少折扣才能防止其转向竞争对手来提供帮助。同样，在“捆绑销售”中，算法可以计算出哪些客户即使被强迫购买一个“无用”的额外物品也仍会购买原产品。这使得这些反竞争策略变得更加精准和有效。

  - [12:45 - 12:53] **The Ethics of Anti-Competitive Practice**

    Is this ethical? One student argued that these practices harm social welfare by raising prices and reducing choice, and they stifle innovation. Another student noted that it could involve bias or discrimination—a point we will explore later regarding personalized pricing. However, there's another side. A firm is not a charity; it is in business to make a profit. From the firm's perspective, this behavior might be seen as the "freedom to do business" and fulfilling their duty to shareholders.

    There's also the question of "information symmetry." If consumers use their own research and tools to respond to what firms are doing, the ethical balance might shift. However, currently, firms often have a massive data advantage. Competition law tries to balance these tensions: preventing bad practices while not being unnecessarily restrictive of what firms can do to be successful. We will stop here for a 10-minute break and return at 1:02 PM.

    这符合伦理吗？一位学生认为，这些行为通过提高价格和减少选择损害了社会福利，并且扼杀了创新。另一位学生指出，这可能涉及偏见或歧视——这一点我们稍后在讨论个性化定价时会进一步探讨。然而，还有另一面。企业不是慈善机构；它的经营目标是获利。从企业的角度来看，这种行为可能被视为“经营自由”，是在履行对股东的责任。

    还存在“信息对称”的问题。如果消费者利用自己的研究和工具来应对企业的行为，伦理天平可能会发生倾斜。然而，目前企业通常拥有巨大的数据优势。竞争法试图平衡这些冲突：既要防止不良行为，又不能对企业的成功经营做出不必要的限制。我们在这里休息 10 分钟，下午 1:02 回来。

  - [13:05 - 13:13] **Ethics Case Study: Algorithmic Wage Suppression**

    To understand the ethical implications of AI in anti-competitive agreements, let's look at a hypothetical scenario involving "RealWorld," an online job platform. Employers use this platform to post job advertisements, and job seekers use it to apply. RealWorld introduces an algorithm called "WageFinder." To participate, employers must upload their current employment contracts and salary data. When an employer wants to post a new job, WageFinder uses this aggregated data to recommend the lowest possible salary the employer should set to attract candidates.

    From an employer's perspective, this is a tool for cost optimization. However, several ethical concerns arise. First, there is a massive information asymmetry; employers have access to market-wide salary data while employees remain in the dark, knowing only the little information companies choose to disclose. Furthermore, this can be viewed as a "Hub-and-Spoke" anti-competitive model. The algorithm acts as the central "hub," and the individual employers are the "spokes." If all major employers for a certain role—say, software engineers with five years of experience—follow the same algorithm's recommendation, they collectively suppress wages. This effectively amounts to a form of price-fixing, specifically "wage-fixing." It also risks amplifying biases; the system may create a "race to the bottom" cycle, where the floor of the salary range is constantly pushed lower, even for roles where demand is high. While employers might negotiate upwards, the net effect of the algorithm is to push the starting point of those negotiations down, which many would regard as unethical.

    为了理解人工智能在反竞争协议中的伦理影响，让我们来看一个涉及“RealWorld”在线招聘平台的假设案例。雇主利用该平台发布招聘广告，求职者利用该平台申请工作。RealWorld 推出了一种名为“WageFinder”的算法。为了参与其中，雇主必须上传其当前的雇佣合同和薪资数据。当雇主想要发布新职位时，WageFinder 会利用这些汇总数据，建议雇主应设定的最低薪资，以吸引候选人。

    从雇主的角度来看，这是一个成本优化工具。然而，这引发了几点伦理担忧。首先，存在巨大的信息不对称；雇主可以获取全市场的薪资数据，而雇员却被蒙在鼓里，只知道公司选择披露的极少信息。此外，这可以被视为一种“轴辐式”反竞争模型。算法充当中央“轴心”，而各个雇主则是“辐条”。如果针对某一特定角色（例如有五年经验的软件工程师）的所有主要雇主都遵循同一个算法的建议，他们就会集体压低工资。这实际上相当于一种形式的价格操纵，具体说是“工资操纵”。它还存在放大偏见的风险；该系统可能会制造一种“逐底竞争”的循环，薪资范围的底线不断被推低，即使是在需求量很大的职位上也是如此。虽然雇主可能会向上协商，但该算法的净效应是降低了谈判的起点，许多人认为这是不道德的。

  - [13:13 - 13:17] **The Theory of Price Discrimination**

    Moving on to a topic that affects us all daily: personalized pricing. To understand this, we must look at the broader economic category of "price discrimination," which is selling the same product under the same conditions to different consumers at different prices. In economic theory, there are three degrees of price discrimination. **Third-degree price discrimination** is based on demographic groups; for example, student discounts at restaurants or cheaper bus fares for the elderly. **Second-degree price discrimination** is based on the quantity or quality purchased; for instance, a telco plan where the first 100 minutes are free but subsequent minutes cost more, or Netflix charging different rates for basic versus premium video quality.

    The "holy grail" for firms is **First-degree (or Perfect) price discrimination**. This is where a firm charges every single individual a different price based exactly on the maximum amount they are willing to pay, known as their "reservation price." Personalized pricing is the attempt to achieve this first-degree discrimination using personal characteristics. For price discrimination to be possible, three conditions must be met: the seller must have some market power (they can't be a pure price taker), they must be able to prevent "arbitrage" (the low-price person reselling to the high-price person), and they must have the ability to segment the market effectively to know who is willing to pay more.

    接下来讨论一个每天都在影响我们的课题：个性化定价。要理解这一点，我们必须审视更广泛的经济学范畴——“价格歧视”，即在相同条件下以不同的价格向不同的消费者销售同一种产品。在经济理论中，价格歧视分为三个等级。**三级价格歧视**是基于人口统计群体的；例如，餐厅的学生折扣或老年人更便宜的巴士车费。**二级价格歧视**是基于购买的数量或质量；例如，前 100 分钟免费但随后分钟收费更高的电信套餐，或者 Netflix 对基础画质和高级画质收取不同的费用。

    企业的“终极目标”是**一级（或完全）价格歧视**。这是指企业根据每个人的“保留价格”（即他们愿意支付的最高金额），向每个人收取不同的价格。个性化定价就是试图利用个人特征来实现这种一级歧视。价格歧视要成为可能，必须满足三个条件：卖方必须具有一定的市场力量（不能纯粹是价格接受者），必须能够防止“套利”（低价购买者转卖给高价购买者），并且必须能够有效地细分市场，以了解谁愿意支付更多。

  - [13:18 - 13:21] **AI and Real-World Personalized Pricing: Instacart & Delta**

    AI significantly changes the landscape because it gives firms the tools to segment markets and predict reservation prices at a massive scale. A recent, famous example involves Instacart. Media investigations found that Instacart’s automated AI pricing system was inflating grocery prices differently for different customers, leading to concerns that it was practicing personalized pricing. Another example is Delta Air Lines, which experimented with AI-powered personalized pricing for flight tickets. While this might have been a limited test, it clearly demonstrates that major firms are eager to use AI to find the maximum price each traveler is willing to pay.

    AI facilitates this by extracting vast amounts of personal data to set rules and by using predictive models to estimate an individual's reservation price. In more traditional, "old school" settings, this was done person-to-person through haggling in a market. But today, if you have a million customers, you need a machine to do that negotiation for you. AI can even handle the "negotiation" process; for instance, if you try to cancel a subscription, the AI might instantly offer you a lower price to retain you.

    人工智能显著改变了这一局面，因为它为企业提供了大规模细分市场和预测保留价格的工具。最近一个著名的例子涉及 Instacart。媒体调查发现，Instacart 的自动化 AI 定价系统对不同客户的杂货定价通胀程度不同，这引发了人们对其进行个性化定价的担忧。另一个例子是达美航空 (Delta Air Lines)，该公司尝试对机票进行人工智能驱动的个性化定价。虽然这可能只是有限的测试，但它清楚地表明，大公司正渴望利用人工智能来寻找每个旅客愿意支付的最高价格。

    AI 通过提取海量的个人数据来设定规则，并利用预测模型来估计个人的保留价格，从而促进了这一过程。在更传统的“老派”场景中，这是通过在市场中讨价还价、人对人完成的。但今天，如果你有一百万个客户，你需要一台机器来为你进行这种谈判。AI 甚至可以处理“谈判”过程；例如，如果你尝试取消订阅，AI 可能会立即为你提供较低的价格以留住你。

  - [13:22 - 13:33] **The Ethics of Personalized Pricing: Transparency and Desperation**

    Why is personalized pricing often considered unethical? One major argument is the lack of transparency, which can border on deception. If a consumer expects a fixed market price but is secretly being charged a variable price based on their data, they are being misled. It becomes even more problematic in "emergency" or essential services. Imagine a dental emergency where a patient is in extreme pain; an AI algorithm could detect that desperation and charge a price very close to the patient's maximum willingness to pay. This is effectively "price gouging" facilitated by technology.

    While dynamic pricing (like surge pricing for Grab) is often transparent because the user sees the price fluctuate based on market demand, personalized pricing is hidden. It targets the individual's specific vulnerabilities. One student pointed out that we don't feel "student discounts" are unfair even though they are discriminatory. The difference lies in the intent—is the firm being "altruistic" by providing access to those who couldn't otherwise afford it, or are they simply trying to "squeeze" every cent of consumer surplus from every individual?

    为什么个性化定价通常被认为是不道德的？一个主要的论点是缺乏透明度，这近乎欺骗。如果消费者预期的是固定的市场价格，但实际上被根据其数据秘密收取了浮动价格，那么他们就被误导了。这在“紧急”或基本服务中变得更加成问题。想象一下牙科急诊，病人正处于剧痛之中；AI 算法可以检测到这种绝望情绪，并收取一个非常接近病人最高支付意愿的价格。这实际上是技术助长的“哄抬物价”。

    虽然动态定价（如 Grab 的高峰加价）通常是透明的，因为用户可以看到价格根据市场需求而波动，但个性化定价是隐蔽的。它针对的是个人的特定弱点。一位学生指出，我们并不觉得“学生折扣”不公平，尽管它们也是歧视性的。区别在于意图——企业是出于“利他主义”，为那些本负担不起的人提供机会，还是仅仅试图从每个人身上“榨取”最后一分钱的消费者剩余？

  - [13:34 - 13:51] **Fairness, Privacy, and Economic Efficiency**

    The ethics of personalized pricing also tie into broader fairness and privacy concerns. Using a "black box" algorithm means even the seller might not be able to explain why Customer A is charged more than Customer B. Furthermore, this practice relies on the constant surveillance of consumers—tracking location, purchase history, and even reviews—often without explicit or meaningful consent.

    On the other hand, there are arguments in favor of the practice. From a purely economic standpoint, price discrimination can actually benefit some people who end up paying less than they would under a fixed-price model. Some argue it contributes to economic growth and volume; if a firm can offer a lower price to someone who wouldn't have bought the product otherwise, it increases total sales and taxation income. There is also the "redistribution" argument: the firm is simply capturing the "consumer surplus" (the extra benefit the buyer was getting). From a free-market perspective, firms are generally free to price as they like. However, some regard the resources spent on developing these "surveillance" pricing systems as a waste—those resources could instead be spent on actually improving the product.

    个性化定价的伦理还涉及到更广泛的公平和隐私问题。使用“黑箱”算法意味着即使是卖方也可能无法解释为什么客户 A 被收取的费用比客户 B 高。此外，这种做法依赖于对消费者的持续监控——跟踪位置、购买历史甚至评价——且通常没有明确或有意义的同意。

    另一方面，也有支持这种做法的论点。从纯粹的经济角度来看，价格歧视实际上可以让一些人受益，因为他们最终支付的费用比固定价格模型下的要少。有人认为这有助于经济增长和销量；如果企业能向原本不会购买产品的人提供较低的价格，就会增加总销售额和税收。还有“再分配”论点：企业只是在获取“消费者剩余”（买家获得的额外收益）。从自由市场的角度来看，企业通常可以随心所欲地定价。然而，一些人认为投入到开发这些“监控”定价系统上的资源是一种浪费——这些资源本可以用于真正改进产品。

  - [13:52 - 13:56] **Case Study: The "Zomana" Platform Scenario**

    Let's conclude with a final hypothetical: "Zomana," an online retail platform. Zomana allows sellers to set a price range (e.g., $10 to $20) for a product. An algorithm then decides the specific price for each buyer based on their past purchases, location, payment method, and reviews. Zomana notifies the buyers that an algorithm is being used and what data it considers.

    Even with this transparency, is it ethical? One might argue that it is still an excessive use of personal data. Even if consent is technically given, the "black box" nature remains: the buyer knows *what* data is used, but not *how* that data translates into the specific price they see. This highlights that transparency about the *existence* of an algorithm does not necessarily make the *outcome* of that algorithm ethical.

    让我们以最后一个假设案例作为结束：“Zomana”，一个在线零售平台。Zomana 允许卖家为产品设定一个价格范围（例如 10 到 20 美元）。然后，算法根据每个买家的过往购买记录、位置、支付方式和评价来决定具体价格。Zomana 会告知买家正在使用算法以及算法考虑了哪些数据。

    即便有这种透明度，这符合伦理吗？有人可能会认为，这仍然是对个人数据的过度使用。即使技术上给予了同意，“黑箱”性质依然存在：买家知道使用了*哪些*数据，但不知道这些数据是*如何*转化为他们看到的具体价格的。这凸显出，关于算法*存在*的透明度并不一定能使该算法的*结果*符合伦理。
## Week 4
  - [12:07 - 12:09] **The Evolution of Media Manipulation: From Photoshop to Meitu**

    Of course, in more recent times, we have witnessed a significant shift. We now possess technology that allows us to edit images or media much more easily, right? Let's look at a historical example. On the second slide here, you can see a scandal that took place in the context of the Iraq War. A journalist took two separate pictures—which you can see on the left and right—and wanted to create a "spliced image," meaning a combination of these two, because he felt the composite would be more dramatic and dynamic. So, this journalist used, I believe, Photoshop to digitally combine these two source images into the single image you see at the bottom. As a result of fabricating this image to alter the narrative reality, the journalist was fired.

    In even more recent years, we have seen AI technology allow people to alter images with even greater ease. For example, many of you are familiar with the app "Meitu" (Meitu Xiuxiu). This app basically allows you to "dress up" or enhance your own face. If you look at the bottom image on the slide, you see the "before" and "after." You can see changes to the eyebrows, the tone of the skin, the shape of the face, and so on. We have had such technologies for a while. So, the question arises: What are deepfakes, and how are they different from what has come before, like these Photoshop edits or Meitu filters?

    当然，在近代我们见证了巨大的转变。我们现在拥有的技术让编辑图像或媒体变得更加容易，对吧？让我们看一个历史案例。在第二张幻灯片上，你可以看到伊拉克战争期间发生的一起丑闻。一名记者拍摄了两张独立的照片——你可以看到左边和右边的这两张——他想通过“拼接”将两者结合，因为他觉得合成后的照片会更具戏剧性和动态感。于是，这名记者利用（我认为是）Photoshop 将这两张原图合成为了底部的那张。由于伪造图像以此篡改了叙事现实，这名记者最终被解雇了。

    在更近的几年里，我们看到 AI 技术让人们能更轻松地篡改图像。例如，你们很多人都熟悉“美图秀秀”（Meitu）这款应用。它基本上允许你“装扮”或美化自己的脸。如果你看幻灯片底部的图片，那是“修改前”和“修改后”的对比。你可以看到眉毛、肤色、脸型等方面的变化。这种技术我们已经司空见惯了。那么问题来了：什么是“深度伪造”（Deepfakes）？它与之前的 Photoshop 编辑或美图滤镜究竟有何不同？

  - [12:09 - 12:12] **Defining Deepfakes and Emerging Threats (Grok, Sora, and Telegram)**

    When we talk about "deepfakes," we are referring specifically to AI-generated fake media. We are not just talking about altering existing media, but creating content that is completely synthetic, completely fabricated. This can manifest as images, audio, or video. In the past two years, we have seen a surge in AI tools becoming available that allow people to easily produce these things. For instance, Grok—the chatbot integrated into X (formerly Twitter)—is quite infamous for this capability, and tools like OpenAI's Sora can also be used to generate realistic video.

    While this technology has come to the forefront, governments are now starting to take action. On the left side of the slide, you can see a newspaper article discussing the ready availability of "nudes." One of the most prevalent negative use cases of deepfakes is to create what we call "non-consensual pornography" or "intimate image abuse." This is where someone uploads a normal image of a person and then uses an AI system or tool to strip the clothing or alter the image to make the person look like they are nude. These are deepfake nudes, and this is a phenomenon that has proliferated significantly over Telegram, the chat application.

    On top of deepfake nudes, there are also concerns about deepfake fraud. This is where people use the likeness of others to commit financial crimes. For example, I could create a deepfake video of a company director or CEO and use that video to convince an employee of the company to transfer money to a specific bank account. That is a prime example of fraud. In the third image on the slide, you can see a very recent example of countries taking action: Malaysia and Indonesia blocked access to the Grok app (or threatened to) because it was clear that people were using this app specifically to produce explicit deepfake images. This is an issue that regulators are actively scrutinizing.

    当我们谈论“深度伪造”时，我们特指 AI 生成的虚假媒体。我们指的不仅仅是修改现有媒体，而是创造完全合成的、完全虚构的内容。这可以表现为图像、音频或视频。在过去两年里，我们看到越来越多的 AI 工具问世，让人们能够轻易地制作这些内容。例如，集成在 X（前推特）中的聊天机器人 Grok 就因这种能力而臭名昭著，而像 OpenAI 的 Sora 这样的工具也可以用来生成逼真的视频。

    虽然这项技术已经走到台前，但各国政府现在也开始采取行动。在幻灯片左侧，你可以看到一篇关于“裸照”唾手可得的报导。深度伪造最普遍的负面用例之一就是制造所谓的“非自愿色情内容”或“私密图像滥用”。即某人上传一张某人的普通照片，然后利用 AI 系统或工具去除衣物或修改图像，使照片中的人看起来像是裸体的。这就是深度伪造裸照，这种现象在聊天软件 Telegram 上已经非常泛滥。

    除了深度伪造裸照，还有关于深度伪造欺诈的担忧。这是指人们利用他人的肖像进行金融犯罪。例如，我可以制作一个公司董事或 CEO 的深度伪造视频，利用该视频说服公司员工将资金转入特定银行账户。这是一个典型的欺诈案例。在幻灯片的第三张图中，你可以看到各国采取行动的最新案例：马来西亚和印度尼西亚封锁了 Grok 应用（或威胁封锁），因为很明显人们在使用该应用专门制作露骨的深度伪造图像。这是监管机构正在积极审查的问题。

  - [12:12 - 12:23] **The "Pervert's Dilemma": Ethics of Private Creation Without Distribution**

    I suppose this issue gives rise to a number of ethical problems, and I have tried to compartmentalize these into data questions. Let's begin with a discussion of the first distinction: the creation of a deepfake versus its distribution. The question I would like us to consider is: Is the creation of a deepfake for your own private use unethical? And if so, why might it be unethical?

    Let's consider the point of misrepresentation. We will come back to this, but imagine a scenario where an individual uses a deepfake app to produce a nude image of someone they know. However, he does not share this nude image. That new image is strictly for his personal consumption. In that scenario, you probably could not argue that there is "misrepresentation" in the strict sense, right? Because the idea behind misrepresentation is that you are actually communicating some sort of falsehood to a third party. In a case involving creation without distribution, there is no communication, and thus no representation to an audience.

    One of the authors in our readings (referencing the "Pervert's Dilemma") argues that since it's not the actual private parts of that person, they can't exactly claim a privacy violation in the traditional sense. It's just a generated fiction. You can analogize this to mental imagination. If you just imagine someone naked in your mind, is that unethical? Or, if you want to go a step further, what if you are a very skilled artist and you hand-draw a realistic nude image of that person for private use? If we accept that your mental imagination or your hand-drawn artwork is not necessarily unethical, is there something special about deepfakes that makes them ethically distinct? From a consequentialist point of view, there is really no harm done to the victim because the content hasn't been shared.

    **Student:** Patient consent—or rather, subject consent. Even if you don't distribute it, you still put someone famous or a specific person into a deepfake video. You need consent from this person regardless of what you do with it.

    **Professor:** Okay. And why is that consent necessary if no one sees it?

    **Student:** Because, as you taught us last semester, consent regarding a person's image is crucial. Even if it's public knowledge or their face is public, putting it into a video requires permission.

    **Professor:** Legally speaking, yes, subject to exceptions. But from a moral standpoint?

    **Student:** It feels unethical. You are using somebody's face information without permission.

    **Professor:** But if we look strictly at consequentialism—what is the harm? You mentioned "negative consequences," but what are they if it stays on my hard drive?

    **Student:** Well, although you faked his body, the face is still part of his anatomy. It's part of his identity being depicted in a compromising way.

    **Professor:** Right. It is difficult to make the case that creation without distribution is unethical purely on consequentialist grounds. If you want to make that case, you probably have to resort to Deontology or Virtue Ethics—specifically the idea of human dignity. The argument is that regardless of consequences, it is fundamentally undignified to create a pornographic image of a person, treating them as an object for consumption, even if they never find out. That violates the Categorical Imperative—treating a person as a means to an end rather than an end in themselves.

    Another possible line of argument is the "risk of accidental distribution." You create the image, but then you might lose your laptop, or you get hacked, or you accidentally share it. The mere act of creation introduces a non-zero risk of harm to the victim. That is a consequentialist argument regarding risk.

    **Student:** It's also about how it affects the creator. It might cause you to treat real people as less human. If you habitually objectify people via deepfakes, it degrades your own virtue. A respectful person would not do this.

    **Professor:** Exactly. That’s a virtue ethics argument. A virtuous person would not engage in the non-consensual sexualization of others. And finally, there’s the argument about the individual's autonomy over their own image—using their biometric data to train an AI or generate content violates their right to control their digital self.

    我想这个问题引发了一系列伦理难题，我试图将这些问题分类讨论。让我们从第一个区别开始讨论：深度伪造的“制作”与“传播”的区别。我希望大家思考的问题是：仅供私人使用的深度伪造制作是否不道德？如果是，为什么？

    让我们考虑“虚假陈述”（Misrepresentation）这一点。想象一个场景：某人使用深度伪造应用制作了熟人的裸体图像，但他没有分享这张照片。这张新图像仅供他个人消费。在这种情况下，你可能很难说存在严格意义上的“虚假陈述”，对吧？因为虚假陈述的核心在于你向第三方传达了某种虚假信息。在“只制作不传播”的案例中，不存在传播行为，因此也不存在向受众的陈述。

    我们在阅读材料中的一位作者（涉及“变态者的困境”理论）认为，既然这并非该人真实的私密部位，他们就不能在传统意义上主张隐私侵权。这只是生成的虚构内容。你可以将其类比为心理想象。如果你只是在脑海中想象某人裸体，这不道德吗？或者更进一步，如果你是一位技艺高超的画家，你手绘了一张该人逼真的裸体画像供私人使用呢？如果我们接受心理想象或手绘艺术并不一定是不道德的，那么深度伪造是否有某种特殊性使其在伦理上与之不同？从结果论（Consequentialist）的角度来看，因为内容未被分享，受害者并没有受到实际伤害。

    **学生：** 还需要考虑当事人同意——或者是主体同意。即使你不传播，你依然把名人或特定的人放入了深度伪造视频中。无论你做什么，都需要获得这个人的同意。

    **教授：** 好的。但如果没人看到，为什么同意是必要的？

    **学生：** 因为，正如您上学期教我们的，关于个人形象的同意至关重要。即使他们的脸是公开的，将其放入视频仍需许可。

    **教授：** 从法律上讲是的，但这受限于某些例外。但从道德立场来看呢？

    **学生：** 这感觉是不道德的。你在未经允许的情况下使用了别人的面部信息。

    **教授：** 但如果我们严格从结果论来看——伤害在哪里？你提到了“负面后果”，但如果文件只留在我的硬盘里呢？

    **学生：** 尽管身体是伪造的，但脸依然是他解剖结构的一部分。这是他的身份以一种不体面的方式被描绘。

    **教授：** 对。单纯依据结果论很难证明“只制作不传播”是不道德的。如果你想证明这一点，你可能必须求助于义务论（Deontology）或美德伦理学（Virtue Ethics）——特别是人类尊严（Human Dignity）的概念。这种观点认为，无论结果如何，制作某人的色情图像，将其视为消费对象（即使他们从未发现），这在本质上是有损尊严的。这违反了康德的绝对命令——将人视为手段而非目的。

    另一个可能的论点是“意外传播的风险”。你制作了图像，但你可能会丢失笔记本电脑，或者被黑客攻击，或者不小心分享出去。制作行为本身就给受害者引入了非零的伤害风险。这是一个关于风险的结果论论点。

    **学生：** 这也关乎它如何影响创作者。这可能会导致你把真人看得不那么像人。如果你习惯性地通过深度伪造物化他人，这会降低你自己的美德。一个尊重他人的人不会这样做。

    **教授：** 没错。这就是美德伦理学的论点。一个有美德的人不会未经同意对他人的性形象进行处理。最后，还有关于个人对其形象的自主权（Autonomy）的论点——利用他们的生物特征数据来训练 AI 或生成内容，侵犯了他们控制数字自我的权利。

  - [12:23 - 12:27] **Conditions for Permissible Distribution: Consent, Context, and Labels**

    Next question: Now we are looking at distribution. You are not only creating the deepfake, but sharing it. When might this be regarded as ethically permissible?

    **Student:** It could be, for example, a person depicting a historical figure from a long time ago, and it's clearly labeled, and it has some positive use. For instance, in education, having Einstein teach physics and math might make it more interesting. That could be a case where distribution is okay.

    **Professor:** Okay, so one idea is if the person is already deceased, or a historical figure. We will get back to this in the second-to-last question.

    **Student:** Another point is about the reason you are doing it. The intent matters.

    **Professor:** Exactly. What else might make the distribution permissible?

    **Student:** Essentially some sort of declaration that this is a deepfake or AI-generated picture.

    **Professor:** Right. So, if we look at the first article in your readings, the author mentions—and I'm paraphrasing—three conditions that might make the distribution of deepfakes morally permissible:

    1. **Consent:** You have the permission of the subject.
    2. **Declaration:** It is clearly labeled as AI-generated or fake.
    3. **No Malicious Intent:** You aren't doing it to harm.

    下一个问题：现在我们要看的是传播。你不仅制作了深度伪造内容，还分享了它。在什么情况下，这可能被视为在伦理上是允许的？

    **学生：** 比如，描绘一个很久以前的历史人物，并且有清晰的标注，且具有积极用途。例如在教育中，让爱因斯坦来教物理和数学可能会更有趣。这可能是一个允许传播的案例。

    **教授：** 好的，所以一个观点是如果此人已经去世，或者是历史人物。我们会在倒数第二个问题中回头讨论这一点。

    **学生：** 另一点是关于你这样做的理由。意图很重要。

    **教授：** 没错。还有什么能让传播变得允许？

    **学生：** 本质上是某种声明，说明这是深度伪造或 AI 生成的图片。

    **教授：** 对。如果我们看阅读材料中的第一篇文章，作者提到了（我意译一下）三个可能使深度伪造传播在道德上被允许的条件：

    1. **同意：** 你获得了主体的许可。
    2. **声明：** 清晰标注这是 AI 生成的或伪造的。
    3. **无恶意意图：** 你这样做不是为了造成伤害。

  - [12:27 - 12:38] **Harms of Non-Consensual Distribution and the "Liar's Dividend"**

    Okay, so let's explore the third question: What exactly is wrong about non-consensual distribution? This takes us back to the right of someone to represent themselves accurately. What kinds of harms can diverge from the non-consensual sharing of a deepfake?

    **Student:** Personal reputation. For example, if I deepfake a doctor's video having them say something unprofessional, that creates reputational damage to him because it fabricates reality.

    **Professor:** Reputation is key. You want to protect credibility. You don't want people creating a deepfake of you saying something that would destroy that standing.

    **Student:** Humiliation and distress.

    **Professor:** Yes, distress or humiliation. Obviously, if someone puts up a derogatory image of you online, most people feel somewhat distressed—unless you are very thick-skinned. But most would be annoyed or humiliated.

    Now, regarding the need for declaration: Why should we declare a deepfake is fake, even if the sharing is consensual?

    **Student:** If somebody sees it and they are unsure whether it's a deepfake or not, and they share it with somebody else... it's important for the audience to know. It tells people how they should interpret it. If you declare the image as fake, people will likely not try to spread it as news.

    **Student:** Also, we can't assume everyone is technically capable of understanding what is a deepfake. For example, the elderly, or my parents—they aren't necessarily trained the way I am to be skeptical. So labeling protects the vulnerable, the non-technically savvy.

    **Professor:** That's a valid point regarding digital literacy inequality. If these pockets of the population accept these fakes as reality, it distorts their worldview.

    **Student:** And there was a point made in the reading about the "negative dividend"—or the "Liar's Dividend." If we know there are so many deepfakes around that aren't declared, then if you actually say something true that was controversial, you can just claim, "Oh, that was a deepfake." It decreases the overall trust threshold.

    **Professor:** Yes, the **Liar's Dividend**. There is an overall corruption of the information ecosystem. Over the past century, we have become accustomed to accepting that a photograph or recording reflects something real about the world. We rely on these media as evidence. If deepfakes become widespread and unlabeled, we can no longer rely on such media as sources of truth. There is always that doubt in the back of the mind.

    **Student:** Professor, from the perspective of the three parts of ethics—Consequentialism, Deontology, and Virtue—what would you consider a violation of *autonomy*? I'm trying to conceptualize where "autonomy" fits.

    **Professor:** Autonomy doesn't strictly fall into just one bucket; it's a foundational principle often linked to Deontology (rights/duties). But autonomy is important because people have a right to control their own identity, image, and voice. You cannot just put something out there which is not from the actual person. It is a form of dishonesty and deception regarding their self-representation. We all have an interest in how our community sees us.

    好的，让我们探讨第三个问题：未经同意的传播究竟错在哪里？这回到了一个人准确代表自己的权利。未经同意分享深度伪造内容会产生什么样的伤害？

    **学生：** 个人声誉。例如，如果我深度伪造了一个医生的视频，让他说一些不专业的话，这会对他造成声誉损害，因为它伪造了现实。

    **教授：** 声誉是关键。你要保护公信力。你不希望别人通过深度伪造让你说出一些会毁掉你地位的话。

    **学生：** 羞辱和痛苦。

    **教授：** 是的，痛苦或羞辱。显然，如果有人在网上发布你的贬损形象，大多数人都会感到痛苦——除非你脸皮非常厚。但大多数人会感到恼火或受到羞辱。

    现在，关于声明的必要性：即使分享是经当事人同意的，为什么我们仍应声明这是深度伪造？

    **学生：** 如果有人看到它，但不确定它是否是深度伪造，然后他们把它分享给别人……受众知情很重要。这告诉人们应该如何解读它。如果你声明图像是假的，人们可能就不会把它当作新闻传播。

    **学生：** 此外，我们不能假设每个人都有技术能力去理解什么是深度伪造。例如，老年人，或者我的父母——他们不像我这样受过训练去保持怀疑。所以标签保护了弱势群体和非技术精通者。

    **教授：** 关于数字素养不平等的观点很站得住脚。如果这些人群将伪造品视为现实，这会扭曲他们的世界观。

    **学生：** 阅读材料中还提到了关于“负红利”——或者说“说谎者的红利”（Liar's Dividend）的观点。如果我们知道周围有很多未声明的深度伪造，那么当你真的说了一些有争议的真话时，你大可以声称：“噢，那是深度伪造的。”这降低了整体的信任门槛。

    **教授：** 是的，**说谎者的红利**。这导致了信息生态系统的整体腐败。在过去的一个世纪里，我们已经习惯于接受照片或录音反映了世界的某种真实情况。我们依赖这些媒体作为证据。如果深度伪造泛滥且未加标注，我们就再也无法依赖这些媒体作为真理的来源。脑海中总会存有疑虑。

    **学生：** 教授，从伦理学的三个部分——结果论、义务论和美德伦理学——的角度来看，您认为什么是对“自主权”（Autonomy）的侵犯？我正试图概念化“自主权”的位置。

    **教授：** 自主权并不严格属于某一个类别；它是一个通常与义务论（权利/义务）联系在一起的基本原则。但自主权很重要，因为人们有权控制自己的身份、形象和声音。你不能随便发布不属于该人真实意愿的内容。这是一种关于自我表达的欺诈和欺骗。我们都对自己在他者眼中的形象享有利益。

  - [12:41 - 12:47] **The Ethical Trap of "Dog Whistling" (Consensual but Harmful)**

    I think it is interesting to explore the residual problem: Can the consensual *plus* declared distribution of deepfakes still be unethical? Suppose the subject agrees, and I label it "fake." Why can't I just distribute it?

    **Student:** I think it depends a lot on what it's used for. You can use it in manipulative ways to mass manipulate people.

    **Professor:** This suggests an interesting scenario: Imagine a politician. A politician consents to the making of a deepfake of himself, and he declares that the video is fake. In the video, he presents himself saying some sort of horrible thing—maybe making a racist statement or saying that "we should go to war with that country."

    The concern here is that even if the subject has consented and it is labeled, the video could still result in negative consequences because it speaks to a certain base. This is akin to **"dog whistling."** The politician can say, "Oh, I never said this! It's labeled fake!" But he still obviously sanctions the video.

    **Student:** What is dog whistling? And secondly, consent cannot be withdrawn easily once something is shared. It feels like autonomy is lost.

    **Professor:** On the second point, yes, once distributed, you can't really retract it. Regarding **dog whistling**: The idea is that you are saying something unacceptable, but in a subtle way that is understandable only to the specific audience you are trying to reach. It comes from the actual dog whistle, which you blow and only dogs can hear.

    **Student:** Even if labeled, it creates social harm because you are encouraging a particular point of view, but hiding behind plausible deniability.

    **Professor:** Exactly. The person expressing the point of view can hide behind the "fake" label to avoid accountability, while still energizing their base with the inflammatory message.

    我认为探索这个遗留问题很有趣：**经过同意且已声明**的深度伪造传播仍然可能是不道德的吗？假设当事人同意，我也标注了“虚假”。为什么我还是不能传播它？

    **学生：** 我认为这很大程度上取决于它的用途。你可能用它以操纵性的方式来大规模蛊惑人心。

    **教授：** 这提示了一个有趣的场景：想象一位政治家。这位政治家同意制作一段关于他自己的深度伪造视频，并且他声明该视频是假的。在视频中，他表现得自己在说一些可怕的话——比如发表种族主义言论，或者说“我们应该向那个国家开战”。

    这里的担忧在于，即使当事人同意且已标注，该视频仍可能导致负面后果，因为它在向特定的选民基础喊话。这类似于**“狗哨政治”（Dog Whistling）**。政治家可以说：“噢，我从未说过这话！上面标了是假的！”但他显然仍然默许了该视频的存在。

    **学生：** 什么是“狗哨”？其次，一旦分享，同意很难撤回。感觉自主权丧失了。

    **教授：** 关于第二点，是的，一旦传播，你很难真正撤回。关于**狗哨**：它的意思是你在说一些不可接受的话，但用一种微妙的方式，只有你试图接触的特定受众能听懂。这个词来源于实际的犬笛，你吹响它，只有狗能听见。

    **学生：** 即使标注了，它也会造成社会危害，因为你在鼓励某种特定观点，却躲在“合理的推诿”（plausible deniability）背后。

    **教授：** 没错。表达该观点的人可以躲在“虚假”标签后面逃避责任，同时仍利用煽动性信息激励其支持者。

  - [12:47 - 12:57] **Ethics of "Deadfakes": Legacy and the Deceased**

    We've covered the living. Now, what happens if the subject is deceased? Legally, you cannot defame a dead person. Does this mean creating a "deadfake" is always ethically permissible?

    **Student:** I'm drawing an example of a famous public figure like Michael Jackson. Or even locally. I would challenge anyone to make a deepfake video of **Mr. Lee Kuan Yew** and see what happens. Even if autonomy is "out the window" because they are dead, you have a problem with dignity, legacy, and manipulation.

    **Professor:** Right. There are two ways of arguing this. The first is to "bite the bullet" and argue that the deceased retain certain moral rights, even if they aren't around to claim them. This is a difficult philosophical position.

    The second way is to focus on **legacy** and the **historical record**. You should not create a deepfake of someone recently deceased because it causes distress to their loved ones. For someone long deceased, like Julius Caesar, there are no living relatives to distress. However, pollution of the historical record is a concern. If we flood the zone with fake videos of historical figures, it degrades future generations' ability to understand history accurately.

    **Student:** Hypothetically, let's say I want to make a deepfake of a deceased person for a YouTube channel. If I strictly abide by the thinking that "dead people have no rights," I can do whatever I want.

    **Professor:** That's one perspective. But practically, you consider social norms. In Singapore, could you get sued? Yes. Not necessarily for defamation, but perhaps under the **Protection from Harassment Act (POHA)**, depending on the content.

    **Student:** What if I duplicate myself—create a digital twin—so my future grandchildren can speak with my old version? The blame isn't there because I consented, but the consequences—trauma, "going rogue"—are unpredictable.

    **Professor:** In that scenario, you consented to the legacy avatar. The reason is legitimate (family connection). As long as you address the risks, it seems perfectly fine.

    我们已经讨论了活人。现在，如果主体去世了呢？在法律上，你不能诽谤死者。这是否意味着制作“死者伪造”（Deadfake）在伦理上总是允许的？

    **学生：** 我举个著名公众人物的例子，比如迈克尔·杰克逊。或者就在本地。我敢挑战任何人在新加坡制作一段**李光耀先生**的深度伪造视频，看看会发生什么。即使因为他们去世了导致自主权“失效”，你仍然面临尊严、遗产和操纵的问题。

    **教授：** 对。有两种论证方式。第一种是“硬着头皮”（bite the bullet）论证死者保留某些道德权利，即使他们无法主张这些权利。这是一个困难的哲学立场。

    第二种方式是关注**遗产**和**历史记录**。你不应该制作近期去世者的深度伪造，因为这会给他们的亲人带来痛苦。对于像尤利乌斯·凯撒这样去世已久的人，没有在世亲属会感到痛苦。然而，历史记录的污染是一个问题。如果我们用历史人物的虚假视频充斥这一领域，这会降低后代准确理解历史的能力。

    **学生：** 假设我想为 YouTube 频道制作一个死者的深度伪造视频。如果我严格遵守“死人没有权利”的想法，我可以为所欲为。

    **教授：** 那是一个角度。但实际上，你要考虑社会规范。在新加坡，你会由被起诉吗？是的。不一定是诽谤，但根据内容，可能依据**《防止骚扰法案》（POHA）**被起诉。

    **学生：** 如果我复制我自己——创造一个数字双胞胎——这样我未来的孙辈就可以和我（旧版本）对话呢？因为我同意了，所以没有指责，但后果——创伤、AI“失控”——是不可预测的。

    **教授：** 在那种情况下，你同意了制作遗产化身。理由是合法的（家庭联系）。只要你解决了风险，这似乎完全没问题。

  - [12:57 - 13:04] **The "Gun" Argument: Is Releasing AI Tools Unethical?**

    Finally, is creating and sharing the *tool* itself unethical? Think of OpenAI releasing **Sora**, or X releasing **Grok**.

    **Student:** Whether you are a deontologist, consequentialist, or virtue ethicist, you've broken every rule in the book. It facilitates dishonesty (virtue), violates consent duties (deontology), and creates risk (consequentialism).

    **Professor:** But is OpenAI wrong? The argument is often: "We make the gun, we don't kill people." They are selling a tool.

    **Student:** But these tools allow infringement of rights, like making Disney characters without paying Disney.

    **Student:** But Photoshop exists. You can use Photoshop to make fake things. It's just a tool to make the process easier. If Adobe can release Photoshop with no consequence, why is it wrong when Grok releases a tool?

    **Professor:** That is the key counter-argument. Photoshop and AI generators are functionally similar in purpose. The difference is the **barrier to entry**. Photoshop requires skill; you actually need to know what to do to fake a picture. Grok is text-to-image: you just type "make this image." It democratizes the ability to create harm.

    **Student:** I feel like it's covered in law. If you use Photoshop to break the law, you are liable. It's on the user, not the tool.

    最后，创造和分享*工具*本身是不道德的吗？想想 OpenAI 发布 **Sora**，或者 X 发布 **Grok**。

    **学生：** 无论你是义务论者、结果论者还是美德伦理学家，你都打破了书中的每一条规则。它助长了不诚实（美德），违反了同意义务（义务论），并制造了风险（结果论）。

    **教授：** 但 OpenAI 错了吗？通常的论点是：“我们制造枪支，我们不杀人。”他们只是在销售工具。

    **学生：** 但这些工具允许侵犯权利，比如不付钱给迪士尼就制作迪士尼角色的图像。

    **学生：** 但是 Photoshop 是存在的。你可以用 Photoshop 制作虚假的东西。它只是一个让过程更容易的工具。如果 Adobe 发布 Photoshop 没有后果，为什么 Grok 发布工具就是错的？

    **教授：** 这是关键的反驳论点。Photoshop 和 AI 生成器在目的上功能相似。区别在于**准入门槛**。Photoshop 需要技能；你需要真正知道如何操作才能伪造图片。Grok 是“文生图”：你只需输入“制作这张图片”。它使制造伤害的能力大众化了。

    **学生：** 我觉得法律已经涵盖了这点。如果你用 Photoshop 违法，你就要负责。责任在于用户，而非工具。

  - [13:04 - 13:07] **Singapore's Legal Response: Elections and Online Safety**

    I'll wrap this up by clarifying the legal standpoint in Singapore. The first direct attempt to police deepfakes was the **Elections (Integrity of Online Advertising) (Amendment) Act 2024**. The idea is that during an election period, if you publish online election advertising that realistically represents a candidate saying or doing something they did not actually say or do (using digital means), you commit an offense. This prohibits realistic deepfakes of candidates.

    More recently, there is the **Online Safety Bill**. This targets "online harmful activities," including **"intimate image abuse"** (deepfake porn) and **"inauthentic material abuse."** For intimate images, you are not allowed to communicate material that causes harassment, alarm, or distress. The law explicitly includes generative AI data in its illustrations. For inauthentic material abuse, it targets misleading content that causes harm because it is false. These are Singapore's attempts to deal with the problematic uses of deepfake technology without preventing legitimate uses.

    Okay, I should probably pause here for a short break. Let's come back in 10 minutes.

    最后，我通过澄清新加坡的法律立场来总结。新加坡管制深度伪造的第一次直接尝试是**《2024年选举（在线广告诚信）（修正）法案》**。其核心思想是，在选举期间，如果你发布在线选举广告，利用数字手段逼真地通过深度伪造展示候选人说了或做了他们实际上没有说或做的事，你就犯法了。这禁止了对候选人的逼真深度伪造。

    最近，还有**《在线安全法案》**（Online Safety Bill）。该法案针对“在线有害活动”，包括**“私密图像滥用”**（深度伪造色情内容）和**“不真实材料滥用”**。对于私密图像，法律禁止传播会导致骚扰、恐慌或痛苦的材料。法律在说明中明确包含了生成式 AI 数据。对于不真实材料滥用，它针对的是因为虚假而造成伤害的误导性内容。这些是新加坡在不阻止合法用途的情况下处理深度伪造技术问题的尝试。

    好的，我应该在这里暂停休息一下。我们 10 分钟后回来。

  - [13:18 - 13:21] **Facial Recognition: Verification vs. Identification & Real-World Cases**

    We can explain what facial recognition is essentially: it is the use of an AI system to automate the recognition of a person from their facial image. In terms of what it is used for, I think you are also quite familiar, so I don't need to belabor the point. First, normally we use it for **verification**. For example, unlocking your phone—that is a common use case. Or if you are transiting through an airport, like Changi, and they use a biometric sensor to detect and verify who you are against your passport. That is a use case based on verification.

    Separately—and this is the one which is more problematic—facial recognition is also used for **identification**. In particular, it is used for surveillance and policing. In some countries, you already see police forces using cameras that facilitate automatic facial recognition on the streets. I think, in particular, the UK has been quite enthusiastic about rolling out live facial recognition.

    Let's look at some interesting pieces of news relating to this. On the left of the slide, you can see a case involving two ladies who were caught at a supermarket in Singapore—specifically **Sheng Siong**. They were shoplifting, and apparently, the way they were caught was because the supermarket privately used some sort of facial recognition technology to identify who they were. So that is an interesting, and I guess positive, use case for law enforcement.

    However, on the other hand, we have the same situation but a different outcome. In the second article, someone was apparently ejected from a UK supermarket—**Sainsbury's**—because the facial recognition technology malfunctioned. The system falsely identified this innocent shopper as someone else who, I guess, had shoplifted before. The staff approached him and kicked him out. So this is a negative example of the fallibility of facial recognition. And of course, the third article discusses the UK police force rolling out their facial recognition vans, which has caused significant controversy regarding civil liberties.

    我们可以解释一下人脸识别本质上是什么：它是利用 AI 系统自动从面部图像中识别一个人的技术 。关于它的用途，我想大家都很熟悉，所以我不需要赘述。首先，通常我们用它来进行**验证（Verification）** 。例如，解锁你的手机——这是一个常见的用例。或者如果你在机场（比如樟宜机场）过境，他们使用生物识别传感器将你的脸与护照进行比对。这是基于验证的用例。

    另外——这也是更有问题的一种——人脸识别也被用于**识别（Identification）** 。特别是，它被用于监控和警务。在一些国家，你已经可以看到警察部队使用支持自动人脸识别的摄像头。我认为，特别是英国，在推广实时人脸识别方面一直非常积极 。

    让我们看一些相关的新闻。在幻灯片左侧，你可以看到一个案例，涉及两名在新加坡超市——具体是**昇菘超市（Sheng Siong）**——被抓获的女士 。她们在偷窃，显然，她们被抓获的原因是超市私下使用了某种人脸识别技术来确认她们的身份 。所以这是一个有趣的，我想也是积极的执法用例。

    然而，另一方面，同样的情况却有不同的结果。在第二篇文章中，有人显然被驱逐出了英国的一家超市——**森宝利（Sainsbury's）**——因为人脸识别技术发生了故障 。系统错误地将这位无辜的购物者识别为之前偷窃过的人 。工作人员走上前将他赶了出去。所以这是人脸识别可能出错的一个负面例子。当然，第三篇文章讨论了英国警方推出的面部识别车，这引发了关于公民自由的巨大争议 。

  - [13:21 - 13:24] **The Immutability of Biometrics & The Right to Obscurity**

    Okay, so that takes us to our discussion about facial recognition. Largely, the problem is the privacy problem. So we can begin with the first question: What kinds of privacy concerns does facial recognition technology raise? What is so problematic about it?

    **Student:** From the privacy standpoint, things like your face, your fingerprints, your DNA, and so on are inherently problematic because you cannot really change them. Or you can change them, but with great difficulty—I guess you can get plastic surgery. But in general, you can't really change these features of yourself. So the risk is that, unlike a password where if it's hacked you can just change the password, once your biometric data is leaked, you can no longer use it safely as a form of verification. It is compromised forever.

    **Student:** There is also the "right to obscurity." One of the authors argues that even though you are in a public space, you have a right to go unnoticed. Facial recognition makes you become "known" and noticed instantly. It essentially strips away your anonymity in the crowd.

    **Professor:** Right. So I guess one of the reasons people dislike facial recognition is because it causes public places to lose their privacy. When you walk on the street normally, generally people don't know who you are unless you bump into someone that you know. You have a level of anonymity. But if there is facial recognition—if there are cameras everywhere with this technology enabled—there is a sense that wherever you go in public, someone (or some system) knows exactly who you are and where you are. This is an extension of CCTV, but CCTV alone usually requires a human to look at the footage. With facial recognition, identification becomes automated and scalable.

    好的，这把我们带入了关于人脸识别的讨论。很大程度上，问题在于隐私问题。所以我们可以从第一个问题开始：人脸识别技术引发了什么样的隐私担忧？它有什么问题？

    **学生：** 从隐私的角度来看，像你的脸、指纹、DNA 等东西本质上是有问题的，因为你无法真正改变它们。或者你可以改变，但难度很大——我想你可以整容。但一般来说，你无法真正改变这些特征。所以风险在于，不像密码，如果密码被黑客入侵，你只需更改密码即可；一旦你的生物识别数据泄露，你就再也无法安全地将其用作验证形式了。它永远被泄露了。

    **学生：** 还有一个“隐姓埋名的权利”（Right to Obscurity）。其中一位作者认为，即使你在公共场所，你也有权不被注意。人脸识别让你瞬间变得“众所周知”并被注意到。它本质上剥夺了你在人群中的匿名性。

    **教授：** 对。我想人们不喜欢人脸识别的原因之一，是因为它导致公共场所失去了隐私。当你正常走在街上时，通常人们不知道你是谁，除非你撞见熟人。你拥有一定程度的匿名性。但是如果有面部识别——如果到处都有启用这项技术的摄像头——就会有一种感觉：无论你在公共场合走到哪里，某人（或某个系统）都确切知道你是谁、你在哪里。这是闭路电视（CCTV）的延伸，但 CCTV 通常需要人类查看录像。有了人脸识别，身份识别变得自动化且可大规模扩展。

  - [13:24 - 13:30] **Passive Surveillance, Algorithmic Bias & The Panopticon Effect**

    **Student:** It's also very passive. For example, if I use my fingerprint, it is very active; I have to voluntarily go there and press my finger. But facial recognition can identify me from far away without me knowing.

    **Professor:** Okay, so it is a passive thing. Why is that a problem?

    **Student:** Because with a fingerprint, I would at least know I'm being tracked unless my finger was chopped off. But with facial recognition, I can't really know if I got detected. I might be scanned by surveillance without consenting or knowing.

    **Professor:** Exactly. Your image and location are being collected in a surreptitious way. Now, regarding the storage of the image: strictly speaking, the algorithm doesn't necessarily store the *image* itself. It breaks your facial features down into data points—a "face map"—and compares that. Unless the camera is also recording the video feed (which CCTV usually does), the risk isn't just the image storage, but the identification data.

    **Student:** Back to the article, the collection of face data can leak into training data without consent. Also, there is the issue of discrimination.

    **Professor:** Yes, there is the potential risk that your facial image will be used for discrimination. Research has shown that facial recognition technology works better on certain demographics (e.g., white males) than others (e.g., women or people of color). If you belong to a demographic that the technology is not accurate for, you are more likely to be the subject of errors—false positives or false negatives. This creates a specific harm of algorithmic bias.

    **Student:** I think this technology places a mental burden on people. If people know they are being recognized by cameras, it creates a "Panopticon" effect. It eventually affects the connection between privacy and liberty. If you know that you are always being watched, you will change your behavior. You will feel like you cannot do things that you otherwise might want to. For example, you may think twice before shaking someone's hand, or attending a protest, if you think that action will be logged.

    **Professor:** Yes, the "chilling effect." If surveillance is ubiquitous, you self-censor. It depends on the circumstances. If you use FaceID on your iPhone just to unlock it, that seems fine. But if the government is secretly tracking everywhere you walk, that gives rise to serious privacy concerns regarding civil liberties.

    **学生：** 它也非常被动。例如，如果我使用指纹，那是很主动的；我必须自愿去那里按下手指。但人脸识别可以在我很远的地方识别我，而我却毫不知情。

    **教授：** 好的，所以它是被动的。为什么这是一个问题？

    **学生：** 因为用指纹的话，我至少知道我被追踪了（除非我的手指被砍掉了）。但对于人脸识别，我无法真正知道我是否被检测到了。我可能在不知情或未同意的情况下被监控扫描。

    **教授：** 没错。你的图像和位置是以一种秘密的方式被收集的。现在，关于图像的存储：严格来说，算法不一定会存储*图像*本身。它将你的面部特征分解为数据点——一张“人脸地图”——并进行比对。除非摄像头同时也在录制视频流（CCTV 通常会这样做），否则风险不仅仅是图像存储，而是身份识别数据。

    **学生：** 回到文章，面部数据的收集可能会在未经同意的情况下泄露到训练数据中。此外，还有歧视的问题。

    **教授：** 是的，存在你的面部图像被用于歧视的潜在风险。研究表明，人脸识别技术在某些人口统计群体（如白人男性）身上的效果优于其他群体（如女性或有色人种）。如果你属于该技术不准确的人群，你就更有可能成为错误（误报或漏报）的受害者。这造成了算法偏见的特定伤害。

    **学生：** 我认为这项技术给人们带来了精神负担。如果人们知道自己被摄像头识别，就会产生“全景监狱”（Panopticon）效应。它最终会影响隐私与自由之间的联系。如果你知道自己一直被监视，你就会改变你的行为。你会觉得你不能做你本来想做的事情。例如，如果你认为握手或参加抗议会被记录下来，你可能会在做这些事之前三思而后行。

    **教授：** 是的，“寒蝉效应”。如果监控无处不在，你就会自我审查。这取决于具体情况。如果你只是用 iPhone 的 FaceID 解锁手机，那似乎没问题。但如果政府秘密追踪你走过的每一个地方，这就会引发关于公民自由的严重隐私担忧。

  - [13:32 - 13:35] **The Illusion of Consent in Ubiquitous Surveillance**

    **Professor:** Let's move to the second point: Notice and Consent. Let's say the supermarket puts up a sign saying, "If you walk in this shop, we have facial recognition. By entering, you agree to being tracked." Does that necessarily make the use of facial recognition ethical?

    **Professor:** Hardly. As we discussed, even if you give consent—you see the cameras, you see the sign—it doesn't change the fact that you lose that sense of obscurity. Furthermore, notice and consent may not be *real* consent. If you need to buy food, or you need to walk on that street to get home, you don't really have a choice. It seems like in some circumstances, consent is coerced by necessity.

    **Student:** Sorry, I just want to add that how prevalent the technology is makes a difference. Like you mentioned, if we need to go to the shop or walk the street... but if *all* the streets have this technology, then it's almost like we cannot avoid our face being recognized. That becomes a huge privacy concern. In countries like Singapore, CCTV and smart cameras are so prevalent that they are there all the time. Some of us might have this false sense that we are private, but actually, that's not true.

    **Professor:** Yes, what you mentioned raises the point about ubiquity. When these technologies become ubiquitous, there is no escape. Even if they offer an "opt-out"—"don't walk here"—if there is no practical alternative route, the choice is an illusion.

    **教授：** 让我们进入第二点：通知与同意 。假设超市贴了一个告示说：“如果你走进这家店，我们有人脸识别。进入即表示你同意被追踪。”这是否必然使人脸识别的使用符合伦理？

    **教授：** 很难说。正如我们讨论的，即使你给予了同意——你看到了摄像头，看到了告示——这并不能改变你失去隐秘感的事实。此外，通知和同意可能并不是*真正的*同意。如果你需要买食物，或者你需要走那条街回家，你真的没有选择。在某些情况下，同意似乎是被迫的。

    **学生：** 对不起，我只想补充一点，技术的普及程度会有很大影响。就像您提到的，如果我们需要去商店或走在街上……但如果*所有*街道都有这种技术，那么我们几乎无法避免自己的脸被识别。这就成了一个巨大的隐私问题。在像新加坡这样的国家，CCTV 和智能摄像头非常普及，它们无时无刻不在。我们中的一些人可能会有一种虚假的隐私感，但实际上那不是真的。

    **教授：** 是的，你提到的这一点引出了关于普遍性（Ubiquity）的问题。当这些技术变得无处不在时，就无处可逃了。即使他们提供“选择退出”的选项——“不要走这里”——如果没有实际的替代路线，这种选择就是一种幻觉。

  - [13:36 - 13:44] **The Slippery Slope Argument & Function Creep**

    **Professor:** The last question is specific and comes from the articles: the "Slippery Slope". What is the slippery slope argument as applied to facial recognition?

    **Student:** The slippery slope suggests a tendency that the government may monitor people's lives in unlimited fields. It will invade our privacy not only in specific areas like business or security but also limit freedom in our daily life.

    **Professor:** Right. The argument is that we eventually reach a point of total or heavy surveillance. The question is: How do we get there? We get there through a slippery slope. You start *here* with limited surveillance—perhaps just for terrorists. But because you have already spent the money and built the infrastructure (positioned the cameras), this lowers the cost of the next step.

    **Student:** It connects to data privacy. Like the case we discussed last semester about the marketing company. They start collecting minimal personal info, and then proceed to collect more sensitive data for profiling. It starts very small but becomes an avalanche.

    **Professor:** Yes, that is the concept of **"Function Creep."** You collect data for one legitimate purpose (e.g., verifying identity at a border). But once you have that database, you suddenly realize, "Oh, I already have this data. I can use this for something else." And then the original limitation is lost.

    **Student:** Or they might argue for a starting point using an extreme case, like a terrorist or a hellfire missile targeting a kidnapper. Nobody will argue against catching a terrorist. But they use these morally extreme cases to get permission, and then later on down the line, they hide their intention to use it for parking tickets or advertising. It's one step to "catch the killer," another step to "catch the jaywalker."

    **Professor:** Exactly. They use proportionate reasons initially. "If you don't do this, terrorists will win." That starts the process. Once the cameras are up, the logic shifts: "Now that we have this camera, what else can we use it for? What other types of criminals can we catch?" If you are aware of this slippery slope, it should impact your decision-making at the start. You have to account for the possibility that this technology *will* allow for expanded surveillance later.

    As far as policing is concerned, there is this ethical tension between wanting individuals to be private versus wanting to catch criminals and terrorists. That is a tight ethical balance for any police force to strike.

    **教授：** 最后一个问题很具体，来自文章：“滑坡效应”（Slippery Slope）。应用于人脸识别时的滑坡论证是什么？

    **学生：** 滑坡效应表明政府可能会在无限的领域监控人们的生活。它不仅会在商业或安全等特定领域侵犯我们的隐私，还会限制我们日常生活的自由。

    **教授：** 对。这个论点是说我们最终会达到全面或重度监控的地步。问题是：我们是如何到达那里的？我们通过滑坡到达。你从*这里*开始，进行有限的监控——也许只是为了抓恐怖分子。但因为你已经花了钱，建立了基础设施（安装了摄像头），这就降低了下一步行动的成本。

    **学生：** 这与数据隐私有关。就像我们上学期讨论的那家营销公司的案例。他们开始只收集极少的个人信息，然后进而收集更敏感的数据用于画像。它开始时非常小，但最后变成了雪崩。

    **教授：** 是的，这就是**“功能泛化”（Function Creep）**的概念。你为了一个合法的目的（例如边境身份验证）收集数据。但一旦你拥有了那个数据库，你突然意识到：“噢，我已经有这些数据了。我可以把它用在别的地方。”然后最初的限制就消失了。

    **学生：** 或者他们可能会用一个极端的案例作为起点，比如抓恐怖分子或用“地狱火”导弹打击绑架者。没有人会反对抓捕恐怖分子。但他们利用这些道德上极端的案例获得许可，然后在之后隐藏他们将其用于停车罚单或广告的意图。从“抓捕杀人犯”到“抓捕乱穿马路者”只有一步之遥。

    **教授：** 没错。他们最初使用相称的理由。“如果你不这样做，恐怖分子就会赢。”这启动了流程。一旦摄像头安装好，逻辑就变了：“既然我们有了这个摄像头，我们还能用它做什么？我们还能抓到什么类型的罪犯？”如果你意识到了这种滑坡效应，这应该会影响你一开始的决策。你必须考虑到这项技术在未来*必然*会导致监控扩大的可能性。

    就警务而言，存在着这种伦理张力：既希望个人享有隐私，又希望抓捕罪犯和恐怖分子。对于任何警察部队来说，这都是一个需要把握的严峻的伦理平衡。

  - [13:44 - 13:48] **Recommender Systems: Addiction, Polarization & Pay-for-Play**

    **Professor:** Let me finish up with the next part. This relates to **Recommender Systems**. If you have used any sort of online platform, you will have interacted with one. A recommender system is a system that automates how the platform recommends content to the user.

    This is how many modern platforms work. Facebook didn't use to work this way; in the early days, your feed was just who you were following or your friends—chronological order. There was no "For You" algorithm. But today, Netflix, TikTok, and Instagram all rely on it. TikTok is entirely driven by recommendations, which is why it is so popular.

    I've given three examples in the slides.

    1. **TikTok & Addiction:** The EU charged TikTok for making their app too addictive. It is almost designed to be addictive through its "infinite scroll" and "autoplay" features.
    2. **Political Bias:** A private study showed that platforms like TikTok and X (Twitter) presented views which lead towards the **far-right** in the midst of a German federal election. This raises the concern that recommender systems can tilt people politically.
    3. **Spotify & Fairness:** A class action lawsuit in America alleges that Spotify is not fair in its recommendations. It seems the system is weighted towards labels or artists who are effectively "paying" them (a modern "payola" or "pay-for-play"), rather than purely based on user taste.

    So, based on your personal understanding, what kinds of ethical concerns arise when you use a recommender system?

    **教授：** 让我来结束下一部分。这与**推荐系统（Recommender Systems）**有关 。如果你使用过任何类型的在线平台，你都会与它互动。推荐系统是一种自动向用户推荐内容的系统 。

    这就是许多现代平台的运作方式。Facebook 以前不是这样运作的；在早期，你的信息流只是你关注的人或你的朋友——按时间顺序排列。没有“为你推荐”算法。但今天，Netflix、TikTok 和 Instagram 都依赖它。TikTok 完全由推荐驱动，这就是它如此受欢迎的原因 。

    我在幻灯片中给出了三个例子 。

    1. **TikTok 与成瘾：** 欧盟指控 TikTok 使其应用过于令人上瘾 。它几乎就是通过“无限滚动”和“自动播放”功能被设计成令人上瘾的 。
    2. **政治偏见：** 一项私人研究表明，像 TikTok 和 X（Twitter）这样的平台在德国联邦选举期间展示了倾向于**极右翼**的观点 。这引发了推荐系统可能在政治上倾斜人群的担忧。
    3. **Spotify 与公平性：** 美国的一起集体诉讼指控 Spotify 的推荐不公平。看来该系统倾向于那些实际上在“付钱”给他们的唱片公司或艺术家（一种现代的“收受贿赂”或“付费播放”），而不是纯粹基于用户口味 。

    那么，根据你的个人理解，当你使用推荐系统时会出现什么样的伦理问题？

  - [13:48 - 13:52] **Echo Chambers, Polarization & Platform Incentives**

    **Student:** One thing that is really bothering me is that it can easily have a tendency to bias the population and polarize it. If you are a little bit left-leaning, for instance, and you click on left-leaning media, you will get a lot more of it and see no other side. The same piece of work for the right-leaning media. You don't get the balance. It polarizes society because you only see one side of the newspaper, so to speak.

    **Professor:** Right. The recommender system tries to show you things that look like what you looked at before. If you looked at right-wing content, it shows you more of the same. The theory is that this leads to a greater degree of **polarization** because people stop seeing the other side. This creates an **echo chamber**. Why might this happen?

    **Professor:** Ideally, we would want the system to be transparent. But I suspect it may be impossible for certain systems to be transparent. This goes into the incentives. If you are a platform and you rely on views and ads, your incentive is to keep people on the platform as long as possible. Your incentive is to make your platform *addictive*. That's why we've seen the EU take action.

    **学生：** 有一件事真的很困扰我，那就是它很容易产生偏见并使人群极化。例如，如果你稍微倾向左翼，并且你点击了左翼媒体，你会得到更多此类内容，而看不到另一方。右翼媒体也是如此。你得不到平衡。它使社会极化，因为可以说你只看到了报纸的一面。

    **教授：** 对。推荐系统试图向你展示与你之前看过的东西相似的内容。如果你看了右翼内容，它会向你展示更多相同的内容。理论上，这会导致更大程度的**极化（Polarization）**，因为人们不再看到另一方。这创造了一个**回声室（Echo Chamber）**。为什么会发生这种情况？

    **教授：** 理想情况下，我们希望系统是透明的。但我怀疑某些系统可能根本无法做到透明。这涉及激励机制。如果你是一个平台，并且你依赖浏览量和广告，你的动机就是让人们尽可能长时间地留在平台上。你的动机是让你的平台*令人上瘾*。这就是为什么我们看到欧盟采取行动。

  - [13:52 - 13:57] **Loss of Agency, Accountability & Filter Bubbles**

    **Student:** I thought one step before the polarization is actually the loss of the **freedom to choose**. Like what you mentioned when Facebook started—you get to see all the feeds from all your friends. Then slowly they decide to recommend to you, so you don't see the rest. Now your freedom to choose is lost and delegated to the recommendation system.

    **Professor:** Interesting. Could you counter-argue that if you don't like the platform, you just leave?

    **Student:** But if all your friends are on it, then to get out of it means you're missing out socially.

    **Professor:** Right, the **network effect**. If everyone uses the app, you may feel excluded if you don't. So you effectively don't have a choice.

    **Student:** Also, because power is concentrated on these platforms, they are often not **accountable** for the recommendations. From an e-commerce perspective, if they recommend certain shoes that are really not suitable for me, they are not held accountable. For social media, if they push a certain narrative during an election and I choose a candidate that is not suitable based on bad info, they aren't liable. It is very difficult for authorities to chase after every wrong recommendation.

    **Professor:** It is a regulatory concern. The networks have grown to such an extent that it is quite difficult to practically regulate them given their immense economic power. One of the older terms used for this is the **"Filter Bubble,"** where the algorithm filters out everything not related to what you watched before. You end up with a curated, distorted view of the world.

    **学生：** 我认为在极化之前的一步实际上是**选择自由**的丧失。就像你提到的 Facebook 刚开始的时候——你可以看到所有朋友的所有动态。然后慢慢地，他们决定向你推荐，所以你看不到了其余的内容。现在你的选择自由丧失了，被让渡给了推荐系统。

    **教授：** 有意思。你能反驳说，如果你不喜欢这个平台，你离开就是了吗？

    **学生：** 但如果你所有的朋友都在上面，那么退出意味着你在社交上错过了。

    **教授：** 对，**网络效应（Network Effect）**。如果每个人都使用该应用程序，如果你不使用，你可能会感到被排斥。所以你实际上没有选择。

    **学生：** 此外，由于权力集中在这些平台上，它们通常不对推荐**负责**。从电子商务的角度来看，如果他们推荐了某些确实不适合我的鞋子，他们不承担责任。对于社交媒体，如果他们在选举期间推行某种叙事，而我基于错误信息选择了不合适的候选人，他们也不负责任。当局很难追究每一个错误的推荐。

    **教授：** 这是一个监管问题。网络已经发展到如此程度，鉴于其巨大的经济实力，实际上很难对它们进行监管。用于描述这一点的旧术语之一是**“过滤气泡”（Filter Bubble）**，算法会过滤掉所有与你之前观看的内容无关的东西。你最终得到的是一个被策划的、扭曲的世界观。

  - [13:57 - 14:04] **Impact on Creators, Market Manipulation & Data Privacy**

    **Professor:** There is a broader point: Creators are reliant on the platform. They have to **game the algorithm**. If you are a YouTuber, you can guess what videos YouTube will recommend: use clickbait titles, make shorter videos, or controversial ones. The platforms are responsible for shaping the media landscape, perhaps in ways that are not good. Videos that are objective or boring don't get incentivized.

    **Student:** I think it is directly related to last week's topic on **manipulating prices and personalized pricing**. The recommendation engine is the main tool used to steer users. It makes the platform producers more powerful. They can steer the price higher for users by recommending more expensive products. Finally, the big producers can take the whole market.

    **Professor:** Yes, the system infers what the business needs. After they collect enough data about you, they make inferences about your willingness to pay.

    **Student:** Sometimes we are talking about something, and then an ad appears. It feels like they are listening.

    **Professor:** That's a potential concern—not directly related to the recommender logic itself, but the **data collection** that feeds into it. They collect data to recommend things better, but it raises privacy issues if they disclose that or infer sensitive information you didn't want shared.

    **教授：** 还有一个更广泛的观点：创作者依赖于平台。他们必须**博弈算法**。如果你是一个 YouTuber，你能猜到 YouTube 会推荐什么样的视频：使用诱饵标题（clickbait），制作更短的视频，或有争议的视频。平台负责塑造媒体格局，但这可能并非以好的方式。客观或无聊的视频不会受到激励。

    **学生：** 我认为这与上周关于**操纵价格和个性化定价**的话题直接相关。推荐引擎是用来引导用户的主要工具。它使平台生产者更加强大。他们可以通过推荐更昂贵的产品来推高用户的价格。最终，大生产商可以占据整个市场。

    **教授：** 是的，系统会推断企业的需求。在他们收集了关于你的足够数据后，他们会推断你的支付意愿。

    **学生：** 有时我们在谈论某事，然后广告就出现了。感觉他们在监听。

    **教授：** 这是一个潜在的担忧——虽然与推荐逻辑本身没有直接关系，但与为其提供养料的**数据收集**有关。他们收集数据是为了更好地推荐东西，但如果他们泄露了数据或推断出你不希望分享的敏感信息，这就引发了隐私问题。

  - [14:05 - 14:07] **The "Superstar" Effect & Conclusion**

    **Professor:** I want to raise one last point particular to media platforms like Spotify or YouTube. There is a concern about how these systems tend to favor songs that are already popular. This means songs which are not really popular become even less visible.

    There is a very, very **"long tail"** of unpopular songs, and a few "head" songs that are very popular and always recommended. It has the effect of excluding creators who have not achieved a certain degree of fame. You might regard that as an unfair way to treat new artists.

    Okay, we are actually done for today's lesson. I won't see you next week (Recess Week). I will see you the following week. To all of you who are celebrating, **Happy Chinese New Year**.

    **教授：** 我想提出针对 Spotify 或 YouTube 等媒体平台的最后一点。人们担心这些系统往往会偏向那些已经很流行的歌曲 。这意味着原本就不流行的歌曲会变得更加不可见。

    存在一条非常非常长的冷门歌曲**“长尾”（Long Tail）**，以及少数非常流行且总是被推荐的“头部”歌曲。这种效应会将那些尚未达到一定知名度的创作者排除在外。你可能会认为这是对待新艺术家的不公平方式。

    好的，我们今天的课实际上已经结束了。下周我见不到你们（休会周）。我会在下下周见到你们。祝所有庆祝节日的人**农历新年快乐**。
***

# Reading
## Week 1
  - Introduction to AI
    - Adriana Placani, ‘Anthropomorphism in AI: hype and fallacy’ (2024) 4 AI and Ethics 691.
    - Alva Markelius and others, ‘The mechanisms of AI hype and its planetary and social costs’ (2024) 4 AI and Ethics 727.
    - Blake Richards and others, ‘The Illusion of AI’s Existential Risk’ (Noema, 2023).
  - Introduction to AI ethics
    - Sheila Bonde and Paul Firenze, ‘A Framework for Making Ethical Decisions’ (2013).
    - (OPTIONAL) Frank Aragbonfoh Abumere, ‘Utilitarianism’ in George Matthews (ed), Introduction to Philosophy: Ethics (2019).
    - (OPTIONAL) Joseph Kranak, ‘Kantian Deontology’ in George Matthews (ed), Introduction to Philosophy: Ethics (2019).
    - (OPTIONAL) Douglas Giles, ‘How Can I Be a Better Person? On Virtue Ethics’ in George Matthews (ed), Introduction to Philosophy: Ethics (2019).
  - Introduction to AI governance
    - Organisation for Economic Co-operation and Development, ‘Recommendation of the Council on Artificial Intelligence’ (2025), section 1.
    - Association of Southeast Asian Nations, ‘ASEAN Guide on AI Governance and Ethics’ (2024) at pages 10-16.
    - Marco Pasqua, ‘Bridging Soft and Hard Law in AI Governance’ (2025).
## Week 2
  - Ethics – autonomous weapons
    - Peter Asaro, ‘Autonomous Weapons and the Ethics of Artificial Intelligence’ in S Matthew Liao (ed), Ethics of Artificial Intelligence (Oxford University Press 2020).
    - Michael Haiden and Florian Richter, ‘Autonomous weapons: considering the rights and interests of soldiers’ (2025) 27 Ethics and Information Technology 52.
  - Ethics – autonomous vehicles
    - Sven Ove Hansson, Matts-Åke Belin and Björn Lundgren, ‘Self-Driving Vehicles – an Ethical Overview’ (2021) 34 Philosophy and Technology 1383.
    - Alexander Hevelke and Julian Nida-Rümelin, ‘Responsibility for Crashes of Autonomous Vehicles: An Ethical Analysis’ (2015) 21 Science and Engineering Ethics 619.
    - Antje von Ungern-Sternberg, ‘Autonomous driving: regulatory challenges raised by artificial decision-making and tragic choices’ in Woodrow Barfield and Ugo Pagallo (eds), Research Handbook on the Law of Artificial Intelligence (Elgar 2018).
## Week 3
  - **Ethics – AI-facilitated anti-competitive business practices.** What does it mean for a business practice to be anti-competitive? What anti- competitive business practices does AI facilitate, and how does AI facilitate those practices? Why are such anti-competitive business practices unethical?
    - Maurice E Stucke and Ariel Ezrachi, ‘Antitrust, algorithmic pricing and tacit collusion’ in Woodrow Barfield and Ugo Pagallo (eds), Research Handbook on the Law of Artificial Intelligence (Elgar 2018).
    - Thomas K Cheng and Julian Nowag, ‘Algorithmic Predation and Exclusion’ (2023) 25 University of Pennsylvania Journal of Business Law 41.
  - **Ethics – AI-facilitated personalised pricing.** What is personalised pricing? How does AI facilitate personalised pricing? Why might personalised pricing be unethical (and is it always so)?
    - Akiva A Miller, ‘What Do We Worry About When We Worry About Price Discrimination? The Law and Ethics of Using Personal Information for Pricing’ (2014) 19 Journal of Technology Law and Policy 41.
    - Jeffery Moriarty, ‘Why online personalized pricing is unfair’ (2021) 23 Ethics and Information Technology 495.
## Week 4
  - **Ethics – Deepfakes** What are the harms of deepfakes, and are the ethical issues with deepfakes principally about their potentially harmful consequences? Might the making of deepfakes be intrinsically wrongful?
    - Adrienne de Ruiter, ‘The Distinct Wrong of Deepfakes’ (2021) 34 Philosophy & Technology 1311.
    - Daniel Story and Ryan Jenkins, ‘Deepfake Pornography and the Ethics of Non‑Veridical Representations’ (2023) 36 Philosophy & Technology 55.
    - Bobby Chesney and Danielle Citron, ‘Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security’ (2019) California Law Review 1753.
  - **Ethics – Facial recognition** What might be the ethical problems with the use of facial recognition technology? Does it matter (i) who is using it, (ii) where it is used, (iii) and why it is used? What are the social implications if the use of facial recognition technology is widespread?
    - Philip Brey, ‘Ethical Aspects of Facial Recognition Systems in Public Places’ (2004) 2 Journal of Information, Communication and Ethics in Society 97.
    - Evan Selinger and Brenda Leong, ‘The Ethics of Facial Recognition Technology’ in Carissa Véliz (ed), The Oxford Handbook of Digital Ethics (Oxford University Press 2024).
***
