# __DSA5204 Deep Learning and Applications___
***

# Info
  Course description
  This course is designed to provide students with a comprehensive understanding of deep learning, a pivotal machine learning technique with extensive applications in artificial intelligence and data sciences. The curriculum focuses on the introduction of fundamental concepts, numerical algorithms, and computing frameworks pertinent to deep learning. Special emphasis is placed on numerical algorithms and their implementation within industrial computing frameworks, as well as the analysis of data-intensive problems derived from real-world applications.

  Core topics covered in this course include: Foundations of Neural Networks; Optimization Strategies in Deep Learning; Representative Network Architectures; Regularization and Generalization; Model Evaluation and Tuning; Deep Learning Frameworks and Implementation Applications in Natural Language Processing and Computer Vision.
***

# Lectures
## Week 1
  * [14:10–14:14] **Course logistics: assessments, Q&A, textbook**
    **English** At the end of the semester (Week 13), you must submit two items: (1) a 15-minute recorded video presentation of your project, and (2) a formal written report. The Q&A tool briefly stopped working but was restored. Pay attention to deadlines: there is a late-submission policy with a 30% penalty per day late. For example, if an assignment is 7/10 on time, one day late reduces it to about 4/10. For the group project, most of the time everyone in a group receives the same grade, but there will be an end-of-semester peer survey to flag unequal contributions. Lectures are recorded and uploaded to Canvas. Questions should be asked in class, during consultation, or via the Canvas discussion board. For roughly the first 10 lectures, we will follow the textbook *Deep Learning* (the “Deep Learning Book”), which is free online at deeplearningbook.org. The test format will be explained later; it will be open-book with no strict limit on what hard copies you can bring.

    **Chinese** 学期末（第 13 周）你们要交两样东西：（1）一个 15 分钟的项目视频展示；（2）一份正式的书面报告。课堂的 Q&A 功能刚才一度出问题，后来恢复了。注意截止日期：迟交每天扣 30%。比如原本 7/10，迟交一天大概只剩 4/10。小组项目通常全组同分，但学期末会有同伴互评/问卷，用来识别有人贡献特别多或特别少的情况。课程会录制并上传到 Canvas。提问可以课堂举手、咨询时间问，或在 Canvas 讨论区发帖（推荐）。前 10 讲左右主要跟一本教材：*Deep Learning*（可在 deeplearningbook.org 免费下载）。考试形式后面再细说：是开卷，能带多少纸质资料基本不限制。

  * [14:14–14:21] **What are AI, ML, Deep Learning? Turing Test and motivation**
    **English** Deep learning is a branch of machine learning (ML), which itself is part of artificial intelligence (AI). AI’s goal is to understand and replicate intelligence. People are inspired by biological neural networks (brains), and popular culture has long imagined AI—from human-like robots to more “useful but not human-looking” systems, and even chatbots that feel human-like. Alan Turing reframed “Can machines think?” into a more concrete operational test: the Turing Test (Imitation Game). There are three parties: a computer (A), a human (B), and an interrogator (C) who can only communicate via text and must decide which is the human. If the machine fools the interrogator sufficiently often (commonly cited as >70%), it is called “intelligent” under this criterion. Modern LLMs may appear to pass, but the lecturer argues there are still ways to distinguish them (e.g., breadth and style of responses). Machine learning then becomes: how can machines *learn from data* to do tasks that thinking beings do. Deep learning is a subset of ML focusing on (deep) neural networks.

    **Chinese** 深度学习是机器学习（ML）的一个分支，而机器学习属于人工智能（AI）的大框架。AI 的目标是理解并尝试复现“智能”。生物学里研究大脑神经网络，也启发了人工神经网络。电影/文化里对 AI 的想象也经历变化：早期更“像人”的 AI，后来更务实，变成“能做事但不一定长得像人”的系统；现在的聊天机器人在语言上越来越像人。图灵把“机器能思考吗？”这种不够清晰的问题，改成更可操作的图灵测试（模仿游戏）：有电脑 A、人类 B、审问者 C。C 只能用文字对话，判断谁是人。如果机器能在足够高比例（常见说法是超过 70%）骗过审问者，就按这个标准算“智能”。如今的大语言模型看似很接近，但老师认为仍有办法分辨（比如回答风格、覆盖面等）。机器学习的核心就变成：机器如何从数据中学习，去完成“有智能的行为/任务”。深度学习则是机器学习里专门研究（深层）神经网络的一部分。

  * [14:21–14:29] **Why deep learning became successful: history, data, compute, software**
    **English** Neural networks are not new: early models date back to the 1940s; key training ideas such as backpropagation emerged in the 1950s; by the 1990s, architectures like CNNs and RNNs already existed. However, they were not dominant until around 2012, when deep CNNs achieved state-of-the-art performance in computer vision (ImageNet era). After that, deep learning rapidly advanced across areas: GANs for image generation, reinforcement learning systems (e.g., AlphaGo in 2016), transformer-based language models (e.g., BERT), protein folding (AlphaFold), diffusion models, and large language models. A key reason for post-2012 success is scale: deep models have many parameters and can learn features automatically, but they need lots of data; digitization produced that data. To implement deep learning effectively, you also need hardware (GPUs) and software frameworks. This course will mainly use TensorFlow and Keras (especially for assignments/demos), though projects may use other libraries.

    **Chinese** 神经网络并不是新东西：1940 年代就有早期模型；1950 年代提出了关键训练思想（反向传播相关思想）；到 1990 年代，CNN、RNN 等结构基本都出现了。但它们直到 2012 年左右才真正“统治”，典型标志是深度 CNN 在视觉任务上达到/超过当时最强水平（ImageNet 那一波）。之后深度学习在多个方向爆发：GAN 做图像生成；强化学习系统（如 2016 年 AlphaGo）；基于 Transformer 的语言模型（如 BERT）；蛋白质折叠（AlphaFold）；扩散模型；以及现在的大语言模型等。为什么 2012 后突然这么强？核心是“规模”：深度模型参数多、表达能力强，可以自动学特征，但前提是需要海量数据；社会数字化带来了数据。同时还需要算力（GPU）和好用的软件框架。本课主要用 TensorFlow + Keras（作业/演示尽量统一方便批改），项目可用其他库。

  * [14:29–14:36] **Applications and limitations; course goals; project expectations; prerequisites**
    **English** Deep learning has many real applications: autonomous driving (still not fully solved), medical imaging diagnosis, translation (strong for major languages), and reinforcement learning for decision-making/games. However, models have limitations such as adversarial examples: small malicious perturbations can cause confident wrong predictions, which can be dangerous in safety-critical systems. The course aims to cover important deep learning innovations, architectures, and learning algorithms, plus practical issues like overfitting and generalization; regularization will take about two lectures. The group project is designed to build “learning how to learn”: choose a research paper not covered in class, understand it, reproduce results, and make a small extension (e.g., tweak the model or test on a slightly different dataset/problem). Expected background: undergraduate-level linear algebra and probability; basic Python; familiarity with NumPy/SciPy and plotting tools is helpful (the transcript mentions pandas/seaborn for visualization). TensorFlow will be used in the course, and tutorials are provided on Canvas.

    **Chinese** 深度学习应用很多：自动驾驶（仍未完全解决）、医学影像诊断、机器翻译（主流语言已经很强）、强化学习做决策/玩游戏等。但也要看到限制，比如对抗样本：输入做一点点“恶意但很小”的改动，模型就可能非常自信地输出错误结果；如果在自动驾驶等安全场景会很危险。课程目标是介绍深度学习的重要创新：网络结构、学习算法，以及实践中最关键的问题——过拟合与泛化；正则化会用大约两讲来系统讲。小组项目的目的之一是训练“学会学习”：选一篇课堂没覆盖的论文，读懂它、复现结果，并做一个小扩展（比如小改模型，或换一个相近数据集/问题）。背景要求：本科水平线性代数与概率；会基础 Python；熟悉 NumPy/SciPy 和画图工具更好（记录里提到用 pandas/seaborn 做可视化）。课上用 TensorFlow，Canvas 上也会给入门教程。

  * [14:37–14:47] **Definition of machine learning: Task–Experience–Performance (T–E–P)**
    **English** A poll checks whether students have taken prior ML courses, since the first two lectures review ML basics. The standard definition: a program learns from experience **E** with respect to a task **T** and performance measure **P** if performance at **T**, measured by **P**, improves with **E**. ML is “inverse” of traditional programming: instead of hand-coding rules, you provide input–output examples and the system learns a mapping/program. Examples of tasks: prediction (e.g., house price), transcription (e.g., CAPTCHA/handwritten text), translation, and generation (e.g., realistic faces). Experience is data; data are ultimately represented as numbers (e.g., images as pixel matrices). Performance measures: regression often uses mean squared error; classification often uses accuracy (for evaluation); generation is harder to evaluate because you want realism *and* diversity. Break: 10 minutes, then regression as the anchor example.

    **Chinese** 老师先做了个投票，看看大家是否上过 DSA5102/5105 之类的机器学习基础课，因为前两讲会复习基础。机器学习的经典定义是 T–E–P：如果一个程序在任务 **T** 上、用指标 **P** 衡量的性能，会随着经验 **E**（数据/训练样本）的增加而提升，那么它就在“学习”。机器学习可以看作和传统编程相反：传统是人写规则+输入→输出；机器学习是给大量输入–输出样本，让系统自己学出映射/“程序”。任务例子：预测（房价）、转写（手写识别/CAPTCHA）、翻译、生成（生成逼真的人脸）等。经验 E 就是数据；而数据最终都要用数字表示（例如图像就是像素矩阵）。性能指标方面：回归常用均方误差；分类评估常用准确率；生成类任务更难，因为既要“像真的”也要“有多样性”。接着休息 10 分钟，然后用回归做一个核心示例。

  * [15:03–15:36] **Regression as anchor: hypothesis space, loss, ERM, linear/affine models, basis functions, generalization**
    **English** In regression, there is an unknown true relationship (f^*) mapping input (x) to output (y). We observe a dataset of (n) samples ((x_i, y_i)) (optionally with noise). We choose a **hypothesis space** ( \mathcal{H} ): a set of candidate functions (f) (e.g., all linear functions). To measure how close a model is, define a **loss** for one example; for regression a common choice is squared loss ((f(x)-y)^2). The average loss over the dataset is the **empirical risk**. Learning is framed as **empirical risk minimization (ERM)**: find (\hat f) that minimizes average loss on the training data.

    For linear regression with (f(x)=w^\top x), ERM has a closed-form solution (least squares): with matrix (X) and vector (y), the optimum is (w = (X^\top X)^{-1}X^\top y) (when invertible). This is a special “luxury”; for neural networks you can compute gradients, but you usually cannot solve (\nabla=0) exactly.

    The lecture distinguishes **linear** (through origin) vs **affine** models (w^\top x + b). Affine regression can be solved similarly by augmenting (x) with a constant 1 (add a column of ones to (X), and add (b) into the parameter vector).

    Then, model capacity: if the hypothesis space is too small, you get **underfitting** (e.g., fitting a line to quadratic data). A simple way to increase expressiveness is **linear basis models**: apply a feature map (\phi(x)) (possibly nonlinear) and fit a linear model in (\phi(x)). Examples include polynomial basis (\phi_j(x)=x^j), Gaussian basis, sigmoid basis, sine/cosine, etc. With sufficiently rich bases (e.g., polynomials of high degree), classical results (Weierstrass approximation idea) say you can approximate any continuous function arbitrarily well. Linear basis models still have a closed-form least squares solution if you replace (X) by the transformed design matrix (\Phi).

    However, high capacity can cause **overfitting**: a very high-degree polynomial can fit training points perfectly (zero empirical risk) but perform poorly between points (bad generalization). This motivates distinguishing **empirical risk** (training) vs **population risk** (expected loss over the true data distribution) and the **generalization gap**. We cannot minimize population risk directly, but we can estimate it by a **train/test split**: train on training data only, evaluate on held-out test data. As dataset size (n \to \infty), empirical risk should converge toward population risk, so one way to reduce overfitting is getting more data. The lecture notes that generalization is central in deep learning (models are large), and will be addressed via architecture choices, explicit regularization, loss/training changes, and data augmentation. Break: 5 minutes, then classification.

    **Chinese** 回归任务里，真实关系 (f^*) 把输入 (x) 映射到输出 (y)，但我们不知道 (f^*)。我们只看到数据集里的 (n) 个样本 ((x_i,y_i))（可能有噪声）。接着选一个**假设空间** ( \mathcal{H} )：也就是你允许模型长什么样（比如“所有线性函数”）。为了衡量模型好坏，先定义单个样本的**损失函数**；回归里常用平方损失 ((f(x)-y)^2)。把所有样本的损失取平均，就是**经验风险**（empirical risk）。学习就被表述成 **ERM：经验风险最小化**——在假设空间里找一个 (\hat f)，让训练集平均损失最小。

    线性回归 (f(x)=w^\top x) 很特殊：ERM 有闭式解（最小二乘公式）。把输入堆成矩阵 (X)、输出堆成向量 (y)，最优解是 (w=(X^\top X)^{-1}X^\top y)（可逆时）。这在神经网络里通常做不到：你能算梯度，但很难直接解出“梯度=0”的解析解。

    老师也区分了**线性模型**（过原点）和**仿射模型**（affine）(w^\top x+b)。仿射模型也好解：给每个输入向量补一个常数 1（相当于在 (X) 里加一列全 1），把 (b) 合并进参数向量，就能用同样的最小二乘形式求解。

    接着讲模型容量：假设空间太小会**欠拟合**（比如用直线去拟合二次函数数据）。一个简单增大表达力的方法是**线性基函数模型**：先做特征映射 (\phi(x))（可以是非线性的），再在 (\phi(x)) 上做线性模型。例子：多项式基函数 (\phi_j(x)=x^j)、高斯基、sigmoid 基、正余弦等。若基函数足够丰富（如允许很高次数的多项式），经典结论（类似魏尔施特拉斯逼近思想）说明：任意连续函数都可以被逼近到任意精度。线性基函数模型依然有闭式解：把 (X) 换成变换后的设计矩阵 (\Phi) 就行。

    但容量大也会带来**过拟合**：高次多项式可能把训练点全部穿过（经验风险为 0），但在点与点之间表现很差（泛化差）。因此要区分**经验风险**（训练集）和**总体风险**（population risk：对真实分布下“任意新样本”的期望损失），以及两者之间的**泛化差距**。总体风险不能直接优化，但可以用**训练/测试集划分**来估计：只用训练集训练，再用测试集评估。样本量 (n) 越大，经验风险越接近总体风险，所以“更多数据”通常能缓解过拟合。老师强调泛化在深度学习里非常核心（模型太大太容易过拟合），后面会通过结构选择、显式正则化、改损失/训练算法、数据增强等系统讲。之后休息 5 分钟，进入分类部分。

  * [15:44–16:08] **Classification: from argmax to softmax; one-hot; cross-entropy vs square loss**
    **English** For K-class classification, a linear approach generalizes binary linear classifiers: use K weight vectors (w_1,\dots,w_K) to produce K scores, then predict the class with the maximum score (argmax). In matrix form, collect weights into (W), compute scores (W\phi(x)), and apply (g=\arg\max). Two problems if you try to train using squared loss on integer labels and argmax outputs:
    (1) **Label encoding problem (nominal vs ordinal):** class IDs (0–9) are nominal; there is no meaningful notion that “6 is farther from 0 than 1 is.” Squared error would penalize wrong classes unequally.
    (2) **Optimization problem:** argmax is non-differentiable, which breaks gradient-based training.

    Fixes: encode labels as **one-hot vectors** so all classes are equidistant (no artificial order). Make the model output a **probability vector** using **softmax**, a differentiable function (s(z)_k = \exp(z_k)/\sum_j \exp(z_j)), ensuring positive entries summing to 1. The model becomes ( \text{softmax}(W\phi(x))).

    For training, accuracy is fine for evaluation but is not differentiable as a training loss. We need a differentiable loss comparing probability vectors. Squared loss works but cross-entropy is preferred. With true label (y) as one-hot and predicted probabilities (p), cross-entropy is (L=-\sum_k y_k\log p_k), which reduces to (-\log p_{\text{true}}). Practical reason: cross-entropy yields stronger gradients, especially when the model is “almost correct” (e.g., 80% confidence) and you still want to push it toward near-certainty; square loss gradients can become too small. A theoretical reason is its connection to maximum likelihood. The lecture ends by summarizing: TEP definition; regression uses squared loss; classification uses cross-entropy; hypothesis spaces; capacity; overfitting; empirical vs population risk; train/test split; then transitions to a linear regression demo using Python tooling.

    **Chinese** K 类分类可以看作二分类线性分类器的推广：用 (w_1,\dots,w_K) 产生 K 个分数，然后取最大分数对应的类别（argmax）。矩阵写法就是把权重堆成 (W)，算分数 (W\phi(x))，再用 (g=\arg\max) 输出类别。若你直接把类别当整数（0–9）并用平方损失训练，会有两个问题：
    (1) **标签编码问题（名义 vs 序数）：** 类别是“名义型”（nominal），没有“0 更接近 1 而远离 6”的意义。用平方损失会导致不同错误被罚得不一样，不合理。
    (2) **优化问题：** argmax 不可导，没法做基于梯度的训练。

    解决办法：用 **one-hot** 编码标签，让类别之间没有人为顺序（各类别“距离”一致）。同时让模型输出**概率向量**，用 **softmax** 把任意实数向量变成概率分布：(s(z)_k=\exp(z_k)/\sum_j \exp(z_j))，每一维都 >0 且总和为 1。模型就变成 ( \text{softmax}(W\phi(x)))。

    训练时：准确率适合作为训练后评估指标，但不适合作为训练损失（不可导、0/1 跳变）。需要一个可导的“概率向量差异”损失。平方损失可以用，但更常用的是**交叉熵**：真实标签 (y) 是 one-hot，预测概率是 (p)，交叉熵 (L=-\sum_k y_k\log p_k)，由于 one-hot 的性质，它等价于 (-\log p_{\text{true}})。实践原因：交叉熵在模型“差一点就对”（比如 80%）时梯度更大，更容易把模型推到“非常自信且正确”；平方损失在这里梯度可能太小，训练推进慢。理论原因：最小化交叉熵等价于最大化似然。最后老师总结了：TEP 定义；回归常用平方损失；分类常用交叉熵；需要先定义假设空间与容量；过拟合与泛化；经验风险 vs 总体风险；用 train/test split 估计泛化；然后进入线性回归的 Python 演示。

  * [16:08–16:28] **Demo: Linear regression on Singapore HDB resale prices; preprocessing; visualization; underfitting; feature expansion + one-hot**
    **English** The demo uses common Python ML tools (scikit-learn, pandas, matplotlib; the transcript mentions seaborn; NumPy). Data source: Singapore HDB resale prices (GovTech dataset). Target: resale price. Features include town (location), storey range, floor area (sqm), remaining lease (years/months), etc. Two preprocessing steps convert strings to numbers: remaining lease “61 years 4 months” → ~61.33; storey range “10 TO 12” → average storey 11.
    Visualization:
    • Bar plot of town vs price (expensive areas like Bukit Timah/Bishan vs cheaper like Yishun, etc.).
    • Bar/plot of storey vs price (higher floor tends to cost more).
    • Scatter of floor area vs price, colored by remaining lease (newer flats tend to be pricier at same size).
    Then split train/test (e.g., 90/10) with shuffling.
    Model 1: linear regression using only floor area (1D). Plot shows large scatter; normalized RMSE ~26% on both train and test. Similar train/test errors imply **underfitting**.
    Model 2: polynomial regression (degree 3) on the same 1D feature shows little improvement (~26%). Reason: for a fixed floor area, prices vary widely due to other factors; increasing polynomial degree won’t fix missing dimensions. Lesson: visualize data; scalar error alone doesn’t tell whether the issue is nonlinearity or missing features.
    Model 3: add more features (remaining lease, storey, and town). Town is categorical, so apply one-hot encoding (e.g., `get_dummies`). With higher-dimensional input and linear regression, normalized error improves to ~18%, with train/test still similar (no strong overfitting). Conclude: form project groups early; end of lecture.

    **Chinese** 演示用到常见 Python 机器学习工具（scikit-learn、pandas、matplotlib；记录里也提到 seaborn；以及 NumPy）。数据来自新加坡 HDB 转售价格（GovTech 数据）。目标是预测转售价。输入特征包括：town（地区）、storey range（楼层范围）、floor area（面积）、remaining lease（剩余租期）等。两步预处理把字符串变成数值：比如剩余租期 “61 years 4 months” → 约 61.33；楼层范围 “10 TO 12” → 平均楼层 11。
    可视化部分：
    • 地区 vs 价格条形图（如 Bukit Timah/Bishan 较贵，Yishun 等相对便宜）。
    • 楼层 vs 价格：楼层越高通常越贵。
    • 面积 vs 价格散点图，并用颜色表示剩余租期（同面积下，租期更长/更“新”的房通常更贵）。
    然后做 train/test 划分（例如 90/10）并随机打乱。
    模型 1：只用面积做 1 维线性回归。散点很分散，归一化 RMSE 约 26%，训练/测试差不多 → **欠拟合**（主要是模型信息不足，而不是过拟合）。
    模型 2：同样只用面积，换成三次多项式回归，几乎没提升（仍 ~26%）。原因是：同一面积下价格跨度很大，决定价格的因素不止面积；你把曲线弄得再“弯”，也还是 1 维输入，解决不了“缺少关键特征维度”的问题。结论：要先可视化理解数据结构；只看一个误差数字，很难判断到底是“非线性不够”还是“特征维度不够”。
    模型 3：加入更多特征（剩余租期、楼层、地区等）。地区是类别型变量，需要 one-hot（如 `get_dummies`）。在更高维输入下做线性回归，误差降到约 18%，且训练/测试仍接近（没有明显过拟合）。最后提醒尽早组队，课程结束。
## Week 2
  - [14:05 - 14:06] **Recap: Linear Basis Models and the Shift to Neural Networks**

    In our previous session, we discussed how entering input data $x$ into a model doesn't necessarily mean the model remains a simple linear function of that raw $x$. We explored using basis functions like sine and cosine to create more complex mappings. While these allow us to express more intricate functions, we also highlighted significant limitations—specifically, that these bases are fixed before we even look at the data. Today's primary objective is to transition from these fixed models to neural networks, starting with the simplest form: the Fully Connected Neural Network (FCNN). We want to understand the motivation behind this shift—essentially viewing a neural network as an "adaptive" basis model that can outperform the fixed linear basis models we've used so far.

    在上节课中，我们讨论了将输入数据 $x$ 输入模型时，模型并不一定只是关于 $x$ 的简单线性函数。我们探讨了如何利用正弦、余弦基函数来构建更复杂的映射。虽然这增强了模型的表达能力，但我们也提到了其局限性——即这些基函数在观测数据之前就已经固定了。今天的主要目标是从这些固定模型转向神经网络，从最简单的全连接神经网络（FCNN）开始。我们要探讨这种转变背后的动机，即把神经网络看作一种“自适应”基模型，它的性能往往优于我们之前使用的固定线性基模型。

  - [14:06 - 14:12] **Round 1: The "Fixed Basis" Game (3 Integers)**

    To illustrate the difference between fixed and adaptive models, let’s play a two-round game using Poll Everywhere. First, pick three of your favorite integers between 0 and 9. Write them down now and do not change them; these represent your "fixed basis." The goal is to get as close as possible to a "magic number" (the target) by adding up to three of your chosen integers (you can use each integer only once in the sum, or choose not to use some). Your score is the absolute difference between your sum and the magic number. For example, if you chose $\{3, 5, 9\}$ and the magic number is 15, your best sum is $5 + 9 = 14$, giving an error of 1. If the magic number is 1, your best sum is 0 (by choosing nothing), with an error of 1.

    We ran three magic numbers for Round 1: **12**, **1**, and **20**. Looking at the results, the error distribution varied wildly. For the magic number 20, performance was noticeably worse because most fixed sets of three small integers simply couldn't reach that high. Individually, your scores likely fluctuated significantly across these three numbers because your "basis" was fixed and couldn't adapt to the specific target I presented.

    为了说明固定模型和自适应模型之间的区别，我们来玩一个两轮的 Poll Everywhere 游戏。首先，在 0 到 9 之间选出三个你最喜欢的整数。现在就把它们写下来，不要更改；这代表了你的“固定基”。游戏的目标是通过将你选出的整数相加（最多选三个，每个只能用一次，也可以不选），尽可能接近我给出的“幻数”（目标值）。你的得分是你的总和与幻数之间的绝对差值。例如，如果你选了 $\{3, 5, 9\}$，幻数是 15，你能得到的最好结果是 $5 + 9 = 14$，误差为 1。如果幻数是 1，最好的结果是不加任何数（总和为 0），误差也是 1。

    第一轮我们测试了三个幻数：**12**、**1** 和 **20**。从结果来看，误差分布波动很大。对于幻数 20，大家的表现明显较差，因为大多数人选的三个小整数根本凑不到那么大。从个人角度来看，你们在三局中的得分可能差异巨大，因为你们的“基”是固定的，无法根据我给出的具体目标进行调整。

  - [14:12 - 14:17] **Round 2: The "Adaptive Basis" Game (6 Integers)**

    Now, let’s modify the rules for the second version. This time, you pick **six** integers between 0 and 9. However, when I show you the magic number, you can still only use **up to three** of those integers to form your sum. For example, if you chose $\{0, 1, 2, 3, 4, 8\}$ and the magic number is 5, you could pick $2+3$ or $1+4$ for an error of 0. If the magic number is 18, you'd pick $8+4+3=15$ for an error of 3.

    We tested this with magic numbers **4**, **10**, and **19**. What we observed is that performance was much better and more consistent across rounds. Even for the high number 19, the vast majority of you maintained an error of at most 1. In this version, because you had a larger "pool" to select from *after* seeing the target, you effectively "adapted" your choice of the three active basis functions to fit the specific data. This is much closer to how a neural network operates compared to a fixed linear model.

    现在，我们修改一下第二轮的规则。这一次，你在 0 到 9 之间选 **6 个** 整数。但是，当我给出幻数时，你仍然只能从这 6 个数中选出 **最多 3 个** 来求和。例如，如果你选了 $\{0, 1, 2, 3, 4, 8\}$，幻数是 5，你可以选 $2+3$ 或 $1+4$，误差为 0。如果幻数是 18，你会选 $8+4+3=15$，误差为 3。

    我们用幻数 **4**、**10** 和 **19** 进行了测试。我们观察到，各轮的表现要好得多，也更稳定。即使是面对 19 这样大的数字，绝大多数人的误差也控制在 1 以内。在这一版中，由于你在看到目标 *之后* 有更大的选择空间，你实际上是“自适应”地选择了三个基函数来拟合具体的数据。与固定线性模型相比，这更接近神经网络的运作方式。

  - [14:17 - 14:24] **Mathematical Motivation: Efficiency of Approximation**

    Let’s formalize this. In Round 1, you picked three basis functions $\phi_i$ *a priori* (before seeing $f$). Your model was $\sum w_i \phi_i$. In Round 2, you had a larger set but adapted your selection to the target. Neural networks can be framed as linear models on an **adaptive basis**.

    Why not just use a fixed basis with many terms? We know from the Weierstrass Approximation Theorem and Fourier analysis that fixed bases have "universal approximation" properties—they can fit any continuous function given enough terms. However, we care about **efficiency**. In our experiments, a polynomial basis might need $m=9$ terms to fit a wavy function with error $<0.1$, while a cosine basis only needs $m=7$. Conversely, for a simple quadratic-like curve, the polynomial basis reached an error of $10^{-6}$ with $m=3$, whereas the cosine basis lagged behind. No fixed basis is universally efficient for every target function. Since we cannot know the best basis before seeing the data, it is mathematically superior to adapt the basis functions $\phi(x; \theta)$ using trainable parameters $\theta$.

    让我们把这正式化。在第一轮中，你在看到 $f$ 之前先验地选择了三个基函数 $\phi_i$。你的模型是 $\sum w_i \phi_i$。在第二轮中，你有一个更大的集合，并根据目标调整了选择。神经网络可以被看作是建立在 **自适应基** 上的线性模型。

    为什么不直接使用包含很多项的固定基呢？根据 Weierstrass 逼近定理和傅里叶分析，固定基具有“通用逼近”性质——只要项数足够多，它们可以拟合任何连续函数。然而，我们关心的是 **效率**。在我们的实验中，多项式基可能需要 $m=9$ 项才能以 $<0.1$ 的误差拟合一个波动函数，而余弦基只需要 $m=7$ 项。反之，对于一个简单的类二次曲线，多项式基在 $m=3$ 时误差就达到了 $10^{-6}$，而余弦基则表现稍逊。没有任何固定基对所有目标函数都是通用的高效。既然我们在看到数据之前无法知道最好的基是什么，那么使用带有可训练参数 $\theta$ 的自适应基函数 $\phi(x; \theta)$ 在数学上是更优的。

  - [14:24 - 14:29] **The Trade-off: Training Difficulty and Representation Learning**

    In a linear basis model, training is easy because the empirical risk minimization (ERM) problem is linear with respect to the weights $w$, allowing for a closed-form solution. In an adaptive basis model $f(x) = \sum w_i \phi(x; \theta_i)$, we now have to train both $w$ and $\theta$. Because $\phi$ is usually a nonlinear function of $\theta$, the overall model is nonlinear with respect to its parameters, making optimization much more difficult.

    These adaptive $\phi(x; \theta)$ are often called **feature maps**. We say the network is "learning a representation" of the data—identifying edges, colors, or shapes in an image, for instance. This is why the premier neural network conference is called **ICLR** (International Conference on Learning Representations).

    在线性基模型中，训练非常容易，因为经验风险最小化（ERM）问题关于权重 $w$ 是线性的，可以得到闭式解。在自适应基模型 $f(x) = \sum w_i \phi(x; \theta_i)$ 中，我们现在必须同时训练 $w$ 和 $\theta$。由于 $\phi$ 通常是 $\theta$ 的非线性函数，整个模型关于其参数是非线性的，这使得优化变得困难得多。

    这些自适应的 $\phi(x; \theta)$ 通常被称为 **特征映射 (feature maps)**。我们说网络正在学习数据的“表示 (representation)”——例如识别图像中的边缘、颜色或形状。这就是为什么顶级的神经网络会议被称为 **ICLR**（国际学习表示会议）。

  - [14:29 - 14:33] **Failure of Linear Models: The XOR Problem**

    To motivate the structure of a neural network, let's look at the classic **Exclusive OR (XOR)** function. XOR takes two binary inputs and outputs $0$ if they are the same $(\{0,0\}, \{1,1\})$ and $1$ if they are different $(\{0,1\}, \{1,0\})$. If you try to fit this with a standard linear regression $y = w_1x_1 + w_2x_2 + b$, the math fails. For the XOR data, the optimal linear solution ends up being a constant function $y = 0.5$.

    Visually, the two classes (0 and 1) are not **linearly separable**. You cannot draw a single straight line in the 2D plane to separate the zeros from the ones. If you analyze the gradients, you'll find a contradiction: to fit the change from $(0,0) \to (0,1)$, $w_2$ needs to be positive, but to fit $(1,0) \to (1,1)$, $w_2$ needs to be negative. The linear model "gives up" and sets the weights to zero.

    为了探讨神经网络的结构，我们来看看经典的 **异或 (XOR)** 函数。XOR 接收两个二进制输入，如果输入相同（$\{0,0\}$，$\{1,1\}$）则输出 $0$，如果不同（$\{0,1\}$，$\{1,0\}$）则输出 $1$。如果你尝试用标准的线性回归 $y = w_1x_1 + w_2x_2 + b$ 来拟合，数学上会失败。对于 XOR 数据，最优线性解最终会是一个常数函数 $y = 0.5$。

    从直观上看，这两个类别（0 和 1）不是 **线性可分的**。你无法在二维平面上画一条直线将 0 和 1 分开。如果你分析梯度，你会发现一个矛盾：为了拟合 $(0,0) \to (0,1)$ 的变化，$w_2$ 需要是正数；但为了拟合 $(1,0) \to (1,1)$，$w_2$ 则需要是负数。线性模型最终“放弃”了，将权重设为零。

  - [14:33 - 14:40] **Stacking Layers: Why Linearity Isn't Enough**

    We can define a simple neural network as a composition of functions $f(x) = f_2(f_1(x))$. Here, $f_1$ produces an output $h$, known as the **hidden unit**. The dimension $m$ of $h$ is the **width** of the hidden layer.

    A student might ask: "Can we just stack two linear functions?" Let’s test that. If $f_1(x) = Wx + c$ and $f_2(h) = w^T h + b$, then the composition is $f(x) = w^T(Wx + c) + b$. If you expand this out, it simplifies to $w'^T x + b'$. This is still just a linear model! The lesson is: **Never stack linear layers consecutively.** They do not increase the expressivity of the model because the composition of linear functions is always linear. To break this, we must introduce a **nonlinearity** between the layers.

    我们可以将一个简单的神经网络定义为函数的复合 $f(x) = f_2(f_1(x))$。这里 $f_1$ 产生一个输出 $h$，称为 **隐藏单元 (hidden unit)**。$h$ 的维度 $m$ 被称为隐藏层的 **宽度 (width)**。

    可能会有学生问：“我们能不能直接堆叠两个线性函数？”让我们测试一下。如果 $f_1(x) = Wx + c$ 且 $f_2(h) = w^T h + b$，那么复合函数就是 $f(x) = w^T(Wx + c) + b$。展开后，它简化为 $w'^T x + b'$。这仍然只是一个线性模型！教训是：**永远不要连续堆叠线性层。** 它们不会增加模型的表达能力，因为线性函数的复合仍然是线性的。为了打破这一点，我们必须在层与层之间引入 **非线性**。

  - [14:40 - 14:46] **The Solution: Activation Functions and the ReLU**

    To fix this, we modify $f_1$ to include a nonlinear function $g$, called an **activation function**. We typically apply $g$ element-wise, so it doesn't change the dimension. The most popular choice is the **ReLU (Rectified Linear Unit)**: $g(z) = \max(0, z)$.

    With a ReLU hidden layer, our XOR model becomes $y = w^T \max(0, Wx + c) + b$. By choosing specific parameters (e.g., $W = [[1, 1], [1, 1]]$, $c = [0, -1]$), this network can solve the XOR problem exactly. What's happening behind the scenes? The hidden units transform the input space. While the original points were not linearly separable in the $x$-space, the mapping into the $h$-space moves the points such that a simple linear regression on $h$ can now separate them. This is the essence of representation learning: transforming data into a space where the problem becomes easy.

    为了解决这个问题，我们修改 $f_1$，加入一个非线性函数 $g$，称为 **激活函数**。我们通常对每个元素单独应用 $g$，因此它不会改变维度。最流行的选择是 **ReLU (修正线性单元)**：$g(z) = \max(0, z)$。

    通过使用 ReLU 隐藏层，我们的 XOR 模型变为 $y = w^T \max(0, Wx + c) + b$。通过选择特定的参数（例如 $W = [[1, 1], [1, 1]]$, $c = [0, -1]$），这个网络可以完美解决 XOR 问题。背后发生了什么？隐藏单元对输入空间进行了变换。虽然原始点在 $x$ 空间中不是线性可分的，但映射到 $h$ 空间后，点的位置发生了变化，使得对 $h$ 进行简单的线性回归就能将它们分开。这就是表示学习的本质：将数据变换到一个让问题变得简单的空间。

  - [14:46 - 14:55] **Types of Activation Functions and Biological Inspiration**

    In a general shallow neural network, you have two main choices: the activation function $g$ and the hidden layer width $m$. Beyond ReLU, we have:

    - **Leaky ReLU:** $g(z) = z$ if $z > 0$ and $\delta z$ if $z < 0$. It’s a less "drastic" version that prevents dead neurons.
    - **Sigmoid:** Bounded between 0 and 1, used for binary classification in the output layer.
    - **Tanh:** Bounded between -1 and 1, often used in recurrent networks.

    One danger with bounded functions like Sigmoid is **saturation**. If you stack many Sigmoids, the outputs get pushed closer and closer to 1 (or 0), which can cause training issues. For intermediate layers, ReLU is usually the "safe" default.

    Interestingly, ReLU has a biological motivation. In biological neural networks, neurons transmit electrical impulses only if the input signal exceeds a certain threshold. ReLU mimics this—it remains "silent" (zero) for negative inputs and "fires" linearly once the threshold is crossed.

    We've been talking for nearly an hour, so let's take a 10-minute break. I'll turn on the Q&A function on Poll Everywhere so you can post your questions there.

    在一个通用的浅层神经网络中，你主要有两个选择：激活函数 $g$ 和隐藏层宽度 $m$。除了 ReLU，我们还有：

    - **Leaky ReLU:** 当 $z > 0$ 时 $g(z) = z$，当 $z < 0$ 时为 $\delta z$。这是一个不那么“剧烈”的版本，可以防止神经元“死亡”。
    - **Sigmoid:** 限制在 0 和 1 之间，常用于输出层的二分类任务。
    - **Tanh:** 限制在 -1 和 1 之间，常用于循环网络。

    Sigmoid 等有界函数的一个风险是 **饱和 (saturation)**。如果你堆叠多个 Sigmoid，输出会被推向 1（或 0）附近，这会导致训练问题。对于中间层，ReLU 通常是“安全”的默认选择。

    有趣的是，ReLU 也有生物学上的动机。在生物神经网络中，只有当输入信号超过一定阈值时，神经元才会传递电脉冲。ReLU 模拟了这一点——对于负输入保持“沉默”（零），一旦超过阈值则线性“放电”。

    我们已经讲了快一个小时了，休息 10 分钟。我会开启 Poll Everywhere 的问答功能，你们可以把问题发在上面。

  - [15:04 - 15:10] **Theoretical Superiority: Universal Approximation vs. The Curse of Dimensionality**

    We've now introduced the shallow neural network: a model where input $x$ undergoes a linear transformation, followed by an activation function to create a hidden unit $h$, which then undergoes another linear transformation to produce an output. But what do we truly gain over a linear basis model? Both share the **Universal Approximation Theorem**, meaning both can approximate any continuous function to arbitrary precision if they are complex enough (measured by the number of basis functions $m$ for linear models, or the width of the hidden layer $m$ for neural networks).

    The key difference lies in **efficiency** and the **Curse of Dimensionality**. In a linear basis model (like polynomial regression), if you want to decrease your error by a factor of 5 in a 1D problem, you might need 5 times the pieces. In 2D, you need $5^2 = 25$; in 3D, you need $5^3 = 125$. Mathematically, for polynomials, the error is $O(m^{-1/d})$, meaning the number of basis functions $m$ must grow exponentially with the dimension $d$ to maintain accuracy. However, a landmark result by Baron (1993) showed that for neural networks, the error is $O(1/\sqrt{m})$, which is **independent of the dimension $d$**. This is why neural networks revolutionized fields like computer vision; images have massive dimensionality, and traditional models simply couldn't handle them efficiently.

    我们已经介绍了浅层神经网络：输入 $x$ 经过线性变换，接着是激活函数产生隐藏单元 $h$，最后再经过一次线性变换得到输出。但与线性基模型相比，我们究竟得到了什么？两者都符合 **通用逼近定理 (Universal Approximation Theorem)**，即只要模型足够复杂（线性模型取决于基函数数量 $m$，神经网络取决于隐藏层宽度 $m$），它们都能以任意精度逼近任何连续函数。

    关键区别在于 **效率** 和 **维度灾难 (Curse of Dimensionality)**。在线性基模型（如多项式回归）中，如果你想在 1D 问题中将误差降低 5 倍，你可能需要 5 倍的基函数；在 2D 中需要 $5^2 = 25$ 倍；在 3D 中则需要 $5^3 = 125$ 倍。从数学上讲，多项式的误差是 $O(m^{-1/d})$，这意味着为了保持精度，基函数数量 $m$ 必须随维度 $d$ 指数级增长。然而，Baron (1993) 的一项里程碑式研究表明，神经网络的误差是 $O(1/\sqrt{m})$，这与 **维度 $d$ 无关**。这就是为什么神经网络彻底改变了计算机视觉等领域；图像具有极高的维度，传统模型在效率上根本无法胜任。

  - [15:11 - 15:17] **Classification Paradigms: Multiclass vs. Multi-label**

    Adapting a neural network for classification follows the same logic as the linear models we discussed last week. For **Multiclass Classification** (where only one label is correct), we use a **Softmax** output layer because it ensures the outputs sum to 1, forming a probability vector. However, some tasks require **Multi-label Classification**, where multiple labels can be correct simultaneously (e.g., an image containing both "Sun" and "Cloud").

    In the multi-label case, we don't use Softmax. Instead, we apply a **Sigmoid** function to each of the $K$ output nodes independently. This effectively transforms the problem into $K$ separate binary classification tasks. Correspondingly, for training, we use **Binary Cross-Entropy (BCE)** summed over all $K$ classes, whereas for multiclass, we use the standard categorical **Cross-Entropy**.

    将神经网络应用于分类任务遵循与我们上周讨论的线性模型相同的逻辑。对于 **多类别分类 (Multiclass Classification)**（只有一个标签是正确的），我们使用 **Softmax** 输出层，因为它能确保输出之和为 1，构成一个概率向量。然而，有些任务需要 **多标签分类 (Multi-label Classification)**，即多个标签可以同时正确（例如，一张图像中同时包含“太阳”和“云”）。

    在多标签情况下，我们不使用 Softmax。相反，我们对 $K$ 个输出节点分别独立应用 **Sigmoid** 函数。这实际上将问题转化为 $K$ 个独立的二分类任务。相应地，在训练时，我们会对所有 $K$ 个类别使用 **二元交叉熵 (BCE)** 之和，而对于多类别分类，我们则使用标准的 **交叉熵 (Cross-Entropy)**。

  - [15:17 - 15:28] **Why Cross-Entropy? Analyzing Gradient Vanishing**

    Why prefer Cross-Entropy over Mean Squared Error (MSE) for classification? While MSE is a solid default for regression, it performs poorly with Sigmoid/Softmax. Let's look at the math. If we use a Sigmoid model $P = \sigma(z)$ where $z = w^Tx$, and our target is $y=1$:

    - With **Cross-Entropy**, the gradient $\frac{\partial L}{\partial z}$ is simply $P - 1$. If our prediction $P$ is wrong (near 0), the gradient is nearly $-1$, providing a strong signal to learn.
    - With **MSE**, the gradient $\frac{\partial L}{\partial z}$ is $(P - 1) \sigma'(z)$, which expands to $(P - 1)P(1 - P)$.

    Notice the problem: if our prediction is totally wrong ($P \approx 0$), the term $P$ in the MSE gradient makes the entire gradient **vanish to zero**. The model stops learning even though it's incorrect! This "gradient vanishing" issue is a compelling reason to stick with Cross-Entropy for classification.

    为什么在分类任务中，交叉熵优于均方误差 (MSE)？虽然 MSE 是回归任务的可靠默认选择，但它在配合 Sigmoid/Softmax 使用时表现不佳。让我们看下数学推导。如果我们使用 Sigmoid 模型 $P = \sigma(z)$，其中 $z = w^Tx$，且目标 $y=1$：

    - 使用 **交叉熵** 时，梯度 $\frac{\partial L}{\partial z}$ 简化为 $P - 1$。如果预测 $P$ 错误（接近 0），梯度接近 $-1$，提供了强烈的学习信号。
    - 使用 **MSE** 时，梯度 $\frac{\partial L}{\partial z}$ 是 $(P - 1) \sigma'(z)$，展开为 $(P - 1)P(1 - P)$。

    注意问题所在：如果我们的预测完全错误（$P \approx 0$），MSE 梯度中的 $P$ 项会导致整个梯度 **消失为零**。即便模型是错的，它也会停止学习！这种“梯度消失”问题是分类任务坚持使用交叉熵的一个极具说服力的理由。

  - [15:28 - 15:37] **Optimization via Gradient Descent**

    Because neural networks are nonlinear with respect to their parameters $\theta$, we cannot use a simple closed-form formula like the least-squares solution. We must use iterative methods, the most popular being **Gradient Descent**.

    The intuition comes from the Taylor expansion: $R(\theta + \epsilon \phi) \approx R(\theta) + \epsilon \phi^T \nabla R(\theta)$. To minimize the risk $R$ in the next step, the optimal direction $\phi$ is the negative gradient, $-\nabla R(\theta)$. This gives us the update rule: $\theta_{k+1} = \theta_k - \epsilon \nabla R(\theta_k)$, where $\epsilon$ is the **learning rate**. If $\epsilon$ is too small, learning is agonizingly slow. If $\epsilon$ is too large (as seen in our quadratic example $R = \theta^2/2$), the steps can overshot the minimum and cause the model to diverge.

    由于神经网络关于其参数 $\theta$ 是非线性的，我们无法使用像最小二乘法那样的简单闭式公式。我们必须使用迭代法，其中最流行的是 **梯度下降 (Gradient Descent)**。

    其直觉来自泰勒展开：$R(\theta + \epsilon \phi) \approx R(\theta) + \epsilon \phi^T \nabla R(\theta)$。为了在下一步中最小化风险 $R$，最优方向 $\phi$ 是负梯度方向 $-\nabla R(\theta)$。这得到了更新规则：$\theta_{k+1} = \theta_k - \epsilon \nabla R(\theta_k)$，其中 $\epsilon$ 是 **学习率**。如果 $\epsilon$ 太小，学习速度会慢得令人痛苦；如果 $\epsilon$ 太大（正如我们在二次函数示例 $R = \theta^2/2$ 中看到的），步子跨得太大可能会越过最小值，导致模型发散。

  - [15:38 - 16:07] **Deep Dive: Convergence and Eigenvalues in Linear Models**

    While analyzing neural network convergence is complex, we can gain insight by applying gradient descent to a **linear model**. The error $E_k = w_k - w_\infty$ evolves as $E_{k+1} = A E_k$, where $A = I - \epsilon (X^T X)$.

    For the error to vanish ($E_k \to 0$), all eigenvalues of $A$ must have a magnitude less than 1. This leads to a strict upper bound on the learning rate: **$\epsilon < \frac{2}{\lambda_{max}}$**, where $\lambda_{max}$ is the largest eigenvalue of the covariance matrix $X^T X$.

    Furthermore, the **rate of convergence** is dictated by the ratio of the smallest to the largest eigenvalues (the **condition number**). If the covariance matrix has very different eigenvalues, the error contours are elongated ovals rather than circles, making gradient descent extremely slow as it "oscillates" or crawls along the narrow axis. This gives us a theoretical grounding for why data normalization is so important in practice—it helps balance these eigenvalues.

    We'll take another break now. Next, we'll look at how to actually implement and train these networks in Python.

    虽然分析神经网络的收敛性很复杂，但我们可以通过将梯度下降应用于 **线性模型** 来获得启发。误差 $E_k = w_k - w_\infty$ 的演变遵循 $E_{k+1} = A E_k$，其中 $A = I - \epsilon (X^T X)$。

    为了使误差消失（即 $E_k \to 0$），$A$ 的所有特征值的模必须小于 1。这为学习率提供了一个严格的上限：**$\epsilon < \frac{2}{\lambda_{max}}$**，其中 $\lambda_{max}$ 是协方差矩阵 $X^T X$ 的最大特征值。

    此外，**收敛速度** 由最小特征值与最大特征值的比率（即 **条件数**）决定。如果协方差矩阵的特征值差异巨大，误差等值线会呈现细长的椭圆形而非圆形，这会导致梯度下降在窄轴方向上“振荡”或缓慢爬行，速度极慢。这从理论上解释了为什么数据归一化在实践中如此重要——它有助于平衡这些特征值。

    我们现在再休息一下。接下来，我们将探讨如何在 Python 中实际实现并训练这些网络。
## Week 3
  - [14:06 - 14:11] **From Shallow to Deep: Addressing the Curse of Dimensionality**

    The basis functions in our model contain a parameter `θ`, which implies they can be adapted based on the data. We desire this adaptability because, as we observed in last week's experiments, it is not immediately clear how to choose a fixed basis function for a given problem; there is no single "universal" basis function that works well for every target function. Therefore, the core idea is to adapt the basis function itself. Last week, we argued that a shallow neural network can be viewed as a linear basis model with an adaptive basis. Our definition of a shallow neural network for regression involves an input `X`, followed by a linear transform `WX`, and then a non-linear activation function `G`. This produces what we call the "hidden unit." We then perform linear regression on this hidden unit. If we are dealing with a classification problem, the structure inside `G` remains the same, but we apply a Softmax regression on the hidden unit at the end. Essentially, the hidden unit is a transformed version of the input, acting like a basis function. The crucial difference from a standard basis function is that this function `G` contains trainable parameters—the weights `W` and the bias term `b`. These allow the basis to adapt to the data.

    The primary advantage of this neural network-compatible adaptive basis model is that it does not suffer from the "curse of dimensionality." Recall that if you use a fixed polynomial basis, the error increases as the dimensionality increases, and maintaining the same error rate requires an exponential increase in the number of basis functions. Neural networks avoid this. The main disadvantage, however, is that we lose the closed-form ordinary least squares formula due to the model's non-linearity with respect to the parameters. Consequently, we must resort to gradient descent. Our goal is to minimize the empirical risk. In our notation, `f` is the neural network taking input `X`, with parameters `w`, `b` (output layer) and `W`, `c` (hidden layer/adaptive basis).

    In last week's demo using the MNIST dataset (often transcribed as "endless," but referring to the digit dataset), we used gradient descent to train a shallow neural network for 10-class classification. We "cheated" slightly because the full training set has 60,000 samples, which makes standard gradient descent too slow. We only used 5,000 data points to make it feasible. Today, we will address this slowness and explain how TensorFlow computes derivatives via backpropagation. We will start by defining what a deep neural network is, note that they typically require large datasets, and then modify our gradient descent algorithm to work efficiently on this large scale.

    基函数包含参数 `θ`，这意味着它们可以根据数据进行调整。我们需要这种适应性，因为正如上周的实验所示，对于特定问题，往往很难确定该选择什么样的固定基函数；似乎不存在一种对所有目标函数都适用的通用基函数。因此，核心思想是让基函数具有适应性。上周我们要点在于，浅层神经网络可以被视为具有自适应基的线性基模型。我们对回归浅层神经网络的定义是：输入 `X` 经过线性变换 `WX`，再经过非线性激活函数 `G`，生成所谓的“隐藏单元”。然后我们在隐藏单元上进行线性回归。如果是分类问题，`G` 内部的结构保持不变，但在最后会在隐藏单元上应用 Softmax 回归。本质上，隐藏单元是输入的变换版本，类似于基函数。但它与普通基函数的关键区别在于，函数 `G` 包含可训练的参数——权重 `W` 和偏置项 `b`，这使得基函数能够根据数据进行调整。

    这种兼容神经网络的自适应基模型的主要优势在于它不会受到“维数灾难”的影响。回想一下，如果你使用固定的多项式基，误差会随着维度的增加而增加，想要保持相同的误差率，就需要指数级地增加基函数的数量。神经网络则避开了这个问题。主要缺点是，由于模型对参数具有非线性，我们不再拥有普通的最小二乘法闭式解。因此，我们必须依靠梯度下降。我们的目标是最小化经验风险。在我们的符号中，`f` 是输入为 `X` 的神经网络，参数包括 `w`、`b`（输出层）和 `W`、`c`（隐藏层/自适应基）。

    在上周使用 MNIST 数据集（录音中误听为 "endless"）的演示中，我们使用梯度下降训练了一个浅层神经网络来进行 10 类分类。我们当时稍微“取巧”了一下，因为完整的训练集有 60,000 个样本，标准梯度下降太慢了，所以我们只使用了 5,000 个数据点。今天，我们将解决梯度下降速度慢的问题，并解释 TensorFlow 如何计算导数。我们将首先定义什么是深度神经网络，指出它们通常需要大数据集，然后修改我们的梯度下降算法，使其在如此大规模的数据上也能高效工作。

  - [14:11 - 14:16] **Defining Deep Fully Connected Networks**

    Let us define the simplest structure of a deep neural network: the deep fully connected neural network. We previously defined a shallow network as input `X` going through a linear transform and a non-linear function `G` to give one hidden layer `H`. To make a neural network "deep," we simply stack these layers. Instead of one hidden unit, we have `L` hidden units, denoted as `H_1` up to `H_L`. The final step remains the same as in the shallow network: linear regression on the last hidden unit `H_L`.

    The first layer is identical to the shallow network: Input `X` undergoes a linear transform with weights `W_1`, followed by activation `G_1` to produce `H_1`. To add layers in between, we use a very simple idea: `H_2` takes the previous layer `H_1` as its input, goes through a linear transform `W_2`, and an activation function. This pattern repeats recursively. `H_3` takes `H_2` as input with weights `W_3` and activation `G_3`, and so on. This is how you build a deep, fully connected neural network.

    A few technicalities to note: The trainable parameters are the weights `W` and biases `b` for the linear transforms. These are indexed separately for each layer (e.g., `W_1`, `W_2`...), meaning they are all independent of each other. This grants the model a tremendous number of degrees of freedom. Additionally, you have the freedom to choose different activation functions `G` for each layer. In the graphical representation—where every node is a scalar variable and every edge is a trainable weight—a deep network with 3 hidden units (`L=3`) shows separate matrices `W` connecting each layer. The number of degrees of freedom corresponds to the number of arrows (edges) in the graph. In the Keras library ("incur us" in transcript), these are referred to as "Dense" layers because the network is fully connected; every node in one layer connects to every node in the next, making the connections as dense as possible.

    让我们定义深度神经网络最简单的结构：深度全连接神经网络。此前我们定义的浅层网络是输入 `X` 经过线性变换和非线性函数 `G` 得到一个隐藏层 `H`。要让神经网络变“深”，我们只需堆叠这些层。我们不再只有一个隐藏单元，而是有 `L` 个隐藏单元，记为 `H_1` 到 `H_L`。最后一步与浅层网络相同：在最后一个隐藏单元 `H_L` 上进行线性回归。

    第一层与浅层网络完全相同：输入 `X` 经过权重为 `W_1` 的线性变换，随后是激活函数 `G_1`，生成 `H_1`。要在中间添加层，思路非常简单：`H_2` 将上一层 `H_1` 作为输入，经过线性变换 `W_2` 和激活函数。这种模式递归重复。`H_3` 以 `H_2` 为输入，使用权重 `W_3` 和激活函数 `G_3`，依此类推。这就是构建深度全连接神经网络的方法。

    需要注意几个技术细节：可训练参数是线性变换的权重 `W` 和偏置 `b`。它们在每一层都有独立的索引（如 `W_1`, `W_2`...），这意味着它们彼此独立。这赋予了模型极大的自由度。此外，你可以为每一层选择不同的激活函数 `G`。在图形表示中——每个节点是一个标量变量，每条边是一个可训练权重——一个具有 3 个隐藏单元（`L=3`）的深度网络显示了连接每一层的独立矩阵 `W`。自由度的数量对应于图中的箭头（边）数量。在 Keras 库（录音中误听为 "incur us"）中，这些被称为“密集”（Dense）层，因为网络是全连接的；一层的每个节点都连接到下一层的每个节点，使得连接尽可能密集。

  - [14:16 - 14:21] **Why Depth Matters: Intuitions and Challenges**

    Why do we care about deep neural networks? Why add layers? Unfortunately, there is no definitive mathematical answer yet, but there are useful intuitions and partial answers.

    1. **Oscillatory Functions:** We will demonstrate shortly that deep networks are efficient at representing highly oscillatory functions—functions that are extremely sensitive to input changes (high frequency) but are not random.
    2. **Benign Overfitting:** Deep networks typically have a massive number of parameters, putting us in a regime where overfitting should theoretically happen. However, empirically, we observe "benign overfitting," where the solution generalizes well provided we apply appropriate training and regularization strategies.
    3. **Sequential Feature Extraction (Hierarchy):** This is perhaps the most convincing intuition. A deep network imposes a prior of sequential or hierarchical learning. In image recognition, for example, the first layer might learn basic edges and colors. The second layer combines these representations to recognize shapes. The third layer combines shapes to recognize objects. By composing layers one after another, we force the model to learn low-level features first and build up to high-level abstractions. If your task benefits from this hierarchical structure, deep networks are highly effective.

    However, not everything is positive. Deep networks are highly non-convex, meaning the loss landscape likely has many local minima, making gradient descent prone to getting stuck. They are easier to overfit due to their size, and they require significantly more computational resources and time to train.

    我们为什么要关注深度神经网络？为什么要增加层数？遗憾的是，目前还没有确定的数学答案，但存在一些有用的直觉和部分解答。

    1. **振荡函数：** 我们稍后将演示，深度网络能高效地表示高振荡函数——即那些对输入变化极其敏感（高频）但并非随机的函数。
    2. **良性过拟合：** 深度网络通常拥有海量参数，理论上这会让我们处于过拟合的境地。然而，经验上我们观察到了“良性过拟合”现象，即只要应用适当的训练和正则化策略，解的泛化能力依然很好。
    3. **序列特征提取（层级化）：** 这可能是最有说服力的直觉。深度网络施加了一种序列化或层级化学习的先验。例如在图像识别中，第一层可能学习基本的边缘和颜色。第二层结合这些表示来识别形状。第三层结合形状来识别物体。通过一层层地组合，我们强制模型先学习低级特征，再构建高级抽象。如果你的任务受益于这种层级结构，深度网络将非常有效。

    然而，并非一切都是好消息。深度网络是高度非凸的，这意味着损失地形可能有很多局部极小值，导致梯度下降容易陷入其中。由于模型庞大，它们更容易过拟合，并且需要更多的计算资源和时间来训练。

  - [14:21 - 14:26] **Demo: Visualizing Optimization Landscapes**

    We will use a demo to understand what kinds of functions are best represented by shallow versus deep networks. We use `plt.contourf` to plot 2D contour maps of the network outputs.

    First, we build a **Shallow Neural Network** with one hidden layer of 150 units (approx. 601 parameters). The resulting landscape is quite smooth. The path from maximum to minimum (represented by darker colors) is clear. This suggests that gradient descent would perform reasonably well here. Note that for a shallow network, the landscape with respect to inputs is symmetric to the landscape with respect to weights.

    Next, we build a **Deep Neural Network** with depth 3 and width 16 (approx. 600 parameters, to keep complexity comparable). The landscape immediately becomes much more complex. When we boost the depth to 6 (width 10, approx. 591 parameters), the landscape shows regions where contour lines are very close together. This indicates the output is highly sensitive to the input in those regions. This confirms our hypothesis that deep networks are better suited for modeling functions that are highly sensitive to inputs.

    However, this complexity implies difficulty in optimization. If you visualize the optimization landscape with respect to the weights in the first layer, you can see it is highly non-convex. If you initialize in a "saddle" region, it might not be clear whether to turn left or right to decrease the loss. Both directions might seem valid locally, making gradient descent tricky.

    我们将通过演示来理解浅层与深度网络最适合表示什么样的函数。我们使用 `plt.contourf` 来绘制网络输出的 2D 等高线图。

    首先，我们构建一个**浅层神经网络**，包含一个有 150 个单元的隐藏层（约 601 个参数）。生成的景观相当平滑。从最大值到最小值（由深色表示）的路径非常清晰。这表明梯度下降在这里的表现会比较合理。注意，对于浅层网络，关于输入的景观与关于权重的景观是对称的。

    接下来，我们构建一个**深度神经网络**，深度为 3，宽度为 16（约 600 个参数，以保持复杂度可比）。景观立刻变得复杂得多。当我们把深度增加到 6（宽度 10，约 591 个参数）时，景观显示的区域中等高线非常密集。这表明在这些区域输出对输入高度敏感。这证实了我们的假设，即深度网络更适合建模对输入高度敏感的函数。

    然而，这种复杂性也意味着优化的困难。如果你观察关于第一层权重的优化景观，你会发现它是高度非凸的。如果你初始化在一个“鞍点”区域，可能不清楚应该向左还是向右转来降低损失。局部来看两个方向似乎都是对的，这使得梯度下降变得棘手。

  - [14:26 - 14:31] **Skip Connections (Residual Networks)**

    To address the training difficulty, we introduce "skip connections" (or residual connections). Mathematically, we modify the layer transition. Instead of just `H_new = G(W * H_old)`, we define the new hidden unit as `H = H_prev + F(H_prev)`, where `F` represents the dense layer transformation.

    This structure makes it easier for the network to learn the identity function. If the transformation `F` learns to be 0, the layer simply passes the input through (`H = H_prev`). This helps prevent the gradient from vanishing because even if `F` is zero (or random), the gradient flow through the identity path is 1. This makes the landscape much friendlier for gradient descent.

    Implementing this in code involves adding the previous layer's output to the current layer's transformation. When we visualize the landscape of a deep network *with* skip connections, it appears much smoother compared to the deep network without them. Crucially, using the summary method, we see that skip connections do not add any trainable parameters—you are simply copying the last layer to the next.

    为了解决训练困难，我们引入了“跳跃连接”（Skip Connections，或残差连接）。在数学上，我们修改了层的转换方式。不仅仅是 `H_new = G(W * H_old)`，我们将新的隐藏单元定义为 `H = H_prev + F(H_prev)`，其中 `F` 代表密集层变换。

    这种结构使得网络更容易学习恒等函数。如果变换 `F` 学习为 0，该层只是简单地传递输入（`H = H_prev`）。这有助于防止梯度消失，因为即使 `F` 为零（或随机），通过恒等路径的梯度流为 1。这使得地形对梯度下降更加友好。

    在代码中实现这一点包括将上一层的输出加到当前层的变换上。当我们可视化具有跳跃连接的深度网络的景观时，它比没有跳跃连接的网络看起来平滑得多。关键是，使用 summary 方法可以看到，跳跃连接不会增加任何可训练参数——你只是将上一层复制到下一层。

  - [14:31 - 14:38] **Stochastic Gradient Descent (SGD) and Mini-batches**

    Deep networks have complex landscapes and require large datasets for training. However, standard gradient descent is extremely slow on large data. The empirical risk `R` involves computing the average loss over `N` samples (where `N` is the dataset size). To take a single step in gradient descent, you must calculate the gradient of the sum over `N` terms. If `N` is 10,000 or 60,000, this is computationally prohibitive.

    We need an algorithm whose runtime is independent of the dataset size `N`. This brings us to **Stochastic Gradient Descent (SGD)**. The idea is simple: instead of computing the actual gradient (sum over `N`), we compute an *unbiased estimate* of the gradient using a single random sample, and then take a step.

    Let `γ_k` be a uniform random variable from 1 to `N` (a random index). We update our parameter `θ` by subtracting `ε` (learning rate) times the gradient of the loss with respect to *only* sample `γ_k`.

    - Computation cost: 1 loss function calculation per step (no sum).
    - Unbiased nature: The expected value of the gradient of a random sample is precisely the average of the gradients of all samples. Thus, on average, SGD equals gradient descent.

    However, relying on a single sample makes the trajectory extremely volatile and random. To balance stability and speed, we use **Mini-batch Gradient Descent**. Instead of 1 sample, we sample a batch of size `M` (e.g., `M=64`). We average the gradients over these `M` samples. This reduces variance compared to pure SGD but is faster than full gradient descent. In practice, mini-batch is the standard choice.

    深度网络具有复杂的地形，并且需要大数据集进行训练。然而，标准梯度下降在处理大数据时极慢。经验风险 `R` 涉及计算 `N` 个样本（其中 `N` 是数据集大小）的平均损失。要在梯度下降中迈出一步，必须计算 `N` 项之和的梯度。如果 `N` 是 10,000 或 60,000，这在计算上是不可接受的。

    我们需要一种运行时间与数据集大小 `N` 无关的算法。这就引入了**随机梯度下降（SGD）**。思路很简单：我们不计算实际梯度（`N` 项之和），而是使用单个随机样本计算梯度的*无偏估计*，然后迈出一步。

    设 `γ_k` 为从 1 到 `N` 的均匀随机变量（随机索引）。我们通过减去 `ε`（学习率）乘以仅关于样本 `γ_k` 的损失梯度来更新参数 `θ`。

    - 计算成本：每步仅计算 1 个损失函数（无需求和）。
    - 无偏性质：随机样本梯度的期望值正是所有样本梯度的平均值。因此，平均而言，SGD 等于梯度下降。

    然而，依赖单个样本会使轨迹极其波动和随机。为了平衡稳定性与速度，我们使用**小批量梯度下降（Mini-batch Gradient Descent）**。我们不取 1 个样本，而是采样大小为 `M` 的一批（例如 `M=64`）。我们在这些 `M` 个样本上平均梯度。这比纯 SGD 降低了方差，但比全梯度下降更快。在实践中，小批量是标准选择。

  - [14:38 - 14:49] **Convergence Analysis: Gradient Descent on a Quadratic**

    Does SGD converge? Let's analyze this using a simple 1D quadratic example from last week: `R(θ) = θ^2`.

    Standard Gradient Descent update: `θ_{k+1} = (1 - ε)θ_k`.

    Explicit formula: `θ_k = (1 - ε)^k * θ_0`.

    Convergence: As long as `ε < 2`, `θ_k` converges to 0 (the global minimum).

    Now, let's adapt this for SGD. We define a noisy loss function for each sample `i`: `R_i(θ) = 1/2 * (θ - θ_i)^2`.

    - `θ` is the trainable parameter.
    - `θ_i` represents the noise inherent in sample `i`.
    - Assumptions: The average of `θ_i` is 0 (mean noise is zero), and the average of `θ_i^2` is 1 (variance of noise is 1).

    Think of this as linear regression where the input `X=1` is constant, and we are predicting a constant target. The true target should be 0, but the data has noise `θ_i` with variance 1.

    First, let's verify Gradient Descent on this specific problem. The global objective `R(θ)` is the average of `R_i(θ)`.

    Expanding `(θ - θ_i)^2`: We get `θ^2 - 2θθ_i + θ_i^2`.

    Averaging over `N` samples:

    1. `θ^2` term averages to `θ^2` (constant w.r.t `i`).

    2. `-2θθ_i` term averages to 0 (since mean of `θ_i` is 0).

    3. `θ_i^2` term averages to 1 (since variance is 1).

       Result: `R(θ) = 1/2 * θ^2 + 1/2`.

       The gradient of `R` is simply `θ`. Thus, the GD update is identical to the simple case: `θ_{k+1} = (1 - ε)θ_k`. It converges deterministically to 0.

    SGD 收敛吗？让我们用上周的一个简单的 1D 二次函数示例来分析：`R(θ) = θ^2`。

    标准梯度下降更新：`θ_{k+1} = (1 - ε)θ_k`。

    显式公式：`θ_k = (1 - ε)^k * θ_0`。

    收敛性：只要 `ε < 2`，`θ_k` 就会收敛到 0（全局最小值）。

    现在，让我们针对 SGD 调整这个例子。我们为每个样本 `i` 定义一个含噪声的损失函数：`R_i(θ) = 1/2 * (θ - θ_i)^2`。

    - `θ` 是可训练参数。
    - `θ_i` 代表样本 `i` 固有的噪声。
    - 假设：`θ_i` 的平均值为 0（平均噪声为零），`θ_i^2` 的平均值为 1（噪声方差为 1）。

    可以把这看作是输入 `X=1` 为常数的线性回归，我们预测一个常数目标。真实目标应该是 0，但数据包含方差为 1 的噪声 `θ_i`。

    首先，验证此特定问题上的梯度下降。全局目标 `R(θ)` 是 `R_i(θ)` 的平均值。

    展开 `(θ - θ_i)^2`：得到 `θ^2 - 2θθ_i + θ_i^2`。

    对 `N` 个样本求平均：

    1. `θ^2` 项平均为 `θ^2`（关于 `i` 是常数）。

    2. `-2θθ_i` 项平均为 0（因为 `θ_i` 的均值为 0）。

    3. `θ_i^2` 项平均为 1（因为方差为 1）。

       结果：`R(θ) = 1/2 * θ^2 + 1/2`。

       `R` 的梯度仅仅是 `θ`。因此，GD 更新与简单情况相同：`θ_{k+1} = (1 - ε)θ_k`。它确定性地收敛到 0。

  - [14:49 - 14:57] **SGD Analysis: Expectation**

    Now, how does SGD behave? We take the gradient with respect to a *single* random sample `R_i`.

    Gradient of `R_i` w.r.t `θ` is `(θ - θ_i)`.

    The SGD iteration becomes:

    `θ_{k+1} = θ_k - ε * (θ_k - θ_{γ_k})`

    where `γ_k` is the random index.

    Simplifying: `θ_{k+1} = (1 - ε)θ_k + ε * θ_{γ_k}`.

    We can expand this recursion all the way back to `θ_0`.

    `θ_k = (1 - ε)^k * θ_0 + ε * [Sum from J=1 to K of (1-ε)^(J-1) * θ_{γ_{k-J}}]`.

    This formula has two parts:

    1. **Deterministic part:** `(1 - ε)^k * θ_0`. This is identical to Gradient Descent.
    2. **Random part:** The sum term involving random noise samples `θ_{γ}`.

    If we take the **Expected Value** of `θ_k`:

    The expectation of the noise term `θ_{γ}` is 0 (as defined by our noise model). Therefore, the entire sum term vanishes in expectation.

    Result: `E[θ_k] = (1 - ε)^k * θ_0`.

    **Conclusion:** On average, SGD is exactly equal to Gradient Descent. It converges to 0 in expectation.

    现在，SGD 表现如何？我们对*单个*随机样本 `R_i` 求梯度。

    `R_i` 关于 `θ` 的梯度是 `(θ - θ_i)`。

    SGD 迭代变为：

    `θ_{k+1} = θ_k - ε * (θ_k - θ_{γ_k})`

    其中 `γ_k` 是随机索引。

    化简得：`θ_{k+1} = (1 - ε)θ_k + ε * θ_{γ_k}`。

    我们可以将这个递归一直展开回 `θ_0`。

    `θ_k = (1 - ε)^k * θ_0 + ε * [从 J=1 到 K 的求和 (1-ε)^(J-1) * θ_{γ_{k-J}}]`。

    这个公式有两部分：

    1. **确定性部分：** `(1 - ε)^k * θ_0`。这与梯度下降完全相同。
    2. **随机部分：** 涉及随机噪声样本 `θ_{γ}` 的求和项。

    如果我们计算 `θ_k` 的**期望值**：

    噪声项 `θ_{γ}` 的期望值为 0（根据我们的噪声模型定义）。因此，整个求和项在期望中消失了。

    结果：`E[θ_k] = (1 - ε)^k * θ_0`。

    **结论：** 平均而言，SGD 完全等于梯度下降。它在期望上收敛到 0。

  - [14:57 - 15:12] **SGD Analysis: Variance and Trade-offs**

    Since SGD has randomness, we must analyze the variance. We need to compute `E[θ_k^2]`.

    Squared expansion of `θ_k`:

    1. First term squared: `((1-ε)^k θ_0)^2`.
    2. Cross term: Contains `θ_{γ}`, so expectation is 0.
    3. Last term (Random Sum) squared: This creates a double sum.
       - If indices `J` and `L` are different, the expectation `E[θ_{γ_J} * θ_{γ_L}]` is 0 due to independence (I.I.D assumption).
       - If indices are the same (`J=L`), we get `E[θ_{γ}^2]`, which is the variance of the noise = 1.

    This simplifies the variance calculation to a single geometric sum:

    `Variance(θ_k) = ε^2 * Sum_{J=1 to K} [(1 - ε)^(2J - 2)]`.

    Using the geometric series formula, as `k -> infinity`, the variance converges to:

    `Limit Variance ≈ ε^2 / (2ε - ε^2) ≈ ε / 2`.

    This is a crucial result. The variance is of order `ε`. The standard deviation (fluctuation) is of order `sqrt(ε)`.

    **The Trade-off:**

    - **Large `ε`:** Fast convergence initially, but large fluctuations around 0 (noise floor). You will never settle perfectly at the minimum; you will bounce around it with error proportional to `sqrt(ε)`.
    - **Small `ε`:** Slower convergence, but smaller fluctuations once you arrive.

    This explains the behavior of SGD: it converges to a "noise floor" determined by the learning rate.

    由于 SGD 具有随机性，我们必须分析方差。我们需要计算 `E[θ_k^2]`。

    `θ_k` 的平方展开：

    1. 第一项平方：`((1-ε)^k θ_0)^2`。
    2. 交叉项：包含 `θ_{γ}`，所以期望为 0。
    3. 最后一项（随机求和）平方：这产生一个双重求和。
       - 如果索引 `J` 和 `L` 不同，由于独立性（I.I.D 假设），期望 `E[θ_{γ_J} * θ_{γ_L}]` 为 0。
       - 如果索引相同（`J=L`），我们得到 `E[θ_{γ}^2]`，即噪声的方差 = 1。

    这将方差计算简化为单个几何级数求和：

    `Variance(θ_k) = ε^2 * Sum_{J=1 to K} [(1 - ε)^(2J - 2)]`。

    利用几何级数公式，当 `k -> 无穷大` 时，方差收敛于：

    `极限方差 ≈ ε^2 / (2ε - ε^2) ≈ ε / 2`。

    这是一个至关重要的结果。方差是 `ε` 量级的。标准差（波动）是 `sqrt(ε)` 量级的。

    **权衡：**

    - **大 `ε`：** 初始收敛快，但在 0 附近的波动大（噪声基底）。你永远无法完美停留在最小值；你会以正比于 `sqrt(ε)` 的误差在它周围跳动。
    - **小 `ε`：** 收敛较慢，但一旦到达，波动较小。

    这解释了 SGD 的行为：它收敛到由学习率决定的“噪声基底”。

  - [15:25 - 15:28] **Learning Rate Decay (Annealing)**

    In the stochastic gradient descent (SGD) landscape, the gradient will pull you towards the center of the noise distribution (e.g., negative 1.8 in our specific example). However, because you are picking random samples, the update will fluctuate in this region even after convergence. We observed that with a learning rate of 0.5, you converge quickly (in a single step) but have large noise around 0. If we decrease the learning rate by a factor of 5 to 0.1, it takes 3-4 steps to converge, but the fluctuations after convergence are much smaller. This confirms our derivation that the standard deviation of the noise is of the order `sqrt(ε)`.

    The smaller the `ε`, the slower the convergence, but the smaller the final fluctuations. However, the fluctuations never truly go to zero with a constant learning rate. How do we resolve this? The solution is to use a **decaying learning rate** (or annealing). Instead of `ε` being constant, we use `ε_k`, where `ε_k` is large initially to ensure fast convergence, and then gets smaller and smaller as `k` increases. This dampens the fluctuations over time, allowing the system to eventually settle at 0.

    Mathematically, there is a sufficient condition for the rate of decay to guarantee convergence to the global minimum. The sum of the learning rates must diverge (`Σ ε_k = ∞`), ensuring you can reach the minimum from anywhere, but the sum of the squares must converge (`Σ ε_k^2 < ∞`), ensuring the variance dies out. An example function that satisfies this is the harmonic series, where `ε_k = C / k`.

    在随机梯度下降（SGD）的地形中，梯度会将你拉向噪声分布的中心（在我们的例子中是负 1.8）。然而，由于你是随机选取样本，即使收敛后，更新值也会在这个区域内波动。我们观察到，当学习率为 0.5 时，你收敛得很快（一步到位），但在 0 附近的噪声很大。如果我们把学习率减小 5 倍到 0.1，虽然需要 3-4 步才能收敛，但收敛后的波动要小得多。这证实了我们的推导，即噪声的标准差是 `sqrt(ε)` 量级的。

    `ε` 越小，收敛越慢，但最终的波动越小。然而，使用恒定学习率时，波动永远不会真正变为零。如何解决这个问题？解决方案是使用**衰减的学习率**（或退火）。我们不再让 `ε` 保持常数，而是使用 `ε_k`，其中 `ε_k` 在初始时较大以确保快速收敛，随着 `k` 增加而逐渐变小。这会随时间抑制波动，使系统最终稳定在 0。

    在数学上，有一个关于衰减率的充分条件可以保证收敛到全局最小值。学习率之和必须发散（`Σ ε_k = ∞`），确保你可以从任意位置到达最小值，但平方和必须收敛（`Σ ε_k^2 < ∞`），确保方差最终消失。满足这一条件的函数示例是调和级数，即 `ε_k = C / k`。

  - [15:28 - 15:39] **Momentum and Adaptive Optimizers**

    We have solved the issue of large data making gradient descent slow by using SGD or mini-batch SGD. Now, we want to improve SGD further by incorporating other ideas. One major concept is **Momentum**. Another is **Adaptive Learning Rates**, where the learning rate is different for every trainable parameter. The model automatically varies the learning rate; the user only sets a maximum global rate. Examples of such algorithms include Adadelta ("delta" in transcript) and the very popular **Adam** algorithm. Adam combines the ideas of both momentum and adaptive learning rates, making it a very good default optimizer to use.

    Let's focus on Momentum. Recall from our analysis of linear regression that the convergence rate depends on the condition number (ratio of eigenvalues) of the covariance matrix. If the eigenvalues in different directions are vastly different, you get a "narrow valley" landscape. For example, if we minimize `R(θ) = 1/2 * (θ_1^2 + λ * θ_2^2)` where `λ` is very large, the gradient in the vertical direction (`θ_2`) is much larger than in the horizontal direction (`θ_1`). Gradient descent will overshoot back and forth in the steep direction while making slow progress in the flat direction, resulting in a **zigzagging** behavior.

    This zigzagging is inefficient. If we imagine a ball rolling down a hill in the physical world, it doesn't zigzag wildly because it has mass and inertia—it has **momentum**. Momentum is preserved from one time step to the next, smoothing out the trajectory. Inspired by Newton's Second Law (`F = ma`), we modify our update rule. In discrete time, we introduce a velocity term `v_k`. The update becomes `v_{k+1} = α * v_k - ε * Gradient`. The parameter `α` (usually between 0 and 1) represents friction or memory. If `α` is non-zero, the system remembers the previous step. This makes the system "second-order," effectively dampening the oscillations and speeding up convergence along the flat direction.

    我们已经通过 SGD 或小批量 SGD 解决了大数据导致梯度下降过慢的问题。现在，我们希望通过通过结合其他思想进一步改进 SGD。一个主要概念是**动量（Momentum）**。另一个是**自适应学习率**，即每个可训练参数的学习率都是不同的。模型会自动调整每个参数的学习率；用户只需设置一个最大的全局比率。此类算法的例子包括 Adadelta（录音中简称 "delta"）和非常流行的 **Adam** 算法。Adam 结合了动量和自适应学习率的思想，使其成为一个非常好的默认优化器。

    让我们专注于动量。回想一下我们对线性回归的分析，收敛速度取决于协方差矩阵的条件数（特征值的比率）。如果不同方向的特征值差异巨大，你就会得到一个“狭长山谷”般的地形。例如，如果我们最小化 `R(θ) = 1/2 * (θ_1^2 + λ * θ_2^2)`，其中 `λ` 非常大，那么垂直方向（`θ_2`）的梯度将远大于水平方向（`θ_1`）。梯度下降会在陡峭的方向上来回过冲，而在平坦的方向上进展缓慢，导致**之字形（zigzagging）**行为。

    这种之字形路径是低效的。如果我们想象一个球在物理世界中滚下山坡，它不会剧烈地走之字形，因为它有质量和惯性——它有**动量**。动量会从一个时间步保留到下一个时间步，从而平滑轨迹。受牛顿第二定律（`F = ma`）启发，我们修改更新规则。在离散时间中，我们引入速度项 `v_k`。更新变为 `v_{k+1} = α * v_k - ε * Gradient`。参数 `α`（通常在 0 到 1 之间）代表摩擦或记忆。如果 `α` 非零，系统会记住上一步。这使得系统变为“二阶”系统，有效地抑制震荡并加速在平坦方向上的收敛。

  - [15:39 - 15:42] **Mini-batch Implementation and Epochs**

    In practice, whether you use Momentum or Adam, you ultimately incur a "batch size" parameter. This determines whether you are doing pure SGD (batch size 1) or mini-batch GD (e.g., 64 or 128). Larger batch sizes are slower per step but more stable.

    How is sampling done in practice? Theoretically, SGD samples randomly. Practically, we sample **without replacement**. We define an **Epoch** as one full sweep through the entire dataset. If your dataset has `N` points and batch size is `M`, you first shuffle the data, then take the first `M` points as batch 1, the next `M` as batch 2, and so on. In Keras, when you call the `.fit()` method, you specify the number of epochs. The number of gradient descent steps per epoch is `N / M`. If `N=1000` and `M=100`, you take 10 steps per epoch.

    在实践中，无论你使用动量还是 Adam，你最终都会遇到“批量大小”（batch size）这个参数。这决定了你是做纯 SGD（批量大小为 1）还是小批量 GD（例如 64 或 128）。较大的批量每步较慢，但更稳定。

    实际上是如何进行采样的？理论上，SGD 是随机采样的。但在实践中，我们进行**不放回采样**。我们将**Epoch（轮）**定义为对整个数据集的一次完整扫描。如果你的数据集有 `N` 个点，批量大小为 `M`，你首先打乱数据，然后取前 `M` 个点作为第 1 批，接下来的 `M` 个作为第 2 批，依此类推。在 Keras 中，当你调用 `.fit()` 方法时，你需要指定 epoch 的数量。每个 epoch 包含的梯度下降步数是 `N / M`。如果 `N=1000` 且 `M=100`，那么每个 epoch 你将采取 10 步。

  - [15:42 - 15:52] **Computational Graphs and Efficient Backpropagation**

    All variants of gradient descent (Momentum, Adam, SGD) require calculating the gradient of the loss on each sample. For deep neural networks, which are compositions of many functions, we use the **Backpropagation** algorithm. To understand this, we use computational graphs. Nodes represent variables, and edges represent operations.

    - *Example 1:* `Z = X * Y`.
    - *Example 2 (Logistic Regression):* Inputs `X, W, b` -> `U1 = X·W` -> `U2 = U1 + b` -> `Y_hat = Sigma(U2)`.
    - *Example 3 (Regularization):* Output `R = λ * ||W||^2`. This graph squares every component of `W` and sums them.

    How do we efficiently compute gradients? The network is a composition: `Z = F(G(X))`. The chain rule tells us `dZ/dX = dZ/dY * dY/dX`. However, a naive application of the chain rule is inefficient because it often recomputes the same values multiple times.

    Consider `f(f(f(W)))`. If you blindly apply the chain rule, the term `f(W)` appears in multiple places in the derivative expansion. If you don't store it, you have to recompute it every time.

    **The Core Idea:** Efficient backpropagation trades memory for computation.

    1. **Forward Pass:** As you compute the output, **store** the intermediate values (e.g., `f(W)`) in memory.
    2. **Backward Pass:** When computing gradients, retrieve these stored values from memory instead of recomputing them.

    所有的梯度下降变体（动量、Adam、SGD）都需要计算每个样本上的损失梯度。对于由许多函数组合而成的深度神经网络，我们使用**反向传播（Backpropagation）**算法。为了理解这一点，我们使用计算图。节点代表变量，边代表操作。

    - *示例 1：* `Z = X * Y`。
    - *示例 2（逻辑回归）：* 输入 `X, W, b` -> `U1 = X·W` -> `U2 = U1 + b` -> `Y_hat = Sigma(U2)`。
    - *示例 3（正则化）：* 输出 `R = λ * ||W||^2`。这个图将 `W` 的每个分量平方并求和。

    我们如何高效地计算梯度？网络是一个复合函数：`Z = F(G(X))`。链式法则告诉我们 `dZ/dX = dZ/dY * dY/dX`。然而，简单粗暴地应用链式法则效率很低，因为它经常重复计算相同的值。

    考虑 `f(f(f(W)))`。如果你盲目地应用链式法则，项 `f(W)` 会在导数展开式中多次出现。如果你不保存它，每次都需要重新计算。

    **核心思想：** 高效的反向传播是用空间换时间。

    1. **前向传播：** 当你计算输出时，将中间值（例如 `f(W)`）**存储**在内存中。
    2. **反向传播：** 计算梯度时，从内存中检索这些存储的值，而不是重新计算它们。

  - [15:52 - 16:09] **Backpropagation Walkthrough: A Linear Example**

    Let's demonstrate this on a simple 1D linear neural network (all scalars, no activation functions).

    **Setup:**

    - `H1 = W1 * X`
    - `H2 = W2 * H1`
    - `H3 = W3 * H2`
    - `Y_hat = W * H3`
    - Loss `R` is based on `(Y_hat - Y)^2`.

    **Step 1: Forward Propagation**

    Given data `X` and parameters `W_i`:

    1. Compute `H1 = W1 * X`. **Store H1 in memory.**

    2. Compute `H2 = W2 * H1`. Use the stored `H1`. **Store H2.**

    3. Compute `H3 = W3 * H2`. Use the stored `H2`. **Store H3.**

    4. Compute `Y_hat = W * H3`. Use the stored `H3`. **Store Y_hat.**

       Now, memory contains `H1, H2, H3, Y_hat`.

    **Step 2: Backward Propagation**

    We want gradients of `R` with respect to all weights `W`. We compute gradients recursively starting from the output.

    1. Compute `dR/dY_hat`. This depends on `Y_hat` (which is in memory) and `Y`. **Store this result as `P_hat`.**
    2. Compute `dR/dH3`. By chain rule: `(dR/dY_hat) * (dY_hat/dH3)`.
       - `dR/dY_hat` is `P_hat` (retrieved from memory).
       - `dY_hat/dH3` is `W`.
       - Result: `P_hat * W`. **Store as `P3`.**
    3. Compute `dR/dH2`. Chain rule: `(dR/dH3) * (dH3/dH2)`.
       - `dR/dH3` is `P3`.
       - `dH3/dH2` is `W3`.
       - Result: `P3 * W3`. **Store as `P2`.**
    4. Repeat for `P1`.

    **Step 3: Compute Weight Gradients**

    Now we combine the stored values from Step 1 (Forward) and Step 2 (Backward) to get the final gradients for the weights.

    - `dR/dW` = `(dR/dY_hat) * (dY_hat/dW)` = `P_hat * H3`. (Both `P_hat` and `H3` are in memory).
    - `dR/dW3` = `(dR/dH3) * (dH3/dW3)` = `P3 * H2`. (Both `P3` and `H2` are in memory).
    - `dR/dW2` = `P2 * H1`.
    - `dR/dW1` = `P1 * X`.

    All necessary variables are already in memory, so these computations are extremely fast. This avoids the quadratic complexity of naive chain rule application, resulting in linear complexity (one forward pass, one backward pass).

    让我们在一个简单的 1D 线性神经网络（全是标量，无激活函数）上演示这一点。

    **设置：**

    - `H1 = W1 * X`
    - `H2 = W2 * H1`
    - `H3 = W3 * H2`
    - `Y_hat = W * H3`
    - 损失 `R` 基于 `(Y_hat - Y)^2`。

    **第一步：前向传播**

    给定数据 `X` 和参数 `W_i`：

    1. 计算 `H1 = W1 * X`。**将 H1 存入内存。**

    2. 计算 `H2 = W2 * H1`。使用存储的 `H1`。**存储 H2。**

    3. 计算 `H3 = W3 * H2`。使用存储的 `H2`。**存储 H3。**

    4. 计算 `Y_hat = W * H3`。使用存储的 `H3`。**存储 Y_hat。**

       现在，内存中包含 `H1, H2, H3, Y_hat`。

    **第二步：反向传播**

    我们需要 `R` 关于所有权重 `W` 的梯度。我们从输出开始递归计算梯度。

    1. 计算 `dR/dY_hat`。这取决于 `Y_hat`（在内存中）和 `Y`。**将结果存为 `P_hat`。**
    2. 计算 `dR/dH3`。根据链式法则：`(dR/dY_hat) * (dY_hat/dH3)`。
       - `dR/dY_hat` 是 `P_hat`（从内存取）。
       - `dY_hat/dH3` 是 `W`。
       - 结果：`P_hat * W`。**存为 `P3`。**
    3. 计算 `dR/dH2`。链式法则：`(dR/dH3) * (dH3/dH2)`。
       - `dR/dH3` 是 `P3`。
       - `dH3/dH2` 是 `W3`。
       - 结果：`P3 * W3`。**存为 `P2`。**
    4. 对 `P1` 重复此步骤。

    **第三步：计算权重梯度**

    现在我们结合第一步（前向）和第二步（反向）的存储值来获得权重的最终梯度。

    - `dR/dW` = `(dR/dY_hat) * (dY_hat/dW)` = `P_hat * H3`。（`P_hat` 和 `H3` 都在内存中）。
    - `dR/dW3` = `(dR/dH3) * (dH3/dW3)` = `P3 * H2`。（`P3` 和 `H2` 都在内存中）。
    - `dR/dW2` = `P2 * H1`。
    - `dR/dW1` = `P1 * X`。

    所有必要的变量都已在内存中，因此这些计算极快。这避免了简单链式法则应用的二次复杂度，实现了线性复杂度（一次前向，一次反向）。

  - [16:09 - 16:21] **Summary and Conclusion**

    To summarize backpropagation: It efficiently computes gradients by trading memory for computation. It stores intermediate states to prevent re-computation. If you have memory constraints, there are variants (checkpointing) that trade some computation back for memory savings.

    Although our example was scalar and linear, the principle holds for:

    1. **Non-linearities:** You simply add the derivative of the activation function `F` (e.g., `dF/dW`, `dF/dH`) into the chain.
    2. **Vectors/Tensors:** If `W` and `H` are vectors, the derivatives become Jacobian matrices (gradients), but the graph structure and flow remain identical.

    The good news (TL;DR) is that modern frameworks like TensorFlow and PyTorch handle this automatically. You only need to define the forward function and its local derivative; the framework manages the computational graph and backpropagation.

    **Lecture Summary:**

    1. **Deep Networks:** Composition of layers, hierarchical feature extraction, efficient for high-frequency functions.
    2. **Optimization:** SGD addresses the slowness of batch GD on large data.
    3. **Improvements:** Variance in SGD requires learning rate decay. Momentum helps with zigzagging in ill-conditioned landscapes.
    4. **Implementation:** Backpropagation allows efficient gradient calculation on a single sample using 2 passes of the graph.

    Next week, we will discuss **Convolutional Neural Networks (CNNs)**, which are a very powerful architecture compared to the fully connected networks we discussed so far.

    总结反向传播：它通过用空间换时间来高效计算梯度。它存储中间状态以防止重复计算。如果你有内存限制，存在一些变体（如检查点技术）可以用部分计算量换取内存节省。

    虽然我们的例子是标量和线性的，但其原理同样适用于：

    1. **非线性：** 你只需将激活函数 `F` 的导数（如 `dF/dW`, `dF/dH`）加入链式法则中。
    2. **向量/张量：** 如果 `W` 和 `H` 是向量，导数变为雅可比矩阵（梯度），但图结构和流程保持不变。

    好消息（长话短说）是，像 TensorFlow 和 PyTorch 这样的现代框架会自动处理这些。你只需定义前向函数及其局部导数；框架会管理计算图和反向传播。

    **讲座总结：**

    1. **深度网络：** 层的组合，层级特征提取，对高频函数高效。
    2. **优化：** SGD 解决了批量 GD 在大数据上速度慢的问题。
    3. **改进：** SGD 的方差需要学习率衰减。动量有助于解决病态地形中的之字形问题。
    4. **实现：** 反向传播允许通过图的两次遍历高效计算单个样本的梯度。

    下周，我们将讨论**卷积神经网络（CNNs）**（录音中误听为 "conclusion on neural network check"），相比我们目前讨论的全连接网络，这是一种非常强大的架构。
## Week 4
  - [14:04 - 14:05] **Output Layer Design and Transition to Computer Vision**

    The size of the output you want will determine the dimensions of the weights in the final layer, and then you have the choice of the activation function. After you stack many of these layers, eventually, the configuration of the last layer will depend on whether you are solving a regression problem or a classification problem, right? If it is a regression task, the last layer is typically just a linear regression output based on the last hidden unit. Otherwise, if it is a classification task, it is a Softmax regression. We previously mentioned several reasons why Deep Neural Networks might perform better on certain types of functions, such as highly oscillatory functions. However, the truth is that historically, the first time a neural network model actually achieved state-of-the-art performance was in the field of Computer Vision, and that occurred before the popularization of the Convolutional Neural Network. So today, we will be looking at the motivation and construction of Convolutional Neural Networks.

    你想要的输出大小将决定最后一层权重的维度，接着你需要选择激活函数。当你堆叠了许多层之后，最终层的配置将取决于你是在解决回归问题还是分类问题，对吧？如果是回归任务，最后一层通常只是基于最后一个隐藏单元的线性回归输出。否则，如果是分类任务，那就是 Softmax 回归。我们之前提到过深度神经网络在某些类型的函数（如高度震荡的函数）上表现更好的几个原因。然而，事实是，历史上神经网络模型第一次真正达到最先进（SOTA）性能是在计算机视觉领域，而且那是发生在卷积神经网络普及之前。所以今天，我们将探讨卷积神经网络的动机和构建。

  - [14:05 - 14:08] **Permutation Invariance in Fully Connected Networks**

    To motivate Computational Neural Networks (Convolutional Neural Networks), we first want to look at something funny about Fully Connected Neural Networks. First, we need to define what a permutation is. If we have N objects, a permutation is a one-to-one function on these objects, right? It is a bijection. For example, if the objects are labeled 1, 2, 3 up to 9, then the permutation is simply a reordering of the objects from 1 to 9. It is just a reordering. Now, there is a possibly surprising property about the hypothesis space of Fully Connected Neural Networks called "Permutation Invariance."

    What is the statement? It says: Suppose given a function `f` in our hypothesis space, and given any random permutation `P`, where `P` permutes the index of the input. Your input data `x` is D-dimensional. We have a permutation `P` among indices 1 up to D, so it will permute the index of each entry across the dimensions of the input `x`. The statement says that if we consider the output of `f` on the permuted dataset, there exists some function `f_P` in the same hypothesis space which can act on the original dataset and give us this new output. In other words, if `f` is some trained neural network, there is another neural network `f_P`—another fully connected network, possibly with different weights—that can achieve this property. Another way to frame this is that we can swap the positions: First, I train a fully connected network `f` on the original data. Suppose your friend messes up the data by permuting the input dimensions. My claim is there is some neural network `f_P` that, if you feed it the permuted dataset, the output does not change compared to the original case.

    为了引入卷积神经网络，我们首先要看看全连接神经网络的一个有趣之处。首先，我们需要定义什么是置换（permutation）。如果我们有 N 个对象，置换就是这些对象上的一对一函数，对吧？它是一个双射。例如，如果对象被标记为 1 到 9，那么置换仅仅是对这些对象从 1 到 9 的重新排序。现在，关于全连接神经网络的假设空间，有一个可能令人惊讶的性质，称为“置换不变性”（Permutation Invariance）。

    这个陈述是什么呢？它说：假设在我们的假设空间中有一个函数 `f`，并且给定任意随机置换 `P`，其中 `P` 对输入的索引进行置换。你的输入数据 `x` 是 D 维的。我们在 1 到 D 之间有一个置换 `P`，所以它会打乱输入 `x` 各个维度的索引。该陈述表明，如果我们考虑 `f` 在置换后数据集上的输出，那么在同一个假设空间中存在某个函数 `f_P`，它可以作用于原始数据集并给出这个新的输出。换句话说，如果 `f` 是某个经过训练的神经网络，那么存在另一个神经网络 `f_P`——另一个可能具有不同权重的全连接网络——可以实现这一性质。另一种表述方式是我们交换位置：首先，我在原始数据上训练一个全连接网络 `f`。假设你的朋友通过打乱输入维度弄乱了数据。我的主张是，存在某个神经网络 `f_P`，如果你将置换后的数据集输入给它，其输出与原始情况相比不会改变。

  - [14:09 - 14:11] **Visualizing Weight Swapping**

    Why is this true? Actually, it is very simple. You should be convinced with this tiny example. Here we have a shallow Fully Connected Neural Network where the input `x` is 2-dimensional. Suppose you train some neural network `f` with this architecture. After training, the weights are fixed. Let's suppose for simplicity that from the first input dimension, we have some blue weights, and from the second one, we have some gray weights connecting to the next layer. The question is: if we permute the input—here there is only one choice, which is to swap the positions of `x1` and `x2`—how do you design another neural network to give us the same output?

    We can have a neural network with the exact same architecture. The only thing is that previously from the first node we had blue weights, and from the second we had red (gray) weights. Now we just swap them all. From the first input node, we connect the red weights; from the second, we connect the blue weights. You can imagine the output should not change because we just need to swap the weights accordingly. It is like dragging the edges together with the nodes. The statement is actually a bit stronger than what was in the slides just now. It is not just *any* fully connected network; if we define a hypothesis space with a *fixed* architecture, I can always retrain the neural network on the permuted dataset and the output remains the same. To put it in words: Fully Connected Neural Networks seem to not care about permuting the signal components or input components. If you can pick one permutation, you can pick any permutation.

    为什么这是真的？其实很简单。通过这个小例子你应该会被说服。这里我们有一个浅层全连接神经网络，输入 `x` 是二维的。假设你用这种架构训练了某个神经网络 `f`。训练后，权重是固定的。为了简单起见，假设从第一个输入维度出发我们有一些蓝色权重，从第二个出发我们有一些灰色权重连接到下一层。问题是：如果我们置换输入——这里只有一种选择，即交换 `x1` 和 `x2` 的位置——你如何设计另一个神经网络来给我们相同的输出？

    我们可以拥有一个具有完全相同架构的神经网络。唯一的区别是，之前从第一个节点出发我们有蓝色权重，从第二个出发有红色（灰色）权重。现在我们把它们全部交换。从第一个输入节点，我们连接红色权重；从第二个连接蓝色权重。你可以想象输出不应该改变，因为我们要做的只是相应地交换权重。这就像把边和节点一起拖动一样。这个陈述实际上比刚才幻灯片里的更强。它不仅仅是*任何*全连接网络；如果我们定义一个具有*固定*架构的假设空间，我总是可以在置换后的数据集上重新训练神经网络，输出保持不变。用语言来表达就是：全连接神经网络似乎不在乎信号分量或输入分量的置换。如果你可以选择一种置换，你就可以选择任何置换。

  - [14:12 - 14:14] **Dense Features: The Concrete Strength Example**

    Does this make sense or not? Let's look at some examples of data. In this example, we have some information about concrete. Each row is a data point, each column is some feature, and we have features about what the concrete is made from: cement, water, fly ash, and some other things. The age of the product also matters in order to predict the strength. So, what do we mean by a random permutation over the dimensions of the input? That means I am randomly permuting the columns of the table.

    The claim is that if I give the table on top to you, and I give the permuted table below to your friend, and if you are both using the same Fully Connected Neural Network architecture to train on these two datasets, you should get the same performance. Is this sensible? In other words, do we think the information within the dataset above and below is the same? I think it looks the same to me. They seem to represent the same information. Why is it that the ordering of the columns seems to not matter at all here? Because this is an example of what we call **Dense Features**, meaning that each feature on its own is already quite informative. They are like stand-alone features, and the ordering of the columns doesn't really matter. In this case, using a Fully Connected Neural Network actually makes sense.

    这有意义吗？让我们看一些数据示例。在这个例子中，我们有一些关于混凝土的信息。每一行是一个数据点，每一列是一个特征，我们有关于混凝土成分的特征：水泥、水、粉煤灰和其他一些东西。产品的龄期对于预测强度也很重要。那么，输入维度的随机置换是什么意思呢？这意味着我随机打乱表格的列。

    我的主张是，如果我把上面的表格给你，把下面打乱的表格给你的朋友，并且如果你们都使用相同的全连接神经网络架构在这两个数据集上进行训练，你们应该获得相同的性能。这合理吗？换句话说，我们是否认为上面和下面数据集中的信息是相同的？我觉得对我来说看起来是一样的。它们似乎代表了相同的信息。为什么列的顺序在这里似乎根本不重要？因为这是我们所谓的**密集特征**（Dense Features）的一个例子，这意味着每个特征本身就已经很有信息量了。它们就像独立的特征，列的顺序并不重要。在这种情况下，使用全连接神经网络实际上是有意义的。

  - [14:14 - 14:17] **Structured Data: Images and Time Series**

    What if we have images? Here we have a cat and a dog. Suppose you are trying to build a binary classifier where you feed in images and it tells you what animal it is. Each dimension of the data would be a pixel coordinate. So, the corresponding thing for each column from the last slide would now be a pixel. Randomly permuting the columns or input dimensions corresponds to randomly permuting the positions of the pixels. If you do that, you get the dataset on the right.

    What does the Fully Connected Neural Network hypothesis say? It says that if I give you the dataset on the left, and I give your friend the dataset on the right, and you both train the same architecture, you get the same performance. I think this is quite surprising. Because if you give the original dataset to me, or probably any human being, we will do well. But if you give us the one on the right, we won't be able to tell which is a cat or a dog. This is extremely weird. While there is nothing in principle "wrong" with this mathematically, philosophically we don't quite like this. It tells us that if you use a Fully Connected Neural Network for image recognition, it certainly doesn't work like the human brain. It ignores the spatial structure; it doesn't look at local patches to understand shapes or edges. It might just be classifying based on global pixel intensity—how green or yellow the image is.

    What about Time Series data? Here you are tracking the price of something across time, maybe the price of Silver. The top data shows the past month until four days ago, and below is the price from four days ago until now. There is a clear trend—one is a bull market, one is a bear market. But if we permute across time, you just get some seemingly random signals on the right side. We lose the information if you do this permutation. Fully Connected Neural Networks seem to ignore the spatial or temporal structure of your data.

    如果我们有图像呢？这里有一只猫和一只狗。假设你试图构建一个二元分类器，输入图像，它告诉你是什么动物。数据的每个维度将是一个像素坐标。所以，上一张幻灯片中每一列对应的东西现在就是一个像素。随机置换列或输入维度对应于随机置换像素的位置。如果你这样做，你会得到右边的数据集。

    全连接神经网络假说是怎么说的？它说如果我把左边的数据集给你，把你朋友右边的数据集给他，你们都训练相同的架构，你们会得到相同的性能。我认为这相当令人惊讶。因为如果你把原始数据集给我，或者任何人类，我们会做得很好。但是如果你给我们右边的那个，我们将无法分辨哪个是猫还是狗。这非常奇怪。虽然在数学原则上这没有什么“错”，但在哲学上我们不太喜欢这样。它告诉我们，如果你使用全连接神经网络进行图像识别，它的工作方式肯定不像人脑。它忽略了空间结构；它不看局部斑块来理解形状或边缘。它可能只是基于全局像素强度——图像有多绿或多黄——来进行分类。

    那时间序列数据呢？假设你在追踪某物随时间变化的价格，也许是白银的价格。上面的数据显示的是过去一个月直到四天前的情况，下面是四天前到现在的情况。有一个明显的趋势——一个是牛市，一个是熊市。但是如果我们跨时间进行置换，你在右边只会得到一些看似随机的信号。如果你做这种置换，我们就丢失了信息。全连接神经网络似乎忽略了数据的空间或时间结构。

  - [14:19 - 14:23] **Pop Quiz: The Fixed Weight Trap**

    Let me just have a quick poll just to test understanding. Here is the question: Suppose that we train a Fully Connected Neural Network with a certain architecture on your original data. After you train, you **fix the weights**. Then, your friend permutes the input data. The question is: how will your Fully Connected Neural Network behave on the permuted dataset? Would the output change on the permuted dataset or not? The key point is that we fixed the weights of the original model that you trained.

    (Pause for student responses)

    Okay, let's look at the responses. It's kind of 50/50. Recall the example in the slides. The reason we have permutation invariance is that if you swap the data, you would have to swap the edges (weights) accordingly. If the weights are fixed, then there is no reason why the output shouldn't change. So, the answer is actually "Exactly False." It is a false statement. It is a kind of trick question. What is the permutation invariance property here? It is a property of the **Hypothesis Space**, not a property of a specific, fixed model. It really means that you would need to **retrain** the model on the new permuted dataset, and *then* the performance should not change. The weights need to change to adapt to the permuted dataset. Since here we fix the weights, the correct answer is False.

    让我做一个快速投票来测试大家的理解。问题是：假设我们在原始数据上训练了一个具有特定架构的全连接神经网络。训练后，你**固定权重**。然后，你的朋友置换了输入数据。问题是：你的全连接神经网络在置换后的数据集上会如何表现？在置换后的数据集上输出会改变吗？关键点是我们固定了你训练的原始模型的权重。

    （等待学生回答）

    好的，让我们看看回答。大概是一半对一半。回想一下幻灯片中的例子。我们拥有置换不变性的原因是，如果你交换数据，你必须相应地交换边（权重）。如果权重是固定的，那么输出没有理由不改变。所以，答案实际上是“完全错误”（Exactly False）。这是一个错误的陈述。这有点像是个陷阱问题。这里的置换不变性属性是什么？它是**假设空间**的一个属性，而不是特定、固定模型的属性。它真正的意思是，你需要在这个新的置换数据集上**重新训练**模型，*然后*性能应该不会改变。权重需要改变以适应置换后的数据集。因为这里我们固定了权重，正确答案是“错”。

  - [14:23 - 14:28] **Keras Demo: Preprocessing Layers**

    Let us look at the demo on permutation invariance. We want to demonstrate what this funny permutation invariance thing is doing, and at the same time, introduce some more efficient ways of doing preprocessing. We are looking at the MNIST dataset again—the 10-class classification problem of handwritten digits. Last week, we did preprocessing manually: we divided every pixel by 255 to normalize, and we reshaped the 28x28 matrix into a vector. Doing this manually and storing different versions is not efficient. One way to make preprocessing more efficient is to incorporate it as part of your model layers.

    To do that, we will be using two new types of Keras layers: the `Lambda` layer and the `Flatten` layer. The `Lambda` layer is a very flexible layer where you just need to define the input and output. It is extremely customizable, but if the function is complex, it is not easy to use. For normalization ($x / 255$), it is a simple function, so we use `Lambda`. Note that we are using the Sequential API (or Keras API). This acts as the first layer given the raw input X. The next thing is reshaping the image into a long vector. There is a layer called `Flatten` which is the simplest one to use here. After that, we add Dense layers just like last week. We use the `compile` method to specify the loss function (Cross Entropy for 10 classes) and the optimizer (Adam). Everything else is the same. We use the `fit` method to train. Something new today is the `evaluate` method. It tells us the loss and accuracy on the train and test sets. On the train set, we are perfect; on the test set, 98%.

    让我们看看关于置换不变性的演示。我们要演示这个有趣的置换不变性是做什么的，同时，介绍一些更高效的预处理方法。我们再次查看 MNIST 数据集——手写数字的 10 类分类问题。上周，我们手动进行了预处理：我们将每个像素除以 255 进行归一化，并将 28x28 矩阵重塑为向量。手动执行此操作并存储不同版本并不高效。使预处理更高效的一种方法是将其作为模型层的一部分。

    为此，我们将使用两种新类型的 Keras 层：`Lambda` 层和 `Flatten` 层。`Lambda` 层是一个非常灵活的层，你只需要定义输入和输出。它是高度可定制的，但如果函数很复杂，就不太好用了。对于归一化（$x / 255$），这是一个简单的函数，所以我们使用 `Lambda`。注意我们使用的是 Sequential API（或 Keras API）。这是给定原始输入 X 后的第一层。接下来是将图像重塑为长向量。有一个叫做 `Flatten` 的层，这是这里最简单的用法。之后，我们要像上周一样添加 Dense 层。我们使用 `compile` 方法来指定损失函数（10 类分类用交叉熵）和优化器（Adam）。其他一切都是一样的。我们使用 `fit` 方法进行训练。今天的新内容是 `evaluate` 方法。它告诉我们训练集和测试集上的损失和准确率。在训练集上，我们是完美的；在测试集上，是 98%。

  - [14:28 - 14:34] **Destroying Performance with Permutation**

    Now let's plot two images: a '5' and a '0'. What do these look like if we randomly permute or shuffle the pixels? I painstakingly wrote my own function to get more control over this process. We view permutations as composing pairwise swaps. Any permutation of N objects can be decomposed as a composition of pairwise swaps. We do 5000 pairwise swaps here. Looking at the resulting images, I can't tell that this was a 5 and this was a 0. It is quite well shuffled.

    If we look at the performance of the model we trained just now *without* retraining, it is horrible. Previously it was 98%, now it is like 9%. A random guess would be 10%, so this is worse than random guessing. This is because, as explained in the poll, we did not retrain the model. What I want you to do in your own time is extend this demo: retrain the model on the permuted dataset and see what performance you get. There is code provided that keeps track of the indices swapped and swaps the weights of the weight matrix accordingly. If you mathematically swap the weights and re-evaluate, the output does not change—performance remains 98%.

    现在让我们绘制两张图像：一个“5”和一个“0”。如果我们随机置换或打乱像素，这些看起来像什么？我费劲地自己写了一个函数来更好地控制这个过程。我们将置换视为成对交换（pairwise swaps）的组合。任何 N 个对象的置换都可以分解为成对交换的组合。我们在这里做了 5000 次成对交换。看着结果图像，我看步出来这是 5 还是 0。它被打乱得很彻底。

    如果我们看看刚才训练的模型在*没有*重新训练的情况下的表现，那是极其糟糕的。之前是 98%，现在大概是 9%。随机猜测应该是 10%，所以这比随机猜测还差。这是因为，正如在投票中解释的那样，我们没有重新训练模型。我希望你们自己在课后做的是扩展这个演示：在置换后的数据集上重新训练模型，看看能得到什么性能。提供的代码可以跟踪交换的索引并相应地交换权重矩阵的权重。如果你在数学上交换权重并重新评估，输出不会改变——性能保持在 98%。

  - [14:35 - 14:39] **Introduction to Convolution: The Spaceship Example**

    Now that we see what is surprising about Fully Connected networks (ignoring spatial structure), let's look at Convolutional Neural Networks (CNNs). To do that, we need to define what a Convolution is. Let's consider a problem of tracking a spaceship flying in the sky using a laser sensor. You get data `x(t)`, which is the position of the spaceship at time `t`. One problem is that the signal from the sensor is very noisy due to vibrations or clouds. We want to devise a method to denoise the measurement. The claim is that convolutions are a good way to do that.

    Mathematically, we define the convolution between two functions `x` and `w` (the filter). Let's take an example where `w` acts as a weighted average. The function $w(t - a)$ puts maximum weight when $t = a$ (current time). If $a > t$ (future time), we give zero weight. If $a < t$ (historical time), we give some non-zero weight, decreasing as we go further back in time. This is effectively a weighted sum. Think of `x` as the sensor data and `w` as the filter you design to smoothen it. If your signal is noisy (orange line), doing this convolution with `w` produces an output (green line) that smoothens out the noise and is closer to the underlying truth.

    既然我们看到了全连接网络的奇怪之处（忽略空间结构），让我们来看看卷积神经网络（CNN）。为此，我们需要定义什么是卷积。让我们考虑一个用激光传感器追踪天空中飞行的飞船的问题。你得到数据 `x(t)`，即飞船在时间 `t` 的位置。一个问题是，由于震动或云层，来自传感器的信号非常嘈杂。我们想设计一种方法来对测量进行去噪。我的主张是，卷积是实现这一点的好方法。

    在数学上，我们定义两个函数 `x`和 `w`（滤波器）之间的卷积。让我们举一个例子，其中 `w` 充当加权平均值。函数 $w(t - a)$ 在 $t = a$（当前时间）时赋予最大权重。如果 $a > t$（未来时间），我们赋予零权重。如果 $a < t$（历史时间），我们赋予一些非零权重，随着时间推移权重递减。这实际上是一个加权和。把 `x` 看作传感器数据，把 `w` 看作你设计用来平滑它的滤波器。如果你的信号是嘈杂的（橙色线），用 `w` 做这个卷积会产生一个输出（绿色线），它平滑了噪声并且更接近潜在的真实值。

  - [14:39 - 14:41] **Symmetry and Feature Maps**

    The notion of convolution as a weighted average is just one use case. Mathematically, given any two real-valued functions, we can define a convolution. We know this is a symmetric operation, meaning $x * w$ is the same as $w * x$. Mathematically, it doesn't matter which one is the signal or which one is the filter. However, in Machine Learning, we tend to think of `x` as the data and `w` as the filter designed by us (or learned). It is useful to think of the convolution operation as trying to learn some useful features from the data `x`. We tend to think of the output of the convolution as a **Feature Map**.

    卷积作为加权平均值的概念只是一个用例。在数学上，给定任何两个实值函数，我们都可以定义卷积。我们知道这是一个对称操作，意味着 $x * w$ 与 $w * x$ 是一样的。在数学上，哪一个是信号哪一个是滤波器并不重要。然而，在机器学习中，我们倾向于认为 `x` 是数据，`w` 是我们设计（或学习）的滤波器。将卷积操作视为试图从数据 `x` 中学习一些有用的特征是很有用的。我们倾向于将卷积的输出称为**特征图**（Feature Map）。

  - [14:41 - 14:44] **Discrete 1D Convolution Calculation**

    In reality, our signals are not continuous; they are discrete vectors. We just replace the integral with a sum. Here is the straightforward definition of a 1D convolution. Suppose input `x` is a 5-dimensional vector `[1, 2, 1, 4, 1]` and our filter `w` is a 3-dimensional vector `[1, 4, 1]`. What does convolution mean here?

    It means I will take `w` and match it onto the first patch of length 3 of my data `x` (which is `[1, 2, 1]`). I take a dot product: $1*1 + 4*2 + 1*1 = 1 + 8 + 1 = 10$. (Correction from transcript: Lecturer said 9 initially but corrected calculation implies logic). *Correction: Let's follow the transcript calculation closely.* The transcript says: "Match onto first patch... dot product gives 9." Let's check: $1*1 + 2*4 + 1*1 = 1 + 8 + 1 = 10$. The lecturer might have misspoke "9", then later calculated correctly. Let's trace the next step. "One step to the right... match onto `[2, 1, 4]`... I get $1 + 8 + 1 = 10$." (Wait, transcript says "121 match onto 141"). Let's stick to the concept: You match the filter to the left-most part, take a dot product to get one output dimension. Shift one step right, take dot product, get second output. Repeat until the end. Note that the output size decreases compared to the input size. If you use a length 3 filter, output is length 2 less than input ($5 - 3 + 1 = 3$).

    在现实中，我们的信号不是连续的；它们是离散向量。我们只需用求和代替积分。这是 1D 卷积的直观定义。假设输入 `x` 是一个 5 维向量 `[1, 2, 1, 4, 1]`，我们的滤波器 `w` 是一个 3 维向量 `[1, 4, 1]`。这里的卷积意味着什么？

    这意味着我将把 `w` 匹配到数据 `x` 的第一个长度为 3 的片段上（即 `[1, 2, 1]`）。我进行点积运算。然后向右移动一步，再次进行点积。重复此过程直到结束。请注意，与输入大小相比，输出大小减小了。如果你使用长度为 3 的滤波器，输出长度比输入少 2。

  - [14:44 - 14:46] **Padding Strategies**

    For certain applications, like text-to-speech transcription or image segmentation, you want the input and output size to be the same. To achieve this, we change what happens at the boundaries using **Padding**. One method is **Circular Convolution**: we pad the ends of `x` wrapping around. On the left, pad with the last coordinate of `x`; on the right, pad with the first coordinate. It becomes like a circle. Another very common and simple method is **Zero Padding**: just padding the boundaries with zeros. This ensures the output magnitude (dimensions) doesn't change.

    对于某些应用，如文本到语音转录或图像分割，你希望输入和输出大小相同。为了实现这一点，我们使用**填充**（Padding）来改变边界处的处理方式。一种方法是**循环卷积**（Circular Convolution）：我们把 `x` 的两端包裹起来填充。在左边，用 `x` 的最后一个坐标填充；在右边，用第一个坐标填充。它变得像一个圆。另一种非常常见且简单的方法是**零填充**（Zero Padding）：只是用零填充边界。这确保了输出的幅度（维度）不会改变。

  - [14:47 - 14:51] **2D Convolution on Images**

    Now, applied to images (2D matrices). Input is an image `I`, and our filter (also called a **Kernel**) is a smaller matrix, typically 3x3, 5x5, etc. The definition involves summing over two indices. Suppose we have a 3x4 image and a 2x2 kernel. I match the kernel onto the top-left corner patch of the image. I take the element-wise product and sum them up. That gives me one real number (one pixel of the output). Then I slide one unit down, or one unit right. You repeat this process top-to-bottom, left-to-right. In this case, with a 3x4 input and 2x2 kernel, the output is 2x3. The length reduces by 1 in each dimension.

    现在，应用于图像（2D 矩阵）。输入是图像 `I`，我们的滤波器（也称为**核 Kernel**）是一个较小的矩阵，通常是 3x3、5x5 等。定义涉及对两个索引求和。假设我们有一个 3x4 的图像和一个 2x2 的核。我将核匹配到图像的左上角片段。我进行逐元素乘积并将它们相加。这给了我一个实数（输出的一个像素）。然后我向下滑动一个单位，或向右滑动一个单位。你从上到下、从左到右重复这个过程。在这种情况下，输入为 3x4，核为 2x2，输出为 2x3。每个维度的长度减少了 1。

  - [14:51 - 14:53] **Kernel Sizes: Why Odd Numbers?**

    In practice, we tend to use odd-length filters (3x3, 5x5, 7x7). Nobody really uses 2x2 or 4x4. Why? Because if you have an odd length, there is a unique center pixel. Each time you match a patch, there is a center. Also, filters are trainable parameters. We like to give them the possibility to be symmetric features centered around a point. Odd lengths allow for that geometric property more naturally.

    在实践中，我们倾向于使用奇数长度的滤波器（3x3, 5x5, 7x7）。没人真的用 2x2 或 4x4。为什么？因为如果你有奇数长度，就会有一个唯一的中心像素。每次你匹配一个片段，都有一个中心。此外，滤波器是可训练的参数。我们喜欢赋予它们围绕某一点成为对称特征的可能性。奇数长度更自然地允许这种几何属性。

  - [14:53 - 14:56] **The "Lie": Convolution vs. Cross-Correlation**

    I have been making a small lie to you so far. We have been saying "Convolution" everywhere. In math, convolution involves **flipping** (mirroring) the kernel in both directions before applying it. However, what I have been showing you in the visualizations is actually **Cross-Correlation**. In Machine Learning libraries (like TensorFlow/PyTorch), we always implement Cross-Correlation, but we call it Convolution.

    Why does this sloppiness not affect us? Because the kernels `K` are **learned** by the model. If we really wanted the theoretical effect of convolution (with the flip), the model can simply learn the "flipped" version of the kernel to achieve the same result. It doesn't reduce expressivity. In traditional computer vision, where kernels were designed by hand, this distinction mattered significantly. But in Deep Learning, since the model learns the weights, we implement Cross-Correlation and just call it Convolution.

    到目前为止，我其实撒了一个小谎。我们到处都在说“卷积”。在数学中，卷积涉及在应用之前在两个方向上**翻转**（镜像）核。然而，我在可视化中向你们展示的实际上是**互相关**（Cross-Correlation）。在机器学习库（如 TensorFlow/PyTorch）中，我们总是实现互相关，但我们称之为卷积。

    为什么这种不严谨不会影响我们？因为核 `K` 是由模型**学习**的。如果我们真的想要卷积的理论效果（带有翻转），模型可以简单地学习核的“翻转”版本来达到相同的结果。这不会降低表达能力。在传统的计算机视觉中，核是手工设计的，这种区别非常重要。但在深度学习中，由于模型学习权重，我们实现互相关并将其简称为卷积。

  - **Break and Attendance**

    Now we know what a convolution means for vectors (1D) and matrices (2D). We will take a 10-minute break now, and then talk about Tensor Convolution. If you are funded by SSG (SkillsFuture Singapore), remember to take attendance.

    现在我们知道了卷积对于向量（1D）和矩阵（2D）意味着什么。我们要休息 10 分钟，然后讨论张量卷积。如果你是由 SSG（新加坡技能创前程）资助的，记得进行考勤。

  - [15:08 - 15:14] **Tensor Convolutions: Handling Color Images**

    So, what if the image is a color image? Then you have multiple channels, like an RGB image. We have three channels to tell you how red, how green, and how blue it is. It is like three images being stacked onto each other. This is what we call a **Tensor**. There are three dimensions now. The question is: how do we define a convolution on a tensor? The answer is that in this case, the filters (kernels) will also be a tensor.

    Basically, the language we use is that we only specify the width and height of the filters. For example, normally we just call these "3x3 filters." However, the **depth**—the number of channels of each filter—will automatically be the same as the depth (or number of channels) of the input image. Here, the input image is $6 \times 6 \times 3$, so there are three channels: red, blue, and green. Therefore, the depth of the filter is 3.

    Let's first understand what a single filter is doing. Ignore the bottom part for a moment; let's just look at the top part. What do we mean by "the filter convolved with our input image"? Note that the depth is always going to be the same. The output will be a **single image** (a matrix). Given one filter and one input tensor, the output is a matrix.

    How do we define this operation? Basically, your filter will also have the corresponding red, blue, and green channels. Each channel of the filter will perform a 2D convolution with the corresponding channel of the input. The green channel of the filter goes through a 2D convolution with the green channel of the image; that gives us an output matrix. The blue one does the same with the blue channel, and the red one with the red channel. This gives you a bunch of matrices (feature maps). Then, you **sum up** these three matrices to give us a single output matrix. This is how we get this yellow image in the diagram—it is that sum.

    Now, if we have **multiple filters**, we repeat this. Note that all filters will have the same size (e.g., 3x3). Each filter gives us a separate output image (matrix). If I have two filters, then I am just going to stack the two output images together. The final output tensor is formed by stacking the outputs from each filter. Consequently, the number of channels in the output image is equal to the number of filters that you use.

    Another way to visualize this is that now your filters are like blocks—some cuboids. Basically, I am matching a cuboid (the filter) to a corresponding cuboid patch in the input. If the filter is $3 \times 3 \times 3$, I match it with a $3 \times 3 \times 3$ patch of the image. I take the dot product of these 27 numbers, and I get one output number. This is an equivalent definition.

    如果图像是彩色图像怎么办？那样你就有多个通道，比如 RGB 图像。我们有三个通道分别告诉你它有多红、多绿和多蓝。这就像三张图像堆叠在一起。这就是我们所说的**张量**（Tensor）。现在有三个维度了。问题是：我们如何在张量上定义卷积？答案是，在这种情况下，滤波器（核）也将是一个张量。

    基本上，我们的术语是，我们只指定滤波器的宽度和高度。例如，通常我们就叫这些“3x3 滤波器”。但是，**深度**——即每个滤波器的通道数——将自动与输入图像的深度（或通道数）相同。这里，输入图像是 $6 \times 6 \times 3$，所以有三个通道：红、蓝、绿。因此，滤波器的深度也是 3。

    让我们先理解单个滤波器在做什么。先忽略底部部分；我们只看顶部。我们说的“滤波器与输入图像卷积”是什么意思？注意深度总是相同的。输出将是一张**单张图像**（一个矩阵）。给定一个滤波器和一个输入张量，输出是一个矩阵。

    我们如何定义这个操作？基本上，你的滤波器也将具有对应的红、蓝、绿通道。滤波器的每个通道将与输入的对应通道进行 2D 卷积。滤波器的绿色通道与图像的绿色通道进行 2D 卷积，这给我们一个输出矩阵。蓝色通道与蓝色通道做同样的事，红色与红色也是。这会给你一系列矩阵（特征图）。然后，你将这三个矩阵**相加**，从而给我们一个单一的输出矩阵。这就是我们在图中得到那个黄色图像的方式——它是那个总和。

    现在，如果我们有**多个滤波器**，我们重复这个过程。注意所有滤波器的大小都是一样的（比如 3x3）。每个滤波器给我们一个单独的输出图像（矩阵）。如果我有两个滤波器，我就把这两个输出图像堆叠在一起。最终的输出张量是通过堆叠每个滤波器的输出来形成的。因此，输出图像中的通道数等于你使用的滤波器数量。

    另一种可视化方法是，现在的滤波器就像块——一些长方体。基本上，我将一个长方体（滤波器）与输入中对应的长方体块进行匹配。如果滤波器是 $3 \times 3 \times 3$，我就把它与图像的 $3 \times 3 \times 3$ 块匹配。我对这 27 个数字进行点积，得到一个输出数字。这是等价的定义。

  - [15:14 - 15:18] **Quiz: Calculating Trainable Parameters**

    If not, I'll ask you some questions to test your understanding. Here we have 3 input channels, and the shape of our filters is width and height 5x5. The goal is to get 8 output channels. So, how many **trainable parameters** do you need to achieve this using a convolution (ignoring bias)? In other words, how many filters do I need and what is the size of each filter?

    (Pause for student responses)

    Okay, let's look at the responses. The majority thinks it is $3 \times 5 \times 5 \times 8$. Yeah, that is the correct answer. Very good. Because, well, in order to get 8 output channels, that means we need 8 filters, right? So that is where the "times 8" comes from. And now, every filter has width and height 5x5. The depth is equal to 3 because the input image has depth 3 (three channels). So, yeah: $3 \times 5 \times 5 \times 8$.

    The next one is an open question. Maybe in less than 6 words you can try to describe what a 1x1 convolution is on a tensor input. I can use up to 5 words maybe.

    (Student response: "Identity operation")

    "Identity operation"... well, if I'm using just one filter... it's a 1x1 convolution, but the input is a tensor with multiple channels...

    如果没有问题，我问大家一些问题来测试理解程度。这里我们有 3 个输入通道，我们滤波器的形状是宽和高为 5x5。目标是得到 8 个输出通道。那么，忽略偏置（bias），你需要多少个**可训练参数**来实现这一点？换句话说，我需要多少个滤波器，每个滤波器的大小是多少？

    （等待学生回答）

    好的，让我们看看回答。大多数人认为是 $3 \times 5 \times 5 \times 8$。是的，这是正确答案。非常好。因为，为了得到 8 个输出通道，这意味着我们需要 8 个滤波器，对吧？所以这就是“乘以 8”的由来。而且，每个滤波器的宽度和高度都是 5x5。深度等于 3，因为输入图像的深度是 3（三个通道）。所以，是 $3 \times 5 \times 5 \times 8$。

    下一个是一个开放式问题。也许你可以尝试用少于 6 个字来描述张量输入上的 1x1 卷积是什么。

    （学生回答：“恒等操作”）

    “恒等操作”……嗯，如果我只使用一个滤波器……这是一个 1x1 卷积，但输入是一个具有多个通道的张量……

  - [15:18 - 15:23] **[Break in Recording / Off-Topic Discussion]**

    *(There is a gap in the source transcript between 15:18 and 15:23. The discussion likely continued on the topic of 1x1 convolutions or student questions before returning to the comparison with Dense layers.)*

    *（源文本在 15:18 到 15:23 之间存在空白。讨论可能继续围绕 1x1 卷积或学生提问进行，然后回到了与密集层的比较。）*

  - [15:23 - 15:24] **Revisiting Permutation Invariance**

    ...of the Dense layer (Fully Connected Network). The reason why it has permutation invariance is because if you swap a pair of inputs, you just swap the corresponding rows of the weight matrix, and the output doesn't change. That is correct. That's because you allow any general linear operation, so swapping two rows is still valid.

    But if you restrict yourself to convolution operations, and we swap row 1 and row 3 of the input... then swapping row 1 and row 3 of the matrix is no longer a convolution. It no longer satisfies this "constant diagonals" criteria (Toeplitz structure) required for convolution. Therefore, actually, convolutions **do not** have this permutation invariance property of the hypothesis space. That is something you can test in the demo at home.

    ……关于密集层（全连接网络）。它具有置换不变性的原因是因为如果你交换一对输入，你只需交换权重矩阵的对应行，输出不会改变。这是正确的。那是因为你允许任何一般的线性操作，所以交换两行仍然是有效的。

    但是，如果你将自己限制在卷积操作中，并且我们交换输入的第 1 行和第 3 行……那么交换矩阵的第 1 行和第 3 行就不再是一个卷积了。它不再满足卷积所需的这种“常数对角线”标准（Toeplitz 结构）。因此，实际上，卷积假设空间**不**具备这种置换不变性属性。这是你们可以在家里的演示中测试的东西。

  - [15:24 - 15:31] **Motivation 0: Effective Feature Extractors**

    Right, so now let's talk about some major motivations as to why we would care about convolutions.

    **Motivation 0** is that convolutions can be effective **Feature Extractors**. Consider the input image of this Golden Retriever. Let's consider a specific filter. For simplicity, let's say I hand-designed this filter, and it applies the same matrix for every channel. What is this? The bright spot is a large value, and the dark spot is a small value. This is like a Gaussian shape, right? With the highest value in the center and smallest value away from the center. There will also be a bias on it.

    If we apply this type of kernel to the image, what do we get? I want to say that this kernel is basically the 2D version of the one that we saw with tracking the spaceship—it is a decaying exponent. So you expect a similar type of **smoothing behavior**, right? And you are not wrong. If you apply this filter to the image, it smoothens out the image or it blurs the image somewhat. It is denoising in the sense that if you look at some of the grass patch, maybe you think the ISO is too high and there is background noise; the effect on that is actually desirable. It smoothens it up. However, you don't want smoothing everywhere. Especially on the outline of the dog—now you smoothen it, and it becomes very blurry. In that sense, it is not desirable.

    The question is: is this all that convolutions can do? Here is a different type of feature (filter). It is the opposite extreme. Let's understand it as: on the right of this matrix I have $+1$, on the left I have $-1$. If we use this convolution on the image, what do you expect the output to look like? Let's try to visualize this. Here we get like an **outline** of the dog, but we lose out almost all the other information, like the color. Technically, this is a **Vertical Edge Detector**. Why is this the case? If we take this filter $W_2$ and match it on a patch where there is no vertical edge—meaning the pixel values on the left and right are the same—then what happens? On the left, we have $-1 \times \text{pixel}$; on the right, we have $+1 \times \text{pixel}$. We take the dot product, it will cancel out and give you 0. The only time you get a non-zero output is where the pixel values on the left and right are different. That is precisely when you have a vertical edge.

    So you see that the kernels can learn vastly different things. They are actually very difficult to design by hand, so it is a good idea that the neural networks learn them for us. They are expressive and can learn natural things like edges at different angles. If you have a much bigger kernel like $7 \times 7$ or $9 \times 9$, you can imagine that you can start to learn more sophisticated curves or textures.

    好了，现在让我们谈谈我们为什么要关注卷积的一些主要动机。

    **动机 0** 是卷积可以是有效的**特征提取器**。考虑这只金毛猎犬的输入图像。让我们考虑一个特定的滤波器。为了简单起见，假设我是手工设计这个滤波器的，并且它对每个通道应用相同的矩阵。这是什么？亮点是大值，暗点是小值。这就像一个高斯形状，对吧？中心值最高，远离中心值最小。上面也会有一个偏置（bias）。

    如果我们对图像应用这种类型的核，我们会得到什么？我想说这个核基本上我们在追踪飞船时看到的那个核的 2D 版本——它是一个衰减指数。所以你预期会有类似类型的**平滑行为**，对吧？你没得错。如果你将此滤波器应用于图像，它会平滑图像或使图像变得模糊。这是一种去噪，从某种意义上说，如果你看草地部分，也许你觉得 ISO 太高了，有背景噪音；对此的效果实际上是理想的。它将其平滑了。然而，你并不希望到处都平滑。特别是在狗的轮廓上——现在你把它平滑了，它变得非常模糊。在这个意义上，这就不理想了。

    问题是：这就是卷积能做的一切吗？这里有一种不同类型的特征（滤波器）。这是相反的极端。让我们把它理解为：在这个矩阵的右边我有 $+1$，在左边我有 $-1$。如果我们在图像上使用这个卷积，你预期输出看起来像什么？让我们试着可视化一下。在这里，我们得到了狗的**轮廓**，但我们几乎丢失了所有其他信息，比如颜色。从技术上讲，这是一个**垂直边缘检测器**。为什么会这样？如果我们取这个滤波器 $W_2$ 并将其匹配到没有垂直边缘的块上——意味着左边和右边的像素值相同——那么会发生什么？在左边，我们有 $-1 \times \text{像素}$；在右边，我们有 $+1 \times \text{像素}$。我们取点积，它会抵消并给你 0。只有当左边和右边的像素值不同时，你才会得到非零输出。这正是你有垂直边缘的时候。

    所以你可以看到，核可以学习截然不同的东西。它们实际上很难手工设计，所以让神经网络为我们学习它们是个好主意。它们具有表达力，可以学习像不同角度的边缘这样的自然事物。如果你有一个更大的核，比如 $7 \times 7$ 或 $9 \times 9$，你可以想象你可以开始学习更复杂的曲线或纹理。

  - [15:32 - 15:38] **Motivation 1: Sparse Interactions and Parameter Sharing**

    **Motivation 1: Sparse Interactions.** As we said, Dense layers and Convolution layers are both linear operations; it is just that convolution is a restricted type. In particular, there are a lot more zeros in the matrix representation of a convolution. That means there are a lot fewer multiplications of real numbers that you need to do, so it is a lot cheaper to compute.

    If we look at the computational graph comparison, the cost for a convolution layer is $\text{Input Size} \times \text{Kernel Size}$. For a Dense layer, it is $\text{Input Size} \times \text{Output Size}$ (or size squared if input equals output), which is normally a lot more. So it is faster computation. Also, the interactions are very local. In the graph, a node only influences a few nodes in the next layer. Conversely, an output node only depends on a small number of input nodes—this is called the **Receptive Field**. If the kernel is 3x3, each output pixel depends on only 9 input pixels. You might worry: "It seems the output is not making use of the entire input." However, this is not a concern because CNNs are always deep. After stacking many layers, the final layer's output will implicitly be connected to the entire input image.

    **Parameter Sharing (Tied Weights):** Another thing to do with the special structure of the matrix is that it is not just sparse; there are also only a small number of degrees of freedom because the diagonal entries should be equal. This is called "Tied Weights." In the computational graph, all the edges pointing in the same direction share the same weight (e.g., all edges pointing bottom-left to top-right are $W_2$). In a Dense layer, every edge is independent. So, convolution gives us **Memory Efficiency**. The number of trainable parameters is just the size of the kernel, which is independent of the input size. If you had no parameter sharing, you would need a separate kernel for every pixel.

    **动机 1：稀疏交互（Sparse Interactions）。** 正如我们所说，密集层和卷积层都是线性操作；只是卷积是一种受限类型。特别是，卷积的矩阵表示中有很多零。这意味着你需要做的实数乘法要少得多，因此计算起来要便宜得多。

    如果我们看计算图的比较，卷积层的成本是 $\text{输入大小} \times \text{核大小}$。对于密集层，它是 $\text{输入大小} \times \text{输出大小}$（如果输入等于输出，则是大小的平方），这通常要多得多。所以计算速度更快。此外，交互是非常局部的。在图中，一个节点只影响下一层中的几个节点。反过来说，一个输出节点只取决于少数几个输入节点——这被称为**感受野**（Receptive Field）。如果核是 3x3，每个输出像素只取决于 9 个输入像素。你可能会担心：“看起来输出没有利用整个输入。”然而，这不用担心，因为 CNN 总是很深的。堆叠许多层后，最终层的输出将隐含地连接到整个输入图像。

    **参数共享（权重绑定 Tied Weights）：** 关于矩阵特殊结构的另一件事是，它不仅是稀疏的；自由度的数量也很少，因为对角线条目应该相等。这被称为“权重绑定”。在计算图中，所有指向相同方向的边共享相同的权重（例如，所有从左下指向右上的边都是 $W_2$）。在密集层中，每条边都是独立的。所以，卷积给了我们**内存效率**。可训练参数的数量仅仅是核的大小，这与输入大小无关。如果没有参数共享，你需要为每个像素配备一个单独的核。

  - [15:38 - 15:52] **Motivation 2: Equivariance and Invariance**

    Now we move on to the last and perhaps most important motivation: **Equivariance and Invariance of functions**. This takes a little bit of math definitions.

    - **Equivariance:** We say function $f$ is equivariant with respect to transformation $g$ if $f(g(x)) = g(f(x))$. The order doesn't matter.
    - **Invariance:** We say function $f$ is invariant to $g$ if $f(g(x)) = f(x)$. The output doesn't change even if I transform the input.

    Examples:

    - **Scaling:** Let $g(x) = \lambda x$. Let $f(x) = \text{ReLU}(x)$. It is clear $f$ and $g$ are equivariant. Whether I scale first or apply ReLU first doesn't matter (for positive $\lambda$).
    - **Rotation:** Let $g(x)$ be a rotation matrix. Let $f(x) = ||x||^2$ (length of vector). $f$ is invariant to $g$ because length doesn't change with rotation.
    - **Translation:** Let $g(x)$ be a translation (shifting every pixel by a constant amount). Consider the function $f(x) = \text{sum}(x)$. Summing all entries is invariant to translation.

    We will look at components of a CNN with respect to **Translation**.

    It is not hard to show that the **Convolution operation is equivariant to translation**. If I translate the image first and then convolve, it gives the same output as if I convolve first and then translate. (Strictly speaking, this holds for circular convolution or infinite signals; for zero-padding, it fails at the boundaries, but for conceptual purposes, we assume it holds).

    Also, the **Activation function** (like ReLU) is element-wise, so it is clearly equivariant to translation.

    Here is a mathematical fact: If $f_1$ and $f_2$ are both equivariant to $g$, then their composition $f_1 \circ f_2$ is also equivariant.

    This means a sequence of Convolutions and Activations is equivariant to translation.

    Finally, suppose at the end of the model we have a function $F$ (like Global Average Pooling or a sum) that is **Invariant** to translation.

    **Claim:** An Invariant function composed with an Equivariant function is Invariant.

    Therefore, the entire model structure (Conv layers + Global Pooling) gives us a good chance to be **Translation Invariant**.

    现在我们要谈谈最后一个也许也是最重要的动机：**函数的等变性（Equivariance）和不变性（Invariance）**。这需要一点数学定义。

    - **等变性：** 如果 $f(g(x)) = g(f(x))$，我们说函数 $f$ 关于变换 $g$ 是等变的。顺序不重要。
    - **不变性：** 如果 $f(g(x)) = f(x)$，我们说函数 $f$ 对 $g$ 是不变的。即使我变换输入，输出也不会改变。

    例子：

    - **缩放：** 设 $g(x) = \lambda x$。设 $f(x) = \text{ReLU}(x)$。很明显 $f$ 和 $g$ 是等变的。无论我是先缩放还是先应用 ReLU 都不重要（对于正数 $\lambda$）。
    - **旋转：** 设 $g(x)$ 是旋转矩阵。设 $f(x) = ||x||^2$（向量长度）。$f$ 对 $g$ 是不变的，因为长度不随旋转改变。
    - **平移：** 设 $g(x)$ 是平移（将每个像素移动一个常数量）。考虑函数 $f(x) = \text{sum}(x)$。求和所有条目对平移是不变的。

    我们将查看 CNN 组件关于**平移**的性质。

    不难证明**卷积操作对平移是等变的**。如果我先平移图像然后卷积，其输出与我先卷积然后平移是一样的。（严格来说，这适用于循环卷积或无限信号；对于零填充，它在边界处不成立，但在概念上，我们假设它成立）。

    此外，**激活函数**（如 ReLU）是逐元素的，所以它显然对平移是等变的。

    这里有一个数学事实：如果 $f_1$ 和 $f_2$ 都对 $g$ 等变，那么它们的组合 $f_1 \circ f_2$ 也是等变的。

    这意味着卷积和激活的序列对平移是等变的。

    最后，假设在模型末端我们有一个函数 $F$（如全局平均池化或求和），它对平移是**不变的**。

    **主张：** 一个不变函数与一个等变函数的组合是不变的。

    因此，整个模型结构（卷积层 + 全局池化）让我们很有机会实现**平移不变性**。

  - [15:52 - 15:58] **Priors and Limitations**

    Why do we like Translation Invariance? Consider the "Oracle" function $f^*$—the Truth. It takes an image and tells you if it is a cat or a dog. If we translate the dog a little bit, it is still a dog. So $f^*$ is translation invariant. Therefore, it is a good idea to impose this **Prior** in our model.

    Let $H$ be the hypothesis space of Fully Connected Networks. Let $H'$ be the CNN hypothesis space (a subset of $H$ restricted to convolutions). $H'$ represents the set of functions that satisfy this translation invariance property. Since our target function $f^*$ lies in this set, restricting our search to $H'$ (imposing the prior) allows us to get a better approximation given the same computational resources.

    However, CNNs are not perfect. They are only approximately invariant to small translations. Also, they are **not** naturally invariant to other transformations like Rotation or Scaling.

    Good news: There are sophisticated models like "Steerable CNNs" for general invariances, but they are inefficient to train.

    In practice, we use a training trick called **Data Augmentation** to handle rotation and scaling (we will discuss this in a few lectures).

    There is still one more ingredient of a typical CNN, and I will tell you what this is after the break. Let's take a 10-minute break now.

    为什么我们喜欢平移不变性？考虑“神谕”函数 $f^*$——即真理。它接收图像并告诉你它是猫还是狗。如果我们稍微平移狗，它仍然是一只狗。所以 $f^*$ 是平移不变的。因此，在我们的模型中施加这种**先验（Prior）**是一个好主意。

    设 $H$ 为全连接网络的假设空间。设 $H'$ 为 CNN 假设空间（$H$ 的一个子集，限制为卷积）。$H'$ 代表满足这种平移不变性属性的函数集。由于我们的目标函数 $f^*$ 位于这个集合中，将我们的搜索限制在 $H'$（施加先验）允许我们在给定相同计算资源的情况下获得更好的近似。

    然而，CNN 并不完美。它们只是对小幅平移近似不变。此外，它们天生对旋转或缩放等其他变换**不**具备不变性。

    好消息：存在像“可操纵 CNN”（Steerable CNNs）这样针对一般不变性的复杂模型，但它们训练效率低下。

    在实践中，我们使用一种称为**数据增强**（Data Augmentation）的训练技巧来处理旋转和缩放（我们将在几节课后讨论这个问题）。

    典型的 CNN 还有一个组成部分，休息后我会告诉大家这是什么。现在我们休息 10 分钟。

  - [16:10 - 16:12] **Max Pooling: Definition and Mechanics**

    Let me just explain Max Pooling. What is this? I think it is much easier to look at the visualizations. In the visualization, we have Max Pooling with stride length 2. What that means is that if it is a vector input, I will look at a patch of length 2. For example, if I have `[3, 1]`, I take the maximum, so I get 3. Then I move two steps to the right (because stride is 2). I look at `[4, 1]`, take the maximum, I get 4. I move another 2 steps, look at `[5, 9]`, I get 9. So the length of the output will be half if the stride is 2 compared to the input.

    If it is an image, then I look at a 2x2 patch. Suppose the values are `3, 1, 5, 9`. I take the biggest one, I get 9. Then I move two steps to the right for the next 2x2 patch. Suppose it is `4, 1, 2, 6`. I take the biggest unit, so I have 6, and so on. If it is an image and I apply Max Pooling with stride 2, then the width and the height of the image will be halved. Note that this operation has **no trainable parameters**, unlike convolution where you have kernels that need to be trained. This is really just some form of downsampling operation.

    让我解释一下最大池化（Max Pooling）。这是什么？我觉得看可视化图会容易得多。在可视化中，我们有步幅（stride）长度为 2 的最大池化。这意味着如果是向量输入，我会看一个长度为 2 的片段。例如，如果我有 `[3, 1]`，我取最大值，所以我得到 3。然后我向右移动两步（因为步幅是 2）。我看 `[4, 1]`，取最大值，得到 4。再向右移动两步，看 `[5, 9]`，得到 9。所以如果步幅是 2，输出的长度将是输入的一半。

    如果是图像，我就看一个 2x2 的块。假设数值是 `3, 1, 5, 9`。我取最大的一个，得到 9。然后我向右移动两步看下一个 2x2 的块。假设是 `4, 1, 2, 6`。我取最大的单元，所以我得到 6，依此类推。如果是图像，我应用步幅为 2 的最大池化，那么图像的宽度和高度将会减半。请注意，这个操作**没有可训练的参数**，不像卷积那样有需要训练的核。这实际上只是一种形式的下采样操作。

  - [16:12 - 16:16] **Why Max Pooling? Invariance and "Detection"**

    You know that this will result in local invariance. Any deformations—in fact, any permutation, and in particular any **translation**—within that small patch will result in no change to the maximum value. So this will build in approximate translation invariance into the model.

    That is one way to motivate this. Another way to motivate this is an analogy—I don't know if this is from a textbook, but let me explain. It tries to explain why taking the Max across a patch of pixels is desirable. The analogy is: if you are trying to detect the image of a digit '5', maybe I have a database of many types of '5's. Given my input image, I compare it with the database and get some outputs. I just take the maximum output because I just want to know: **is there a '5' in the image or not?**

    In reality, what is happening is that the Max is taken across pixels after the convolution. The convolution layer's kernel is trying to learn a feature. After you apply convolution, each output pixel is telling you whether that feature is occurring on a specific patch centered on a certain input pixel. What Max Pooling is trying to say is: "I don't actually care exactly where within this 2x2 or 3x3 patch this feature occurs. I just want to know, **does it appear here?**" Taking the Max answers that question.

    Typically, this combination—Convolution, followed by Activation Function (like ReLU), followed by Pooling—is called a **Convolutional Layer** or a "Detector Stage" in the literature. Sometimes it is called a Convolutional Block. You repeat many of these blocks.

    你知道这会导致局部不变性。任何变形——实际上是任何置换，特别是该小块内的任何**平滑**——都不会导致最大值发生变化。所以这会在模型中建立近似的平滑不变性。

    这是激发这种做法的一种方式。另一种方式是一个类比——我不知道这是不是教科书里的，但我解释一下。它试图解释为什么要跨像素块取最大值。类比是：如果你试图检测数字“5”的图像，也许我有一个包含多种“5”的数据库。给定输入图像，我将其与数据库进行比较并得到一些输出。我只取最大输出，因为我只想知道：**图像中是否有“5”？**

    在现实中，发生的事情是最大值是在卷积之后跨像素获取的。卷积层的核试图学习特征。应用卷积后，每个输出像素都告诉你该特征是否出现在以某个输入像素为中心的特定块上。最大池化试图表达的是：“我实际上并不关心这个特征在这个 2x2 或 3x3 块内的确切位置。我只想知道，**它是否出现在这里？**”取最大值回答了这个问题。

    通常，这种组合——卷积，后跟激活函数（如 ReLU），后跟池化——在文献中被称为**卷积层**或“检测阶段”。有时它被称为卷积块。你会重复许多这样的块。

  - [16:16 - 16:18] **Typical Architecture: Shrinking W/H, Increasing Channels**

    This is the typical architecture of a Convolutional Neural Network (CNN). You have Convolution, followed by Pooling, followed by another Convolution, followed by another Pooling. This happens many times. At the end, we Flatten it to a vector, and then we have a Fully Connected Neural Network (Dense Layers).

    One obvious thing is that because of the Max Pooling, the image sizes—the width and height—will always decrease. The depth (the number of channels) is something the user chooses, and we normally choose it to be more and more. We get **thicker and thicker images** (more channels) but smaller spatial dimensions.

    Recall that the computational complexity depends on the image size (Width x Height). Max Pooling keeps the computational cost in check. If we didn't reduce the size, the cost would explode because we are also increasing the number of channels. So, using Max Pooling is a trade-off, a balancing act to control computational costs while allowing us to increase the number of channels.

    这是卷积神经网络（CNN）的典型架构。你有卷积，后跟池化，再跟另一个卷积，再跟另一个池化。这会重复多次。最后，我们将其展平（Flatten）为一个向量，然后连接一个全连接神经网络（密集层）。

    显而易见的一点是，由于最大池化，图像尺寸——宽度和高度——总是会减小。深度（通道数）是用户选择的，我们通常选择越来越多的通道。我们得到**越来越厚**（更多通道）但空间尺寸更小的图像。

    回想一下，计算复杂度取决于图像大小（宽 x 高）。最大池化控制了计算成本。如果我们不减小尺寸，成本就会爆炸，因为我们还在增加通道数量。所以，使用最大池化是一种权衡，一种控制计算成本的平衡行为，同时允许我们增加通道数量。

  - [16:20 - 16:24] **The Logic of Feature Hierarchy: Edges to Objects**

    Why do we structure it like this? Research shows this is how CNNs actually learn. On the raw input image, you learn **Low Level Features** such as edges and lines at different angles. This is not surprising. Maybe I only need 10 or 20 channels because there are only so many interesting edges (vertical, horizontal, diagonal).

    In the next layer, the model will combine this information. If a vertical edge and a horizontal edge occur simultaneously on a patch, it is likely there is a **Shape**, like a rectangle. So the next layer learns shapes. If I have 10-20 interesting edges, I can probably compose hundreds of interesting shapes. I need a separate channel for each interesting shape, which is why the next layer needs **more channels**.

    The layer after that will combine interesting shapes to learn about **Interesting Objects**. If I have hundreds of shapes, I probably have thousands of objects (cars, chairs, people) composed from these shapes. As I go deeper, I need even more channels. That is the intuition.

    为什么我们要这样构建？研究表明这正是 CNN 实际学习的方式。在原始输入图像上，你学习**低级特征**，如不同角度的边缘和线条。这并不奇怪。也许我只需要 10 或 20 个通道，因为有趣的边缘（垂直、水平、对角线）只有那么多。

    在下一层，模型将结合这些信息。如果垂直边缘和水平边缘同时出现在一个块上，那么很可能有一个**形状**，比如矩形。所以下一层学习形状。如果我有 10-20 个有趣的边缘，我大概可以组合出数百种有趣的形状。我需要为每种有趣的形状配备一个单独的通道，这就是为什么下一层需要**更多通道**。

    之后的一层将结合有趣的形状来学习**有趣的对象**。如果我有数百种形状，我可能有数千种由这些形状组成的对象（汽车、椅子、人）。随着我深入，我需要更多的通道。这就是直觉。

  - [16:24 - 16:28] **Resolution vs. Abstraction Trade-off**

    Why is it okay for the width and height to shrink? In the deeper layers, we are trying to tell if there is a **High Level Feature**, for example: "Is there a car in this part of the image?" The belief is that high-level features like cars or tables normally do not need too fine a resolution to be detected.

    Does it make sense? If I give you a low-resolution image, it is easier to detect whether there is a car compared to detecting whether there is a vertical or horizontal edge. Differentiating edges requires high resolution; detecting a car does not. A pixel in the later layers represents a **large patch** of the original input image. Summarizing this information over a big patch is okay for high-level objects.

    Finally, why is it okay to have a Dense layer at the end? If you are already learning high-level features (car, bus, chair), these are **Dense Features**. You can put them as columns of a table and just ask a binary question: "Is it there or not?" At this stage, they are independent of their precise spatial positions. So it makes sense to use a Dense layer to classify based on the presence of these features.

    为什么宽度和高度缩小是可以的？在更深层，我们试图判断是否存在**高级特征**，例如：“图像的这部分有一辆车吗？”我们相信，像汽车或桌子这样的高级特征通常不需要太精细的分辨率就能检测到。

    这有道理吗？如果我给你一张低分辨率图像，检测是否有车比检测是否有垂直或水平边缘更容易。区分边缘需要高分辨率；检测汽车则不需要。后层中的一个像素代表原始输入图像的一个**大块**。对于高级对象，在这个大块上总结信息是可以的。

    最后，为什么最后有一个密集层是可以的？如果你已经学到了高级特征（汽车、公交车、椅子），这些就是**密集特征**。你可以把它们作为表格的列，然后只问一个二元问题：“它在那里吗？”在这个阶段，它们已经独立于它们的精确空间位置了。所以使用密集层根据这些特征的存在进行分类是有意义的。

  - [16:28 - 16:30] **Limitations and Alternatives**

    Of course, we are assuming that high-level features can be detected with low resolution. That may not always be true. In certain applications (like medical imaging), this is not like that. If that assumption doesn't hold, we need more sophisticated architectures.

    - **Strided Convolutions:** Learnable downsampling (more expensive but smarter).
    - **U-Net or ResNet:** Modern architectures that help with resolution issues or training depth. Chapter 9 of the Deep Learning book has details.

    当然，我们要假设高级特征可以在低分辨率下被检测到。但这可能并不总是真的。在某些应用（如医学成像）中，情况并非如此。如果这个假设不成立，我们需要更复杂的架构。

    - **步幅卷积（Strided Convolutions）：** 可学习的下采样（更昂贵但更智能）。
    - **U-Net 或 ResNet：** 帮助解决分辨率问题或训练深度问题的现代架构。深度学习书籍的第 9 章有详细介绍。

  - [16:30 - 16:33] **History: ImageNet**

    Let's say a bit about the history. There is the **ImageNet Challenge**: 1 million images, 20,000 categories. It is a very difficult classification problem. For a long time, the best model was not a neural network (e.g., XRCE) with 26% error. Humans have about 5% error.

    In 2012, **AlexNet** (the first CNN to win) achieved 16% error. It was the first time a neural network had state-of-the-art performance in Computer Vision.

    From then on, everything was CNNs. A particular highlight is **ResNet** in 2015, which beat human performance. It incorporates residual layers (skip connections) to help with the Gradient Vanishing problem.

    我们讲一点历史。有一个 **ImageNet 挑战赛**：100 万张图像，20,000 个类别。这是一个非常困难的分类问题。很长一段时间里，最好的模型不是神经网络（例如 XRCE），误差率为 26%。人类的误差率大约是 5%。

    2012 年，**AlexNet**（第一个获胜的 CNN）达到了 16% 的误差率。这是神经网络首次在计算机视觉领域取得最先进的性能。

    从那时起，一切都是 CNN。一个特别的亮点是 2015 年的 **ResNet**，它击败了人类的性能。它结合了残差层（跳跃连接）来帮助解决梯度消失问题。

  - [16:33 - 16:41] **Demo 1: MNIST with CNN**

    Let's build a simple CNN for MNIST. We need `Conv2D` and `MaxPooling2D` layers.

    1. **Lambda Layer:** Normalization.
    2. **Reshape Layer:** Reshape 28x28 input to 28x28x1. The Conv layer expects a Tensor (height, width, channels), so we need to add that channel dimension explicitly.
    3. **Conv2D:** We choose 64 filters, kernel size 3x3, activation ReLU.
    4. **MaxPooling2D:** Default stride is 2.

    Let's check our understanding of **Trainable Parameters**:

    - Pooling: 0 parameters.
    - Conv2D: 64 filters, size 3x3x1 (since input has 1 channel). Plus bias terms. So: $(3 \times 3 \times 1 + 1) \times 64$.

    Let's track the **Output Shape**:

    - Input: 28x28x1.
    - Conv2D (assuming padding): 28x28x64 (64 filters).
    - MaxPooling (stride 2): 14x14x64.
    - Next Conv (e.g., 128 filters): 14x14x128.
    - Next Pool: 7x7x128.
    - Flatten: Converts 7x7x128 into a long vector.
    - Dense Layers follow.

    Training performance: We get **99.3%** accuracy. Previously with Fully Connected, it was 98%. There is an improvement. If you train this CNN on the permuted dataset, it should perform poorly because it relies on spatial structure.

    让我们为 MNIST 构建一个简单的 CNN。我们需要 `Conv2D` 和 `MaxPooling2D` 层。

    1. **Lambda 层：** 归一化。
    2. **Reshape 层：** 将 28x28 输入重塑为 28x28x1。卷积层期望张量（高、宽、通道），所以我们需要显式添加那个通道维度。
    3. **Conv2D：** 我们选择 64 个滤波器，核大小 3x3，激活函数 ReLU。
    4. **MaxPooling2D：** 默认步幅为 2。

    让我们检查一下**可训练参数**的理解：

    - 池化：0 个参数。
    - Conv2D：64 个滤波器，大小 3x3x1（因为输入有 1 个通道）。加上偏置项。所以：$(3 \times 3 \times 1 + 1) \times 64$。

    让我们追踪**输出形状**：

    - 输入：28x28x1。
    - Conv2D（假设有填充）：28x28x64（64 个滤波器）。
    - MaxPooling（步幅 2）：14x14x64。
    - 下一个 Conv（例如 128 个滤波器）：14x14x128。
    - 下一个 Pool：7x7x128。
    - Flatten：将 7x7x128 转换为长向量。
    - 后面是 Dense 层。

    训练性能：我们得到了 **99.3%** 的准确率。之前用全连接是 98%。有所改进。如果你在置换后的数据集上训练这个 CNN，它的表现应该很差，因为它依赖于空间结构。

  - [16:41 - 16:53] **Demo 2: Chest X-Ray & ImageDataGenerator**

    The next demo is on Chest X-Ray images. It is a binary classification: **Normal** (healthy lungs) vs. **Pneumonia**.

    We plot some images. To me, even the normal one looks cloudy. I don't know anything about medicine. It is a difficult problem; radiologists train for years to tell them apart.

    **Preprocessing Challenge:** The images come from different machines and have different resolutions. We need to standardize them (e.g., resize to 128x128). The dataset is 2.3 GB. Doing preprocessing naively requires loading all data into memory, which is inefficient.

    **Solution: ImageDataGenerator (On-the-fly loading).**

    We want only the relevant batch (e.g., size 16) to live in memory. After one step of gradient descent, we throw this data away and load the next batch. This doesn't impose a memory constraint.

    You define a `train_generator` specifying the target size (128x128) and batch size. When we call `model.fit`, we pass the generator instead of `x_train, y_train`. Since the generator is infinite, we must specify `steps_per_epoch` (Total Size / Batch Size).

    **Performance:** 70% accuracy. The Confusion Matrix shows it is perfect on Normal patients, but for Pneumonia patients, it declares 1/3 of them as Normal. This is pretty bad (High False Negatives).

    **Why?**

    1. **Drastic Downsampling:** We resized big images to 128x128. We might have lost too much information.
    2. **Assumption Failure:** The assumption that high-level features can be detected at low resolution (like a car) probably does not hold for medical problems. You need fine details. You probably need **U-Net** or **ResNet**.

    **The "No Free Lunch" of Generators:**

    Using `ImageDataGenerator` is memory efficient, but there is a drawback. After every epoch, we need to retrieve the same batch from the disk again. This presents a **Communication Cost** (Disk I/O). If you use SSDs, this cost is small, which is okay.

    下一个演示是关于胸部 X 光图像的。这是一个二元分类：**正常**（健康的肺）与 **肺炎**。

    我们绘制了一些图像。对我来说，即使是正常的看起来也很浑浊。我对医学一无所知。这是一个难题；放射科医生需要训练多年才能区分它们。

    **预处理挑战：** 图像来自不同的机器，分辨率不同。我们需要标准化它们（例如，调整为 128x128）。数据集有 2.3 GB。天真地进行预处理需要将所有数据加载到内存中，这很低效。

    **解决方案：ImageDataGenerator（动态加载）。**

    我们希望只有相关的批次（例如大小为 16）存在于内存中。在梯度下降的一步之后，我们丢弃这些数据并加载下一批。这不会施加内存限制。

    你定义一个 `train_generator`，指定目标大小（128x128）和批次大小。当我们调用 `model.fit` 时，我们传递生成器而不是 `x_train, y_train`。由于生成器是无限的，我们必须指定 `steps_per_epoch`（总大小 / 批次大小）。

    **性能：** 70% 的准确率。混淆矩阵显示它在正常患者上是完美的，但在肺炎患者上，它将 1/3 的人判定为正常。这相当糟糕（高假阴性）。

    **为什么？**

    1. **剧烈的下采样：** 我们将大图像调整为 128x128。我们可能丢失了太多信息。
    2. **假设失效：** 高级特征可以在低分辨率下被检测到（像汽车那样）的假设可能不适用于医学问题。你需要精细的细节。你可能需要 **U-Net** 或 **ResNet**。

    **生成器的“没有免费午餐”：**

    使用 `ImageDataGenerator` 具有内存效率，但也有缺点。在每个 epoch 之后，我们需要再次从磁盘检索相同的批次。这带来了**通信成本**（磁盘 I/O）。如果你使用 SSD，这个成本很小，这还可以。
***
