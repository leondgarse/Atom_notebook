# __DSA5204___
***

Course description
This course is designed to provide students with a comprehensive understanding of deep learning, a pivotal machine learning technique with extensive applications in artificial intelligence and data sciences. The curriculum focuses on the introduction of fundamental concepts, numerical algorithms, and computing frameworks pertinent to deep learning. Special emphasis is placed on numerical algorithms and their implementation within industrial computing frameworks, as well as the analysis of data-intensive problems derived from real-world applications.

Core topics covered in this course include: Foundations of Neural Networks; Optimization Strategies in Deep Learning; Representative Network Architectures; Regularization and Generalization; Model Evaluation and Tuning; Deep Learning Frameworks and Implementation Applications in Natural Language Processing and Computer Vision.

# Lectures
## Week1
  * [14:10–14:14] **Course logistics: assessments, Q&A, textbook**
    **English** At the end of the semester (Week 13), you must submit two items: (1) a 15-minute recorded video presentation of your project, and (2) a formal written report. The Q&A tool briefly stopped working but was restored. Pay attention to deadlines: there is a late-submission policy with a 30% penalty per day late. For example, if an assignment is 7/10 on time, one day late reduces it to about 4/10. For the group project, most of the time everyone in a group receives the same grade, but there will be an end-of-semester peer survey to flag unequal contributions. Lectures are recorded and uploaded to Canvas. Questions should be asked in class, during consultation, or via the Canvas discussion board. For roughly the first 10 lectures, we will follow the textbook *Deep Learning* (the “Deep Learning Book”), which is free online at deeplearningbook.org. The test format will be explained later; it will be open-book with no strict limit on what hard copies you can bring.

    **Chinese** 学期末（第 13 周）你们要交两样东西：（1）一个 15 分钟的项目视频展示；（2）一份正式的书面报告。课堂的 Q&A 功能刚才一度出问题，后来恢复了。注意截止日期：迟交每天扣 30%。比如原本 7/10，迟交一天大概只剩 4/10。小组项目通常全组同分，但学期末会有同伴互评/问卷，用来识别有人贡献特别多或特别少的情况。课程会录制并上传到 Canvas。提问可以课堂举手、咨询时间问，或在 Canvas 讨论区发帖（推荐）。前 10 讲左右主要跟一本教材：*Deep Learning*（可在 deeplearningbook.org 免费下载）。考试形式后面再细说：是开卷，能带多少纸质资料基本不限制。

  * [14:14–14:21] **What are AI, ML, Deep Learning? Turing Test and motivation**
    **English** Deep learning is a branch of machine learning (ML), which itself is part of artificial intelligence (AI). AI’s goal is to understand and replicate intelligence. People are inspired by biological neural networks (brains), and popular culture has long imagined AI—from human-like robots to more “useful but not human-looking” systems, and even chatbots that feel human-like. Alan Turing reframed “Can machines think?” into a more concrete operational test: the Turing Test (Imitation Game). There are three parties: a computer (A), a human (B), and an interrogator (C) who can only communicate via text and must decide which is the human. If the machine fools the interrogator sufficiently often (commonly cited as >70%), it is called “intelligent” under this criterion. Modern LLMs may appear to pass, but the lecturer argues there are still ways to distinguish them (e.g., breadth and style of responses). Machine learning then becomes: how can machines *learn from data* to do tasks that thinking beings do. Deep learning is a subset of ML focusing on (deep) neural networks.

    **Chinese** 深度学习是机器学习（ML）的一个分支，而机器学习属于人工智能（AI）的大框架。AI 的目标是理解并尝试复现“智能”。生物学里研究大脑神经网络，也启发了人工神经网络。电影/文化里对 AI 的想象也经历变化：早期更“像人”的 AI，后来更务实，变成“能做事但不一定长得像人”的系统；现在的聊天机器人在语言上越来越像人。图灵把“机器能思考吗？”这种不够清晰的问题，改成更可操作的图灵测试（模仿游戏）：有电脑 A、人类 B、审问者 C。C 只能用文字对话，判断谁是人。如果机器能在足够高比例（常见说法是超过 70%）骗过审问者，就按这个标准算“智能”。如今的大语言模型看似很接近，但老师认为仍有办法分辨（比如回答风格、覆盖面等）。机器学习的核心就变成：机器如何从数据中学习，去完成“有智能的行为/任务”。深度学习则是机器学习里专门研究（深层）神经网络的一部分。

  * [14:21–14:29] **Why deep learning became successful: history, data, compute, software**
    **English** Neural networks are not new: early models date back to the 1940s; key training ideas such as backpropagation emerged in the 1950s; by the 1990s, architectures like CNNs and RNNs already existed. However, they were not dominant until around 2012, when deep CNNs achieved state-of-the-art performance in computer vision (ImageNet era). After that, deep learning rapidly advanced across areas: GANs for image generation, reinforcement learning systems (e.g., AlphaGo in 2016), transformer-based language models (e.g., BERT), protein folding (AlphaFold), diffusion models, and large language models. A key reason for post-2012 success is scale: deep models have many parameters and can learn features automatically, but they need lots of data; digitization produced that data. To implement deep learning effectively, you also need hardware (GPUs) and software frameworks. This course will mainly use TensorFlow and Keras (especially for assignments/demos), though projects may use other libraries.

    **Chinese** 神经网络并不是新东西：1940 年代就有早期模型；1950 年代提出了关键训练思想（反向传播相关思想）；到 1990 年代，CNN、RNN 等结构基本都出现了。但它们直到 2012 年左右才真正“统治”，典型标志是深度 CNN 在视觉任务上达到/超过当时最强水平（ImageNet 那一波）。之后深度学习在多个方向爆发：GAN 做图像生成；强化学习系统（如 2016 年 AlphaGo）；基于 Transformer 的语言模型（如 BERT）；蛋白质折叠（AlphaFold）；扩散模型；以及现在的大语言模型等。为什么 2012 后突然这么强？核心是“规模”：深度模型参数多、表达能力强，可以自动学特征，但前提是需要海量数据；社会数字化带来了数据。同时还需要算力（GPU）和好用的软件框架。本课主要用 TensorFlow + Keras（作业/演示尽量统一方便批改），项目可用其他库。

  * [14:29–14:36] **Applications and limitations; course goals; project expectations; prerequisites**
    **English** Deep learning has many real applications: autonomous driving (still not fully solved), medical imaging diagnosis, translation (strong for major languages), and reinforcement learning for decision-making/games. However, models have limitations such as adversarial examples: small malicious perturbations can cause confident wrong predictions, which can be dangerous in safety-critical systems. The course aims to cover important deep learning innovations, architectures, and learning algorithms, plus practical issues like overfitting and generalization; regularization will take about two lectures. The group project is designed to build “learning how to learn”: choose a research paper not covered in class, understand it, reproduce results, and make a small extension (e.g., tweak the model or test on a slightly different dataset/problem). Expected background: undergraduate-level linear algebra and probability; basic Python; familiarity with NumPy/SciPy and plotting tools is helpful (the transcript mentions pandas/seaborn for visualization). TensorFlow will be used in the course, and tutorials are provided on Canvas.

    **Chinese** 深度学习应用很多：自动驾驶（仍未完全解决）、医学影像诊断、机器翻译（主流语言已经很强）、强化学习做决策/玩游戏等。但也要看到限制，比如对抗样本：输入做一点点“恶意但很小”的改动，模型就可能非常自信地输出错误结果；如果在自动驾驶等安全场景会很危险。课程目标是介绍深度学习的重要创新：网络结构、学习算法，以及实践中最关键的问题——过拟合与泛化；正则化会用大约两讲来系统讲。小组项目的目的之一是训练“学会学习”：选一篇课堂没覆盖的论文，读懂它、复现结果，并做一个小扩展（比如小改模型，或换一个相近数据集/问题）。背景要求：本科水平线性代数与概率；会基础 Python；熟悉 NumPy/SciPy 和画图工具更好（记录里提到用 pandas/seaborn 做可视化）。课上用 TensorFlow，Canvas 上也会给入门教程。

  * [14:37–14:47] **Definition of machine learning: Task–Experience–Performance (T–E–P)**
    **English** A poll checks whether students have taken prior ML courses, since the first two lectures review ML basics. The standard definition: a program learns from experience **E** with respect to a task **T** and performance measure **P** if performance at **T**, measured by **P**, improves with **E**. ML is “inverse” of traditional programming: instead of hand-coding rules, you provide input–output examples and the system learns a mapping/program. Examples of tasks: prediction (e.g., house price), transcription (e.g., CAPTCHA/handwritten text), translation, and generation (e.g., realistic faces). Experience is data; data are ultimately represented as numbers (e.g., images as pixel matrices). Performance measures: regression often uses mean squared error; classification often uses accuracy (for evaluation); generation is harder to evaluate because you want realism *and* diversity. Break: 10 minutes, then regression as the anchor example.

    **Chinese** 老师先做了个投票，看看大家是否上过 DSA5102/5105 之类的机器学习基础课，因为前两讲会复习基础。机器学习的经典定义是 T–E–P：如果一个程序在任务 **T** 上、用指标 **P** 衡量的性能，会随着经验 **E**（数据/训练样本）的增加而提升，那么它就在“学习”。机器学习可以看作和传统编程相反：传统是人写规则+输入→输出；机器学习是给大量输入–输出样本，让系统自己学出映射/“程序”。任务例子：预测（房价）、转写（手写识别/CAPTCHA）、翻译、生成（生成逼真的人脸）等。经验 E 就是数据；而数据最终都要用数字表示（例如图像就是像素矩阵）。性能指标方面：回归常用均方误差；分类评估常用准确率；生成类任务更难，因为既要“像真的”也要“有多样性”。接着休息 10 分钟，然后用回归做一个核心示例。

  * [15:03–15:36] **Regression as anchor: hypothesis space, loss, ERM, linear/affine models, basis functions, generalization**
    **English** In regression, there is an unknown true relationship (f^*) mapping input (x) to output (y). We observe a dataset of (n) samples ((x_i, y_i)) (optionally with noise). We choose a **hypothesis space** ( \mathcal{H} ): a set of candidate functions (f) (e.g., all linear functions). To measure how close a model is, define a **loss** for one example; for regression a common choice is squared loss ((f(x)-y)^2). The average loss over the dataset is the **empirical risk**. Learning is framed as **empirical risk minimization (ERM)**: find (\hat f) that minimizes average loss on the training data.

    For linear regression with (f(x)=w^\top x), ERM has a closed-form solution (least squares): with matrix (X) and vector (y), the optimum is (w = (X^\top X)^{-1}X^\top y) (when invertible). This is a special “luxury”; for neural networks you can compute gradients, but you usually cannot solve (\nabla=0) exactly.

    The lecture distinguishes **linear** (through origin) vs **affine** models (w^\top x + b). Affine regression can be solved similarly by augmenting (x) with a constant 1 (add a column of ones to (X), and add (b) into the parameter vector).

    Then, model capacity: if the hypothesis space is too small, you get **underfitting** (e.g., fitting a line to quadratic data). A simple way to increase expressiveness is **linear basis models**: apply a feature map (\phi(x)) (possibly nonlinear) and fit a linear model in (\phi(x)). Examples include polynomial basis (\phi_j(x)=x^j), Gaussian basis, sigmoid basis, sine/cosine, etc. With sufficiently rich bases (e.g., polynomials of high degree), classical results (Weierstrass approximation idea) say you can approximate any continuous function arbitrarily well. Linear basis models still have a closed-form least squares solution if you replace (X) by the transformed design matrix (\Phi).

    However, high capacity can cause **overfitting**: a very high-degree polynomial can fit training points perfectly (zero empirical risk) but perform poorly between points (bad generalization). This motivates distinguishing **empirical risk** (training) vs **population risk** (expected loss over the true data distribution) and the **generalization gap**. We cannot minimize population risk directly, but we can estimate it by a **train/test split**: train on training data only, evaluate on held-out test data. As dataset size (n \to \infty), empirical risk should converge toward population risk, so one way to reduce overfitting is getting more data. The lecture notes that generalization is central in deep learning (models are large), and will be addressed via architecture choices, explicit regularization, loss/training changes, and data augmentation. Break: 5 minutes, then classification.

    **Chinese** 回归任务里，真实关系 (f^*) 把输入 (x) 映射到输出 (y)，但我们不知道 (f^*)。我们只看到数据集里的 (n) 个样本 ((x_i,y_i))（可能有噪声）。接着选一个**假设空间** ( \mathcal{H} )：也就是你允许模型长什么样（比如“所有线性函数”）。为了衡量模型好坏，先定义单个样本的**损失函数**；回归里常用平方损失 ((f(x)-y)^2)。把所有样本的损失取平均，就是**经验风险**（empirical risk）。学习就被表述成 **ERM：经验风险最小化**——在假设空间里找一个 (\hat f)，让训练集平均损失最小。

    线性回归 (f(x)=w^\top x) 很特殊：ERM 有闭式解（最小二乘公式）。把输入堆成矩阵 (X)、输出堆成向量 (y)，最优解是 (w=(X^\top X)^{-1}X^\top y)（可逆时）。这在神经网络里通常做不到：你能算梯度，但很难直接解出“梯度=0”的解析解。

    老师也区分了**线性模型**（过原点）和**仿射模型**（affine）(w^\top x+b)。仿射模型也好解：给每个输入向量补一个常数 1（相当于在 (X) 里加一列全 1），把 (b) 合并进参数向量，就能用同样的最小二乘形式求解。

    接着讲模型容量：假设空间太小会**欠拟合**（比如用直线去拟合二次函数数据）。一个简单增大表达力的方法是**线性基函数模型**：先做特征映射 (\phi(x))（可以是非线性的），再在 (\phi(x)) 上做线性模型。例子：多项式基函数 (\phi_j(x)=x^j)、高斯基、sigmoid 基、正余弦等。若基函数足够丰富（如允许很高次数的多项式），经典结论（类似魏尔施特拉斯逼近思想）说明：任意连续函数都可以被逼近到任意精度。线性基函数模型依然有闭式解：把 (X) 换成变换后的设计矩阵 (\Phi) 就行。

    但容量大也会带来**过拟合**：高次多项式可能把训练点全部穿过（经验风险为 0），但在点与点之间表现很差（泛化差）。因此要区分**经验风险**（训练集）和**总体风险**（population risk：对真实分布下“任意新样本”的期望损失），以及两者之间的**泛化差距**。总体风险不能直接优化，但可以用**训练/测试集划分**来估计：只用训练集训练，再用测试集评估。样本量 (n) 越大，经验风险越接近总体风险，所以“更多数据”通常能缓解过拟合。老师强调泛化在深度学习里非常核心（模型太大太容易过拟合），后面会通过结构选择、显式正则化、改损失/训练算法、数据增强等系统讲。之后休息 5 分钟，进入分类部分。

  * [15:44–16:08] **Classification: from argmax to softmax; one-hot; cross-entropy vs square loss**
    **English** For K-class classification, a linear approach generalizes binary linear classifiers: use K weight vectors (w_1,\dots,w_K) to produce K scores, then predict the class with the maximum score (argmax). In matrix form, collect weights into (W), compute scores (W\phi(x)), and apply (g=\arg\max). Two problems if you try to train using squared loss on integer labels and argmax outputs:
    (1) **Label encoding problem (nominal vs ordinal):** class IDs (0–9) are nominal; there is no meaningful notion that “6 is farther from 0 than 1 is.” Squared error would penalize wrong classes unequally.
    (2) **Optimization problem:** argmax is non-differentiable, which breaks gradient-based training.

    Fixes: encode labels as **one-hot vectors** so all classes are equidistant (no artificial order). Make the model output a **probability vector** using **softmax**, a differentiable function (s(z)_k = \exp(z_k)/\sum_j \exp(z_j)), ensuring positive entries summing to 1. The model becomes ( \text{softmax}(W\phi(x))).

    For training, accuracy is fine for evaluation but is not differentiable as a training loss. We need a differentiable loss comparing probability vectors. Squared loss works but cross-entropy is preferred. With true label (y) as one-hot and predicted probabilities (p), cross-entropy is (L=-\sum_k y_k\log p_k), which reduces to (-\log p_{\text{true}}). Practical reason: cross-entropy yields stronger gradients, especially when the model is “almost correct” (e.g., 80% confidence) and you still want to push it toward near-certainty; square loss gradients can become too small. A theoretical reason is its connection to maximum likelihood. The lecture ends by summarizing: TEP definition; regression uses squared loss; classification uses cross-entropy; hypothesis spaces; capacity; overfitting; empirical vs population risk; train/test split; then transitions to a linear regression demo using Python tooling.

    **Chinese** K 类分类可以看作二分类线性分类器的推广：用 (w_1,\dots,w_K) 产生 K 个分数，然后取最大分数对应的类别（argmax）。矩阵写法就是把权重堆成 (W)，算分数 (W\phi(x))，再用 (g=\arg\max) 输出类别。若你直接把类别当整数（0–9）并用平方损失训练，会有两个问题：
    (1) **标签编码问题（名义 vs 序数）：** 类别是“名义型”（nominal），没有“0 更接近 1 而远离 6”的意义。用平方损失会导致不同错误被罚得不一样，不合理。
    (2) **优化问题：** argmax 不可导，没法做基于梯度的训练。

    解决办法：用 **one-hot** 编码标签，让类别之间没有人为顺序（各类别“距离”一致）。同时让模型输出**概率向量**，用 **softmax** 把任意实数向量变成概率分布：(s(z)_k=\exp(z_k)/\sum_j \exp(z_j))，每一维都 >0 且总和为 1。模型就变成 ( \text{softmax}(W\phi(x)))。

    训练时：准确率适合作为训练后评估指标，但不适合作为训练损失（不可导、0/1 跳变）。需要一个可导的“概率向量差异”损失。平方损失可以用，但更常用的是**交叉熵**：真实标签 (y) 是 one-hot，预测概率是 (p)，交叉熵 (L=-\sum_k y_k\log p_k)，由于 one-hot 的性质，它等价于 (-\log p_{\text{true}})。实践原因：交叉熵在模型“差一点就对”（比如 80%）时梯度更大，更容易把模型推到“非常自信且正确”；平方损失在这里梯度可能太小，训练推进慢。理论原因：最小化交叉熵等价于最大化似然。最后老师总结了：TEP 定义；回归常用平方损失；分类常用交叉熵；需要先定义假设空间与容量；过拟合与泛化；经验风险 vs 总体风险；用 train/test split 估计泛化；然后进入线性回归的 Python 演示。

  * [16:08–16:28] **Demo: Linear regression on Singapore HDB resale prices; preprocessing; visualization; underfitting; feature expansion + one-hot**
    **English** The demo uses common Python ML tools (scikit-learn, pandas, matplotlib; the transcript mentions seaborn; NumPy). Data source: Singapore HDB resale prices (GovTech dataset). Target: resale price. Features include town (location), storey range, floor area (sqm), remaining lease (years/months), etc. Two preprocessing steps convert strings to numbers: remaining lease “61 years 4 months” → ~61.33; storey range “10 TO 12” → average storey 11.
    Visualization:
    • Bar plot of town vs price (expensive areas like Bukit Timah/Bishan vs cheaper like Yishun, etc.).
    • Bar/plot of storey vs price (higher floor tends to cost more).
    • Scatter of floor area vs price, colored by remaining lease (newer flats tend to be pricier at same size).
    Then split train/test (e.g., 90/10) with shuffling.
    Model 1: linear regression using only floor area (1D). Plot shows large scatter; normalized RMSE ~26% on both train and test. Similar train/test errors imply **underfitting**.
    Model 2: polynomial regression (degree 3) on the same 1D feature shows little improvement (~26%). Reason: for a fixed floor area, prices vary widely due to other factors; increasing polynomial degree won’t fix missing dimensions. Lesson: visualize data; scalar error alone doesn’t tell whether the issue is nonlinearity or missing features.
    Model 3: add more features (remaining lease, storey, and town). Town is categorical, so apply one-hot encoding (e.g., `get_dummies`). With higher-dimensional input and linear regression, normalized error improves to ~18%, with train/test still similar (no strong overfitting). Conclude: form project groups early; end of lecture.

    **Chinese** 演示用到常见 Python 机器学习工具（scikit-learn、pandas、matplotlib；记录里也提到 seaborn；以及 NumPy）。数据来自新加坡 HDB 转售价格（GovTech 数据）。目标是预测转售价。输入特征包括：town（地区）、storey range（楼层范围）、floor area（面积）、remaining lease（剩余租期）等。两步预处理把字符串变成数值：比如剩余租期 “61 years 4 months” → 约 61.33；楼层范围 “10 TO 12” → 平均楼层 11。
    可视化部分：
    • 地区 vs 价格条形图（如 Bukit Timah/Bishan 较贵，Yishun 等相对便宜）。
    • 楼层 vs 价格：楼层越高通常越贵。
    • 面积 vs 价格散点图，并用颜色表示剩余租期（同面积下，租期更长/更“新”的房通常更贵）。
    然后做 train/test 划分（例如 90/10）并随机打乱。
    模型 1：只用面积做 1 维线性回归。散点很分散，归一化 RMSE 约 26%，训练/测试差不多 → **欠拟合**（主要是模型信息不足，而不是过拟合）。
    模型 2：同样只用面积，换成三次多项式回归，几乎没提升（仍 ~26%）。原因是：同一面积下价格跨度很大，决定价格的因素不止面积；你把曲线弄得再“弯”，也还是 1 维输入，解决不了“缺少关键特征维度”的问题。结论：要先可视化理解数据结构；只看一个误差数字，很难判断到底是“非线性不够”还是“特征维度不够”。
    模型 3：加入更多特征（剩余租期、楼层、地区等）。地区是类别型变量，需要 one-hot（如 `get_dummies`）。在更高维输入下做线性回归，误差降到约 18%，且训练/测试仍接近（没有明显过拟合）。最后提醒尽早组队，课程结束。
