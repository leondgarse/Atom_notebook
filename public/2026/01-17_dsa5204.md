# __DSA5204 Deep Learning and Applications___
***

# Info
  Course description
  This course is designed to provide students with a comprehensive understanding of deep learning, a pivotal machine learning technique with extensive applications in artificial intelligence and data sciences. The curriculum focuses on the introduction of fundamental concepts, numerical algorithms, and computing frameworks pertinent to deep learning. Special emphasis is placed on numerical algorithms and their implementation within industrial computing frameworks, as well as the analysis of data-intensive problems derived from real-world applications.

  Core topics covered in this course include: Foundations of Neural Networks; Optimization Strategies in Deep Learning; Representative Network Architectures; Regularization and Generalization; Model Evaluation and Tuning; Deep Learning Frameworks and Implementation Applications in Natural Language Processing and Computer Vision.
***

# Lectures
## Week 1
  - [14:10 - 14:13] **Course Logistics, Assessment Breakdown, and Policies**

    At the end of the semester, specifically on the last day of week 13, you are required to submit two major components. First, you must record and submit a 15-minute video presentation of your group project. Second, you also have to write and submit a formal final report. Somehow the Q&A feature was turned off just now—I'm not sure why—but it is turned back on, so I can see that it is active now. Regarding the assessment components, please keep the due dates strictly in mind because we have a very specific late submission policy. For any deadline that you miss, there is a 30% penalty for every day that the submission is late. For example, if you submit an assignment one day late and your raw score was 7 out of 10, your adjusted grade will drop to 4 out of 10.

    There is a good question coming in on the chat regarding the group project: "Does every group member get the same grade?" The answer is: in most cases, yes. However, at the end of the semester, I will conduct a peer survey. This allows you to indicate if there was a group member who was exceptional and did most of the work, or conversely, a member who didn't contribute much. So, while everyone usually gets the same grade, we have this mechanism to adjust for participation. All resources, including slides and Python demos, will be uploaded to Canvas. If you have questions during the lecture, use the PollEV Q&A or raise your hand. For questions outside of class, the best method is to use the "Discussions" tab on Canvas rather than email, or come to the consultation slots. For the first ten lectures, we will be following the "Deep Learning" textbook by Ian Goodfellow, which is free to download at deeplearningbook.org. Regarding the final test format: it will be open book, meaning you can bring any hard copies with no limit on the quantity.

    在学期末，也就是第13周的最后一天，你们需要提交两样东西。首先，你们必须录制并提交一份15分钟的小组项目展示视频。其次，你们还需要撰写一份正式的期末报告。刚才问答（Q&A）功能不知为何关闭了，但现在已经重新开启，我可以看到它是活跃的。关于评估部分，请务必牢记截止日期，因为我们有严格的迟交政策。任何错过的截止日期，每迟交一天都会受到30%的分数扣除惩罚。举个例子，如果你的作业原本得了10分中的7分，但迟交了一天，你的最终成绩就会变成4分。

    聊天框里有个关于小组项目的好问题：“每个组员的得分是否相同？”答案是：大多数情况下是的。但是，在学期末我会进行一次同行评议调查（Peer Survey）。你们可以在调查中指出是否有组员表现特别出色、承担了大部分工作，或者是否有组员贡献寥寥。因此，虽然通常大家分数相同，但我们有这个机制来根据参与度调整分数。所有的课程资源，包括幻灯片和Python演示代码，都会上传到Canvas上。如果在课上有疑问，请使用PollEV提问或直接举手。课后如果有问题，最好的方式是使用Canvas上的“讨论”板块，或者来参加答疑。在前十次讲座中，我们将使用Ian Goodfellow编写的《Deep Learning》作为教材，这本书可以在deeplearningbook.org免费下载。关于期末考试的形式：它是开卷考试，意味着你们可以携带任何纸质资料进场，且不限数量。

  - [14:14 - 14:16] **Introduction to AI: From Biological Roots to Sci-Fi Evolution**

    Let's begin by defining our subject. Deep Learning is a branch of Machine Learning, which itself is a subset of the broader field of Artificial Intelligence (AI). The ultimate goal of AI is to understand intelligence and try to replicate it. Biologists have actually been trying to do this for a long time; for example, they study the biological neural networks of a fruit fly's brain to understand how intelligence functions biologically. This notion of replicating intelligence has also been widely explored in movies.

    If we look at movies in chronological order, from older ones to newer ones, we see an interesting trend. Initially, movies were very ambitious, depicting AI that was indistinguishable from a human. Then, we became a bit more "down to earth" about our capabilities, moving towards robots that looked like humans but were clearly machines. Later, we saw robots that didn't look human at all—like *WALL-E*—but could perform intelligent, useful tasks. Finally, we have movies like *Her*, where a man falls in love with a chatbot. I think this is roughly where we have achieved success today; our chatbots are now very clever and rather human-like. It seems we are trying to move back towards that original ambition of intelligent agents that can interact naturally with us.

    让我们先定义一下这门课的主题。深度学习（Deep Learning）是机器学习（Machine Learning）的一个分支，而机器学习又是人工智能（Artificial Intelligence, AI）这一广泛领域的子集。AI的终极目标是理解智能并尝试复制它。生物学家其实很早就开始尝试这么做了，比如他们通过研究果蝇大脑的生物神经网络来试图理解智能的生物学原理。这种复制智能的概念在电影中也有广泛的体现。

    如果我们按时间顺序看这些电影，会发现一个有趣的趋势。起初，电影的设想非常宏大，描绘的AI几乎和人类一模一样。后来，我们对复制智能的能力变得更加“脚踏实地”，电影中的机器人虽然像人，但明显是机器。再后来，我们看到了像《瓦力》（WALL-E）这样完全不像人，但能执行智能且有用任务的机器人。最后，我们看到了像《Her》这样的电影，讲述了一个男人爱上聊天机器人的故事。我认为这正是我们目前所达到的阶段——现在的聊天机器人非常聪明，而且相当拟人化。看起来我们正努力朝最初的宏大愿景回归，试图创造出能与我们自然交互的智能体。

  - [14:16 - 14:19] **The Turing Test: Can Machines Think?**

    Alan Turing addressed this fundamental question: "Can machines think?" However, he argued that this question is not well-defined because the terms "machine" and "think" are ambiguous. For example, consider a motorized standing desk that moves up and down—is that a machine? Or consider this chair over here; what does it mean for a chair to "think"? You might think about neural networks all day, but you don't have a mouth to speak, so how do we know if you are thinking? Because these definitions are slippery, asking "Can machines think?" is dangerous and unproductive.

    Turing's contribution was to resolve this by proposing the "Imitation Game," now known as the Turing Test, which makes the question concrete. In this test, there are three participants: a computer (A), a human (B), and an interrogator (C). The interrogator's job is to determine which participant is the computer and which is the human via a text-based conversation. Turing defined a machine as "intelligent" if it could fool the interrogator more than 70% of the time. With modern Large Language Models (LLMs) like ChatGPT, you might think the Turing Test is obsolete because they are already so clever. However, I would argue it is actually quite easy to tell a human and ChatGPT apart. If you ask a human about a broad range of subjects, they will likely only know one or two well. But ChatGPT has a concept of almost everything. So, ironically, its excessive knowledge gives it away.

    艾伦·图灵（Alan Turing）曾探讨过一个核心问题：“机器能思考吗？”但他认为这个问题本身定义不清，因为“机器”和“思考”这两个词太模棱两可了。例如，一张能升降的电动站立式办公桌，它算机器吗？或者看看这边这把椅子，对于椅子来说，“思考”意味着什么？你可能整天都在思考神经网络，但如果你没有嘴巴说出来，我们怎么知道你在思考？正因为这些定义含糊不清，直接问“机器能思考吗”是危险且无效的。

    图灵的贡献在于他提出了“模仿游戏”（Imitation Game），也就是后来著名的“图灵测试”，将这个问题具体化了。测试中有三个参与者：一台计算机（A）、一个人（B）和一个询问者（C）。询问者的任务是通过纯文本对话来判断哪一个是计算机，哪一个是人。图灵定义，如果机器能在超过70%的时间里欺骗询问者，那它就是“智能”的。有了现在的像ChatGPT这样的大语言模型，你可能觉得图灵测试已经过时了，因为它们太聪明了。但我认为，其实很容易区分人类和ChatGPT。如果你问一个人类广泛领域的问题，他可能只精通其中一两个；但ChatGPT几乎懂所有概念。所以讽刺的是，正是它那过于渊博的知识暴露了它不是人类。

  - [14:19 - 14:21] **Defining Machine Learning and Its History**

    So, Turing shifted the question from "Can machines think?" to "Can machines do what thinking beings do?" This leads us to the definition of Machine Learning. As defined by Tom Mitchell, Machine Learning is the study of how machines can learn to perform tasks that thinking beings do. Deep Learning is a subset of Machine Learning, specifically focusing on the study of deep neural networks. Looking at the Venn diagram, AI is the largest sphere, encompassing any system that mimics intelligence—even a simple rule-based database could be AI if it's useful enough. Machine Learning sits inside AI as models that adapt to data, and Deep Learning sits inside ML.

    Why study neural networks? Are they new? Actually, no. The first artificial neural network was introduced in the 1940s, nearly 80 years ago. The Backpropagation algorithm, which is the key ingredient for training these networks, was introduced in the 1950s. By the 1990s, we already had sophisticated architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The models existed, but they weren't popular because they didn't perform well at the time.

    图灵将问题从“机器能思考吗”转变为“机器能做有思维的生物所做的事吗？”这就引出了机器学习的定义。正如Tom Mitchell所定义的，机器学习研究的是机器如何学会执行有思维生物所做的任务。深度学习是机器学习的一个子集，专门研究深度神经网络。看这张韦恩图，AI是最大的圆圈，包含任何模拟智能的系统——哪怕是一个简单的基于规则的数据库，只要足够有用，也可以算作AI。机器学习位于AI内部，指那些能适应数据的模型，而深度学习则位于机器学习内部。

    为什么要研究神经网络？它们是新事物吗？其实不是。第一个人工神经网络早在1940年代就出现了，距今快80年了。用于训练这些网络的关键——反向传播算法（Backpropagation），在1950年代就已提出。到了1990年代，我们已经有了像卷积神经网络（CNN）和循环神经网络（RNN）这样复杂的架构。模型虽然早就存在，但当时并不流行，因为它们那时的表现并不好。

  - [14:22 - 14:25] **The Deep Learning Boom (2012-Present)**

    The turning point came in 2012 with AlexNet, a Convolutional Neural Network that became state-of-the-art in image recognition, beating all traditional models. This was the first time a neural network truly dominated. Since then, Deep Learning has achieved state-of-the-art results in various fields. We have Generative Adversarial Networks (GANs) for image generation, and Reinforcement Learning models like AlphaGo, which beat human champions at the game of Go in 2016. We have Transformers like BERT for translation, and recently, the protein folding problem was solved using deep learning, which famously won the Nobel Prize in Chemistry (referring to the 2024 prize awarded to AlphaFold researchers).

    More recently, we've seen diffusion models and, of course, Large Language Models (LLMs) like ChatGPT, which is essentially a Transformer combined with Reinforcement Learning. Why did Deep Learning only take off after 2012? The difference lies in the approach. In traditional Machine Learning, a human expert had to manually design feature maps to extract useful features from an image (e.g., edges, corners) before feeding it to a classifier. Deep Learning models, however, are massive with millions of parameters. We feed the raw image directly into the model, and because it has so many degrees of freedom, it learns to extract the features itself. This skips the manual feature engineering step. However, because these models are so huge, they require massive amounts of data to learn effectively. Before 2012, we didn't have enough data. With the rapid digitization of society, we now have the data scale required for these models to perform.

    转折点出现在2012年，AlexNet——一种卷积神经网络——在图像识别领域达到了最先进水平（State-of-the-Art），击败了所有传统模型。这是神经网络第一次真正展现出统治力。从那以后，深度学习在各个领域都取得了突破。我们有用于图像生成的生成对抗网络（GAN），有像AlphaGo这样的强化学习模型——它在2016年击败了人类围棋冠军。我们有用于翻译的Transformer模型（如BERT），最近，蛋白质折叠问题也通过深度学习得到了解决，这也获得了诺贝尔化学奖（指2024年AlphaFold研究者获奖）。

    更近期的发展包括扩散模型（Diffusion Models），当然还有像ChatGPT这样的大语言模型（LLM），它本质上是Transformer加上强化学习。为什么深度学习直到2012年后才腾飞？区别在于方法。在传统机器学习中，人类专家必须手动设计特征图（Feature Maps）来从图像中提取有用特征（如边缘、角点），然后再输入分类器。而深度学习模型非常庞大，拥有数百万个参数。我们将原始图像直接输入模型，由于它拥有极高的自由度，它能自己学会提取特征。这省去了手动特征工程的步骤。然而，正因为模型如此巨大，它们需要海量数据才能有效学习。在2012年之前，我们没有足够的数据。随着社会的快速数字化，我们现在终于有了能让这些模型发挥作用的数据规模。

  - [14:28 - 14:33] **Tools, Applications, and Adversarial Limitations**

    To implement deep learning, you need two things: hardware (GPUs) and software. We will be using TensorFlow and Keras in this course. These libraries make it incredibly easy to train neural networks. If you were doing this 10-15 years ago, you would have to write the backpropagation algorithm from scratch mathematically, which is extremely tiring and error-prone. Nowadays, the software handles the differentiation for you.

    Let's look at some applications. Autonomous driving is a major, albeit still open, problem. It involves processing video feeds to instantly recognize road markings, cars, and pedestrians. Medical imaging is another area; models can predict diseases from X-rays. This is a non-trivial task because a human radiologist trains for several years to do this, yet models are achieving comparable performance. Language translation is now considered almost a solved problem for major languages. However, we must be aware of limitations. Deep Learning models can be fragile. For example, if you take an image of a panda that the model correctly classifies, and add a tiny layer of carefully crafted noise—imperceptible to the human eye—the model might suddenly classify it as a "gibbon" with 99% confidence. This is an adversarial attack. Imagine if this happens to an autonomous vehicle seeing a stop sign; it could be extremely dangerous. We will discuss "Adversarial Training" later in the course to counter this.

    要实现深度学习，你需要两样东西：硬件（GPU）和软件。本课程我们将使用TensorFlow和Keras。这些库让训练神经网络变得非常简单。如果你在10到15年前做这个，你必须从头手写反向传播算法的数学代码，那是极其累人且容易出错的。现在，软件会自动为你处理微分计算。

    让我们看一些应用。自动驾驶是一个主要但尚未完全解决的问题。它涉及处理视频流以即时识别道路标记、车辆和行人。医学影像是另一个领域；模型可以从X光片中预测疾病。这不是一个简单的任务，因为人类放射科医生需要训练多年才能做到这一点，而模型现在已经能达到类似的表现。语言翻译对于主要语言来说几乎已经被视为一个已解决的问题。然而，我们必须意识到局限性。深度学习模型可能是脆弱的。例如，如果你拿一张模型能正确分类的熊猫图片，加上一层精心设计的微小噪声——人眼根本看不出来——模型可能会突然以99%的置信度将其分类为“长臂猿”。这就是对抗性攻击（Adversarial Attack）。试想如果这种情况发生在看到停车标志的自动驾驶汽车上，那将是极度危险的。我们稍后会在课程中讨论“对抗训练”来应对这一问题。

  - [14:33 - 14:37] **Course Structure, Research Focus, and Project Scope**

    In this course, we will cover architectures, learning algorithms, and practical techniques. A very serious problem in deep learning is overfitting, so we will dedicate two full lectures to regularization techniques to address this. Deep Learning is an extremely active field of research, with new models coming out all the time. Therefore, "learning how to learn" is crucial.

    This is the main purpose of the group project. Detailed instructions are on Canvas, but essentially, you are expected to pick a research paper that covers a topic *not* covered in our lectures. You must read it, understand it, replicate its results, and then do a small extension. An extension could be, for example, making a small tweak to the model architecture to address a slightly different problem, or applying it to a different dataset. Regarding prerequisites: you need Linear Algebra and Probability (Year 1 or 2 undergrad level). If you need a refresher, Chapters 2 and 3 of the Deep Learning Book are excellent. For programming, you need basic Python knowledge, and familiarity with libraries like NumPy, Pandas, and Seaborn is helpful.

    在本课程中，我们将涵盖架构、学习算法和实用技术。深度学习中一个非常严重的问题是过拟合（Overfitting），因此我们将专门用两节课来讲正则化（Regularization）技术来解决这个问题。深度学习是一个研究极其活跃的领域，新模型层出不穷。因此，“学会如何学习”是至关重要的。

    这也是小组项目的主要目的。Canvas上有详细说明，但基本上，你们需要选择一篇我们课程并未涵盖的研究论文。你们必须阅读并理解它，复现其结果，然后做一个小的扩展。扩展可以是，例如，对模型架构做一个小调整以解决稍有不同的问题，或者将其应用于不同的数据集。关于预备知识：你需要线性代数和概率论（本科一二年级水平）。如果你需要复习，教材《Deep Learning Book》的第2章和第3章非常棒。在编程方面，你需要掌握基础Python，熟悉NumPy、Pandas和Seaborn等库会有很大帮助。

  - [14:37 - 14:41] **Poll on Prerequisites and Recap of Machine Learning Definition**

    I'm going to run a quick poll just to test the system and see your background. Have you taken DSA5102 or DSA5105 before? These are foundational machine learning courses. If most of you have taken them, I can move slightly faster through the basics today.

    Let's recap the formal definition of Machine Learning by Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. This concept is somewhat the inverse of traditional programming. In traditional programming, you give the computer a program (logic) and inputs, and it calculates the output. In Machine Learning, you give the computer the inputs and the desired outputs (the answers), and the computer learns the program (the logic) that connects them.

    我现在做一个快速投票，测试一下系统顺便了解你们的背景。你们之前修过DSA5102或DSA5105吗？这些是机器学习的基础课程。如果你们大多数人都修过，我今天讲基础部分就可以稍微快一点。

    让我们重温一下Mitchell对机器学习的正式定义：如果一个计算机程序在某类任务T上的性能（用P衡量）随着经验E的增加而提高，那么我们就说它从经验E中学习了。这个概念与传统编程有点相反。在传统编程中，你给计算机程序（逻辑）和输入，它计算出输出。而在机器学习中，你给计算机输入和期望的输出（答案），计算机自己去学习连接这两者的程序（逻辑）。

  - [14:43 - 14:47] **Task Examples: From House Prices to Google's reCAPTCHA**

    Let's look at concrete examples of Task (T), Experience (E), and Performance (P). A classic task is **Prediction** (Regression), such as predicting the price of a house based on its size. Another is **Transcription**, like recognizing handwritten digits or text. A fun fact about transcription: Google's reCAPTCHA—where you have to click on images containing traffic lights or cars—is actually Google making use of you to label their data for autonomous driving. They are collecting labeled data for free! Other tasks include **Translation** and **Image Generation** (generating realistic human faces).

    In these cases, Experience (E) is simply your data. For an image, the data is represented numerically as a matrix of pixel values. As for Performance (P): for a Regression task (like house prices), we typically use Mean Squared Error (MSE). For Classification, we use Accuracy. However, for Image Generation, it is not so straightforward. How do you evaluate a generated face? We want it to be realistic, but we also want diversity (creativity) so it doesn't just generate the same face every time. We will discuss these complex metrics later in the course. Okay, it's 47 minutes past the hour. Let's take a 10-minute break now. After the break, we will deep dive into Linear Regression as an anchor model to understand these basics concretely.

    让我们看一些关于任务（T）、经验（E）和性能（P）的具体例子。一个经典的任务是**预测**（回归），比如根据房屋大小预测房价。另一个是**转录**，比如识别手写数字或文本。关于转录有个有趣的事实：Google的reCAPTCHA——就是让你点击包含红绿灯或汽车的图片的那个验证码——实际上是Google在利用你为他们的自动驾驶数据打标签。他们在免费收集标注数据！其他任务还包括**翻译**和**图像生成**（生成逼真的人脸）。

    在这些案例中，经验（E）就是你的数据。对于图像，数据以像素值矩阵的数值形式表示。至于性能（P）：对于回归任务（如房价），我们通常使用均方误差（MSE）。对于分类任务，我们使用准确率（Accuracy）。然而，对于图像生成，评价标准就不那么直观了。你如何评价一张生成的人脸？我们要它逼真，但我们同时也需要多样性（创造力），不能让它每次都生成同一张脸。我们会在课程后面讨论这些复杂的指标。好了，现在是47分。让我们休息10分钟。休息回来后，我们将深入探讨线性回归，以此作为锚点模型来具体理解这些基础知识。

  - [15:03 - 15:09] **Formalizing Regression: Hypothesis Space and Empirical Risk**

    Let's formalize the regression problem. If we are predicting a real number, like the price of a house, we assume there is some underlying relationship from the input `x` to the output `y`. We denote this true, underlying function as `f*`. However, `f*` is unknown to us. The only thing we can see is the "experience," which is our dataset consisting of `N` samples of inputs `x_i` and outputs `y_i`. Our goal is to learn a function `f` that is close to `f*`, meaning we want a function that fits our data well. In practice, data often contains noise (denoted by `ε`), but for today we will ignore the noise and assume a clean relationship.

    Before we define performance, we need to introduce the concept of the **Hypothesis Space**, denoted by `H`. The hypothesis space is the collection of all candidate functions `f` that we are considering to approximate the true `f*`. For example, in simple linear regression, `H` would just be the set of all possible straight lines, and we are trying to find the single best-fitting line. Once we have a candidate `f` from this space, we need a performance measure `P` to evaluate it. We define this as the distance between our candidate `f` and the true `f*`. The goal of machine learning is to find the specific `f_hat` within `H` that minimizes this distance to the data.

    How do we concretely measure this distance? We use a **Loss Function**, denoted by `L`. The loss function tells us how bad our model's prediction is for a single data point. For regression, a common choice is the **Square Loss**, which is `(f(x) - y)^2`. To measure the distance over the entire dataset, we take the average of this loss over all `N` data points. This average is called the **Empirical Risk**. Therefore, the machine learning process can be phrased as **Empirical Risk Minimization (ERM)**: we want to find the function `f_hat` that minimizes the empirical risk (the average loss) on our observed dataset.

    让我们正式定义回归问题。如果我们是在预测一个实数，比如房价，我们假设输入 `x` 和输出 `y` 之间存在某种潜在的关系。我们将这个真实的、潜在的函数标记为 `f*`。然而，`f*` 对我们来说是未知的。我们要能看到的只是“经验”，也就是包含 `N` 个输入 `x_i` 和输出 `y_i` 样本的数据集。我们的目标是学习一个接近 `f*` 的函数 `f`，也就是说，我们要找一个能很好地拟合数据的函数。在实际应用中，数据通常包含噪声（用 `ε` 表示），但今天我们先忽略噪声，假设关系是纯净的。

    在定义性能之前，我们需要引入 **假设空间（Hypothesis Space）** 的概念，用 `H` 表示。假设空间是我们用来逼近真实 `f*` 的所有候选函数 `f` 的集合。例如，在简单的线性回归中，`H` 就是所有可能的直线的集合，我们要做的就是找到拟合效果最好的那条线。一旦我们在空间中选定了一个候选 `f`，就需要一个性能度量 `P` 来评估它。我们将此定义为候选 `f` 与真实 `f*` 之间的距离。机器学习的目标就是在 `H` 中找到那个能使数据间距离最小化的特定函数 `f_hat`。

    我们如何具体地衡量这个距离呢？我们使用**损失函数（Loss Function）**，记作 `L`。损失函数告诉我们模型对单个数据点的预测有多糟糕。对于回归问题，一个常见的选择是**平方损失（Square Loss）**，即 `(f(x) - y)^2`。为了衡量整个数据集上的距离，我们计算所有 `N` 个数据点损失的平均值。这个平均值被称为**经验风险（Empirical Risk）**。因此，机器学习过程可以表述为**经验风险最小化（ERM）**：我们要找到那个能使观测数据集上的经验风险（平均损失）最小化的函数 `f_hat`。

  - [15:09 - 15:15] **Solving Linear Regression: The Ordinary Least Squares Formula**

    Let's apply this to Linear Regression. Here, our hypothesis space `H` consists of all linear functions. A linear function `f(x)` is defined as `w^T x` (or the dot product of `w` and `x`), where `w` is a weight vector of the same dimension as the input `x`. In simple terms, `w` represents the slope of the line. We want to find the parameter `w` that minimizes the Empirical Risk using the square loss. The formula for Empirical Risk here is the sum of squared differences `(w^T x_i - y_i)^2` averaged over the dataset.

    How do we solve this? We can define it as an optimization problem where we want to find the `w` that minimizes this risk. First, we can write the risk in a compact matrix form. We collect all labels `y_i` into a vector `y`, and we stack all input vectors `x_i` as rows into a matrix `X` (which has `N` rows and `D` columns). The Empirical Risk then becomes the squared norm of `(Xw - y)`. To minimize this, we take the gradient with respect to `w` and set it to zero.

    When you solve this equation `∇w = 0`, you get a closed-form solution: `w = (X^T X)^-1 X^T y`. This is famously known as the **Ordinary Least Squares (OLS)** formula. This is actually a rare luxury in machine learning. For this specific model (Linear Regression), the Empirical Risk Minimization problem is "solved" analytically—we have a direct formula. You just plug in your data `X` and `y`, and you get the optimal model immediately. As we will see later, for neural networks, we do not have this luxury. We will know how to compute the gradient, but we won't be able to analytically solve the equation "gradient equals zero," compelling us to use iterative methods like gradient descent instead.

    让我们将其应用于线性回归。在这里，我们的假设空间 `H` 由所有线性函数组成。线性函数 `f(x)` 定义为 `w^T x`（即 `w` 和 `x` 的点积），其中 `w` 是与输入 `x` 维度相同的权重向量。通俗地说，`w` 代表直线的斜率。我们想要找到能使平方损失下的经验风险最小化的参数 `w`。这里的经验风险公式就是数据集上平方差 `(w^T x_i - y_i)^2` 的平均值。

    我们如何解决这个问题？我们可以将其定义为一个优化问题，目标是找到使风险最小化的 `w`。首先，我们可以用紧凑的矩阵形式书写风险。我们将所有标签 `y_i` 收集到一个向量 `y` 中，并将所有输入向量 `x_i` 作为行堆叠成一个矩阵 `X`（它有 `N` 行和 `D` 列）。这样经验风险就变成了 `(Xw - y)` 的平方范数。为了最小化它，我们要对 `w` 求梯度，并令其为零。

    当你求解这个方程 `∇w = 0` 时，你会得到一个解析解：`w = (X^T X)^-1 X^T y`。这就是著名的**普通最小二乘法（OLS）**公式。这在机器学习中其实是一种难得的奢侈。对于线性回归这个特定模型，经验风险最小化问题是可以被“解析求解”的——我们有一个直接的公式。你只需要代入数据 `X` 和 `y`，就能立即得到最优模型。正如我们稍后会看到的，对于神经网络，我们将不再拥有这种奢侈。我们虽然知道如何计算梯度，但无法解析求解“梯度等于零”的方程，这迫使我们只能使用像梯度下降这样的迭代方法。

  - [15:15 - 15:23] **Affine Models, Capacity, and Linear Basis Models**

    Strictly speaking, the linear models we just discussed (`w^T x`) define straight lines that must pass through the origin. Often, we want lines that have a non-zero y-intercept. These are mathematically called **Affine Models**, defined as `w^T x + b`, where `b` is a bias term. In machine learning, we often use the terms "linear" and "affine" interchangeably, but it's good to be precise. Fortunately, solving for an affine model is just as easy. We simply add an extra parameter to `w` (for the bias) and add a column of 1s to our design matrix `X`. This allows the same OLS formula to solve for the bias as well. In Deep Learning, we often get a bit sloppy and assume the bias is implicitly there without drawing it every time, but technically, this is how it's handled.

    Now, we must ask: Is our hypothesis space `H` big enough? This brings up the concept of **Model Capacity**. Consider two datasets: one generated by a linear function and one by a quadratic function. A linear model fits the linear data perfectly but fails miserably on the quadratic data. This is called **Underfitting**—our hypothesis space is too small and lacks the expressive power to capture the underlying relationship.

    To fix this, we can increase the complexity using **Linear Basis Models**. The idea is to transform the input `x` using a function `φ` (Phi) before feeding it into the linear model. `φ` is a "feature map" or "basis function" that includes non-linearities. For example, if `x` is 1-dimensional, we can choose a **Polynomial Basis** where `φ(x) = [1, x, x^2, x^3...]`. Our model then becomes a linear function of `φ(x)`: `f(x) = w^T φ(x)`. This strictly generalizes linear models. Importantly, the **Universal Approximation Theorem** tells us that if we choose polynomials as our basis and allow the degree to be high enough, we can approximate *any* continuous function to arbitrary precision. This makes Linear Basis Models extremely powerful and expressive.

    严格来说，我们要刚才讨论的线性模型（`w^T x`）定义的直线必须经过原点。通常，我们需要有非零截距的直线。这在数学上称为**仿射模型（Affine Models）**，定义为 `w^T x + b`，其中 `b` 是偏置项（Bias）。在机器学习中，我们经常互换使用“线性”和“仿射”这两个词，但保持精确是好事。幸运的是，求解仿射模型同样简单。我们只需要在 `w` 中增加一个额外的参数（用于偏置），并在设计矩阵 `X` 中增加一列全是1的列。这样我们就可以用同样的最小二乘法公式来求解偏置了。在深度学习中，我们经常会偷懒，假设偏置隐含在其中而不每次都画出来，但在技术上，这就是处理它的方式。

    现在，我们必须问：我们的假设空间 `H` 够大吗？这就引出了**模型容量（Model Capacity）\**的概念。考虑两个数据集：一个由线性函数生成，另一个由二次函数生成。线性模型可以完美拟合线性数据，但在二次函数数据上表现极差。这就是所谓的\**欠拟合（Underfitting）**——我们的假设空间太小，缺乏足够的表达能力来捕捉潜在的关系。

    为了解决这个问题，我们可以使用**线性基模型（Linear Basis Models）\**来增加复杂度。其核心思想是在将输入 `x` 送入线性模型之前，先用一个函数 `φ`（Phi）对其进行转换。`φ` 是一个包含非线性的“特征映射”或“基函数”。例如，如果 `x` 是一维的，我们可以选择\**多项式基**，其中 `φ(x) = [1, x, x^2, x^3...]`。这样我们的模型就变成了关于 `φ(x)` 的线性函数：`f(x) = w^T φ(x)`。这严格地推广了线性模型。重要的是，**通用逼近定理（Universal Approximation Theorem）**告诉我们，如果我们选择多项式作为基，并允许次数足够高，我们可以以任意精度逼近*任何*连续函数。这使得线性基模型极其强大且具有极高的表达力。

  - [15:23 - 15:29] **Solving Basis Models and The Phenomenon of Overfitting**

    The great news is that Linear Basis Models are also easy to solve. Even though the model is non-linear with respect to the input `x`, it is still linear with respect to the trainable parameter `w`. We simply define a new matrix `Φ` (capital Phi) where each row is the transformed input `φ(x_i)`. The Empirical Risk Minimization formula looks exactly the same as before, just replacing `X` with `Φ`: `w = (Φ^T Φ)^-1 Φ^T y`. So, we have a highly expressive model that comes with a closed-form solution. You might wonder: if these models are so good, why do we even need Neural Networks? We will answer that next week, but let's look at a caveat first.

    Since we have the Universal Approximation Theorem, you might be tempted to just use a massive polynomial, say of degree 99, to solve everything. Let's see what happens if we fit a degree-99 polynomial to a small dataset generated from a simple quadratic function. The resulting model (the blue curve) will look extremely weird. It will oscillate wildly. However, it will pass through every single data point exactly. This means its Empirical Risk is technically **zero**. It has solved the minimization problem perfectly on the training data. But obviously, if you look at the spaces between the data points, the predictions are garbage compared to the true underlying function (the orange line). This is the classic **Overfitting** phenomenon. Our model fits the training data perfectly but fails to generalize to unseen data.

    好消息是，线性基模型也很容易求解。虽然模型对于输入 `x` 是非线性的，但它对于可训练参数 `w` 仍然是线性的。我们只需定义一个新的矩阵 `Φ`（大写 Phi），其中每一行是转换后的输入 `φ(x_i)`。经验风险最小化公式看起来和以前完全一样，只是把 `X` 换成了 `Φ`：`w = (Φ^T Φ)^-1 Φ^T y`。所以，我们拥有了一个既具有高表达力又有解析解的模型。你可能会问：如果这些模型这么好，我们为什么还需要神经网络？我们要下周才会回答这个问题，但让我们先看一个警示。

    既然我们有通用逼近定理，你可能会想直接用一个巨大的多项式（比如99次多项式）来解决所有问题。让我们看看如果用99次多项式去拟合一个由简单二次函数生成的小数据集会发生什么。结果模型（蓝色的曲线）看起来会非常怪异。它会剧烈震荡。然而，它会精准地穿过每一个数据点。这意味着它的经验风险在技术上是**零**。它在训练数据上完美地解决了最小化问题。但很明显，如果你看数据点之间的空隙，其预测结果与真实的潜在函数（橙色线）相比简直是垃圾。这就是经典的**过拟合（Overfitting）**现象。我们的模型完美拟合了训练数据，但无法泛化到未见过的数据。

  - [15:29 - 15:36] **Generalization: The Gap Between Empirical and Population Risk**

    This leads to a crucial realization: Machine Learning is not just about optimization (Empirical Risk Minimization). If it were, the degree-99 polynomial would be the perfect model because it has zero error. But clearly, it's not. What we *actually* want to solve is **Population Risk Minimization**. We want to minimize the expected loss on unseen data from the true population distribution. The problem is, we cannot solve this directly because we don't know the true distribution—we only see samples.

    This creates a **Generalization Gap**—the difference between the Empirical Risk (which we can optimize) and the Population Risk (which we care about). If this gap is large, like in our polynomial example, it means we are overfitting. Since we can't calculate the Population Risk directly, how do we estimate it? We assume that as the dataset size `N` goes to infinity, the Empirical Risk converges to the Population Risk. In practice, we estimate this by splitting our data into a **Training Set** and a **Test Set**. We train the model *only* on the training set. Then, we act like the test set is "unseen" data and evaluate the model on it. This gives us an objective estimate of the Population Risk. It is crucial that the model *never* sees the test set during training.

    Comparing the training error and the test error tells us about the Generalization Gap. If the gap is big (low training error, high test error), you are overfitting. This is a central problem in deep learning because neural networks are huge and prone to overfitting. We will spend two full lectures on **Regularization** techniques—like altering the architecture, changing the loss function, or data augmentation—specifically to close this gap. Okay, let's take a 5-minute break. When we come back, we will talk about classification and look at the demo.

    这就引出了一个至关重要的认识：机器学习不仅仅是优化（经验风险最小化）。如果是的话，那个99次多项式就是完美的模型，因为它的误差为零。但显然它不是。我们*真正*想要解决的是**总体风险最小化（Population Risk Minimization）**。我们希望最小化来自真实总体分布的未见数据的期望损失。问题是，我们无法直接求解这个，因为我们不知道真实的分布——我们只能看到样本。

    这就产生了**泛化差距（Generalization Gap）**——即经验风险（我们可以优化的）与总体风险（我们要关注的）之间的差异。如果这个差距很大，就像我们的多项式例子那样，那就意味着我们过拟合了。既然我们无法直接计算总体风险，我们该如何估计它呢？我们假设随着数据集大小 `N` 趋向于无穷大，经验风险会收敛于总体风险。在实践中，我们要通过将数据划分为**训练集（Training Set）**和**测试集（Test Set）**来估计它。我们*仅*在训练集上训练模型。然后，我们将测试集当作“未见”数据，在其上评估模型。这给了我们要总体风险的一个客观估计。至关重要的是，模型在训练期间*决不能*看到测试集。

    比较训练误差和测试误差可以告诉我们泛化差距的大小。如果差距很大（训练误差低，测试误差高），那就是过拟合。这是深度学习中的一个核心问题，因为神经网络非常庞大，很容易过拟合。我们将专门用两节课来讲**正则化（Regularization）**技术——比如改变架构、修改损失函数或数据增强——专门用来缩小这个差距。好了，我们休息5分钟。回来后，我们将讨论分类问题并看演示。

  - [15:44 - 15:47] **Linear Models for Classification: From Regression to Argmax**

    Now let's discuss how to adapt the linear model for a K-class classification problem. Recall that in linear regression, we used a single linear model `w^T x` (or `w^T φ(x)` for basis models) to predict a continuous output. For classification with K classes, we can extend this idea by using K different linear functions instead of just one. Essentially, we will have K different weight vectors `w_1, w_2, ..., w_K`.

    For a given input `x` (or transformed input `φ(x)`), we compute the output for each of the K linear functions, resulting in K values: `y_1, y_2, ..., y_K`. The model's final prediction is simply the index of the largest value among them. This is often called the **Argmax Model**. Geometrically, you can think of this as projecting `φ(x)` onto K different lines and picking the one that gives the maximum response. If you are familiar with Support Vector Machines (SVMs), this is conceptually similar. An SVM typically does binary classification by checking if a point is above or below a single line (hyperplane). The Argmax model is a generalization of that concept to K classes.

    现在让我们讨论如何调整线性模型以解决K类分类问题。回想一下，在线性回归中，我们使用单个线性模型 `w^T x`（或者用于基模型的 `w^T φ(x)`）来预测连续输出。对于有K个类别的分类问题，我们可以通过使用K个不同的线性函数来扩展这个想法。本质上，我们将拥有K个不同的权重向量 `w_1, w_2, ..., w_K`。

    对于给定的输入 `x`（或转换后的输入 `φ(x)`），我们计算每个线性函数的输出，得到K个值：`y_1, y_2, ..., y_K`。模型的最终预测就是这些值中最大值的索引。这通常被称为**Argmax模型**。从几何上讲，你可以将其视为将 `φ(x)` 投影到K条不同的线上，并选取响应最大的那一条。如果你熟悉支持向量机（SVM），这在概念上是类似的。SVM通常通过检查点是在直线的上方还是下方来进行二分类（超平面）。Argmax模型是这一概念在K类上的推广。

  - [15:47 - 15:53] **Issues with Square Loss for Classification (Nominal vs. Ordinal)**

    We can rewrite this model compactly. Let's collect all K weight vectors into a matrix `W` (where each row is a `w_k`). Let `g` be the Argmax function. Our model becomes `f(x) = g(W φ(x))`. Now, how do we train this? A naive approach might be to use the Square Loss we used for regression: take the predicted integer index from the model, subtract the true integer label, square the difference, and minimize this average loss.

    However, there are two major problems with this. The first and most critical issue is that it treats class labels as integers with a meaningful order. This works for **Ordinal Data** (where order matters, like "small, medium, large"), but classification usually involves **Nominal Data** (where order doesn't matter, like "cat, dog, car"). If the true label is Class 0 (e.g., "cat"), and Model A predicts Class 1 ("dog") while Model B predicts Class 6 ("truck"), the square loss would penalize Model B significantly more `(6-0)^2 = 36` than Model A `(1-0)^2 = 1`. But is predicting "truck" really 36 times worse than predicting "dog"? No. Both are just "wrong." There is no inherent order that makes Class 6 "further" from Class 0 than Class 1. Using Square Loss on integer labels imposes a penalty structure that doesn't exist in reality.

    我们可以将这个模型写得更紧凑些。让我们把所有K个权重向量收集到一个矩阵 `W` 中（每一行是一个 `w_k`）。设 `g` 为Argmax函数。我们的模型就变成了 `f(x) = g(W φ(x))`。现在，我们该如何训练它？一个天真的方法可能是沿用回归中使用的平方损失：取模型预测的整数索引，减去真实的整数标签，计算差的平方，然后最小化这个平均损失。

    然而，这样做有两个主要问题。第一个也是最关键的问题是，它将类别标签视为具有意义顺序的整数。这对**有序数据（Ordinal Data）**（顺序很重要，如“小、中、大”）是有效的，但分类通常涉及**名义数据（Nominal Data）**（顺序不重要，如“猫、狗、车”）。如果真实标签是类别0（例如“猫”），模型A预测类别1（“狗”），而模型B预测类别6（“卡车”），平方损失会对模型B的惩罚 `(6-0)^2 = 36` 远大于模型A `(1-0)^2 = 1`。但预测“卡车”真的比预测“狗”糟糕36倍吗？不。两者都只是“错了”。实际上并不存在让类别6比类别1距离类别0“更远”的内在顺序。在整数标签上使用平方损失强加了一种现实中并不存在的惩罚结构。

  - [15:53 - 15:59] **The Solution: One-Hot Encoding and Softmax Activation**

    The second problem with the Argmax approach is that the `argmax` function is "hard" and discrete—it jumps from one integer to another. This makes it non-differentiable, so we cannot use Gradient Descent to train it. To fix both issues, we introduce two changes: **One-Hot Encoding** and the **Softmax** function.

    First, instead of using integers, we encode labels as **One-Hot Vectors**. A one-hot vector has a 1 in the correct class position and 0s elsewhere (e.g., `[0, 1, 0, 0]`). This solves the ordinality problem because the Euclidean distance between any two one-hot vectors is exactly the same (sqrt(2)). No class is "closer" or "further" from another; they are all equidistant. Second, we replace the hard Argmax with the **Softmax** function. Softmax takes a vector of raw scores (logits) and squashes them into a probability vector where all components are positive and sum to 1. It’s a "soft" version of max that is fully differentiable. So our new model outputs a probability vector: `f(x) = Softmax(W φ(x))`.

    Argmax方法的第二个问题是，`argmax` 函数是“硬”且离散的——它在整数之间跳跃。这使得它不可微，因此我们无法使用梯度下降来训练它。为了解决这两个问题，我们引入了两个改变：**独热编码（One-Hot Encoding）\**和\**Softmax**函数。

    首先，我们不再使用整数，而是将标签编码为**独热向量（One-Hot Vectors）**。独热向量在正确类别的位置是1，其他位置是0（例如 `[0, 1, 0, 0]`）。这解决了顺序性问题，因为任意两个独热向量之间的欧几里得距离都是完全相同的（根号2）。没有哪个类别比另一个“更近”或“更远”；它们都是等距的。其次，我们将硬Argmax替换为**Softmax**函数。Softmax将原始分数向量（logits）压缩成一个概率向量，其中所有分量都为正且和为1。这是最大值的“软”版本，且完全可微。所以我们的新模型输出的是一个概率向量：`f(x) = Softmax(W φ(x))`。

  - [16:00 - 16:06] **Cross-Entropy Loss vs. Square Loss for Classification**

    Now that our model outputs probabilities, how do we measure performance? Accuracy is the ultimate metric, but like Argmax, it’s discrete (0 or 1) and non-differentiable, making it useless for training gradients. We need a surrogate loss function. We *could* still use Square Loss between the predicted probability vector and the true one-hot vector. It works and satisfies the basic properties (0 loss if perfect, differentiable).

    However, the standard and better choice is **Cross-Entropy Loss**. Defined as `-log(p_correct)`, it measures the divergence between the predicted probability distribution and the true distribution. Why is it better than Square Loss? Look at the gradients. If your model predicts the correct class with only 80% confidence, you want the gradients to push it towards 99%. Square Loss has a very flat gradient when the prediction is somewhat close (like 0.8), meaning the model learns slowly. Cross-Entropy, being unbounded (it goes to infinity as probability approaches 0), has much steeper gradients even when the error is small. This pushes the model more aggressively towards perfection. That’s why Cross-Entropy is the default for classification.

    既然我们的模型输出的是概率，我们如何衡量性能呢？准确率（Accuracy）是最终指标，但就像Argmax一样，它是离散的（0或1）且不可微，因此无法用于训练梯度。我们需要一个代理损失函数。我们*可以*仍然在预测概率向量和真实独热向量之间使用平方损失。它能用，也满足基本属性（完美时损失为0，可微）。

    然而，标准且更好的选择是**交叉熵损失（Cross-Entropy Loss）**。定义为 `-log(p_correct)`，它衡量预测概率分布与真实分布之间的差异。为什么它比平方损失好？看看梯度就知道了。如果你的模型以80%的置信度预测正确类别，你希望梯度能推动它达到99%。当预测比较接近（如0.8）时，平方损失的梯度非常平缓，意味着模型学习得很慢。而交叉熵是无界的（当概率趋近于0时趋向无穷大），即使误差很小，它也有更陡峭的梯度。这能更积极地推动模型趋向完美。这就是为什么交叉熵是分类问题的默认选择。

  - [16:08 - 16:20] **Linear Regression Demo: HDB Resale Prices (Part 1)**

    Let's move to the Python demo on Linear Regression using the HDB Resale Prices dataset from GovTech. We use libraries like Scikit-Learn, Pandas, and Seaborn. The dataset contains features like 'Town', 'Storey Range', 'Floor Area', and 'Remaining Lease', and we want to predict 'Resale Price'. First, we do some preprocessing: converting 'Remaining Lease' (e.g., "61 years 04 months") into a float (61.33 years) and 'Storey Range' (e.g., "10-12") into an average integer (11).

    We visualize the data. Bar plots show expected trends: central towns like Bukit Timah are expensive, remote ones like Yishun are cheaper. Scatter plots show a positive correlation between Floor Area and Price. However, interestingly, for a fixed floor area (e.g., 100 sqm), there is still a massive range of prices. This suggests that floor area alone isn't enough to determine price.

    We start with a simple model using only **one feature**: Floor Area. We split the data (90% train, 10% test). After training, the Mean Squared Error (normalized) is around 26% for both train and test sets. This is quite high. Since the train and test errors are similar and high, this indicates **Underfitting**. The model is too simple.

    让我们转到使用GovTech的组屋转售价格（HDB Resale Prices）数据集进行的线性回归Python演示。我们会用到Scikit-Learn、Pandas和Seaborn等库。数据集包含“市镇（Town）”、“楼层范围（Storey Range）”、“楼面面积（Floor Area）”和“剩余租期（Remaining Lease）”等特征，我们想要预测“转售价格（Resale Price）”。首先，我们做一些预处理：将“剩余租期”（如“61年04个月”）转换为浮点数（61.33年），将“楼层范围”（如“10-12”）转换为平均整数（11）。

    我们对数据进行可视化。条形图显示了预期的趋势：像武吉知马（Bukit Timah）这样的中心市镇价格昂贵，而像义顺（Yishun）这样的偏远市镇则便宜。散点图显示楼面面积与价格之间存在正相关。然而有趣的是，对于固定的楼面面积（例如100平方米），价格范围仍然巨大。这表明仅靠楼面面积不足以决定价格。

    我们从一个只使用**单一特征**（楼面面积）的简单模型开始。我们划分数据（90%训练，10%测试）。训练后，训练集和测试集的均方误差（归一化后）都在26%左右。这相当高。由于训练误差和测试误差都很高且相近，这表明出现了**欠拟合（Underfitting）**。模型太简单了。

  - [16:20 - 16:28] **Linear Regression Demo: Polynomials vs. More Features (Part 2)**

  To fix underfitting, we assume the model lacks complexity, so we try **Polynomial Regression** (degree 3) on that single feature. The result? The error remains exactly the same, around 26%. Why? Even if we used a degree-100 polynomial, it wouldn't help. The problem isn't that the relationship between area and price is non-linear; the problem is that **one dimension is not enough**. A single curve, no matter how wiggly, cannot capture the variance caused by location, lease, and storey height.

  The visualization saves us here: seeing the massive price spread for a fixed area tells us we need **more input dimensions**, not more complex curves. So, we add the other features: Remaining Lease, Storey, and Town. Since 'Town' is categorical, we use **One-Hot Encoding** (via `get_dummies`). With these added features, the error drops significantly to 18%. The train and test errors are still very close, meaning we still haven't overfitted. This suggests we could arguably increase the model complexity even further to get better results. That concludes today's lecture. Remember to form your project groups soon!

  为了解决欠拟合，我们假设模型缺乏复杂度，所以我们对那个单一特征尝试**多项式回归**（3次）。结果呢？误差几乎完全没变，还是26%左右。为什么？即使我们用100次多项式也没用。问题不在于面积和价格之间的关系是非线性的；问题在于**一个维度不够**。无论一条曲线多么弯曲，它都无法捕捉到由位置、租期和楼层高度引起的差异。

  可视化在这里拯救了我们：看到固定面积下巨大的价格差异，我们就知道我们需要的是**更多的输入维度**，而不是更复杂的曲线。因此，我们加入了其他特征：剩余租期、楼层和市镇。由于“市镇”是分类变量，我们使用**独热编码（One-Hot Encoding）**（通过 `get_dummies`）。加入这些特征后，误差显著下降到了18%。训练误差和测试误差仍然非常接近，这意味着我们还没有过拟合。这表明我们甚至可以进一步增加模型复杂度来获得更好的结果。今天的课就到这里。记得尽快组建你们的项目小组！
## Week 2
  - [14:05 - 14:06] **Recap: Limitations of Linear Basis Models**

    In our previous discussion, we talked about how inputting data `x` directly into a linear model often isn't enough. Instead, we transform `x` using basis functions, like the sine and cosine basis we looked at. Last time, we established that this approach is beneficial because it allows us to express much more complex functions than a simple linear line. However, we also highlighted some significant limitations. I want to remind you of those limitations today because they serve as the perfect motivation for why we need something better.

    在上一次的讨论中，我们谈到了直接将数据 `x` 输入线性模型通常是不够的。相反，我们会使用基函数（例如正弦和余弦基函数）对 `x` 进行转换。上次我们确定了这种方法的好处，因为它允许我们要比简单的线性模型表达更复杂的函数。然而，我们也强调了一些明显的局限性。今天我想再次提醒大家这些局限性，因为它们是我们引入更优模型的重要动机。

  - [14:06 - 14:17] **Interactive Game: Fixed vs. Adaptive Basis**

    The main purpose of today's lecture is to really dig into neural networks. Specifically, we are going to look at the simplest form: the fully connected neural network. To understand why a neural network is superior to a linear basis model, we are going to view the neural network as what is called an "adaptive basis model." To motivate this concept, let's play a game. I'll explain the rules, and you can join in on Poll Everywhere. There will be two rounds.

    **Round 1:** First, I want you to pick three of your favorite integers between 0 and 9. Write them down and do not change them—this is your "fixed basis." Now, I will give you a "magic number." Your goal is to get as close to this magic number as possible by adding up to three of the integers you chose. You can add 0, 1, 2, or all 3 of your chosen numbers. Your score is the absolute difference between your sum and the magic number.

    - *Example:* If you chose {3, 5, 9} and I give you the number 15, the closest you can get is 5 + 9 = 14. Your score (error) is |15 - 14| = 1.
    - *Example:* If you chose {3, 5, 9} and the magic number is 1, the closest you can get is by adding nothing (0), so your score is |1 - 0| = 1.

    Let's play. I'll give you three magic numbers: 12, then another, and finally 20. Looking at the results, the error distribution is quite spread out. Individually, your performance likely varies wildly depending on whether the fixed numbers you chose happened to fit the magic number I gave.

    **Round 2:** Now, let's modify the game. This time, instead of picking three numbers, I want you to pick **six** integers between 0 and 9. When I give you the magic number, you can select *any* three from your pool of six to sum up and get close to the target.

    - *Example:* If you picked {0, 1, 2, 3, 4, 8} and the magic number is 5, you can dynamically choose 2 + 3, giving you an error of 0. If the magic number is 18, you can pick 3, 4, and 8 to get 15, giving an error of 3.

    Let's play with magic numbers 4, 10, and 19. Looking at the results now, the performance is consistently better. The errors are much smaller across the board. Why? Because you have a larger pool of options and you can *adapt* your choice of which numbers to use based on the target I gave you. In Round 1, your "basis" was fixed before you saw the data. In Round 2, you could adapt your basis to the specific data instance. This is the fundamental difference between fixed linear basis models and neural networks.

    今天课程的主要目的是深入探讨神经网络。具体来说，我们将研究最简单的形式：全连接神经网络。为了理解为什么神经网络优于线性基函数模型，我们将把神经网络看作一种“自适应基函数模型”。为了引入这个概念，我们来玩一个游戏。我会解释规则，大家可以在 Poll Everywhere 上参与。游戏分为两轮。

    **第一轮：** 首先，请在 0 到 9 之间选出三个你最喜欢的整数。把它们写下来，不要更改——这是你的“固定基底”。现在，我会给出一个“魔术数字”。你的目标是通过从你选择的整数中相加（最多三个），使其尽可能接近这个魔术数字。你可以选择不加、加一个、加两个或加三个数字。你的得分是你得到的和与魔术数字之间的绝对差值。

    - *例子：* 如果你选了 {3, 5, 9} 而我给的数字是 15，你能得到的最接近的数是 5 + 9 = 14。你的得分（误差）是 |15 - 14| = 1。
    - *例子：* 如果你选了 {3, 5, 9} 而魔术数字是 1，你能得到的最接近的数是不加任何数（即 0），所以你的得分是 |1 - 0| = 1。

    我们来试一下。我会给出三个魔术数字：12，然后是另一个，最后是 20。观察结果可以发现，误差分布非常分散。从个人角度来看，你的表现可能波动很大，这完全取决于你最初固定的数字是否恰好适合我给出的魔术数字。

    **第二轮：** 现在修改一下游戏规则。这次，请在 0 到 9 之间选出 **六个** 整数。当我给出魔术数字时，你可以从这六个数的池中挑选 *任意* 三个来相加，以接近目标。

    - *例子：* 如果你选了 {0, 1, 2, 3, 4, 8} 而魔术数字是 5，你可以动态选择 2 + 3，误差为 0。如果魔术数字是 18，你可以选 3、4 和 8 得到 15，误差为 3。

    我们用魔术数字 4、10 和 19 来试玩一下。现在的测试结果显示，大家的表现普遍更好，误差显著降低。为什么？因为你的选择空间更大了，而且你可以根据我给的具体目标数据来 *调整* 你使用的数字。在第一轮中，你的“基底”在看到数据之前就固定了；而在第二轮中，你可以根据具体的数据实例来调整你的基底。这就是固定线性基函数模型与神经网络之间的本质区别。

  - [14:17 - 14:24] **Comparison of Polynomial vs. Cosine Basis Efficiency**

    So, why is a fixed basis model not necessarily good? We mentioned last week that linear basis models, like polynomial bases, have the "universal approximation property." This means that with enough basis functions (a high enough degree polynomial), you can approximate any reasonable function arbitrarily well. But the key question isn't just *can* we approximate it, but *how efficiently* can we do it? We might have limited data or limited computational budget, so we care about efficiency—specifically, how many basis functions `m` do we need to get a low error?

    Let's look at two experiments. We have two choices of basis sets: a polynomial basis `φ_m(x) = (1, x, x^2, ..., x^m)` and a cosine basis `φ_m(x) = (1, cos(x), cos(2x), ..., cos(mx))`.

    **Experiment 1:** Our target function `f*` is a very wavy curve.

    - If we use the polynomial basis, to get the Mean Squared Error (MSE) below 0.1, we need `m = 9`.
    - If we use the cosine basis, which naturally fits wavy patterns better, we only need `m = 7` to achieve the same error. Here, the cosine basis is more efficient.

    **Experiment 2:** Now, let's change the target function `f*` to a simple, smooth blue curve that looks somewhat quadratic.

    - With the polynomial basis, we achieve a tiny error (less than `10^-6`) with just `m = 3`.
    - With the cosine basis, we need `m = 5` to achieve comparable accuracy. In this case, the polynomial basis is more efficient.

    What do these experiments tell us? They tell us that the efficiency of learning depends heavily on the match between your chosen basis and the unknown target function. No single fixed basis works universally well for all functions. Since we don't know the target function beforehand, it is impossible to pre-select the "best" basis. This leads us to the conclusion that it is better to *adapt* the choice of basis `φ` to the data itself.

    那么，为什么固定基函数模型未必是好的呢？上周我们提到，线性基函数模型（如多项式基底）具有“通用逼近性质”。这意味着只要基函数足够多（多项式的阶数足够高），你可以任意精确地逼近任何合理的函数。但关键问题不仅仅是 *能不能* 逼近，而是我们能 *多高效* 地逼近？我们可能受限于数据量或计算预算，因此我们关注效率——具体来说，我们需要多少个基函数 `m` 才能获得较低的误差？

    来看两个实验。我们要对比两组基底：多项式基底 `φ_m(x) = (1, x, x^2, ..., x^m)` 和余弦基底 `φ_m(x) = (1, cos(x), cos(2x), ..., cos(mx))`。

    **实验 1：** 我们的目标函数 `f*` 是一条波动很大的曲线。

    - 如果使用多项式基底，要将均方误差（MSE）降到 0.1 以下，我们需要 `m = 9`。
    - 如果使用余弦基底（它天然更适合波动形态），我们只需要 `m = 7` 就能达到同样的误差。在这个例子中，余弦基底更高效。

    **实验 2：** 现在我们将目标函数 `f*` 换成一条简单平滑的蓝色曲线，看起来像二次函数。

    - 使用多项式基底，仅需 `m = 3` 就能获得极小的误差（小于 `10^-6`）。
    - 使用余弦基底，我们需要 `m = 5` 才能达到相当的精度。在这种情况下，多项式基底反而更高效。

    这些实验告诉我们什么？它们说明学习的效率很大程度上取决于你选择的基底与未知目标函数的匹配程度。没有一种固定的基底能对所有函数都通用地表现良好。既然我们无法预知目标函数，就无法预先选择“最好”的基底。这引导我们得出一个结论：根据数据来 *自适应* 地调整基底 `φ` 的选择会更好。

  - [14:24 - 14:29] **Adaptive Basis Models and Training Challenges**

    Mathematically, here is how we define an adaptive basis model. The standard linear model is `f(x) = w^T φ(x)`. For an adaptive model, we introduce a new parameter `θ` into the basis function, making it `f(x) = w^T φ(x; θ)`. This `θ` allows the shape of the basis function itself to change based on the data.

    Consider a toy example where the target function is `f* = 1 + x^10`.

    - If you use a standard fixed polynomial basis (1, x, x^2...), you would need `m = 11` (all terms up to degree 10) to represent this perfectly.
    - However, with an adaptive polynomial basis `φ(x; θ) = (x^θ_0, x^θ_1, ...)`, you theoretically only need `m = 2`. The model could simply "learn" that `θ_0 = 0` (making the first term 1) and `θ_1 = 10` (making the second term `x^10`). This is incredibly efficient.

    But there is a catch. The second model is much harder to train. For linear basis models, finding the optimal weights `w` is easy—it's a convex optimization problem with a closed-form solution (the normal equations). But for adaptive models, the function is no longer linear with respect to the parameter `θ`. Optimizing `θ` is a difficult, non-linear problem. That is the key drawback.

    We call these adaptive bases "feature maps." They try to learn a good representation of the data. For example, in image recognition, these features might learn to detect edges, colors, or shapes. This concept of "Learning Representations" is so central to modern AI that one of the top conferences in the field is named ICLR: International Conference on Learning Representations.

    在数学上，我们这样定义自适应基函数模型。标准的线性模型是 `f(x) = w^T φ(x)`。对于自适应模型，我们在基函数中引入一个新的参数 `θ`，使其变为 `f(x) = w^T φ(x; θ)`。这个 `θ` 允许基函数本身的形状根据数据发生改变。

    考虑一个简单的例子，目标函数是 `f* = 1 + x^10`。

    - 如果你使用标准的固定多项式基底（1, x, x^2...），你需要 `m = 11`（即包含直到 10 次幂的所有项）才能完美表示它。
    - 然而，如果使用自适应多项式基底 `φ(x; θ) = (x^θ_0, x^θ_1, ...)`，理论上你只需要 `m = 2`。模型可以简单地“学习”到 `θ_0 = 0`（使第一项变为 1）和 `θ_1 = 10`（使第二项变为 `x^10`）。这非常高效。

    但这有个陷阱。第二个模型要难训练得多。对于线性基函数模型，寻找最优权重 `w` 很容易——这是一个凸优化问题，有闭式解（正规方程）。但对于自适应模型，函数关于参数 `θ` 不再是线性的。优化 `θ` 是一个困难的非线性问题。这是主要的缺点。

    我们将这些自适应基底称为“特征图”（Feature Maps）。它们试图学习数据的良好表示。例如，在图像识别中，这些特征可能学习检测边缘、颜色或形状。“学习表示”（Learning Representations）这个概念在现代人工智能中非常核心，以至于该领域的顶级会议之一就命名为 ICLR：国际学习表示会议（International Conference on Learning Representations）。

  - [14:29 - 14:34] **The XOR Problem: Failure of Linear Models**

    Let's introduce our first neural network by looking at a classic problem: the XOR (Exclusive OR) function. This is a highly non-linear function taking two binary variables, `x1` and `x2`.

    - Input: (0,0) -> Output: 0

    - Input: (0,1) -> Output: 1

    - Input: (1,0) -> Output: 1

    - Input: (1,1) -> Output: 0

      Essentially, the output is 1 if the inputs are different, and 0 if they are the same.

    If you try to fit a linear regression model `f(x) = w^T x + b` to this data using the squared loss, the optimal solution you get is mathematically `w = (0, 0)` and `b = 0.5`. In other words, the model just predicts a constant 0.5 for everything. It fails completely.

    Why? Let's look at the geometry. The classes are not linearly separable—you cannot draw a straight line to separate the 0s from the 1s. Alternatively, look at the gradients:

    - Fix `x1 = 0`. As `x2` goes from 0 to 1, the output goes from 0 to 1. This suggests the weight `w2` should be positive.

    - Fix `x1 = 1`. As `x2` goes from 0 to 1, the output goes from 1 to 0. This suggests the weight `w2` should be negative.

      This is a contradiction. The linear model can't satisfy both conditions, so it gives up and sets `w2 = 0`. The same logic applies to `w1`. Thus, linear models are hopeless for XOR.

    让我们通过一个经典问题来介绍我们的第一个神经网络：异或（XOR）函数。这是一个高度非线性的函数，输入两个二进制变量 `x1` 和 `x2`。

    - 输入：(0,0) -> 输出：0

    - 输入：(0,1) -> 输出：1

    - 输入：(1,0) -> 输出：1

    - 输入：(1,1) -> 输出：0

      本质上，如果输入不同则输出 1，输入相同则输出 0。

    如果你尝试用线性回归模型 `f(x) = w^T x + b` 配合平方损失来拟合这个数据，数学上得到的最优解是 `w = (0, 0)` 和 `b = 0.5`。换句话说，模型对所有输入都预测常数 0.5。这完全失败了。

    为什么？让我们看看几何结构。这两类数据不是线性可分的——你无法画一条直线将 0 和 1 分开。或者，看看梯度：

    - 固定 `x1 = 0`。当 `x2` 从 0 变为 1 时，输出从 0 变为 1。这暗示权重 `w2` 应该是正的。

    - 固定 `x1 = 1`。当 `x2` 从 0 变为 1 时，输出从 1 变为 0。这暗示权重 `w2` 应该是负的。

      这是一个矛盾。线性模型无法同时满足这两个条件，所以它只好放弃并将 `w2` 设为 0。同样的逻辑也适用于 `w1`。因此，线性模型对解决异或问题是无能为力的。

  - [14:34 - 14:40] **Constructing a Neural Network: Composition of Functions**

    I claim that the simplest neural network can solve this. We can model a neural network as the composition of two functions: `f = f^{(2)} ∘ f^{(1)}`.

    1. `h = f^{(1)}(x; W, c)`: This function takes input `x` and produces a vector `h`. We call `h` the "hidden units" because it's an intermediate calculation we don't usually see. The dimension of `h`, let's call it `m`, is the "width" of the hidden layer.
    2. `y = f^{(2)}(h; w, b)`: This function takes `h` and predicts the final output `y`.

    Initially, you might think, "Let's just make both `f^{(1)}` and `f^{(2)}` linear functions." So `f^{(1)}` would be a matrix multiplication `Wx + c`, and `f^{(2)}` would be another linear step `w^T h + b`.

    Does this help? If we expand the math: `f(x) = w^T (Wx + c) + b`. We can simplify this to `(w^T W)x + (w^T c + b)`. This can be rewritten as `w' x + b'`.

    The result is just another linear function! Stacking two linear layers is mathematically equivalent to a single linear layer. It does not increase expressivity. We are still stuck with the same limitations as before. To fix this, we need to introduce nonlinearity.

    我声称最简单的神经网络可以解决这个问题。我们可以将神经网络建模为两个函数的复合：`f = f^{(2)} ∘ f^{(1)}`。

    1. `h = f^{(1)}(x; W, c)`：这个函数接收输入 `x` 并生成向量 `h`。我们将 `h` 称为“隐藏单元”（Hidden Units），因为它是我们通常看不见的中间计算结果。`h` 的维度（我们设为 `m`）被称为隐藏层的“宽度”。
    2. `y = f^{(2)}(h; w, b)`：这个函数接收 `h` 并预测最终输出 `y`。

    起初，你可能会想：“我们就让 `f^{(1)}` 和 `f^{(2)}` 都成为线性函数吧。” 那么 `f^{(1)}` 就是矩阵乘法 `Wx + c`，而 `f^{(2)}` 是另一个线性步骤 `w^T h + b`。

    这有用吗？如果我们展开数学公式：`f(x) = w^T (Wx + c) + b`。这可以简化为 `(w^T W)x + (w^T c + b)`。这又可以重写为 `w' x + b'`。

    结果仅仅是另一个线性函数！堆叠两个线性层在数学上等同于单层线性层。它并没有增加表达能力。我们仍然受困于之前的局限性。为了解决这个问题，我们需要引入非线性。

  - [14:40 - 14:48] **Introducing Activation Functions and Solving XOR**

    The only sensible way to break this chain of linearity is to insert a nonlinear function `g` in between. So now, `f^{(1)}(x) = g(Wx + c)`. This function `g` is called an **activation function**. It is usually a simple function applied element-wise to the vector.

    The most popular choice is the **ReLU** (Rectified Linear Unit) function: `g(z) = max(0, z)`. It simply outputs `z` if positive, and 0 if negative.

    Let's see how this solves XOR. We define our neural network model as `f(x) = w^T max(0, Wx + c) + b`. If we smartly choose the parameters—specifically, let `W` be a 2x2 matrix `[[1, 1], [1, 1]]`, `c` be `[0, -1]`, and `w` be `[1, -2]`—this network perfectly reproduces the XOR truth table.

    What is happening behind the scenes? The hidden layer `h` acts as a feature map.

    - In the original `x` space, the points (0,0), (0,1), (1,0), (1,1) are not linearly separable.

    - In the learned `h` space, the coordinates of these points are transformed. The geometry changes such that the points *become* linearly separable.

      Once the data is in `h` space, the second layer (which is just linear regression) can easily draw a line to separate the classes. This is the power of the hidden units: they learn a representation (feature map) that makes the problem easier to solve.

    打破这种线性链条的唯一合理方法是在中间插入一个非线性函数 `g`。所以现在，`f^{(1)}(x) = g(Wx + c)`。这个函数 `g` 被称为 **激活函数**（Activation Function）。它通常是一个逐元素应用的简单函数。

    最流行的选择是 **ReLU**（线性整流单元）函数：`g(z) = max(0, z)`。简单来说，如果是正数它就输出 `z`，如果是负数就输出 0。

    让我们看看这如何解决 XOR 问题。我们将神经网络模型定义为 `f(x) = w^T max(0, Wx + c) + b`。如果我们巧妙地选择参数——具体来说，设 `W` 为 2x2 矩阵 `[[1, 1], [1, 1]]`，`c` 为 `[0, -1]`，`w` 为 `[1, -2]`——这个网络就能完美地复现 XOR 的真值表。

    幕后发生了什么？隐藏层 `h` 充当了特征图。

    - 在原始的 `x` 空间中，点 (0,0), (0,1), (1,0), (1,1) 是不可线性分割的。

    - 在学习到的 `h` 空间中，这些点的坐标被转换了。几何结构发生了变化，使得这些点 *变得* 线性可分了。

      一旦数据进入 `h` 空间，第二层（仅仅是线性回归）就能轻松画出一条线来分割类别。这就是隐藏单元的力量：它们学习了一种能让问题变得更容易解决的表示（特征图）。

  - [14:48 - 14:55] **Review of Activation Functions & Biological Motivation**

    A general shallow neural network has just two main architectural choices: the dimension of the hidden unit `m`, and the choice of the activation function `g`.

    Besides ReLU, we have other classical choices:

    - **Leaky ReLU:** Instead of being 0 for negative inputs, it has a small positive gradient (e.g., `0.1z`). This is a "less drastic" version of ReLU.
    - **Sigmoid:** Outputs a probability between 0 and 1. Useful for the final layer in binary classification.
    - **Hyperbolic Tangent (Tanh):** Bounded between -1 and 1, zero-centered.

    Why do we like ReLU so much for intermediate layers?

    1. **Computational Speed:** It's just `max(0, z)`.
    2. **Simple Gradients:** The gradient is either 0 or 1. This makes optimization very fast compared to functions like Sigmoid, which can saturate. "Saturation" means that if you stack many Sigmoid layers, the values get squashed towards 1 or 0, causing gradients to vanish and learning to stall.
    3. **Biological Motivation:** ReLU mimics how biological neurons work. In biology, a neuron receives signals via dendrites. It sums them up, but it doesn't fire an electrical impulse unless the total signal exceeds a certain threshold. If the signal is below the threshold (negative input), the neuron stays quiet (output 0). If it exceeds the threshold, it fires. ReLU models this "thresholding" behavior remarkably well.

    I think I've been talking for almost an hour. Let's take a 10-minute break. I realized I forgot to turn on the Q&A function on Poll Everywhere, so I'll enable that now. If you have questions, post them there. See you in 10 minutes.

    一个通用的浅层神经网络主要只有两个架构选择：隐藏单元的维度 `m`，以及激活函数 `g` 的选择。

    除了 ReLU，我们还有其他经典选择：

    - **Leaky ReLU**：对于负输入，它不是输出 0，而是保留一个很小的正梯度（例如 `0.1z`）。这是 ReLU 的一个“不那么激进”的版本。
    - **Sigmoid**：输出 0 到 1 之间的概率。常用于二分类的最后一层。
    - **双曲正切 (Tanh)**：值域在 -1 到 1 之间，以 0 为中心。

    为什么我们在中间层这么喜欢用 ReLU？

    1. **计算速度**：仅仅是 `max(0, z)`。
    2. **简单梯度**：梯度要么是 0 要么是 1。这使得优化速度比 Sigmoid 等函数快得多，后者可能会出现“饱和”。所谓“饱和”是指如果你堆叠很多层 Sigmoid，数值会被挤压向 1 或 0，导致梯度消失，学习停滞。
    3. **生物学动机**：ReLU 模仿了生物神经元的工作方式。在生物学中，神经元通过树突接收信号。它将信号累加，但除非总信号超过某个阈值，否则它不会发射电脉冲。如果信号低于阈值（负输入），神经元保持静默（输出 0）。如果超过阈值，它就会发射。ReLU 非常好地模拟了这种“阈值”行为。

    我想我已经讲了快一个小时了。我们休息 10 分钟吧。我意识到我忘了开启 Poll Everywhere 的问答功能，我现在会把它打开。如果你有任何问题，可以在那里发布。10 分钟后见。

  - [15:04 - 15:10] **Universal Approximation and the Curse of Dimensionality**

    So, we have introduced the shallow neural network structure: input `x`, a linear transformation, a nonlinear activation function to get the hidden unit `h`, and finally a linear transformation to get the output. What exactly do we gain by using this over a standard linear basis model? Interestingly, they share a common strength: the **Universal Approximation Theorem**. Just as a polynomial basis model can approximate any function if you have enough basis functions (large `m`), a shallow neural network can approximate any continuous function arbitrarily well, provided the "width" of the hidden layer `m` is large enough.

    However, the key difference lies in efficiency, specifically regarding the **Curse of Dimensionality**. Imagine you are estimating the area under a curve using Riemann sums (rectangles). To reduce your error by a factor of 5 in 1D, you need 5 times as many pieces. In 3D, you need `5^3 = 125` times as many. The computational cost grows exponentially with the dimension of the data.

    Mathematically, a famous result by Barron (1993) shows that for linear basis models (like polynomials), the error is bounded by `O(m^{-1/d})`. This means to maintain a low error as dimension `d` increases, the number of basis functions `m` must explode exponentially. Linear basis models are extremely inefficient for high-dimensional data.

    In contrast, the same paper proved that for neural networks, the error bound is `O(1/m)`, independent of the input dimension `d`. This is a massive theoretical advantage. It suggests that neural networks are uniquely suited for high-dimensional data, such as images, which explains their state-of-the-art performance in computer vision. The trade-off, of course, is that they are much harder to train than linear models.

    我们已经介绍了浅层神经网络的结构：输入 `x`，经过线性变换和非线性激活函数得到隐藏单元 `h`，最后再经过线性变换得到输出。相比于标准的线性基函数模型，我们究竟获得了什么优势？有趣的是，它们都有一个共同的优点：**通用逼近定理**（Universal Approximation Theorem）。正如多项式基函数模型只要基函数数量（`m`）足够多就能逼近任何函数一样，浅层神经网络只要隐藏层的“宽度”`m` 足够大，也能任意精确地逼近任何连续函数。

    然而，关键的区别在于效率，特别是关于 **维数灾难**（Curse of Dimensionality）。想象一下你用黎曼和（矩形）来估算曲线下的面积。在一维情况下，要将误差缩小 5 倍，你需要 5 倍数量的矩形。但在三维情况下，你需要 `5^3 = 125` 倍。计算成本随着数据的维度呈指数级增长。

    从数学上讲，Barron (1993) 的一个著名结果表明，对于线性基函数模型（如多项式），误差的上界是 `O(m^{-1/d})`。这意味着随着维度 `d` 的增加，为了维持低误差，基函数的数量 `m` 必须呈指数级爆炸式增长。线性基函数模型在处理高维数据时效率极低。

    相比之下，同一篇论文证明了对于神经网络，误差上界是 `O(1/m)`，这与输入维度 `d` 无关。这是一个巨大的理论优势。它表明神经网络特别适合处理高维数据（例如图像），这也解释了为什么它们在计算机视觉领域能取得最先进的性能。当然，代价是它们比线性模型更难训练。

  - [15:10 - 15:15] **Neural Network Architectures for Classification**

    How do we modify the regression architecture for classification? It is exactly the same story as we discussed last week with linear models.

    - **Multi-class Classification:** If we have `K` classes where only one is correct (e.g., identifying a digit), we transform the hidden unit `h` into a `K`-dimensional vector using a matrix `V`, and then apply the **Softmax** function. This ensures the outputs sum to 1, representing a probability distribution.
    - **Multi-label Classification:** Suppose you are observing the sky and want to detect objects: Sun, Moon, and Clouds. You might see the Sun AND Clouds simultaneously. The label would look like `[1, 0, 1]`. Here, the outputs should not sum to 1.

    For multi-label problems, the correct model is to apply the **Sigmoid** function independently to every entry of the output vector `z`. Effectively, you are transforming the problem into `K` separate binary classification tasks. The model predicts the probability of "Sun" independently of "Moon."

    我们如何修改回归架构来进行分类？这与我们上周讨论的线性模型完全一样。

    - **多类分类（Multi-class）：** 如果我们有 `K` 个类别且其中只有一个是正确的（例如识别数字），我们将隐藏单元 `h` 通过矩阵 `V` 变换为一个 `K` 维向量，然后应用 **Softmax** 函数。这确保了输出之和为 1，代表一个概率分布。
    - **多标签分类（Multi-label）：** 假设你在观察天空并想检测物体：太阳、月亮和云。你可能同时看到太阳和云。标签可能看起来像 `[1, 0, 1]`。在这里，输出之和不应该为 1。

    对于多标签问题，正确的模型是对输出向量 `z` 的每个分量独立应用 **Sigmoid** 函数。实际上，你是将这个问题转换成了 `K` 个独立的二分类任务。模型会独立于“月亮”来预测“太阳”出现的概率。

  - [15:15 - 15:27] **Loss Functions: Why Cross-Entropy Beats Square Loss**

    When training via Empirical Risk Minimization, we need to choose a loss function. For regression, Mean Squared Error (MSE) is the standard. For classification, while you *could* theoretically use MSE, **Cross-Entropy** (or Binary Cross-Entropy for multi-label) is far superior.

    Why? Besides the statistical justification (maximizing log-likelihood), the main reason is optimization efficiency, specifically regarding **vanishing gradients**.

    Let's analyze the gradient of the loss with respect to the logit `z` (where `p = sigmoid(z)`).

    - **Square Loss:** If we use MSE, the gradient includes a term `p(1-p)` coming from the derivative of the sigmoid. If the model is completely wrong (e.g., label is 1, but prediction `p` is close to 0), the term `p` becomes 0, causing the entire gradient to vanish. The model stops learning precisely when it needs to learn the most!
    - **Cross-Entropy:** The math works out beautifully such that the gradient with respect to `z` is simply `p - 1` (assuming the label is 1). If the prediction is wrong (`p` is close to 0), the gradient is `-1`. This is a strong, non-zero signal that forces the weights to update.

    So, using Square Loss with Sigmoid/Softmax can lead to learning stagnation due to vanishing gradients. Cross-Entropy avoids this and is numerically much more stable.

    在通过经验风险最小化进行训练时，我们需要选择一个损失函数。对于回归，均方误差（MSE）是标准选择。对于分类，虽然理论上你 *可以* 使用 MSE，但 **交叉熵**（或用于多标签的二元交叉熵）要优越得多。

    为什么？除了统计学上的理由（最大化对数似然）外，主要原因是优化效率，特别是关于 **梯度消失** 的问题。

    让我们分析一下损失函数关于对数几率（logit）`z` 的梯度（其中 `p = sigmoid(z)`）。

    - **平方损失：** 如果我们使用 MSE，梯度中会包含一个来自 Sigmoid 导数的项 `p(1-p)`。如果模型完全错了（例如标签是 1，但预测值 `p` 接近 0），`p`这一项会变成 0，导致整个梯度消失。模型恰恰在最需要学习的时候停止了学习！
    - **交叉熵：** 数学推导的结果非常漂亮，关于 `z` 的梯度仅仅是 `p - 1`（假设标签是 1）。如果预测错了（`p` 接近 0），梯度是 `-1`。这是一个强有力的非零信号，会强迫权重进行更新。

    因此，将平方损失与 Sigmoid/Softmax 结合使用可能会因为梯度消失而导致学习停滞。交叉熵避免了这个问题，并且在数值上更加稳定。

  - [15:27 - 15:38] **Gradient Descent and the Learning Rate**

    Since neural networks are nonlinear, we cannot simply solve `Gradient = 0` to find the optimal weights like we did for linear regression. We must resort to iterative methods, primarily **Gradient Descent**.

    The algorithm is simple: Start with an initial guess `θ_0`. At each step, update the parameters by moving in the direction of the negative gradient: `θ_{k+1} = θ_k - ε * ∇R(θ_k)`. Here, `ε` is the **step size** or **learning rate**.

    How do we choose `ε`?

    - **Too small:** The model makes tiny updates. Convergence will take forever.
    - **Too large:** The model overshoots the minimum and can actually diverge (explode).

    Consider a simple quadratic function `R(θ) = θ^2 / 2`. The gradient update becomes a geometric progression: `θ_k = (1 - ε)^k θ_0`. For this to converge to 0, we need `|1 - ε| < 1`, which implies `ε < 2`. This simple example illustrates that there is a strict upper bound on the learning rate. In practice, `ε` is a hyperparameter you must tune.

    由于神经网络是非线性的，我们不能像处理线性回归那样简单地求解 `梯度 = 0` 来找到最优权重。我们必须求助于迭代方法，主要是 **梯度下降**（Gradient Descent）。

    算法很简单：从初始猜测 `θ_0` 开始。在每一步，通过向负梯度的方向移动来更新参数：`θ_{k+1} = θ_k - ε * ∇R(θ_k)`。这里，`ε` 是 **步长** 或 **学习率**。

    我们要如何选择 `ε`？

    - **太小：** 模型更新极其微小。收敛需要极长的时间。
    - **太大：** 模型会冲过极小值点，甚至导致发散（数值爆炸）。

    考虑一个简单的二次函数 `R(θ) = θ^2 / 2`。梯度更新变成了一个几何级数：`θ_k = (1 - ε)^k θ_0`。为了使其收敛到 0，我们需要 `|1 - ε| < 1`，这意味着 `ε < 2`。这个简单的例子说明学习率存在一个严格的上限。在实践中，`ε` 是一个你必须调整的超参数。

  - [15:38 - 15:45] **Convergence Guarantees and Linear Regression Case**

    For general neural networks, theory guarantees that gradient descent will converge to a point with zero gradient (a stationary point) if the step size is small enough and the loss function is smooth (Lipschitz gradient). However, this only guarantees a **local minimum**, not a global one. In deep learning, finding the global minimum is an unsolved problem, but mysteriously, local minima in high dimensions tend to be "good enough" in practice.

    Let's apply gradient descent to a model we *do* fully understand: **Linear Regression**. Even though we have a closed-form solution (Normal Equations), analyzing gradient descent here gives us intuition.

    The update rule for linear regression is: `w_{k+1} = w_k - ε X^T(X w_k - y)`.

    First, does it converge to the right place? Yes. If we set `w_{k+1} = w_k` (steady state) and solve, we recover the exact Least Squares formula: `w = (X^T X)^{-1} X^T y`.

    对于一般的神经网络，理论保证如果步长足够小且损失函数是平滑的（梯度满足 Lipschitz 条件），梯度下降将收敛到一个梯度为零的点（驻点）。然而，这只能保证是 **局部极小值**，而不是全局极小值。在深度学习中，寻找全局极小值是一个未解之谜，但神奇的是，高维空间中的局部极小值在实践中通常已经“足够好”了。

    让我们把梯度下降应用到一个我们 *确实* 完全理解的模型上：**线性回归**。尽管我们有闭式解（正规方程），但在这里分析梯度下降能给我们提供直觉。

    线性回归的更新规则是：`w_{k+1} = w_k - ε X^T(X w_k - y)`。

    首先，它会收敛到正确的地方吗？是的。如果我们令 `w_{k+1} = w_k`（达到稳态）并求解，我们会还原出精确的最小二乘公式：`w = (X^T X)^{-1} X^T y`。

  - [15:45 - 16:07] **Eigenvalue Analysis of Convergence Speed**

    Now, *when* and *how fast* does it converge? We can model the error `e_k` at step `k` as a power iteration: `e_{k+1} = A e_k`, where the matrix `A = I - ε(X^T X)`.

    For the error to vanish (`e_k → 0`), all eigenvalues of `A` must have a magnitude less than 1.

    Let `μ` be an eigenvalue of the covariance matrix `X^T X`. The corresponding eigenvalue of `A` is `1 - εμ`.

    The convergence condition `|1 - εμ| < 1` implies that the step size must satisfy `ε < 2 / μ_max`, where `μ_max` is the largest eigenvalue of the data's covariance matrix. This gives us a precise theoretical upper bound for the learning rate.

    **Speed of Convergence:** The convergence rate is determined by the "Condition Number," which is the ratio of the largest to the smallest eigenvalue of the covariance matrix (`μ_max / μ_min`).

    - If this ratio is close to 1 (circular contours), convergence is very fast.
    - If this ratio is large (long, narrow oval contours), convergence is very slow because you are limited by the largest eigenvalue (forcing small steps) but need to traverse the direction of the smallest eigenvalue (which needs large steps).

    This analysis explains why optimizing models with ill-conditioned data is so painful, and while strictly true for linear regression, it provides strong intuition for why gradient descent can be slow in neural networks too. Next, we will look at how to implement this in Python.

    现在，它在 *何时* 收敛以及收敛得 *有多快*？我们可以将第 `k` 步的误差 `e_k` 建模为幂迭代：`e_{k+1} = A e_k`，其中矩阵 `A = I - ε(X^T X)`。

    为了让误差消失（`e_k → 0`），矩阵 `A` 的所有特征值的模必须小于 1。

    设 `μ` 为协方差矩阵 `X^T X` 的特征值。`A` 的对应特征值就是 `1 - εμ`。

    收敛条件 `|1 - εμ| < 1` 意味着步长必须满足 `ε < 2 / μ_max`，其中 `μ_max` 是数据协方差矩阵的最大特征值。这为我们提供了学习率的一个精确的理论上限。

    **收敛速度：** 收敛速度取决于“条件数”（Condition Number），即协方差矩阵的最大特征值与最小特征值的比率（`μ_max / μ_min`）。

    - 如果这个比率接近 1（圆形等高线），收敛非常快。
    - 如果这个比率很大（细长的椭圆形等高线），收敛会非常慢。因为你受到最大特征值的限制（迫使你步长很小），但你又需要穿越最小特征值的方向（这本需要大步长）。

    这个分析解释了为什么优化病态（ill-conditioned）数据的模型如此痛苦。虽然这严格适用于线性回归，但它为我们提供了强烈的直觉，解释了为什么梯度下降在神经网络中也可能很慢。接下来，我们将看看如何在 Python 中实现这些。
## Week 3
  - [14:06 - 14:10] **Recap: Adaptive Basis and Shallow Neural Networks**

    So, the basis functions have a parameter θ, which means they can be adapted based on what the data is. We basically want this because, from the experiments we looked at last week, it is not clear what the choice of basis function should be given a specific problem. There doesn't seem to be a single basis function that is good for every target function. So, the idea is that we want to adapt the basis function itself. Last week, we argued that you can view a shallow neural network as a linear basis model with an adaptive basis. This was our definition of a shallow neural network for regression: you have your input x, followed by a linear transform Wx, and then followed by a non-linear activation function g. That result is what we call the hidden unit. Then, you perform linear regression on that hidden unit. If it is a classification problem, the hidden unit would be the same—the term inside the g is the same—but at the end, you would have a softmax regression on the hidden unit. So, the hidden unit is a transformed version of the input; it acts like a basis function which transforms your input. The difference between this and a usual basis function is that this basis function g—or the thing inside the g—has some trainable parameters, specifically the weights W and the bias term b. This allows it to be adapted based on the data. We discussed the advantage of a neural network or adaptive basis model: it does not suffer from the curse of dimensionality. Recall that last week we mentioned that if you just use something like a polynomial basis, it suffers from the curse of dimensionality, meaning the error increases as the dimension increases, and to maintain the same error rate, you need to increase the number of basis functions exponentially. The adaptive basis of a neural network avoids this. The main disadvantage is that you no longer have the Ordinary Least Squares (OLS) formula because of the non-linearity of your model; the model is no longer linear in terms of the parameters you need to train. Therefore, we resort to gradient descent. Our goal is to minimize the empirical risk. Our function f is the neural network with parameters w and b (for the output layer) and W and c (for the hidden layer/adaptive basis). In the demo last week using the MNIST dataset, we used gradient descent to train a shallow neural network for 10-class classification. We "cheated" a little because the training set size was 60,000 samples, and standard gradient descent would be too slow, so we only used 5,000 data points to train our model. Today, we will address the slowness of gradient descent and answer the question of how TensorFlow works—specifically, how it computes derivatives efficiently.

    基函数包含参数 θ，这意味着它们可以根据数据进行调整。我们之所以需要这样做，是因为从上周的实验来看，面对一个给定的问题，并不清楚应该选择什么样的基函数。似乎不存在一种对所有目标函数都有效的通用基函数。因此，我们的想法是调整基函数本身。上周我们讨论过，可以将浅层神经网络视为具有自适应基的线性基模型。这是我们对用于回归的浅层神经网络的定义：首先是输入 x，接着是一个线性变换 Wx，然后是一个非线性激活函数 g。这个结果就是我们所说的隐藏单元。之后，你在隐藏单元上进行线性回归。如果是分类问题，隐藏单元是一样的——g 里面的项是一样的——但在最后你会对隐藏单元进行 Softmax 回归。所以，隐藏单元是输入的变换版本；它就像一个变换输入的基函数。它与普通基函数的区别在于，这个基函数 g（或者说 g 里面的部分）具有一些可训练的参数，即权重 W 和偏置项 b。这使得它能够根据数据进行调整。我们讨论了神经网络或自适应基模型的优势：它不受“维数灾难”的影响。回想一下，上周我们要提到，如果仅使用多项式基，它会遭受维数灾难，这意味着误差会随着维度的增加而增加，如果想保持相同的误差率，就需要指数级地增加基函数的数量。神经网络的自适应基避免了这个问题。主要的缺点是，由于模型的非线性，你不再拥有普通最小二乘法（OLS）公式；模型在待训练参数方面不再是线性的。因此，我们求助于梯度下降。我们的目标是最小化经验风险。我们的函数 f 是神经网络，包含参数 w、b（用于输出层）以及 W、c（用于隐藏层/自适应基）。在上周使用 MNIST 数据集的演示中，我们使用梯度下降训练了一个浅层神经网络来进行 10 类分类。我们当时稍微“作弊”了一下，因为训练集大小是 60,000 个样本，标准梯度下降太慢了，所以我们只使用了 5,000 个数据点来训练模型。今天，我们将解决梯度下降速度慢的问题，并回答 TensorFlow 如何工作的问题——具体来说，它是如何高效计算导数的。

  - [14:11 - 14:15] **Introduction to Deep Fully Connected Neural Networks**

    First off, what is a deep neural network? Or rather, what is a deep fully connected neural network, which is the simplest structure of a deep neural network? Last week we had the shallow neural network: input x goes through a linear transform and a non-linear function g to give you the hidden unit h. If it's a regression output, it's just linear regression on h. So, how do we make a neural network deep? You simply repeat the process. Here we have L layers or L hidden units, h^(1) up to h^(L). The last line is the same as the last step of your shallow neural network; it is just linear regression on the last hidden unit h^(L). Now, what about the first line? The first line is really the same as the first line of the shallow neural network: you have x, you apply a linear transform with W^(1), followed by a non-linear activation function g^(1), and that gives you h^(1). To add layers in between, you use a very simple idea. For h^(2), we take h^(1) as the input, go through a linear transform, and then an activation function. So instead of taking x as the input, you take the previous layer h^(1). For h^(3), you take h^(2) as input, apply a linear transform with W^(3) followed by activation function g^(3), and so on. That is how you build a deep, fully connected neural network. It is extremely simple. Some technicalities to point out: for each layer, the trainable parameters are the Ws and the bs. They are indexed separately for each layer, meaning they are all independent of each other. This gives you lots of degrees of freedom. Also, you could choose the activation functions g to be different for each layer. Let's view this with some pictures. In the full representation (ignoring bias and activation function symbols for clarity), every node is a variable, and every edge is a trainable weight/parameter. In a deep neural network example with depth L=3, you have three hidden units. From each layer to the next, you have a separate matrix W that is trainable. All the arrows you see represent independent degrees of freedom.

    首先，什么是深度神经网络？或者更确切地说，什么是深度全连接神经网络？这是深度神经网络最简单的结构。上周我们讲了浅层神经网络：输入 x 经过线性变换和非线性函数 g 得到隐藏单元 h。如果是回归输出，就只是在 h 上进行线性回归。那么，如何让神经网络变“深”呢？仅仅是重复这个过程。这里我们有 L 层或 L 个隐藏单元，从 h^(1) 到 h^(L)。最后一行与浅层神经网络的最后一步相同，只是对最后一个隐藏单元 h^(L) 进行线性回归。那么第一行呢？第一行实际上与浅层神经网络的第一行相同：你有 x，应用线性变换 W^(1)，接着是非线性激活函数 g^(1)，这给你 h^(1)。为了在中间添加层，我们使用一个非常简单的想法。对于 h^(2)，我们将 h^(1) 作为输入，经过线性变换，然后是激活函数。所以你不再取 x 作为输入，而是取上一层 h^(1)。对于 h^(3)，你取 h^(2) 作为输入，应用 W^(3) 的线性变换，接着是激活函数 g^(3)，依此类推。这就是构建深度全连接神经网络的方法。非常简单。指出一些技术细节：对于每一层，可训练参数是 W 和 b。它们对每一层分别索引，意味着它们彼此独立。这给了你很大的自由度。此外，你也可以为每一层选择不同的激活函数 g。让我们通过图片来看一下。在完整表示中（为了清晰起见忽略偏置和激活函数符号），每个节点是一个变量，每条边是一个可训练的权重/参数。在一个深度为 L=3 的深度神经网络示例中，你有三个隐藏单元。从每一层到下一层，都有一个单独的可训练矩阵 W。你看到的所有箭头都代表独立的自由度。

  - [14:16 - 14:20] **Why Deep Networks? (Oscillatory Functions & Feature Extraction)**

    In Keras, we add layers called "Dense" layers. They are called dense because this is a fully connected neural network, meaning every node in a layer is connected to every other node in the next layer; the connections are as dense as possible. Why do we care about deep neural networks? Why do we want to add layers? Unfortunately, we do not have a completely clear answer to this question mathematically, but there are some partial answers and useful intuitions. We will show in the demo later that deep networks are efficient at representing highly oscillatory functions—functions that are very sensitive to input but are not random. Another point is the phenomenon of "benign overfitting," where despite the number of parameters increasing easily and entering the overfitting regime, the solution is typically not bad, provided you apply appropriate training and regularization strategies. The last point, which is perhaps the most convincing, is that having a deep neural network imposes a "prior" called sequential feature extraction. You are forcing your model to learn hierarchically. For example, if you are doing image recognition, the first layer might learn basic features like edges and colors. The next layer combines these to learn something more complicated, like shapes. The third layer builds on the second to recognize objects. This sequential learning pattern goes from low-level features to high-level features. By composing one layer after another, we force the model to learn in this way. If you believe the task at hand is better learned with this hierarchical structure, then a deep network is good. However, deep networks can be harder to train because they are highly non-convex, meaning you might get stuck in local minima. They are also prone to overfitting because the models are big, and they require more computational resources and time to train.

    在 Keras 中，我们添加称为“Dense”（稠密）的层。之所以叫稠密，是因为这是一个全连接神经网络，意味着一层中的每个节点都与下一层中的每个节点相连；连接尽可能地稠密。我们为什么要关心深度神经网络？为什么要增加层数？遗憾的是，我们在数学上对此并没有一个完全清晰的答案，但有一些部分的解答和有用的直觉。稍后我们将演示，深度网络在表示高度振荡的函数方面非常高效——这些函数对输入非常敏感，但并非随机。另一点是“良性过拟合”（benign overfitting）现象，即尽管参数数量很容易增加并进入过拟合区域，但只要应用适当的训练和正则化策略，解通常并不差。最后一点，也许是最令人信服的，是拥有深度神经网络会施加一种称为“顺序特征提取”的先验。你强迫模型进行分层学习。例如，如果你在做图像识别，第一层可能会学习边缘和颜色等基本特征。下一层将这些特征组合起来学习更复杂的东西，比如形状。第三层建立在第二层的基础上识别物体。这种顺序学习模式是从低级特征到高级特征。通过一层接一层的组合，我们强迫模型以这种方式学习。如果你认为手头的任务更适合这种分层结构，那么深度网络就是好的。然而，深度网络可能更难训练，因为它们是高度非凸的，这意味着你可能会陷入局部极小值。它们也容易过拟合，因为模型很大，而且需要更多的计算资源和时间来训练。

  - [14:20 - 14:27] **Demo: Optimization Landscapes of Shallow vs. Deep Networks**

    We will now look at a demo to understand what kind of functions are nice to represent using a shallow or deep network. We use `plt.contourf` to build a 2D contour map of the function output. First, we build a shallow neural network with one hidden layer of 150 units. The input is 2D. The landscape of the output with respect to the input is rather smooth. It is quite clear how to go from maximum to minimum; darker color means smaller output. Gradient descent on this is pretty reasonable. Note that here we are plotting output with respect to input, not weights, but for shallow networks, the relationship is symmetric. Next, we build a deep neural network. We choose the architecture so the number of parameters is around the same (approx. 600). We use a depth of 3, with 16 units each. The landscape is already much more complex compared to the shallow one. Then, we boost the depth to 6 with 10 units each (total 591 parameters). The landscape shows regions where contour lines are very close together, meaning the output is very sensitive to the input. This suggests deep models are better for functions highly sensitive to input. However, this also implies that the optimization landscape (with respect to weights) is highly non-convex. If you are in a certain region, it might seem correct to turn left or right to decrease output, making gradient descent tricky.

    我们现在来看一个演示，以了解什么样的函数适合用浅层或深度网络来表示。我们使用 `plt.contourf` 来构建函数输出的 2D 等高线图。首先，我们要构建一个具有 150 个单元的单隐藏层的浅层神经网络。输入是 2D 的。输出相对于输入的景观相当平滑。从最大值到最小值的路径非常清晰；深色表示较小的输出。在这种情况下，梯度下降是非常合理的。注意，这里我们要绘制的是输出相对于输入的图，而不是相对于权重的图，但对于浅层网络，这种关系是对称的。接下来，我们构建一个深度神经网络。我们选择架构，使参数数量大致相同（约 600 个）。我们使用深度为 3，每层 16 个单元。与浅层相比，景观已经复杂得多。然后，我们将深度增加到 6，每层 10 个单元（总共 591 个参数）。景观显示的区域中等高线非常密集，这意味着输出对输入非常敏感。这表明深度模型更适合表示对输入高度敏感的函数。然而，这也意味着优化景观（相对于权重）是高度非凸的。如果你处于某个区域，向左或向右转以减少输出似乎都是正确的，这使得梯度下降变得棘手。

  - [14:27 - 14:31] **Demo: Skip Connections (ResNet) and Landscape Smoothing**

    We introduce a particular architecture called "Skip Connections" (or Residual Networks). Mathematically, this means we make it easier for the network to learn the identity function. Instead of just `h_new = F(h_old)`, we define `h_new = h_old + F(h_old)`. If `F` is 0, then it is just identity. Another way to say this is that the gradient is more likely to be closer to 1, making it less likely that the gradient will vanish. It makes the landscape more friendly for gradient descent. We implement this change in the depth-6 network. We repeat the block 5 times: let F be the dense layer based on the last hidden unit, then the new hidden unit h is `h + F`. When we look at the landscape of this residual network, it is much smoother compared to the standard deep network without skip connections. This confirms partial results and empirical evidence that skip connections help with training very deep networks. Note that using the `summary` method, we see that skip connections do not add any trainable parameters—you are just copying the last layer to the next. The total number of parameters remains the same. The demo effectively showed efficient approximation for highly oscillatory functions and how the optimization problem can be hard (highly non-convex landscape).

    我们介绍一种称为“跳跃连接”（Skip Connections，或残差网络）的特定架构。在数学上，这意味着我们使网络更容易学习恒等函数。我们不再只是定义 `h_new = F(h_old)`，而是定义 `h_new = h_old + F(h_old)`。如果 `F` 为 0，那么它就是恒等映射。另一种说法是，梯度更有可能接近 1，从而减少梯度消失的可能性。它使景观对梯度下降更加友好。我们在深度为 6 的网络中实现了这一更改。我们将该模块重复 5 次：设 F 为基于上一个隐藏单元的稠密层，那么新的隐藏单元 h 就是 `h + F`。当我们观察这个残差网络的景观时，与没有跳跃连接的标准深度网络相比，它要平滑得多。这证实了部分结果和经验证据，即跳跃连接有助于训练非常深的网络。注意，使用 `summary` 方法，我们看到跳跃连接不增加任何可训练参数——你只是将上一层复制到下一层。参数总数保持不变。演示有效地展示了对高度振荡函数的高效逼近，以及优化问题可能是多么困难（高度非凸的景观）。

  - [14:32 - 14:38] **Stochastic Gradient Descent (SGD) and Mini-batches**

    Since deep networks have a complex optimization landscape, we tend to use lots of data to train them. Gradient descent turns out to be very slow for large data. We introduce Stochastic Gradient Descent (SGD), which is essentially Monte Carlo gradient descent, to speed things up tremendously. Recall that the empirical risk we want to minimize is the average of the loss functions on each sample. For regression, it involves a sum over N terms, where N is the dataset size. If N is 10,000 or much bigger, gradient descent is extremely slow because you must compute a sum over N things before taking a single step. We want an algorithm whose runtime is independent of the dataset size. SGD is such an algorithm. Instead of computing the actual gradient, we compute an unbiased estimate of the gradient and then take a step. This means on average it will be equal to gradient descent, although there will be variance. In SGD, let γ_k be a uniform random variable from 1 to N (a random index). We update θ by looking at the gradient with respect to this random sample γ_k. The computation is just one loss function, no more sum. It is an unbiased estimate because the expected value of the gradient of R_(γ_k) is precisely the average of the gradients, which is the full gradient. However, looking at just one sample is extremely volatile. To make it less volatile, we use **Mini-batch SGD**. Instead of 1 sample, we look at a batch of size M (e.g., M=64). We average the gradients of this batch. This is more stable than SGD but more expensive (O(M)). It balances the two extremes. In practice, we mostly use mini-batch SGD.

    由于深度网络具有复杂的优化景观，我们要倾向于使用大量数据来训练它们。对于大数据，梯度下降变得非常慢。我们引入随机梯度下降（SGD），这本质上是蒙特卡洛梯度下降，可以极大地加速训练。回想一下，我们要最小化的经验风险是每个样本损失函数的平均值。对于回归，它涉及对 N 项求和，其中 N 是数据集大小。如果 N 是 10,000 或更大，梯度下降会非常慢，因为在迈出一步之前，你必须计算 N 项的总和。我们要一种运行时间与数据集大小无关的算法。SGD 就是这样一种算法。我们不计算实际梯度，而是计算梯度的无偏估计，然后迈出一步。这意味着平均而言它等于梯度下降，尽管会有方差。在 SGD 中，设 γ_k 为从 1 到 N 的均匀随机变量（随机索引）。我们通过查看关于这个随机样本 γ_k 的梯度来更新 θ。计算量只是一个损失函数，不再求和。它是无偏估计，因为 R_(γ_k) 梯度的期望值正是梯度的平均值，即全梯度。然而，仅查看一个样本是非常不稳定的。为了减少波动，我们使用**小批量 SGD**（Mini-batch SGD）。我们要查看大小为 M 的批量（例如 M=64），而不是 1 个样本。我们对这批梯度的梯度求平均。这比 SGD 更稳定，但成本更高（O(M)）。它平衡了两个极端。在实践中，我们大多数时候使用小批量 SGD。

  - [14:38 - 14:48] **Mathematical Analysis: GD on a 1D Quadratic Example**

    Gradient descent will converge to a stationary point (local minimum). If the function is convex, it converges to the global minimum. Does SGD converge? Let's analyze a 1D quadratic example. Last week we saw R(θ) = θ^2 converges if ε < 2. Now consider a variant where we have multiple samples. Let each sample i contribute a loss R_i(θ) = 1/2(θ - θ^(i))^2. Here, θ is the trainable parameter and θ^(i) is some constant (noise). We assume the average of θ^(i) is 0, and the variance of θ^(i) (average of (θ^(i))^2) is 1. You can view this as a linear regression where input x=1 and we are predicting a constant. The true answer should be 0, but there is noise θ^(i) in the data with mean 0 and variance 1. Let's see how GD behaves. The full risk R(θ) is the average of R_i. Expanding the square term (θ - θ^(i))^2, we get θ^2 - 2θθ^(i) + (θ^(i))^2. Summing over all N samples: the θ^2 term becomes Nθ^2; the cross term sum of θ^(i) is 0 (since mean is 0); the last term sum of (θ^(i))^2 is N (since variance is 1). Dividing by 2N, R(θ) simplifies to 1/2 θ^2 + 1/2. The gradient of R is just θ. So, GD iterations are θ_(k+1) = θ_k - εθ_k = (1 - ε)θ_k. This solves to θ_k = (1 - ε)^k θ_0. It converges to 0 as expected.

    梯度下降将收敛到一个驻点（局部极小值）。如果函数是凸的，它收敛到全局极小值。SGD 会收敛吗？让我们分析一个一维二次示例。上周我们看到 R(θ) = θ^2 在 ε < 2 时收敛。现在考虑一个变体，我们有多个样本。设每个样本 i 贡献损失 R_i(θ) = 1/2(θ - θ^(i))^2。这里，θ 是可训练参数，θ^(i) 是某个常数（噪声）。我们要假设 θ^(i) 的平均值为 0，θ^(i) 的方差（(θ^(i))^2 的平均值）为 1。你可以将其视为输入 x=1 的线性回归，我们要预测一个常数。真正的答案应该是 0，但数据中存在均值为 0、方差为 1 的噪声 θ^(i)。让我们看看 GD 的表现。总风险 R(θ) 是 R_i 的平均值。展开平方项 (θ - θ^(i))^2，我们要得到 θ^2 - 2θθ^(i) + (θ^(i))^2。对所有 N 个样本求和：θ^2 项变为 Nθ^2；θ^(i) 的交叉项和为 0（因为均值为 0）；(θ^(i))^2 的最后一项和为 N（因为方差为 1）。除以 2N 后，R(θ) 简化为 1/2 θ^2 + 1/2。R 的梯度就是 θ。因此，GD 迭代为 θ_(k+1) = θ_k - εθ_k = (1 - ε)θ_k。解得 θ_k = (1 - ε)^k θ_0。正如预期的那样，它收敛到 0。

  - [14:49 - 14:57] **Mathematical Analysis: SGD Convergence in Expectation**

    Now, how does SGD behave? In SGD, you look at the gradient with respect to just one random sample R_i. The gradient of R_i with respect to θ is (θ - θ^(i)). The SGD iteration is θ_(k+1) = θ_k - ε(θ_k - θ^(γ_k)), where γ_k is a random index drawn uniformly from 1 to N. This simplifies to θ_(k+1) = (1 - ε)θ_k + εθ^(γ_k). Let's try to get a recursion relationship between θ_k and θ_0. Going down one step: θ_k = (1 - ε)θ_(k-1) + εθ^(γ_(k-1)). Going further down, you eventually see the pattern: θ_k = (1 - ε)^k θ_0 + ε * [sum from j=1 to k of (1 - ε)^(j-1) * θ^(γ_(k-j))]. The first term is the same as Gradient Descent (deterministic). The second term contains all the randomness from the noise θ^(γ). Note that the expectation of θ^(γ) is the average of θ^(i), which is 0. Therefore, if we take the expected value of θ_k, the second term vanishes (sum of 0s). E[θ_k] = (1 - ε)^k θ_0. In expectation, SGD is equal to Gradient Descent. It confirms what we said: on average, it behaves like GD.

    现在，SGD 表现如何？在 SGD 中，你只查看关于一个随机样本 R_i 的梯度。R_i 关于 θ 的梯度是 (θ - θ^(i))。SGD 迭代为 θ_(k+1) = θ_k - ε(θ_k - θ^(γ_k))，其中 γ_k 是从 1 到 N 均匀抽取的随机索引。这简化为 θ_(k+1) = (1 - ε)θ_k + εθ^(γ_k)。让我们尝试找出 θ_k 和 θ_0 之间的递归关系。向下推一步：θ_k = (1 - ε)θ_(k-1) + εθ^(γ_(k-1))。继续向下推，你最终会看到规律：θ_k = (1 - ε)^k θ_0 + ε * [从 j=1 到 k 的 (1 - ε)^(j-1) * θ^(γ_(k-j)) 求和]。第一项与梯度下降（确定性）相同。第二项包含来自噪声 θ^(γ) 的所有随机性。注意，θ^(γ) 的期望值是 θ^(i) 的平均值，即 0。因此，如果我们取 θ_k 的期望值，第二项消失（0 的总和）。E[θ_k] = (1 - ε)^k θ_0。在期望上，SGD 等于梯度下降。这证实了我们要说的话：平均而言，它的表现像 GD。

  - [14:57 - 15:12] **Mathematical Analysis: SGD Variance and Learning Rate Trade-off**

    What about the variance? To investigate this, we compute E[θ_k^2]. Squaring the expression for θ_k, we get three parts: 1) The square of the deterministic term: (1 - ε)^(2k) θ_0^2. 2) The cross term: 2 * deterministic_part * random_sum. Since the expectation of the random sum is 0, the cross term vanishes in expectation. 3) The square of the random sum. This becomes ε^2 * double sum (indices j and l). Due to independence of γ_k choices, E[θ^(γ_a) * θ^(γ_b)] is 0 if a is not equal to b. If indices match (j=l), E[(θ^(γ))^2] is the variance of the noise, which is 1. So the double sum collapses to a single sum where j=l. We get ε^2 * [sum from j=1 to k of (1 - ε)^(2j-2)]. This is a geometric series with ratio (1 - ε)^2. Using the geometric series formula, as k goes to infinity (assuming convergence), this sum converges to ε^2 / [1 - (1 - ε)^2]. Expanding the denominator: 1 - (1 - 2ε + ε^2) = 2ε - ε^2. So the variance converges to ε^2 / (2ε - ε^2) = ε / (2 - ε). For small ε, this is approximately ε/2. This means the standard deviation is of order √ε. Conclusion: SGD converges to a region around 0 but will fluctuate forever. The fluctuation error is of order √ε. If you choose a large ε, you converge fast but have large fluctuations (high noise). If you choose a small ε, it takes longer to converge, but the fluctuations around 0 will be smaller.

    方差呢？为了研究这一点，我们要计算 E[θ_k^2]。对 θ_k 的表达式求平方，我们要得到三部分：1）确定性项的平方：(1 - ε)^(2k) θ_0^2。2）交叉项：2 * 确定性部分 * 随机和。由于随机和的期望为 0，交叉项在期望中消失。3）随机和的平方。这变成 ε^2 * 双重求和（索引 j 和 l）。由于 γ_k 选择的独立性，如果 a 不等于 b，则 E[θ^(γ_a) * θ^(γ_b)] 为 0。如果索引匹配（j=l），E[(θ^(γ))^2] 就是噪声的方差，即 1。所以双重求和坍缩为 j=l 的单重求和。我们得到 ε^2 * [从 j=1 到 k 的 (1 - ε)^(2j-2) 求和]。这是一个比率为 (1 - ε)^2 的几何级数。使用几何级数公式，当 k 趋于无穷大时（假设收敛），该和收敛于 ε^2 / [1 - (1 - ε)^2]。展开分母：1 - (1 - 2ε + ε^2) = 2ε - ε^2。所以方差收敛于 ε^2 / (2ε - ε^2) = ε / (2 - ε)。对于小的 ε，这大约是 ε/2。这意味着标准差是 √ε 量级的。结论：SGD 收敛到 0 附近的区域，但会永远波动。波动误差是 √ε 量级的。如果你选择大的 ε，收敛速度快，但波动大（噪声高）。如果你选择小的 ε，收敛需要更长的时间，但 0 附近的波动会更小。

  - [15:25 - 15:28] **SGD Convergence Issues and Learning Rate Decay**

    In the example we discussed, the gradient pulls you towards a central point, say negative 1.8. However, because of the randomness in sampling, you will experience fluctuations in this region even after the algorithm converges. The specific sample you pick determines the direction of the fluctuation. Looking at the plots, when the learning rate epsilon is 0.5, we seem to converge after a single iteration, but we are left with significant noise around zero. If we decrease the learning rate by a factor of 5 to 0.1, it takes 3 or 4 steps to converge, but the noise around zero is much smaller. This aligns with our mathematical derivation that the standard deviation of the noise is of the order of the square root of epsilon. The trade-off is clear: a smaller epsilon means slower convergence but smaller fluctuations. However, the fluctuations never essentially go to zero with a fixed learning rate. To resolve this and ensure the error actually goes to zero, we use a decaying learning rate. Instead of a constant epsilon, we use epsilon_k, which starts large for fast initial convergence and then decreases as k increases. This dampens the fluctuations over time. A sufficient mathematical condition for this to work is that the sum of the learning rates must diverge (be infinite), while the sum of their squares must converge to a finite value. A classic example satisfying this is the harmonic series, where the learning rate is proportional to 1/k.

    在我们讨论的例子中，梯度会将你拉向一个中心点，比如负 1.8。然而，由于采样的随机性，即使算法收敛后，你仍会在该区域经历波动。你选取的具体样本决定了波动的方向。观察图表，当学习率 epsilon 为 0.5 时，我们似乎在一次迭代后就收敛了，但在零点附近留下了巨大的噪声。如果我们把学习率减小 5 倍变成 0.1，收敛需要 3 到 4 步，但零点附近的噪声要小得多。这与我们的数学推导一致，即噪声的标准差与 epsilon 的平方根成正比。权衡显而易见：较小的 epsilon 意味着收敛较慢，但波动也较小。然而，使用固定学习率时，波动本质上永远不会变为零。为了解决这个问题并确保误差真正趋向于零，我们使用衰减学习率。我们不再使用常数 epsilon，而是使用 epsilon_k，它在初始阶段较大以实现快速收敛，随着 k 的增加而减小。这会随着时间的推移抑制波动。使其有效的充分数学条件是：学习率之和必须发散（为无穷大），而学习率的平方和必须收敛到一个有限值。一个满足此条件的经典例子是调和级数，即学习率与 1/k 成正比。

  - [15:28 - 15:30] **Advanced Optimizers: Adaptive Rates and Adam**

    We have solved the issue of gradient descent being too slow for large data by introducing stochastic and mini-batch gradient descent. Now, we want to improve SGD further by incorporating additional ideas. One major concept is **momentum**, which we will discuss in detail. There are also **adaptive learning rate** algorithms, where the learning rate is not global but is different for every trainable parameter. The user sets a maximum learning rate, and the model varies it for each parameter during training. Examples of this include Adagrad and Adadelta. A very popular optimizer nowadays is **Adam**, which combines the ideas of both momentum and adaptive learning rates. It is generally considered a good default optimizer to use.

    我们通过引入随机梯度下降和小批量梯度下降，解决了梯度下降在大数据上太慢的问题。现在，我们希望通过通过结合其他思想来进一步改进 SGD。一个主要的概念是**动量**（Momentum），我们将对此进行详细讨论。还有**自适应学习率**算法，其中的学习率不是全局的，而是针对每个可训练参数都不同。用户设定一个最大学习率，模型在训练过程中为每个参数调整学习率。这方面的例子包括 Adagrad 和 Adadelta。如今非常流行的一个优化器是 **Adam**，它结合了动量和自适应学习率的思想。通常认为它是一个很好的默认优化器。

  - [15:30 - 15:39] **Momentum: Physics Inspiration and Implementation**

    Let's recap our analysis of gradient descent on linear regression. We noted that the convergence rate depends on the ratio of the eigenvalues of the covariance matrix. Consider a 2D optimization landscape where one direction is much steeper than the other—imagine a valley that is narrow in one direction and wide in another. If the eigenvalue lambda is large, the gradient in that vertical direction will be very large. Standard gradient descent will zigzag wildly across the valley, making slow progress towards the minimum. It is inefficient. Now, compare this to a ball rolling down a hill in the real world. A ball has mass and inertia, meaning it has momentum. It does not zigzag; instead, it accumulates velocity in the downhill direction and resists sudden changes. Momentum in optimization is inspired by this physics. We treat the gradient as an external force (gravity) and add a friction term. In the update rule, we introduce a velocity variable v. The new velocity v_(k+1) is a combination of the old velocity (scaled by a momentum parameter alpha) and the current gradient. The parameter alpha, typically between 0 and 1, acts as the "memory" of the system. If alpha is 0, it is just standard gradient descent. If alpha is non-zero, the system remembers the previous direction, smoothing out the zigzagging behavior. This makes the system a second-order system, as the update effectively depends on the history of gradients, not just the current one.

    让我们回顾一下对线性回归梯度下降的分析。我们注意到收敛速度取决于协方差矩阵特征值的比率。考虑一个二维优化景观，其中一个方向比另一个方向陡峭得多——想象一个在一个方向上狭窄而在另一个方向上宽阔的山谷。如果特征值 lambda 很大，那么该垂直方向上的梯度就会非常大。标准的梯度下降会在山谷中剧烈地呈“之”字形移动，向最小值的推进非常缓慢。这是低效的。现在，将其与现实世界中滚下山坡的球进行比较。球有质量和惯性，这意味着它有动量。它不会呈“之”字形移动；相反，它会在下坡方向积累速度并抵抗突然的方向变化。优化中的动量正是受此物理学启发。我们将梯度视为外力（重力）并添加摩擦项。在更新规则中，我们引入速度变量 v。新速度 v_(k+1) 是旧速度（由动量参数 alpha 缩放）与当前梯度的组合。参数 alpha（通常在 0 到 1 之间）充当系统的“记忆”。如果 alpha 为 0，它就是标准的梯度下降。如果 alpha 不为零，系统会记住先前的方向，从而平滑“之”字形行为。这使得系统成为二阶系统，因为更新实际上依赖于梯度的历史，而不仅仅是当前的梯度。

  - [15:39 - 15:42] **Practical Implementation: Mini-batch Sampling and Epochs**

    Regardless of whether you use momentum or Adam, you will inevitably deal with a hyperparameter called **batch size**. This determines whether you are doing stochastic, mini-batch, or full batch gradient descent. In practice, we typically use mini-batch gradient descent with a batch size like 64 or 128. A larger batch size is more stable but slower to compute. How is the sampling actually done? We do not just pick random samples with replacement. Instead, we perform **sampling without replacement**. We shuffle the entire dataset and then iterate through it in chunks of size M. One full sweep through the dataset is called an **epoch**. If your dataset size is N and batch size is M, you take N/M steps of gradient descent in one epoch. For example, if N=1000 and M=100, one epoch consists of 10 update steps. In frameworks like Keras, you specify the number of epochs for training.

    无论你是使用动量还是 Adam，你都不可避免地要处理一个称为**批量大小**（batch size）的超参数。这决定了你是进行随机、小批量还是全批量梯度下降。在实践中，我们通常使用批量大小为 64 或 128 的小批量梯度下降。较大的批量大小更稳定，但计算速度较慢。实际采样是如何进行的？我们不是简单地进行有放回的随机采样。相反，我们执行**无放回采样**。我们打乱整个数据集，然后以大小为 M 的块进行迭代。对数据集的一次完整遍历称为一个 **Epoch**（轮）。如果你的数据集大小为 N，批量大小为 M，那么在一个 epoch 中你要进行 N/M 步梯度下降。例如，如果 N=1000 且 M=100，一个 epoch 包含 10 个更新步骤。在 Keras 等框架中，你需要指定训练的 epoch 数量。

  - [15:42 - 15:47] **Computational Graphs: Visualizing Dependencies**

    All these optimization variants hinge on the ability to compute the gradient of the loss function on a sample. For deep neural networks, the answer to doing this efficiently is the **Backpropagation** algorithm. To understand it, we first need to understand Computational Graphs. In these graphs, nodes represent variables, and edges represent operations, showing the dependency between variables clearly.

    - *Example 1:* Simple function z = xy. X and Y are input nodes pointing to Z.

    - *Example 2:* Logistic Regression. Inputs X, W, b lead to intermediate outputs. u1 = X dot W, u2 = u1 + b, and finally y_hat = sigma(u2).

    - *Example 3:* Dense Layer. Similar to above but with matrix multiplication and a ReLU activation.

    - *Example 4:* Regularization. One branch computes the prediction y_hat, while another computes the regularization term r = lambda * ||W||^2 (sum of squared weights).

      These graphs clarify which variables depend on which, which is crucial for computing derivatives efficiently.

    所有这些优化变体都取决于计算样本上损失函数梯度的能力。对于深度神经网络，高效执行此操作的答案是**反向传播**（Backpropagation）算法。为了理解它，我们首先需要理解计算图（Computational Graphs）。在这些图中，节点代表变量，边代表操作，清晰地展示了变量之间的依赖关系。

    - *例子 1：* 简单函数 z = xy。X 和 Y 是指向 Z 的输入节点。

    - *例子 2：* 逻辑回归。输入 X、W、b 产生中间输出。u1 = X 点乘 W，u2 = u1 + b，最后 y_hat = sigma(u2)。

    - *例子 3：* 稠密层。与上述类似，但包含矩阵乘法和 ReLU 激活函数。

    - *例子 4：* 正则化。一个分支计算预测值 y_hat，另一个分支计算正则化项 r = lambda * ||W||^2（权重的平方和）。

      这些图阐明了变量间的依赖关系，这对于高效计算导数至关重要。

  - [15:47 - 15:52] **The Inefficiency of Naive Chain Rule**

    A deep neural network is a composition of many functions. Mathematically, if z = f(y) and y = g(x), the chain rule tells us dz/dx = (dz/dy) * (dy/dx). This applies to vectors as well using Jacobians and summations. While the chain rule gives us the correct formula, applying it naively is computationally inefficient because it involves repeating many calculations. Consider a function f(f(f(w))). The derivative would be f'(f(f(w))) * f'(f(w)) * f'(w). Notice that the term f(w) appears multiple times in different places. If we do not store it, we have to recompute it from scratch every time it is needed. In a deep network, this redundancy leads to quadratic computational cost. The efficient approach—Backpropagation—is to compute the forward pass once and **store the intermediate values in memory**. When we later compute the gradients during the backward pass, we can simply retrieve these values from memory instead of recomputing them. This trades memory for computation speed.

    深度神经网络是许多函数的组合。在数学上，如果 z = f(y) 且 y = g(x)，链式法则告诉我们 dz/dx = (dz/dy) * (dy/dx)。这也适用于使用雅可比矩阵和求和的向量情况。虽然链式法则给了我们正确的公式，但简单地应用它在计算上是低效的，因为它涉及重复许多计算。考虑一个函数 f(f(f(w)))。其导数将是 f'(f(f(w))) * f'(f(w)) * f'(w)。注意，项 f(w) 在不同地方多次出现。如果我们不存储它，每次需要时都必须从头重新计算。在深度网络中，这种冗余会导致二次方的计算成本。高效的方法——反向传播——是一次性计算前向传播并将**中间值存储在内存中**。当我们稍后在反向传播过程中计算梯度时，我们可以简单地从内存中检索这些值，而不是重新计算它们。这是用内存换取计算速度。

  - [15:52 - 16:09] **Backpropagation Walkthrough: 1D Linear Network Example**

    Let's demonstrate Backpropagation using a simple, 1D linear neural network with no activation functions.

    - **Setup:** h1 = w1*x, h2 = w2*h1, h3 = w3*h2, and output y_hat = w*h3. The goal is to compute the gradient of the loss R with respect to all weights (w1, w2, w3, w).

    **Step 1: Forward Propagation**

    We compute the outputs layer by layer given the data x and current weights.

    1. Compute h1 = x * w1. **Store h1 in memory.**

    2. Compute h2 = w2 * h1 (using stored h1). **Store h2 in memory.**

    3. Compute h3 = w3 * h2 (using stored h2). **Store h3 in memory.**

    4. Compute y_hat = w * h3. **Store y_hat in memory.**

       Now we have four variables stored in memory.

    **Step 2: Backward Propagation (Computing Deltas)**

    We compute derivatives of the loss R with respect to hidden states, moving backward from the output.

    1. Compute dR/dy_hat. **Store as p_hat.**
    2. Compute dR/dh3 = (dR/dy_hat) * (dy_hat/dh3) = p_hat * w. **Store as p3.**
    3. Compute dR/dh2 = (dR/dh3) * (dh3/dh2) = p3 * w3. **Store as p2.**
    4. Compute dR/dh1 = (dR/dh2) * (dh2/dh1) = p2 * w2. **Store as p1.**

    **Step 3: Computing Gradients wrt Parameters**

    Now we compute the gradients we actually need for updates (dR/dw_i) by combining stored values from Step 1 and Step 2.

    1. dR/dw = (dR/dy_hat) * (dy_hat/dw) = p_hat * h3. (Both p_hat and h3 are in memory).
    2. dR/dw3 = (dR/dh3) * (dh3/dw3) = p3 * h2.
    3. dR/dw2 = (dR/dh2) * (dh2/dw2) = p2 * h1.
    4. dR/dw1 = (dR/dh1) * (dh1/dw1) = p1 * x.

    By doing this, we never repeat unnecessary computations. The cost is linear with respect to the number of nodes (just two passes: one forward, one backward).

    让我们使用一个没有激活函数的简单一维线性神经网络来演示反向传播。

    - **设置：** h1 = w1*x，h2 = w2*h1，h3 = w3*h2，输出 y_hat = w*h3。目标是计算损失 R 相对于所有权重（w1, w2, w3, w）的梯度。

    **步骤 1：前向传播**

    给定数据 x 和当前权重，我们逐层计算输出。

    1. 计算 h1 = x * w1。**将 h1 存储在内存中。**

    2. 计算 h2 = w2 * h1（使用存储的 h1）。**将 h2 存储在内存中。**

    3. 计算 h3 = w3 * h2（使用存储的 h2）。**将 h3 存储在内存中。**

    4. 计算 y_hat = w * h3。**将 y_hat 存储在内存中。**

       现在我们在内存中存储了四个变量。

    **步骤 2：反向传播（计算误差项）**

    我们从输出向后移动，计算损失 R 相对于隐藏状态的导数。

    1. 计算 dR/dy_hat。**存储为 p_hat。**
    2. 计算 dR/dh3 = (dR/dy_hat) * (dy_hat/dh3) = p_hat * w。**存储为 p3。**
    3. 计算 dR/dh2 = (dR/dh3) * (dh3/dh2) = p3 * w3。**存储为 p2。**
    4. 计算 dR/dh1 = (dR/dh2) * (dh2/dh1) = p2 * w2。**存储为 p1。**

    **步骤 3：计算相对于参数的梯度**

    现在，我们通过组合步骤 1 和步骤 2 中的存储值来计算更新所需的实际梯度 (dR/dw_i)。

    1. dR/dw = (dR/dy_hat) * (dy_hat/dw) = p_hat * h3。（p_hat 和 h3 都在内存中）。
    2. dR/dw3 = (dR/dh3) * (dh3/dw3) = p3 * h2。
    3. dR/dw2 = (dR/dh2) * (dh2/dw2) = p2 * h1。
    4. dR/dw1 = (dR/dh1) * (dh1/dw1) = p1 * x。

    这样做，我们永远不需要重复不必要的计算。成本与节点数量成线性关系（只需两次传递：一次前向，一次后向）。

  - [16:09 - 16:19] **Generalization to Non-Linearity and TensorFlow**

    The previous example was linear and scalar, but the logic holds for real networks. If you introduce non-linearity (activation functions f), the only difference is that you must include the derivative of the activation function, f', in the chain rule calculation. If you have vectors instead of scalars, the derivatives become gradients or Jacobians, but the structure remains the same. The key takeaway is that Backpropagation creates memory to gain computational efficiency. In frameworks like TensorFlow, you do not need to implement this yourself. You only need to provide the forward function f(x) and its gradient. TensorFlow handles the computational graph and the backpropagation automatically. There are variants that trade off less memory for more computation (e.g., checkpointing) if memory is a constraint, which are discussed in Chapter 6.5 of the Deep Learning textbook.

    前面的例子是线性和标量的，但这套逻辑适用于真实的网络。如果你引入非线性（激活函数 f），唯一的区别是你必须在链式法则计算中包含激活函数的导数 f'。如果你有向量而不是标量，导数就会变成梯度或雅可比矩阵，但结构保持不变。关键的结论是，反向传播通过占用内存来获得计算效率。在 TensorFlow 这样的框架中，你不需要自己实现这一点。你只需要提供前向函数 f(x) 及其梯度。TensorFlow 会自动处理计算图和反向传播。如果内存受限，还有一些变体可以用较少的内存换取较多的计算（例如检查点技术），深度学习教科书的 6.5 章对此进行了讨论。

  - [16:19 - 16:21] **Lecture Conclusion**

    To summarize, today we looked at Deep Neural Networks (compositions of shallow layers) and discussed why they are useful (sequential feature extraction, representing sensitive functions). We then addressed the problem of training on large datasets by introducing Stochastic Gradient Descent, analyzing its variance, and introducing learning rate decay and momentum. Finally, we explained the Backpropagation algorithm as an efficient means to calculate gradients by storing intermediate states. That is all for today. Next week, we will talk about Convolutional Neural Networks (CNNs), which are really powerful compared to everything we have talked about so far.

    总结一下，今天我们研究了深度神经网络（浅层的组合），并讨论了它们为什么有用（顺序特征提取，表示敏感函数）。然后，我们通过引入随机梯度下降解决了在大型数据集上训练的问题，分析了它的方差，并引入了学习率衰减和动量。最后，我们解释了反向传播算法，作为一种通过存储中间状态来高效计算梯度的方法。今天的课就到这里。下周，我们将讨论卷积神经网络（CNN），与我们目前讨论的所有内容相比，它确实非常强大。
## Week 4
  - [14:04 - 14:05] **Recap: Output Layers and Activation Functions**

    To wrap up our discussion on Fully Connected Neural Networks (FCNNs) from the last lecture, remember that the design of your final layer depends heavily on the dimensions of your weight matrix `W` and your choice of activation function. As you stack multiple layers, the configuration of the final output layer is dictated by the nature of your problem: is it a regression task or a classification task? If you are dealing with a regression problem, the last layer is typically just a linear regression layer on the last hidden unit. However, if it is a classification problem, you would use a Softmax regression output. We previously discussed why deep neural networks might perform better on specific complex functions, such as highly oscillatory functions. However, historically speaking, the first time a neural network model actually achieved state-of-the-art performance was in the field of Computer Vision, and this happened before the dominance of Convolutional Neural Networks (CNNs). Today, we will explore the computational structure of these networks.

    为了总结上一讲关于全连接神经网络（FCNN）的内容，大家要记住，最后一层的设计很大程度上取决于权重矩阵`W`的维度以及激活函数的选择。当你堆叠多层网络时，最终输出层的配置取决于你的任务性质：是回归问题还是分类问题？如果是回归问题，最后一层通常只是最后一个隐藏单元上的线性回归层。如果是分类问题，则会使用 Softmax 回归输出。我们之前讨论过为什么深度神经网络在处理某些类型的函数（如高度震荡函数）时表现更好。然而，从历史角度来看，神经网络模型第一次真正达到“最先进”（state-of-the-art）性能是在计算机视觉领域，而且这发生在卷积神经网络（CNN）占据主导地位之前。今天，我们将探讨这些网络的计算结构。

  - [14:05 - 14:09] **Permutation Invariance of the FCNN Hypothesis Space**

    To motivate the need for Convolutional Neural Networks, we must first look at a peculiar property of Fully Connected Neural Networks. First, let us define a permutation. If we have `N` objects, a permutation is a one-to-one function, or a bijection, on these objects. For example, if the objects are labeled 1 through 9, a permutation is simply a reordering of these objects. Now, consider the hypothesis space of an FCNN. There is a possibly surprising property called the "Permutation Invariance of the Hypothesis Space." The statement is as follows: Suppose we have a function `f` in our hypothesis space (a trained FCNN). If given any random permutation `P` that permutes the indices of the `d`-dimensional input `x`, there exists another function `f_P` in the same hypothesis space that can act on the original dataset and produce the same output. In other words, if I train an FCNN `f` on the original data, and then your friend messes up the data by permuting the input dimensions, there exists another neural network `f_P`—with the same architecture but possibly different weights—that will give the exact same performance on the permuted data. This implies that FCNNs essentially do not care about the ordering of the signal components; if we can fit one permutation, we can fit any permutation.

    为了引出卷积神经网络的必要性，我们需要先看看全连接神经网络的一个奇特属性。首先，我们要定义什么是排列（Permutation）。如果我们有`N`个对象，排列就是这些对象上的一一对应函数，或者说双射。例如，如果对象标记为1到9，排列就是这些对象的重新排序。现在，考虑FCNN的假设空间。有一个可能令人惊讶的属性，叫做“假设空间的排列不变性”。这个陈述是这样的：假设我们在假设空间中有一个函数`f`（一个训练好的FCNN）。如果给定任意一个随机排列`P`来打乱`d`维输入`x`的索引，那么在同一个假设空间中必然存在另一个函数`f_P`，它能够处理原始数据集并产生相同的输出。换句话说，如果我在原始数据上训练了一个FCNN `f`，然后你的朋友通过打乱输入维度的顺序搞乱了数据，依然存在另一个神经网络`f_P`——架构相同但权重可能不同——能够在排列后的数据上表现出完全相同的性能。这意味着FCNN本质上并不关心信号分量的顺序；如果我们能拟合一种排列，我们就能拟合任何一种排列。

  - [14:09 - 14:14] **Intuition: Dense Features vs. Structural Data**

    Why is this true? Consider a simple shallow FCNN where the input `x` is 2-dimensional. Suppose from the first input dimension `x_1` we have blue weights, and from the second `x_2` we have red weights connecting to the next layer. If we permute the input by swapping `x_1` and `x_2`, we can simply swap the corresponding weights (drag the edges along with the nodes) to create a new network that produces the identical output. This raises a question: Is this sensible? Let's look at a dataset predicting concrete strength based on features like cement, water, and fly ash. Here, the order of columns (features) does not matter structurally—these are "dense features" where each stands alone. Permuting the columns doesn't lose information, so an FCNN works fine. However, consider an image classification task (e.g., Cat vs. Dog). Each input dimension is a pixel coordinate. If we randomly permute the pixels, we destroy the spatial structure, turning the image into noise. Humans cannot classify this noise, but the FCNN theory says the network can learn the permuted version just as well as the original. This is philosophically unsettling because it suggests FCNNs ignore spatial and temporal structures (like in time-series data of silver prices), treating inputs merely as a bag of global features like pixel intensity rather than local shapes or edges.

    为什么会这样？考虑一个简单的浅层FCNN，输入`x`是二维的。假设从第一个输入维度`x_1`出发我们有蓝色的权重，从第二个`x_2`出发有红色的权重连接到下一层。如果我们通过交换`x_1`和`x_2`来排列输入，我们只需相应地交换权重（把连线和节点一起拖动），就能构建一个新的网络产生完全相同的输出。这引出了一个问题：这合理吗？让我们看一个通过水泥、水、粉煤灰等特征预测混凝土强度的数据集。在这里，列（特征）的顺序在结构上并不重要——这些是“密集特征”，每一个都独立存在。排列列的顺序不会丢失信息，所以FCNN处理得很好。然而，考虑一个图像分类任务（例如猫与狗）。每个输入维度都是一个像素坐标。如果我们随机打乱像素，我们就破坏了空间结构，把图像变成了噪声。人类无法对这种噪声进行分类，但FCNN的理论表明，网络可以像学习原始图像一样好地学习排列后的版本。这在哲学上令人不安，因为它表明FCNN忽略了空间和时间结构（比如银价的时间序列数据），仅仅将输入视为一堆全局特征（如像素强度），而不是局部的形状或边缘。

  - [14:14 - 14:23] **Student Poll: Fixed Weights and Permutation**

    Let's do a quick poll to test your understanding. Suppose we train a fully connected neural network with a certain architecture on your original data. After training, we **fix the weights**. Then, your friend permutes the input data. The question is: Will the output of your fixed-weight neural network change on this permuted dataset? The results look 50/50, so let me clarify. The answer is actually **false** (the output *will* change). This was a bit of a trick question. The "Permutation Invariance" discussed earlier is a property of the **hypothesis space**, not of a specific trained model instance. It means there *exists* a configuration of weights that works, but you would need to **retrain** (or manually permute the weights of) the model to adapt to the new data. If the weights are fixed and you scramble the input, the specific model instance you currently have will fail. The model itself is not permutation invariant; the capacity of the architecture is.

    我们要进行一个小测验来测试大家的理解。假设我们在原始数据上训练了一个具有特定架构的全连接神经网络。训练结束后，我们将**权重固定**。然后，你的朋友打乱了输入数据。问题是：这个固定权重的神经网络在排列后的数据集上的输出会改变吗？结果看起来是一半一半，所以我来澄清一下。答案其实是**错误**（输出*会*改变）。这是一个陷阱题。我们要讨论的“排列不变性”是**假设空间**的一个属性，而不是某个特定训练好的模型实例的属性。它的意思是*存在*一组权重配置可以起作用，但你需要**重新训练**（或者手动排列权重）模型来适应新数据。如果权重是固定的，而你打乱了输入，你当前的这个模型实例就会失效。模型本身并不是排列不变的；是架构的容量具有这种不变性。

  - [14:23 - 14:35] **Demo: MNIST Classification and Permutation**

    Let's look at a code demo using the MNIST dataset (handwritten digits). We define a preprocessing pipeline using the Sequential API (what I referred to as the stack of layers). We introduce two new layers: a `Lambda` layer for custom normalization (dividing pixel values by 255) and a `Flatten` layer to convert the 28x28 image matrix into a vector for the dense layers. Using the Adam optimizer and cross-entropy loss, we achieve 100% accuracy on the training set and 98% on the test set. Now, we simulate the permutation by applying random pairwise swaps to the pixels. When we visualize the permuted "5" and "0", they look like random noise. If we feed this shuffled data into our *original* trained model (without retraining), accuracy drops to ~9%—worse than random guessing. This confirms that the specific model instance is sensitive to order. However, if you were to **retrain** the network on this shuffled data, the permutation invariance property of FCNNs guarantees that you would recover the high accuracy (98%), effectively proving that FCNNs do not rely on the spatial arrangement of pixels to classify them.

    让我们看一个使用MNIST数据集（手写数字）的代码演示。我们使用Sequential API定义了一个预处理流程。我们引入了两个新层：`Lambda`层用于自定义归一化（将像素值除以255），以及`Flatten`层用于将28x28的图像矩阵转换为向量以供密集层使用。使用Adam优化器和交叉熵损失函数，我们在训练集上达到了100%的准确率，在测试集上达到了98%。现在，我们通过对像素应用随机的成对交换来模拟排列。当我们可视化排列后的“5”和“0”时，它们看起来像随机噪声。如果我们将这些打乱的数据输入到我们*原始*的训练模型中（不重新训练），准确率会降至约9%——比随机猜测还差。这证实了特定的模型实例对顺序是敏感的。然而，如果你在这些打乱的数据上**重新训练**网络，FCNN的排列不变性属性保证了你能够恢复高准确率（98%），这实际上证明了FCNN并不依赖像素的空间排列来进行分类。

  - [14:35 - 14:41] **Introduction to Convolution: Denoising a Signal**

    Now we move to Convolutional Neural Networks (CNNs), which are designed to learn more naturally from structured data. To define convolution, consider the problem of tracking a spaceship with a noisy laser sensor. You receive a signal `x(t)` representing the position at time `t`. To denoise this, we can use a weighted average of past measurements. Mathematically, the convolution of a signal `x` and a filter (or weight function) `w` is defined as the integral of `x(a)w(t-a)da`. For example, if `w` is an exponentially decaying function for past times (and zero for future times), this operation computes a running weighted average that smooths out the noise. In machine learning, we treat `x` as the data and `w` as the **filter** (or kernel) designed to extract features. While convolution is mathematically symmetric (`x * w = w * x`), we conceptually distinguish the input signal from the learnable filter.

    现在我们要进入卷积神经网络（CNN），它的设计初衷是从结构化数据中更自然地学习。为了定义卷积，考虑用嘈杂的激光传感器跟踪飞船的问题。你接收到一个信号`x(t)`代表时间`t`的位置。为了去噪，我们可以使用过去测量值的加权平均。在数学上，信号`x`和滤波器（或权重函数）`w`的卷积定义为`x(a)w(t-a)da`的积分。例如，如果`w`是一个对过去时间呈指数衰减的函数（对未来时间为零），这个操作就计算了一个运行中的加权平均值，从而平滑了噪声。在机器学习中，我们将`x`视为数据，将`w`视为用来提取特征的**滤波器**（或核）。虽然卷积在数学上是对称的（`x * w = w * x`），但在概念上我们会区分输入信号和可学习的滤波器。

  - [14:41 - 14:47] **Discrete Convolution and Padding**

    Since our data (signals) are usually discrete time steps, we replace the integral with a sum to define **Discrete Convolution**. For finite signals, we simply calculate the dot product of the filter `w` with a patch of the input `x`, then slide the filter one step to the right and repeat. For instance, if the input `x` is a vector of length 5 and the filter `w` is length 3, the first output value is the dot product of `w` with the first 3 elements of `x`. Sliding it produces the next value. This "Valid" convolution results in an output smaller than the input (length decreases by `kernel_length - 1`). If we want the output size to remain the same (e.g., for text transcription or segmentation), we use **padding**. "Circular convolution" pads the boundaries by wrapping the signal around (left side gets the end of `x`, right side gets the start). A more common approach is **Zero Padding**, where we simply pad the boundaries with zeros to maintain dimensions.

    由于我们的数据（信号）通常是离散的时间步长，我们用求和代替积分来定义**离散卷积**。对于有限信号，我们只需计算滤波器`w`与输入`x`的一个片段的点积，然后将滤波器向右滑动一步并重复该过程。例如，如果输入`x`是长度为5的向量，滤波器`w`的长度为3，第一个输出值就是`w`与`x`的前3个元素的点积。滑动它会产生下一个值。这种“有效”（Valid）卷积会导致输出比输入小（长度减少`kernel_length - 1`）。如果我们希望输出大小保持不变（例如用于文本转录或分割），我们需要使用**填充**（Padding）。“循环卷积”通过将信号环绕来填充边界（左侧填充`x`的末尾，右侧填充开头）。更常见的方法是**零填充**（Zero Padding），即简单地用零填充边界以保持维度。

  - [14:47 - 14:53] **2D Convolution for Images**

    When dealing with images (2D matrices), the filter (or kernel) is also a matrix, typically smaller than the image (e.g., 3x3, 5x5). The 2D convolution operation involves sliding this kernel over the image. At each position (patch), we perform an element-wise multiplication between the kernel and the image patch, then sum the results to get a single output pixel. We slide this window from top-to-bottom, left-to-right to generate the full output feature map. Just like in 1D, we can use zero padding to ensure the output image has the same dimensions as the input. We typically use **odd-length filters** (like 3x3 or 5x5) because they have a unique center pixel, which makes it easier to preserve spatial symmetry and define the "center" of a feature. The filters are the trainable parameters of the model, learning to detect specific spatial features like edges or shapes.

    在处理图像（二维矩阵）时，滤波器（或核）也是一个矩阵，通常比图像小（例如3x3, 5x5）。二维卷积操作涉及在图像上滑动这个核。在每个位置（图块），我们在核与图像图块之间进行逐元素乘法，然后将结果相加以获得单个输出像素。我们从上到下、从左到右滑动这个窗口，生成完整的输出特征图。就像在一维中一样，我们可以使用零填充来确保输出图像具有与输入相同的维度。我们通常使用**奇数长度的滤波器**（如3x3或5x5），因为它们有一个唯一的中心像素，这使得保持空间对称性和定义特征的“中心”变得更容易。滤波器是模型的可训练参数，用于学习检测特定的空间特征，如边缘或形状。

  - [14:53 - 14:57] **Clarification: Convolution vs. Cross-Correlation**

    I must confess a small technical lie: what I have been showing you and calling "convolution" is mathematically known as **Cross-Correlation**. The strict mathematical definition of convolution involves **flipping** (mirroring) the kernel horizontally and vertically before sliding it. However, in deep learning libraries (like TensorFlow or PyTorch), we implement cross-correlation (sliding without flipping) but call it convolution. Why does this sloppiness not matter? Because the kernels are **learned** parameters. If the theoretical optimal filter is the "flipped" version, the neural network will simply learn the flipped weights directly. Since the model learns the weights from scratch, the lack of flipping does not reduce the expressivity of the model. Thus, in machine learning, we interpret "convolution" as this sliding window cross-correlation operation.

    我必须承认一个小小的技术谎言：我一直展示给你们并称之为“卷积”的操作，在数学上被称为**互相关**（Cross-Correlation）。严格的数学卷积定义要求在滑动之前将核在水平和垂直方向上**翻转**（镜像）。然而，在深度学习库（如TensorFlow或PyTorch）中，我们实现的是互相关（滑动而不翻转），但将其称为卷积。为什么这种不严谨并不重要呢？因为核是**学习**到的参数。如果理论上的最优滤波器是“翻转”后的版本，神经网络只需直接学习翻转后的权重即可。由于模型是从头开始学习权重的，不进行翻转并不会降低模型的表达能力。因此，在机器学习中，我们将“卷积”理解为这种滑动窗口的互相关操作。

  - [15:08 - 15:14] **Tensor Convolutions: Handling Color Images**

    So, what happens if the input is a color image? In that case, you have multiple channels—typically an RGB image with three channels telling you how red, green, and blue specific pixels are. You can think of this as three 2D images stacked on top of each other. In mathematical terms, we call this a "tensor," and it has three dimensions (height, width, and depth). The question is, how do we define a convolution on such a tensor? The answer is that the filters (or kernels) must also be tensors. The terminology we use is that we only specify the width and height of the filters—for example, we might call them "3 by 3 filters"—but the *depth* (the number of channels) of the filter is automatically fixed to be the same as the depth of the input image.

    So, if your input image is 6x6x3, meaning it has 3 channels (Red, Green, Blue), then the depth of your filter is also 3. Let's understand what a single filter does first. Ignoring the multiple filters for a moment, let's look at one filter convolving with one input tensor. Since the depth matches, the output will be a single matrix (a 2D image), effectively "flattening" the depth. How is this operation defined? Essentially, your filter splits into its corresponding Red, Blue, and Green channels. The Green channel of the filter performs a standard 2D convolution with the Green channel of the input image. The Blue filter channel convolves with the Blue input channel, and the Red with the Red. This gives you three separate output matrices. Finally, you sum up these three matrices element-wise to produce a single output matrix. That is how we get one "feature map" from one filter.

    Now, what if we use multiple filters? In a standard convolution layer, all filters will have the same size (e.g., 3x3x3). Each distinct filter will perform the operation I just described and produce one separate output image. If I have, say, two filters, I simply stack their resulting output images together. Consequently, the output of this layer becomes a tensor again, where the number of channels in the output is exactly equal to the number of filters you used. Another way to visualize this is to imagine the filters as 3D blocks or cuboids (e.g., 3x3x3). You match this cuboid to a corresponding 3x3x3 patch in the input, take the dot product of all 27 numbers (9 per channel times 3 channels), and that gives you one output number. These definitions are equivalent.

    那么，如果输入的是彩色图像会怎样呢？在这种情况下，你有多个通道——通常是一个RGB图像，包含三个通道，分别告诉你像素的红色、绿色和蓝色分量是多少。你可以将其想象为三张堆叠在一起的二维图像。在数学术语中，我们称之为“张量”（Tensor），它现在有三个维度（高、宽和深）。问题是，我们如何在张量上定义卷积？答案是，在这种情况下，滤波器（或核）本身也必须是一个张量。通常的说法是，我们只指定滤波器的宽度和高度——例如，我们通常称之为“3乘3滤波器”——但滤波器的*深度*（即通道数）会自动固定为与输入图像的深度（或通道数）相同。

    所以，如果你的输入图像是6x6x3，意味着它有3个通道（红、绿、蓝），那么你的滤波器的深度也是3。让我们先理解单个滤波器在做什么。暂时忽略多个滤波器的情况，来看看一个滤波器与一个输入张量的卷积。由于深度是匹配的，输出将是一个单一的矩阵（二维图像），有效地“压平”了深度。这个操作是如何定义的呢？本质上，你的滤波器会分解成对应的红、蓝、绿通道。滤波器的绿色通道与输入图像的绿色通道进行标准的二维卷积。蓝色通道与蓝色通道卷积，红色与红色卷积。这会给你三个独立的输出矩阵。最后，你将这三个矩阵逐元素相加，生成一个单一的输出矩阵。这就是我们如何从一个滤波器得到一张“特征图”。

    现在，如果我们使用多个滤波器呢？在标准的卷积层中，所有滤波器的大小都是相同的（例如3x3x3）。每个独立的滤波器都会执行我刚才描述的操作，并产生一张独立的输出图像。如果我有两个滤波器，我就简单地将它们生成的输出图像堆叠在一起。因此，这一层的输出再次变成了一个张量，其中输出的通道数正好等于你使用的滤波器的数量。另一种可视化的方法是把滤波器想象成3D块或长方体（例如3x3x3）。你将这个长方体与输入中的对应3x3x3图块匹配，计算所有27个数字（每个通道9个，共3个通道）的点积，这就给出了一个输出数值。这两个定义是等价的。

  - [15:14 - 15:18] **Quiz: Calculating Parameters in Tensor Convolution**

    Let's test your understanding with a quick poll. Suppose we have an input with 3 channels. We decide that the shape of our filters (width and height) is 5x5. Our goal is to get 8 output channels—meaning we want an output image with depth 8. The question is: How many trainable parameters do we need to achieve this using a convolution layer? (You can ignore the bias terms for this calculation). Effectively, I am asking you to determine how many filters we need and what the full dimensions of each filter are.

    (Short pause for students to answer)

    Okay, the majority voted for **3 \* 5 \* 5 \* 8**. That is the correct answer. Very good. Let's break it down: To get 8 output channels, we essentially need 8 distinct filters. That gives us the "times 8" part. Now, what is the size of *one* filter? We specified the spatial dimensions as 5x5. Crucially, the depth of the filter must match the input depth. Since the input image has 3 channels, the depth of each filter is 3. So, each filter has dimensions 3x5x5. Multiply that by 8 filters, and you get the total parameter count.

    Next, here is an open-ended question. In less than 6 words, try to describe what a "1 by 1 convolution" does on a tensor input. I see some answers coming in... "Identity operation" is a common one. Yes, if you use a single 1x1 filter with weights of 1, it acts like an identity operation or a simple scaling. More generally, it performs a linear combination (or mixing) of the input channels at every pixel location without looking at the spatial neighbors.

    让我们通过一个小测验来测试大家的理解。假设我们有一个3通道的输入。我们决定滤波器的形状（宽度和高度）是5x5。我们的目标是获得8个输出通道——这意味着我们想要一个深度为8的输出图像。问题是：使用卷积层实现这一目标需要多少个可训练参数？（计算时可以忽略偏置项）。实际上，我是在问你们需要多少个滤波器，以及每个滤波器的完整维度是多少。

    （学生回答的短暂间隙）

    好的，大多数人选择了 **3 \* 5 \* 5 \* 8**。这是正确答案。非常好。我们来分解一下：为了得到8个输出通道，本质上我们需要8个独立的滤波器。这就是“乘以8”的来源。现在，*单个*滤波器的大小是多少？我们指定了空间维度为5x5。关键在于，滤波器的深度必须与输入深度匹配。由于输入图像有3个通道，每个滤波器的深度就是3。所以，每个滤波器的维度是3x5x5。将其乘以8个滤波器，就得到了总的参数数量。

    接下来是一个开放式问题。请尝试用不到6个单词描述“1乘1卷积”在张量输入上的作用。我看到一些答案了……“恒等操作”（Identity operation）是一个常见的答案。是的，如果你使用权重大约为1的单个1x1滤波器，它的作用就像是恒等操作或简单的缩放。更一般地说，它在每个像素位置执行输入通道的线性组合（或混合），而不查看周围的空间邻居。

  - [15:23 - 15:24] **Clarification: Why CNNs aren't Permutation Invariant**

    *(Answering a student question regarding the previous topic)*

    Regarding your question about why Convolutional Neural Networks don't share the permutation invariance property of the Dense layer (Fully Connected Network): In a dense layer, if you swap a pair of inputs (e.g., input node 1 and input node 3), you can simply swap the corresponding rows of the weight matrix, and the output remains unchanged. That is a valid operation because a dense layer allows for *any* general linear operation. However, a convolution operation is a *restricted* type of linear operation. The weight matrix of a convolution has a very specific structure—it must have constant diagonals (a Toeplitz matrix structure) to represent the "sliding window" effect. If you swap row 1 and row 3 of this structured matrix, the resulting matrix will no longer have constant diagonals. It ceases to be a convolution. Therefore, convolutions do not possess this "permutation invariance of the hypothesis space" property. This is actually something you can verify in the demo code I provided; try swapping rows and see if you can recover the performance without breaking the convolution structure.

    （回答学生关于之前话题的提问）

    关于你问的为什么卷积神经网络不具备密集层（全连接网络）那种排列不变性的问题：在密集层中，如果你交换一对输入（例如输入节点1和输入节点3），你只需交换权重矩阵中对应的行，输出保持不变。这是一个合法的操作，因为密集层允许*任何*通用的线性操作。然而，卷积操作是一种*受限*的线性操作。卷积的权重矩阵具有非常特殊的结构——它必须具有恒定的对角线（托普利兹矩阵结构）来表示“滑动窗口”效应。如果你交换这个结构化矩阵的第1行和第3行，生成的矩阵将不再具有恒定的对角线。它就不再是卷积了。因此，卷积不具备这种“假设空间的排列不变性”。这实际上是你们可以在我提供的演示代码中验证的东西；试着交换行，看看是否能在不破坏卷积结构的情况下恢复性能。

  - [15:24 - 15:31] **Motivation 0: Convolutions as Feature Extractors**

    Now, let's discuss the major motivations for using convolutions. **Motivation 0** is that convolutions are effective feature extractors. Consider this input image of a Golden Retriever. Let's design a filter by hand (not learned yet) and apply it to every channel equally. Imagine a filter shaped like a Gaussian curve—a matrix with a large value in the center (the "bright spot") and small values fading out towards the edges. This is essentially the 2D version of the exponential decay filter we saw in the spaceship example. If you apply this kernel to the image, what happens? It smooths or blurs the image. This acts as a denoiser. If you look at a noisy patch of grass in the background, this filter smooths out the high-ISO noise, which is desirable. However, it also blurs the outline of the dog, which might be undesirable because we lose edge information.

    Is that all convolutions can do? No. Let's look at the opposite extreme. Consider a filter where the left columns are -1 and the right columns are +1. What happens if we convolve this with the image? You get an image that looks like a sketch of the dog's outline. This is a **Vertical Edge Detector**. Why? Imagine matching this filter to a patch of the image. If the patch has no vertical edge (e.g., a flat wall), the pixels on the left equal the pixels on the right. When you calculate the dot product, the -1s cancel out the +1s, giving you a 0 output. You only get a non-zero output when the pixels on the left are significantly different from the pixels on the right—which is precisely the definition of a vertical edge.

    The key takeaway is that kernels can perform vastly different tasks, from blurring to edge detection. Designing these by hand is difficult. In deep learning, we let the model *learn* these kernels. A small 3x3 kernel might learn simple edges. A larger kernel, like 7x7 or 9x9, has a larger context and might learn to detect curves or more sophisticated shapes.

    现在，让我们讨论使用卷积的主要动机。**动机0**是卷积是有效的特征提取器。考虑这张金毛寻回犬的输入图像。让我们手工设计一个滤波器（尚未通过学习获得）并将其同样应用于每个通道。想象一个形状像高斯曲线的滤波器——一个中心值较大（“亮点”）而边缘值逐渐减小的矩阵。这本质上就是我们在飞船示例中看到的指数衰减滤波器的二维版本。如果你将这个核应用于图像，会发生什么？它会平滑或模糊图像。这起到了去噪的作用。如果你看背景中嘈杂的草地，这个滤波器会平滑掉高ISO噪声，这是理想的。然而，它也会模糊狗的轮廓，这可能是不希望看到的，因为我们丢失了边缘信息。

    卷积只能做这些吗？不。让我们看看另一个极端的例子。考虑一个左列是-1、右列是+1的滤波器。如果我们将其与图像进行卷积会发生什么？你会得到一张看起来像狗轮廓素描的图像。这是一个**垂直边缘检测器**。为什么？想象将这个滤波器与图像的一个图块匹配。如果图块没有垂直边缘（例如一面平坦的墙），左边的像素等于右边的像素。当你计算点积时，-1和+1相互抵消，给你一个0输出。只有当左边的像素与右边的像素有显著差异时——这正是垂直边缘的定义——你才会得到非零输出。

    关键的结论是，核可以执行截然不同的任务，从模糊到边缘检测。手工设计这些是困难的。在深度学习中，我们让模型*学习*这些核。一个小的3x3核可能会学习简单的边缘。更大的核，如7x7或9x9，具有更大的上下文，可能会学习检测曲线或更复杂的形状。

  - [15:31 - 15:38] **Motivation 1 & 2: Sparse Interactions and Parameter Sharing**

    **Motivation 1 is Sparse Interactions.** Both dense layers and convolution layers are linear operations, but the convolution matrix has a lot more zeros. This means there are significantly fewer multiplications required, making it much cheaper to compute. If you look at the computational graph, a node in a convolutional layer typically connects to only a few nodes in the next layer (e.g., a 3x3 patch), whereas in a dense layer, every input connects to every output.

    - **Computational Cost:** For a convolution, the cost is roughly *Input Size × Kernel Size*. For a dense layer (assuming input and output size are equal), it is *Input Size squared*. Since the kernel is usually much smaller than the image, convolution is vastly faster.
    - **Receptive Field:** You might worry that because the interactions are local (sparse), the output doesn't "see" the whole image. However, because CNNs are deep (stacked layers), the information propagates. A pixel in the first layer sees a 3x3 patch. A pixel in the second layer sees a 3x3 patch of the *first layer*, which effectively covers a wider area of the original input. Eventually, the final layer is implicitly connected to the entire input image.

    **Motivation 2 is Parameter Sharing (or Tied Weights).** This refers to the fact that we use the *same* filter (the same weights) for every position in the image. In the computational graph, this means all edges pointing in a certain direction share the exact same value (e.g., $w_1$, $w_2$, $w_3$).

    - **Memory Efficiency:** In a dense layer, every edge is an independent parameter. In a CNN, the number of parameters is determined only by the kernel size (e.g., 9 parameters for a 3x3 kernel), regardless of how huge the input image is. This drastically reduces the memory needed to store the model.

    **动机1是稀疏交互（Sparse Interactions）。** 密集层和卷积层都是线性操作，但卷积矩阵包含更多的零。这意味着所需的乘法运算要少得多，计算成本大大降低。如果你看计算图，卷积层中的一个节点通常只连接到下一层中的少数几个节点（例如3x3的图块），而在密集层中，每个输入都连接到每个输出。

    - **计算成本：** 对于卷积，成本大致是 *输入大小 × 核大小*。对于密集层（假设输入和输出大小相等），它是 *输入大小的平方*。由于核通常远小于图像，卷积要快得多。
    - **感受野：** 你可能会担心，因为交互是局部的（稀疏的），输出无法“看到”整个图像。然而，因为CNN是深层的（堆叠层），信息会传播。第一层的一个像素看到一个3x3的图块。第二层的一个像素看到*第一层*的一个3x3图块，这实际上覆盖了原始输入更宽的区域。最终，最后一层隐式地连接到了整个输入图像。

    **动机2是参数共享（或绑定权重）。** 这指的是我们在图像的每个位置使用*同一个*滤波器（相同的权重）。在计算图中，这意味着所有指向特定方向的边共享完全相同的值（例如 $w_1$, $w_2$, $w_3$）。

    - **内存效率：** 在密集层中，每条边都是一个独立的参数。在CNN中，参数的数量仅由核大小决定（例如3x3核只有9个参数），无论输入图像有多大。这大大减少了存储模型所需的内存。

  - [15:38 - 15:51] **Motivation 3: Equivariance and Invariance**

    Now we move to the last and perhaps most theoretically important motivation: **Equivariance and Invariance of functions.** Let's define these terms formally.

    - **Equivariance:** We say a function $f$ is equivariant with respect to a transformation $g$ if the order of operations doesn't matter. That is, $f(g(x)) = g(f(x))$. Whether you apply the transformation first or the function first, the result is the same (structurally).
    - **Invariance:** We say a function $f$ is invariant to $g$ if the transformation doesn't change the output at all. That is, $f(g(x)) = f(x)$.

    Let's look at examples where the transformation $g$ is **Translation** (shifting the image or signal by a constant amount).

    1. **Convolution:** It is easy to prove that convolution is **translation equivariant**. If you shift an image and then convolve it, you get the same result as convolving first and then shifting the output map. (Note: This holds strictly for circular convolution, and holds "almost everywhere" for zero-padding except at the boundaries).
    2. **Activation Functions:** Functions like ReLU operate element-wise. It is obvious that shifting the pixels and then applying ReLU is the same as applying ReLU and then shifting. So, activation functions are also translation equivariant.
    3. **Composition:** A mathematical fact is that the composition of two equivariant functions is also equivariant. Since a CNN layer is essentially Convolution followed by Activation ($f_2 \circ f_1$), the entire layer is translation equivariant. By extension, a stack of many CNN layers is also translation equivariant.

    Now, consider the final step of a network. Suppose we have a function $F$ at the end (like Global Average Pooling or a Summation) that sums up all the entries. This summation is **translation invariant** (the sum doesn't change if you shuffle or shift the numbers).

    - **The Punchline:** If you have a stack of equivariant layers followed by one invariant layer, the entire model becomes **Translation Invariant**.

    现在我们要讨论最后一个，也许是在理论上最重要的动机：**函数的等变性（Equivariance）和不变性（Invariance）。** 让我们正式定义这些术语。

    - **等变性：** 如果操作的顺序不影响结果，我们说函数 $f$ 相对于变换 $g$ 是等变的。即，$f(g(x)) = g(f(x))$。无论你是先应用变换还是先应用函数，结果（在结构上）是相同的。
    - **不变性：** 如果变换根本不改变输出，我们说函数 $f$ 对 $g$ 是不变的。即，$f(g(x)) = f(x)$。

    让我们看一些变换 $g$ 是**平移**（将图像或信号移动恒定量）的例子。

    1. **卷积：** 很容易证明卷积是**平移等变**的。如果你移动图像然后进行卷积，得到的结果与先卷积然后移动输出图是一样的。（注：这对于循环卷积严格成立，对于零填充除了边界处几乎处处成立）。
    2. **激活函数：** 像 ReLU 这样的函数是逐元素操作的。很明显，移动像素然后应用 ReLU 与先应用 ReLU 然后移动是一样的。所以，激活函数也是平移等变的。
    3. **组合：** 一个数学事实是，两个等变函数的组合也是等变的。由于 CNN 层本质上是卷积后跟激活（$f_2 \circ f_1$），所以整个层是平移等变的。以此类推，堆叠许多 CNN 层也是平移等变的。

    现在，考虑网络的最后一步。假设我们在末端有一个函数 $F$（如全局平均池化或求和），它将所有条目相加。这个求和是**平移不变**的（如果你打乱或移动数字，总和不会改变）。

    - **结论：** 如果你有一堆等变层，后面跟着一个不变层，整个模型就变成了**平移不变**的。

  - [15:51 - 15:58] **Why Invariance Matters: The "Cat or Dog" Prior**

    Why do we care about translation invariance? Let's assume there exists an "Oracle Function" $f^*$ that represents the absolute truth—for example, it takes an image and correctly labels it "Cat" or "Dog." If we take a picture of a dog and translate (shift) it slightly to the right or up/down, it is still a dog. The label should not change. Therefore, the true function $f^*$ is translation invariant.

    - **Inductive Bias (The Prior):** Since we know the ground truth function has this property, it is a good idea to enforce this property on our hypothesis space.
    - **Hypothesis Space Comparison:** Let $H$ be the space of all Fully Connected Networks (general linear operations). Let $H'$ be the space of CNNs. $H'$ is a subset of $H$, but it is a specific subset containing only functions that respect translation structure. If the truth $f^*$ lies within this specific area, restricting our search to $H'$ (CNNs) allows us to approximate the truth much better given the same computational resources.

    Are there other invariances we might want? Yes, like **Rotation** or **Scaling** (zooming). However, designing models that are strictly invariant to these (like Steerable CNNs) is theoretically possible but often computationally inefficient. In practice, instead of baking these invariances into the model architecture, we use a training trick called **Data Augmentation**—we manually rotate or zoom our training images to force the model to learn these invariances. We will cover that in a future lecture.

    After the break, I will introduce the final ingredient of a typical CNN. We will see that standard CNNs are actually only *approximately* translation invariant (to small shifts), which is actually what we want—if you shift an image too much, it might disappear from the frame!

    我们为什么关心平移不变性？让我们假设存在一个“神谕函数” $f^*$ 代表绝对真理——例如，它接收图像并正确标记为“猫”或“狗”。如果我们拍一张狗的照片，并将其向右或向上/向下稍微平移（移动），它仍然是一只狗。标签不应该改变。因此，真实的函数 $f^*$ 是平移不变的。

    - **归纳偏置（先验）：** 既然我们知道基本事实函数具有这个属性，那么在我们的假设空间中强制执行这个属性是一个好主意。
    - **假设空间比较：** 设 $H$ 为所有全连接网络（通用线性操作）的空间。设 $H'$ 为 CNN 的空间。$H'$ 是 $H$ 的一个子集，但它是一个特定的子集，只包含遵循平移结构的函数。如果真理 $f^*$ 位于这个特定区域内，将我们的搜索限制在 $H'$（CNN）范围内，可以在相同的计算资源下让我们更好地逼近真理。

    还有我们可能想要的其他不变性吗？是的，比如**旋转**或**缩放**（变焦）。然而，设计严格对这些不变的模型（如可操纵 CNN）在理论上是可行的，但往往计算效率低下。在实践中，我们不将这些不变性硬编码到模型架构中，而是使用一种称为**数据增强**的训练技巧——我们手动旋转或缩放训练图像，迫使模型学习这些不变性。我们将在未来的讲座中讨论这一点。

    休息之后，我将介绍典型 CNN 的最后一个组成部分。我们将看到标准的 CNN 实际上只是*近似*平移不变的（对小位移），但这实际上正是我们想要的——如果你将图像移动太多，它可能会从画面中消失！

  - [16:10 - 16:16] **Max Pooling: Definition and Invariance**

    Let me explain **Max Pooling**. It is easiest to understand through visualization. Suppose we perform Max Pooling with a **stride** length of 2. If the input is a vector, we look at a patch of length 2—for example, the values `[3, 1]`. We take the maximum, which is 3. Then we move (stride) two steps to the right to the next pair, say `[4, 1]`, take the maximum (4), and so on. Consequently, the length of the output will be half of the input length if the stride is 2. The same logic applies to an image: we look at a 2x2 patch of pixels (e.g., `3, 1, 5, 9`), pick the largest value (9), and then slide the window two steps to the right to repeat the process.

    Crucially, this operation has **no trainable parameters**—it is a fixed mathematical operation for **down-sampling**. Why do we do this? First, it builds in **local translation invariance**. If I shift the input slightly within that 2x2 patch, the maximum value (e.g., 9) likely remains the same, so the output doesn't change. This enforces the approximate invariance we discussed earlier. Second, consider the intuition: a convolution kernel detects *if* a specific feature exists in a patch. Max pooling effectively asks, "Did this feature appear anywhere in this patch?" without caring *exactly* which pixel it was centered on. It summarizes presence rather than precise location.

    让我解释一下**最大池化（Max Pooling）**。通过可视化来理解它是最容易的。假设我们执行步长（stride）为2的最大池化。如果输入是一个向量，我们看一个长度为2的片段——例如数值`[3, 1]`。我们取最大值，即3。然后我们要向右移动（跨步）两步到下一对数值，比如`[4, 1]`，取最大值（4），依此类推。因此，如果步长为2，输出的长度将是输入长度的一半。同样的逻辑也适用于图像：我们查看一个2x2的像素块（例如 `3, 1, 5, 9`），选取最大值（9），然后将窗口向右滑动两步重复该过程。

    关键在于，这个操作**没有可训练的参数**——它是一个固定的用于**下采样**的数学操作。我们为什么要这样做？首先，它建立了一种**局部平移不变性**。如果我在那个2x2的图块内稍微移动输入，最大值（例如9）很可能保持不变，所以输出不会改变。这强制了我们之前讨论的近似不变性。其次，从直觉上考虑：卷积核检测的是某个特定特征*是否*存在于某个图块中。最大池化实际上是在问：“这个特征是否出现在这个图块的任何地方？”而并不关心它*确切*以哪个像素为中心。它总结的是特征的存在性，而不是精确位置。

  - [16:16 - 16:28] **CNN Architecture and the Feature Hierarchy**

    Typically, a "Convolutional Layer" or block in literature refers to this sequence: Convolution $\to$ Activation (e.g., ReLU) $\to$ Pooling. We stack many of these blocks. As we go deeper, you will notice a specific architectural pattern: the **width and height** of the images (feature maps) decrease due to pooling, but the **number of channels** (depth) increases. Finally, we flatten the output into a vector and pass it through a Dense (Fully Connected) layer.

    Why does this structure make sense? Research suggests CNNs learn a hierarchy of features.

    1. **Early Layers:** Detect **low-level features** like edges and lines. There are only a few types of edges (vertical, horizontal, diagonal), so we only need a few channels (e.g., 10-20) to represent them.
    2. **Middle Layers:** Combine edges to form **shapes** (rectangles, circles). There are hundreds of possible shapes, so we need more channels to dedicate to each shape detector.
    3. **Deep Layers:** Combine shapes to detect **high-level objects** (cars, faces, chairs). There are thousands of object categories, requiring even more channels.

    This explains the trade-off. We need more channels deeper in the network to represent complex objects, which increases computational and memory costs. To balance this, we use Max Pooling to reduce the spatial resolution (width/height). The assumption is that for high-level tasks—like detecting if a "car" is in the top-right corner—we do not need pixel-perfect resolution. A low-resolution feature map is sufficient to say "Yes, a car is here." This balancing act keeps the computational cost in check while allowing the network's semantic capacity to grow.

    通常，文献中的“卷积层”或卷积块指的是这个序列：卷积 $\to$ 激活（如 ReLU） $\to$ 池化。我们会堆叠许多这样的块。随着网络加深，你会注意到一个特定的架构模式：图像（特征图）的**宽度和高度**由于池化而减小，但**通道数**（深度）在增加。最后，我们将输出展平为向量，并通过一个密集（全连接）层。

    为什么这种结构合理？研究表明CNN学习的是特征的层级结构。

    1. **早期层：** 检测**低级特征**，如边缘和线条。边缘的类型很少（垂直、水平、对角线），所以我们只需要很少的通道（例如10-20个）来表示它们。
    2. **中间层：** 组合边缘以形成**形状**（矩形、圆形）。可能的形状有数百种，所以我们需要更多的通道来专门负责每种形状的检测。
    3. **深层：** 组合形状以检测**高级对象**（汽车、人脸、椅子）。对象类别有成千上万种，需要更多的通道。

    这解释了其中的权衡。我们在网络深层需要更多的通道来表示复杂的对象，这增加了计算和内存成本。为了平衡这一点，我们使用最大池化来降低空间分辨率（宽度/高度）。其假设是，对于高级任务——比如检测右上角是否有“汽车”——我们不需要像素级的完美分辨率。一个低分辨率的特征图足以说明“是的，这里有一辆车”。这种平衡做法在控制计算成本的同时，允许网络的语义容量增长。

  - [16:28 - 16:33] **Historical Context: ImageNet and ResNet**

    Of course, the assumption that high-level features work with low resolution isn't always true (e.g., medical imaging may need high-res details). In those cases, we use architectures like **U-Net** or **Strided Convolutions** (learnable down-sampling). Historically, the **ImageNet Challenge** (1M images, 20k categories) was the turning point. Before neural networks, the best error rate was ~26% (XRCE). In 2012, **AlexNet** (a CNN) achieved 16%, marking the first time a neural network achieved state-of-the-art results in a major benchmark. By 2015, **ResNet** (Residual Networks) surpassed human performance (~5% error) by using skip connections to solve the vanishing gradient problem in very deep networks.

    当然，高级特征适用于低分辨率的假设并不总是成立的（例如，医学成像可能需要高分辨率的细节）。在这些情况下，我们会使用像 **U-Net** 或 **跨步卷积（Strided Convolutions）**（可学习的下采样）这样的架构。从历史上看，**ImageNet 挑战赛**（100万张图像，2万个类别）是一个转折点。在神经网络之前，最好的错误率约为26%（XRCE）。2012年，**AlexNet**（一种CNN）达到了16%，标志着神经网络首次在主要基准测试中取得最先进的结果。到了2015年，**ResNet**（残差网络）通过使用跳跃连接解决了极深网络中的梯度消失问题，超越了人类的表现（约5%的错误率）。

  - [16:33 - 16:41] **Demo 1: Building a CNN for MNIST in Keras**

    Let's verify this with code. We build a simple CNN using the Sequential API.

    1. **Preprocessing:** `Lambda` layer to normalize pixels (div 255).
    2. **Reshape:** Convert 28x28 input to 28x28x1 (because Conv2D expects a tensor).
    3. **Conv Block 1:** `Conv2D` with 64 filters, 3x3 kernel, ReLU activation. Followed by `MaxPooling2D`.
    4. **Conv Block 2:** `Conv2D` with 128 filters. The feature maps get smaller (from 14x14 to 7x7 after pooling), but deeper.
    5. **Classifier:** `Flatten` the 7x7x128 tensor into a vector, then pass through Dense layers.

    **Parameter Check:** How many parameters in the first convolution layer? We have 64 filters. Each is 3x3x1 (since input depth is 1). Plus biases.

    **Result:** We achieve **99.3% accuracy**, improving on the 98% of the Fully Connected Network. Crucially, if you were to run this CNN on the *permuted* dataset we saw earlier, it would fail miserably because the permutation destroys the spatial structure that the convolution kernels rely on.

    让我们用代码验证这一点。我们使用 Sequential API 构建一个简单的 CNN。

    1. **预处理：** `Lambda` 层归一化像素（除以255）。
    2. **重塑（Reshape）：** 将28x28的输入转换为28x28x1（因为 Conv2D 期望张量输入）。
    3. **卷积块 1：** 64个滤波器，3x3核，ReLU激活的 `Conv2D`。后面跟着 `MaxPooling2D`。
    4. **卷积块 2：** 128个滤波器的 `Conv2D`。特征图变小了（池化后从14x14变为7x7），但变深了。
    5. **分类器：** 将7x7x128的张量用 `Flatten` 展平成向量，然后通过 Dense 层。

    **参数检查：** 第一个卷积层有多少参数？我们有64个滤波器。每个是3x3x1（因为输入深度为1）。加上偏置项。

    **结果：** 我们达到了 **99.3% 的准确率**，比全连接网络的98%有所提高。关键是，如果你在之前看到的*排列后*的数据集上运行这个CNN，它会彻底失败，因为排列破坏了卷积核所依赖的空间结构。

  - [16:41 - 16:53] **Demo 2: Pneumonia Detection and ImageDataGenerator**

    We next look at a Binary Classification problem: **Chest X-Rays** (Normal vs. Pneumonia). The dataset is large (2.3GB) and images vary in resolution and source machine.

    - **Efficient Loading:** We cannot load 2.3GB into RAM at once. We use the `ImageDataGenerator` class to perform **"Load on the Fly"**. It keeps only the current mini-batch (e.g., size 16) in memory, processes it, updates weights, and then discards it to load the next batch.
    - **Preprocessing:** We resize all images to a target size of 128x128 so they can be batched together.
    - **Performance:** The model achieves only ~70% accuracy. The Confusion Matrix reveals a high number of False Negatives (Pneumonia cases classified as Normal), which is dangerous in medicine.
    - **Why so poor?**
      1. **Aggressive Down-sampling:** Resizing high-res X-rays to 128x128 loses critical texture details needed for diagnosis.
      2. **Wrong Assumptions:** Our CNN assumption (high-level features = low resolution) works for "cats vs dogs" but fails for medical anomalies which might be subtle textures rather than global shapes.
    - **Trade-off:** "Load on the fly" saves Memory (RAM) but incurs a **Communication Cost** (I/O latency) because we must read from the disk repeatedly every epoch. Using an SSD mitigates this.

    接下来我们看一个二分类问题：**胸部X光片**（正常 vs. 肺炎）。数据集很大（2.3GB），而且图像的分辨率和来源机器各不相同。

    - **高效加载：** 我们无法一次性将2.3GB的数据加载到内存中。我们使用 `ImageDataGenerator` 类来执行**“即时加载”（Load on the Fly）**。它只在内存中保留当前的小批量数据（例如大小为16），处理它，更新权重，然后将其丢弃以加载下一批数据。
    - **预处理：** 我们将所有图像调整为128x128的目标大小，以便它们可以被分批处理。
    - **性能：** 模型的准确率只有约70%。混淆矩阵显示有大量的假阴性（肺炎病例被归类为正常），这在医学上是危险的。
    - **为什么表现不佳？**
      1. **激进的下采样：** 将高分辨率的X光片调整为128x128会丢失诊断所需的关键纹理细节。
      2. **错误的假设：** 我们的CNN假设（高级特征 = 低分辨率）适用于“猫与狗”的分类，但对于医学异常检测则失效，因为后者可能依赖细微的纹理而不是全局形状。
    - **权衡：** “即时加载”节省了内存（RAM），但产生了**通信成本**（I/O延迟），因为我们必须在每个epoch重复从磁盘读取数据。使用SSD可以缓解这个问题。
## Week 5
***
