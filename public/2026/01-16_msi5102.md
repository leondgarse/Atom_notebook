# ___MSI5102 Essentials of Machine Learning___
***
## INFO
  ```
  Dear students,

  I would like to be a bit clearer about (1) how to learn AI ethics in this course and (2) how your learning relates to the exam.

  Going forward, I will include guiding questions in the reading lists. Please use these questions to guide your study of the topic. Your primary goal should be to use these questions to acquire a good personal understanding of the topic.

  Your personal understanding, based on these guiding questions, will be required to answer the exam essay question. Here's an example of a (simplified) essay question: "The use of AI often leads to unfair outcomes. Discuss." (Obviously, the actual exam question will be a bit more sophisticated than this, but you get the idea).

  The readings should not be treated as textbook, but as useful resources to help you to answer the guiding questions. You should not feel the need to digest the readings wholesale – many of them are written by philosophers and lawyers for other philosophers and lawyers to read, and so are often not very accessible to the layperson. Nevertheless, I recommend them because they are the highest quality material that I can find. Just extract what you can from them, learn from our discussions in class, and also feel free to look outside the readings to deepen your own understanding.

  I hope that this makes things more straightforward for you, and assuages any concerns about the exam.

  Regards,

  Benjamin
  ```
***

# Lectures
## Week 1
  * [19:05–19:09] **Class format and collaboration setup**

    The class will not be run like a traditional one-way lecture. Instead, the format is designed to be more flexible and interactive, so that everyone can follow more easily.

    In the second half of the class, we will focus more on discussion and deeper understanding. You will also work with shared code bases, so that multiple people can work on the same code at the same time. This is mainly for convenience and collaboration.

    Before moving on, are there any questions about the course plan or the way the class will run?

    这门课不会采用传统的“老师一直讲、学生一直听”的方式，而是一个相对灵活、互动性更强的课堂形式，方便大家理解和参与。

    在课堂后半部分，我们会更多进行讨论和深入讲解。另外，你们会使用共享的代码仓库，允许多人同时在同一套代码上协作，这主要是为了提高效率和方便学习。

    在正式开始之前，大家对课程安排或上课方式有问题吗？

  * [19:09–19:18] **Why machine learning? What is AI?**

    Let us now start with the main topic: machine learning.

    People often ask why machine learning is important. One reason is that AI has become a very popular term in industry and society. Artificial intelligence is about making intelligent machines.

    Intelligence includes many abilities: driving a car, recognizing images and faces, understanding language, accumulating knowledge, reasoning, doing mathematics, and playing games like chess and Go.

    For humans, some tasks like image recognition are easy, while others like complex calculations may be harder. For machines, the difficulty is often reversed. Tasks that are easy for humans can be very difficult for machines.

    现在我们正式进入课程的核心内容：机器学习。

    很多人会问，为什么要学习机器学习？一个很现实的原因是，AI 已经成为工业界和社会中的热门话题。人工智能的目标，是让机器具备“智能”。

    智能包含很多能力，比如自动驾驶、图像和人脸识别、语言理解、知识积累、推理、数学计算，以及下棋、下围棋等。

    对人来说，图像识别这类任务很容易，而复杂计算可能较难；但对机器而言，情况往往正好相反。

  * [19:18–19:24] **General intelligence vs machine intelligence**

    By general intelligence, we mean the ability to learn, solve problems, and adapt solutions to very different environments. This kind of intelligence is still very hard for machines.

    However, AI technology is advancing at an unprecedented speed. With deep learning, machines can now translate languages, recognize faces, and even drive cars reasonably well.

    These successes create many opportunities for AI-based industries, including startups and applied research.

    所谓“通用智能”，指的是能够学习、解决问题，并把解决方案迁移到不同环境中的能力。这种能力对机器来说仍然非常困难。

    但与此同时，AI 技术正以前所未有的速度发展。借助深度学习，机器已经可以很好地完成语言翻译、人脸识别，甚至在一定条件下实现自动驾驶。

    这些突破也带来了大量 AI 相关产业和创业机会。

  * [19:24–19:30] **AI, education, and social impact**

    On the other hand, AI also brings serious challenges. In some areas, machines may outperform humans, which creates pressure on traditional professions and education systems.

    Today, much learning material is freely available online. Students can often learn knowledge without relying solely on teachers. This changes the role of education.

    For example, in the past, traveling required human assistance for booking hotels and planning routes. Today, online platforms and navigation systems make this easy.

    但另一方面，AI 也带来了很大的挑战。在某些领域，机器可能比人做得更好，这对传统职业和教育体系产生了冲击。

    现在，学习资料大量存在于网络上，学生不再只能依赖老师获取知识，教育的角色正在发生变化。

    过去出行时，需要向人询问路线、预订酒店；而现在，搜索引擎和导航系统几乎可以解决所有问题。

  * [19:30–19:38] **AI vs Machine Learning**

    AI is a very broad field and includes many different approaches. Machine learning is one of the most important and dominant approaches.

    Machine learning solves AI problems through mathematical modeling. Instead of explicitly programming rules, we let machines learn patterns from data.

    人工智能是一个非常宽广的领域，包含多种方法。机器学习是其中最重要、也最主流的一种。

    机器学习的核心思想是：用数学模型来解决 AI 问题，而不是人为写出所有规则，让机器从数据中学习规律。

  * [19:38–19:45] **Basic machine learning tasks**

    There are two main types of machine learning tasks.

    The first is supervised learning, such as classification and regression. In classification, we learn rules to assign objects to different categories. This idea is widely used, for example, in biology when classifying species.

    The second is unsupervised learning, where we do not have labels and instead try to discover structure or patterns in data.

    机器学习的基本任务主要分为两类。

    第一类是监督学习，包括分类和回归。分类的目标是把对象分到不同类别中，比如生物学中的物种分类。

    第二类是无监督学习，在这种情况下没有标签，模型需要从数据中自行发现结构或规律。

  * [19:45–19:52] **Traditional ML vs Deep Learning**

    In traditional machine learning, we manually design features and then learn a prediction function.

    In deep learning, feature extraction is done automatically by multiple layers of neural networks. Each layer extracts higher-level features from the previous one.

    This idea is inspired by how the human brain works, where simple neurons combine to form powerful networks.

    在传统机器学习中，特征通常由人手工设计，然后模型学习预测函数。

    在深度学习中，特征由神经网络自动提取，多层结构逐层学习更高级的特征。

    这种结构受到人脑工作方式的启发：大量简单神经元组合在一起，形成强大的计算能力。

  * [19:52–20:05] **Feature engineering and data representation**

    To apply machine learning, we must convert real-world data into numerical vectors. This process is called feature engineering.

    Data can be numerical, categorical, text, images, or audio. Each type needs to be mapped into real-valued vectors so that mathematical models can be applied.

    For example, categorical data without order uses one-hot encoding, while ordered data can be mapped to integers.

    要使用机器学习，必须把现实世界中的数据转换成数值向量，这个过程叫做特征工程。

    数据可以是数值、类别、文本、图像或音频，不同类型需要不同的映射方式，最终都转成实数向量，才能进行数学计算。

    比如无序类别数据用 one-hot 编码，有序数据可以直接映射成整数。

  * [20:05–20:15] **Standardization and log transformation**

    Different features may have very different scales. Standardization ensures each feature has mean zero and unit variance.

    For features with very large ranges, such as house prices, we often apply a logarithmic transformation before standardization. This helps the model focus on relative differences rather than absolute values.

    不同特征的取值范围可能差异很大，因此需要做标准化，使其均值为 0、方差为 1。

    对于跨度特别大的特征（如房价），通常先进行对数变换，再标准化，这样模型更关注相对差异而不是绝对大小。

  * [20:15–20:21] **Wrap-up and next steps**

    Feature engineering helps improve model performance, but it must be validated experimentally.

    Remaining challenges include choosing evaluation metrics, selecting and tuning models, and avoiding overfitting or underfitting. These topics will be discussed later in the course.

    If you have difficulties forming groups or following the material, please email me. Each group should have four to six students.

    特征工程是否有效，需要通过实验验证。

    后续我们还将讨论模型评估指标、模型选择与调参，以及如何避免过拟合和欠拟合。

    如果在分组或学习过程中遇到困难，可以给我发邮件。每组人数为 4 到 6 人。

  * [20:22–20:27] **Project marking and reproducibility**
    **English**
    OK. The project score is evaluated like this—this is how the project will be marked.
    The project is based on some published work. Nowadays, in machine learning, authors usually upload their data and code to a public website, so it is easy for others to verify the results and run the code.
    With AI assistant tools, running code and setting up experiments has become much easier.
    In most cases, the first step is simply: run their code and see whether you can reproduce their reported results.
    But that is not the whole story. Often you also need extra validation—for example, compare methods properly, or test on the same test data in the correct format. If the test data is prepared correctly, building a baseline model can be very easy. I will show you some simple code in the second half—sometimes it’s just one line.
    So that is the second part of the project. There is also a presentation component—basically you present what you have done.
    I will upload a list of candidate works around Week 6. Most of them are deep learning papers—fairly standard.
    Since many of you are part-time students, you may have time constraints. Any questions about the project? It should be clearer now.

    **Chinese**
    好，项目的分数大概是这样评的——这就是项目评分方式。
    这个项目是基于一篇已经发表的工作来做。现在机器学习领域通常会把数据和代码上传到公开网站，方便别人验证结果、复现代码。
    另外，现在有各种 AI 辅助工具，跑代码、搭环境都变得更容易了。
    所以大多数情况下，第一步就是：把对方的代码跑起来，看你能不能复现出论文里报告的结果。
    但这还不够，很多时候还需要做额外验证，比如更规范地做对比实验，或者保证大家都用同一份测试数据、同一种正确格式。只要测试数据准备对，搭一个基线模型其实很简单。我会在后半部分给你们看一些例子代码，有时候真的就一行。
    这就是项目的第二块。还有展示部分——基本就是把你做了什么讲清楚。
    我会在第 6 周左右放出一批可选的论文题目，大部分是深度学习相关的，比较标准。
    考虑到你们很多是兼职学生，时间上可能会有压力。关于项目还有问题吗？现在应该更清楚了。

  * [20:27–20:30] **Scheduling, quiz logistics, and AI-tool restriction**
    **English**
    For scheduling: originally I planned to place something at a certain time. If you have issues, come during the break and tell me.
    It turns out Week 5 works for most of you, but there may still be problems. Does anyone have a conflict?
    Also, Chinese New Year is in Week 6, right?
    One more thing: you cannot use AI tools during the quiz. (For example, do not use AI tools to “make the PDF” / generate answers.)
    OK, no problem. The first quiz is one hour. It is about 15 questions, multiple-choice only—no fill-in-the-blank.

    **Chinese**
    关于时间安排：我一开始是想把某些安排放在某个时间点。如果你有问题，休息时间过来跟我说。
    结果是第 5 周对大多数人来说比较合适，但可能仍然有人有冲突。有谁有时间问题吗？
    另外，第 6 周是春节对吧？
    还有一点：测验时不能使用 AI 工具。（比如用 AI 来生成答案、做成 PDF 之类的都不可以。）
    好的。第一次测验一小时，大概 15 题，全是选择题，没有填空。

  * [20:40–20:41] **Why calculus: learning as optimization**
    **English**
    Machine learning is essentially optimization: you find a model by minimizing a loss function. You may already know this idea, but many people do not really understand what the “loss” is.
    Calculus provides tools to find parameter values that minimize the loss function. This is used for regression models and for training neural networks.

    **Chinese**
    机器学习本质上就是优化：通过最小化一个损失函数来找到模型。你可能听过这个说法，但很多人其实并不真正理解“损失”到底是什么。
    微积分提供的工具可以帮我们找到使损失函数最小的参数值。这在回归模型和神经网络训练中都会用到。

  * [20:41–20:44] **Types of functions and why minima/maxima are hard**
    **English**
    We start with easy functions. Linear functions are easy to understand: the values form a line; in higher dimensions, they form a plane or hyperplane.
    But beyond that, it becomes harder. For example, quadratic functions are curves, and it is not always obvious where the minimum or maximum is, or whether a maximum exists.
    Exponential-type functions can be even harder to visualize—sometimes you see a bell-curve-like shape, but in higher dimensions.

    **Chinese**
    我们先从简单函数开始。线性函数比较好理解：在二维里像一条直线；维度更高时就是平面或超平面。
    但再往后就难了。比如二次函数是曲线，最小点、最大点在哪里并不总是直观，甚至有时你还要判断到底有没有最大值。
    指数类函数更难想象。有时你看到像“钟形曲线”那种形状，但在高维里就更复杂。

  * [20:45–20:47] **Piecewise linear functions (decision trees intuition)**
    **English**
    If your data has a pattern like this, there is no way a single linear function can fit it. What you can do is split the input space into different regions and use different linear functions in different regions—combine them.
    This is what we call a piecewise linear function.
    For example, the absolute value function makes everything nonnegative: when (x \ge 0), it is (x); when (x < 0), it is (-x).
    Another example is a three-piece linear function: one line for (x < -2), another for (-2 \le x \le 0), and another for (x > 0).
    Decision tree models try to learn functions of this “piecewise” type.

    **Chinese**
    如果数据分布是那种“弯来弯去”的形状，用一个线性函数是不可能拟合好的。你能做的是把输入空间分成不同区域，在每个区域用不同的线性表达式，再把它们拼起来。
    这就叫分段线性函数（piecewise linear）。
    比如绝对值函数会把数变成非负：(x \ge 0) 时就是 (x)，(x < 0) 时就是 (-x)。
    还有一种三段式的线性函数：(x < -2) 用一条线，(-2 \le x \le 0) 用一条线，(x > 0) 再用一条线。
    决策树这类模型，本质上就是在学习这种“分段”的函数结构。

  * [20:48–20:52] **Composite functions and deep learning intuition**
    **English**
    Another important idea is that we can form complicated functions by composing simpler functions. A composite function means you apply one function to the output of another.
    In machine learning, you can think of mapping a sample to another feature space, then mapping again to the label space—multiple levels—so naturally we get composite functions.
    Example: let (y = g(x) = x^3), and (z = f(y) = \sin(y)). Then (z = \sin(x^3)) is a composite function (apply (g) first, then (f)).
    Deep learning is built from stacking many simple nonlinear functions; together they can represent very complicated functions.

    **Chinese**
    另一个重要概念是“复合函数”：把一个函数作用在另一个函数的输出上，用简单函数组合出复杂函数。
    在机器学习里，你可以理解为：先把样本映射到某个特征空间，再映射到标签空间，一层一层做，所以天然会出现复合函数。
    例如 (y=g(x)=x^3)，再令 (z=f(y)=\sin(y))，那么 (z=\sin(x^3)) 就是复合函数（先算 (x^3)，再取正弦）。
    深度学习就是把很多简单的非线性函数叠起来，最终得到非常复杂的表达能力。

  * [20:52–20:58] **Derivatives: locating minima/maxima; multi-variable case**
    **English**
    For a continuous function, we talk about minima and maxima. Derivatives tell us the rate of increase or decrease.
    At a minimum point, the derivative is zero. If the derivative at a point is positive, you know you are on an increasing side; if it is negative, you are on a decreasing side—this helps you move toward a minimum.
    In practice, we may not have a closed-form expression for the minimizer, but we can find it numerically by checking derivatives and moving step by step.
    For multivariable functions, you need derivatives in every direction: partial derivatives with respect to (x_1, x_2, \dots, x_m). At a local optimum, the gradient vector is the zero vector.
    The derivative (or gradient) also serves as an indicator: if it is small, you may be close to a minimum.

    **Chinese**
    对连续函数来说，我们讨论最小值和最大值。导数反映函数上升或下降的速度。
    在最小点处，导数等于 0。如果某点导数为正，说明函数在那边是上升的；如果为负，说明在下降——这能指导你往更可能接近最小值的方向移动。
    实际上很多函数没有解析解（没法写出一个明确公式直接给你最小点），但可以用数值方法：不断看导数，逐步迭代逼近最小点。
    多元函数要考虑各个方向的变化，所以需要对 (x_1,x_2,\dots,x_m) 分别求偏导。在局部最优点，梯度向量为零向量。
    导数/梯度也可以当“指示器”：越小通常表示越接近最小点。

  * [20:58–21:01] **Two derivative rules: sum rule and product rule**
    **English**
    To compute derivatives, we mainly use a few rules. First, the derivative of a sum is the sum of derivatives.
    The product rule is slightly more complicated: derivative of (f(x)g(x)) is (f'(x)g(x) + f(x)g'(x)).
    If you do not know this rule, it is hard to differentiate many expressions. But once you know the rule, you apply it systematically.
    For example, if a function is written as a product like (x^2 \cdot (\text{something in } x)), treat it as the first function times the second function and apply the rule.

    **Chinese**
    计算导数主要依靠一些基本法则。第一条：和的导数等于导数的和。
    乘积法则稍微复杂：((f(x)g(x))' = f'(x)g(x) + f(x)g'(x))。
    如果不知道这个法则，很多函数就很难求导；知道之后就可以机械套用。
    比如一个函数写成乘积形式 (x^2 \cdot (\text{某个关于}x\text{的式子}))，就把它当成“第一个函数×第二个函数”，直接用乘积法则。

  * [21:03–21:05] **Chain rule (composite function derivative)**
    **English**
    For composite functions, we use the chain rule. If (z = f(y)) and (y = g(x)), then
    (\dfrac{dz}{dx} = \dfrac{dz}{dy}\dfrac{dy}{dx}).
    For example, if (y = x^3), then (\dfrac{dy}{dx} = 3x^2). You multiply this with (\dfrac{dz}{dy}) from the outer function to get (\dfrac{dz}{dx}).
    This is exactly the kind of calculation behind training neural networks.

    **Chinese**
    复合函数求导用链式法则：如果 (z=f(y))，且 (y=g(x))，那么
    (\dfrac{dz}{dx} = \dfrac{dz}{dy}\dfrac{dy}{dx})。
    例如 (y=x^3)，则 (\dfrac{dy}{dx}=3x^2)。再把外层函数的 (\dfrac{dz}{dy}) 乘上去，就得到 (\dfrac{dz}{dx})。
    这类计算就是神经网络训练（反向传播）背后的数学基础之一。

  * [21:05–21:09] **Multivariable example: partial derivatives and substitution**
    **English**
    For multivariable cases, it is not always clear at first, so let me give an example. Suppose (z = x^2y).
    If you take partial derivative with respect to (x), treat (y) as constant: (\frac{\partial z}{\partial x} = 2xy).
    If you take partial derivative with respect to (y), treat (x^2) as constant: (\frac{\partial z}{\partial y} = x^2).
    Then if you further define variables like (x = s + t), (y = s - t), you can substitute them in, and the derivative becomes a function of (s,t).
    This is the kind of rule-based computation we use.

    **Chinese**
    多变量情况一开始可能不太直观，所以举个例子。假设 (z=x^2y)。
    对 (x) 求偏导时，把 (y) 当常数：(\frac{\partial z}{\partial x}=2xy)。
    对 (y) 求偏导时，把 (x^2) 当常数：(\frac{\partial z}{\partial y}=x^2)。
    如果再定义 (x=s+t)，(y=s-t)，就可以把它们代入，导数会变成关于 (s,t) 的函数。
    这类计算就是按规则一步步推出来的。

  * [21:10–21:14] **Optimization workflow and gradient descent; closing**
    **English**
    So the logic is: you want to optimize the loss function—minimize it. You compute derivatives of the loss function with respect to parameters, set them to zero (in simple cases), and solve equations to get the minimizer.
    In machine learning models, the loss depends on many parameters, so you take derivatives with respect to those parameters.
    Numerically, we solve this using gradient descent. Loss functions can be complicated; sometimes they are not continuous or not differentiable. But most are piecewise differentiable, so we can still apply these methods.
    OK, I think that’s all for today’s lecture.

    **Chinese**
    所以整体逻辑是：你要优化损失函数，也就是把它最小化。简单情况下，你对损失函数关于参数求导，令导数为 0，然后解方程得到最小点。
    机器学习模型里，损失函数通常依赖很多参数，所以你要对这些参数分别求导。
    数值上，我们通常用梯度下降来做。损失函数可能很复杂；有些不连续，或不可导。但大多数是“分段可导”的，所以仍然可以用这些方法。
    好的，我想今天的课就到这里。
## Week 2
  - [19:10–19:13] **From raw features to a learnable function**
     **English**
     Price itself is just a number. What really matters is how we model the relationship between input features and that price.
     In practice, the original feature space is often not suitable for machine learning directly. So we first convert raw features into a numerical feature representation. Once this mapping is done, we try to learn a mathematical function from features to the output.

    This function is unknown in advance, so in practice we often try different functional forms and see which one works well.

    **Chinese**
     房价本身只是一个数字，真正重要的是我们如何刻画“特征”和“价格”之间的关系。
     在实际中，原始特征空间通常并不适合直接用于机器学习，因此我们需要先把原始数据转换成数值特征表示。完成这个映射之后，再去学习一个从特征到输出的数学函数。

    这个函数事先并不知道，所以通常需要尝试不同形式的函数，看看哪一种效果更好。

  - [19:13–19:15] **Feature vectors and labels (house price example)**
     **English**
     Consider a dataset of house sales. For each house, we have a feature vector, such as size, number of bedrooms, and other attributes.
     Mathematically, we write features as a bold symbol, meaning it is a vector rather than a single number.

    The price, on the other hand, is just a scalar value. So the input is a vector, while the output (label) is a single real number.
     This naturally leads us to ask: can we learn a linear function that maps feature vectors to prices?

    **Chinese**
     以房屋销售数据为例，每一套房子都有一组特征，比如面积、卧室数量等。
     在数学中，我们用加粗符号表示特征，这是因为它不是一个数，而是一组数构成的向量。

    房价本身是一个标量值。因此，输入是向量，输出（标签）是一个实数。
     这就引出了一个问题：我们能否学习一个线性函数，把特征向量映射到房价？

  - [19:15–19:16] **Linear regression model**
     **English**
     A typical linear regression model has the form
    $$
    \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots
    $$
    Here, $\theta_0$ is the intercept, and the other $\theta_i$ are coefficients for different features.

    The predicted price $\hat{y}$ is still a single number, even though the input has multiple dimensions. This is the general regression setting.

    **Chinese**
     一个典型的线性回归模型可以写成
    $$
    \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots
    $$
    其中 $\theta_0$ 是截距，其余的 $\theta_i$ 是各个特征对应的系数。

    虽然输入是多维特征，但预测结果 $\hat{y}$ 仍然是一个数。这就是回归问题的一般形式。

  - [19:16–19:17] **Regression vs. classification**
     **English**
     In regression problems, the label is a real number.
     Binary classification is a special case where labels are restricted to 0 or 1.

    In this lecture, we focus on regression, especially linear regression, and later we will discuss classification models separately.

    **Chinese**
     在回归问题中，标签是一个实数。
     二分类问题是一个特殊情况，标签只能取 0 或 1。

    本节课主要讨论回归问题，特别是线性回归，分类模型会在之后单独介绍。

  - [19:17–19:19] **Predicted values and errors**
     **English**
     For each house, we have a real price $y$ and a predicted price $\hat{y}$.
     The difference between these two is the error. Sometimes the model underestimates the real value, and sometimes it overestimates it.

    Our goal is to find parameters $\theta$ such that these errors are as small as possible across all data points.

    **Chinese**
     对于每一套房子，都有一个真实价格 $y$ 和一个预测价格 $\hat{y}$。
     两者之间的差值就是误差。有时模型会低估真实价格，有时会高估。

    我们的目标是找到一组参数 $\theta$，使得在所有数据点上的误差都尽可能小。

  - [19:19–19:21] **Loss function: why we need it**
     **English**
     To formalize “how small the error is,” we introduce a loss function.
     A loss function measures how bad a prediction is for a single data point.

    There are many possible loss functions. If the predicted value equals the real value exactly, the loss should be zero. Otherwise, the loss should be positive.

    **Chinese**
     为了定量描述“误差有多大”，我们引入损失函数。
     损失函数用于衡量单个数据点上的预测有多糟。

    损失函数的形式有很多种。如果预测值与真实值完全一致，损失应为 0；否则，损失应为正数。

  - [19:21–19:23] **Absolute loss (L1 loss / Manhattan distance)**
     **English**
     One simple loss function is the absolute loss, which measures the absolute difference between predicted and real values.
     Geometrically, this corresponds to the Manhattan distance: like walking along streets rather than cutting straight across blocks.

    In one-dimensional problems such as house price prediction, absolute loss can be used directly.

    **Chinese**
     一种简单的损失函数是绝对损失，即预测值与真实值之差的绝对值。
     在几何上，它对应“曼哈顿距离”，类似在城市街道上行走，而不是直接斜穿街区。

    在房价预测这种一维输出的问题中，可以直接使用绝对损失。

  - [19:23–19:29] **Relative loss (percentage error)**
     **English**
     Sometimes we care more about relative error than absolute error.
     Relative loss measures how much the prediction deviates proportionally from the true value.

    For example, if the real price is 2.5 million and the prediction is 3 million, the relative error is about 20%.
     If the prediction is 2 million instead, the relative error is again about 20%, but in the opposite direction.

    **Chinese**
     有时我们更关心相对误差，而不是绝对误差。
     相对损失衡量的是预测值相对于真实值偏离了多少比例。

    例如，真实价格是 250 万，而预测为 300 万，相对误差约为 20%。
     如果预测为 200 万，相对误差同样约为 20%，只是方向相反。

  - [19:29–19:35] **Squared loss and empirical risk**
     **English**
     In practice, squared loss is used very frequently because it has nice mathematical properties.
     For a dataset with many data points, we do not look at the loss of a single point, but sum up all losses and take the average.

    This average loss over the dataset is called the empirical risk.
     Minimizing empirical risk means the model fits the training data well overall.

    **Chinese**
     在实际中，平方损失被大量使用，因为它在数学上非常方便处理。
     当数据点很多时，我们不会只看单个样本的损失，而是把所有损失加起来，再取平均。

    这个平均损失称为经验风险。
     最小化经验风险，意味着模型在整体上对训练数据拟合得较好。

  - [19:35–19:39] **Learning as optimization**
     **English**
     The key idea of machine learning here is optimization.
     We want to find parameter values that minimize the empirical risk.

    For squared loss, the resulting objective function is differentiable, which allows us to use calculus tools such as derivatives and gradients to find the minimum.

    **Chinese**
     这里机器学习的核心思想是优化。
     我们的目标是找到一组参数，使经验风险最小。

    对于平方损失，目标函数是可微的，因此可以使用微积分工具，如导数和梯度，来寻找最小值。

  - [19:35–19:37] **From single-point loss to an optimization problem**
     **English**
     Suppose we only have three training data points (three houses sold). We use these three records to determine the parameters $\theta_0, \theta_1, \theta_2$.

    If we use squared loss, then the empirical risk is the average (or sum) of the squared prediction errors across these three points:
    $$
    L(\theta)=\frac{1}{3}\sum_{i=1}^{3}\big(\hat y_i-y_i\big)^2
    $$
    Here, $\hat y_i = f(x_i;\theta)$ is the predicted price of the $i$-th house and $y_i$ is the real price.

    **Chinese**
     假设我们只有三个训练样本（三套房子的成交记录），我们要用这三条数据来确定参数 $\theta_0,\theta_1,\theta_2$。

    如果采用平方损失，那么经验风险就是这三个点上预测误差平方的平均（或总和）：
    $$
    L(\theta)=\frac{1}{3}\sum_{i=1}^{3}\big(\hat y_i-y_i\big)^2
    $$
    其中 $\hat y_i=f(x_i;\theta)$ 是第 $i$ 套房子的预测价格，$y_i$ 是真实价格。

  - [19:37–19:39] **Why calculus appears: differentiable objective**
     **English**
     To find the best parameters, we want the minimum point of $L(\theta)$.
     For squared loss, $L(\theta)$ is differentiable, so we can compute its partial derivatives with respect to each parameter:
    $$
    \frac{\partial L}{\partial \theta_0},\quad \frac{\partial L}{\partial \theta_1},\quad \frac{\partial L}{\partial \theta_2}
    $$
    At a minimum point (for a smooth function), the gradient must be zero in every direction.

    **Chinese**
     为了找到最优参数，我们需要找到 $L(\theta)$ 的最小点。
     平方损失的好处是 $L(\theta)$ 可微，因此我们可以对每个参数求偏导：
    $$
    \frac{\partial L}{\partial \theta_0},\quad \frac{\partial L}{\partial \theta_1},\quad \frac{\partial L}{\partial \theta_2}
    $$
    对于光滑可微函数，在最小点处，梯度在每个方向都应当为 0。

  - [19:40–19:42] **Concrete example: the 3-house dataset**
     **English**
     Now we plug in the three house records. For each record, we have two features (for instance):

    - $x_1$: number of bedrooms
    - $x_2$: standardized area (e.g., 0.5 corresponds to 500 square feet in some normalized unit)

    The model for each house is:
    $$
    \hat y=\theta_0+\theta_1 x_1+\theta_2 x_2
    $$
    So the first house might have $x_1=1$ bedroom and $x_2=0.5$, with real price $y_1\approx 491$ (in some simplified unit, e.g., “thousands”).
     The second and third houses have their own $(x_1,x_2,y)$ values.

    **Chinese**
     接下来把三条房屋记录代入。每条记录有两个特征（例如）：

    - $x_1$：卧室数量
    - $x_2$：标准化后的面积（比如 0.5 对应 500 平方英尺这一类归一化单位）

    模型对每套房子都写成：
    $$
    \hat y=\theta_0+\theta_1 x_1+\theta_2 x_2
    $$
    比如第一套房可能是 $x_1=1$、$x_2=0.5$，真实价格 $y_1\approx 491$（为方便讲解，把单位做了简化，比如“千”为单位）。
     第二套、第三套同理，各有自己的 $(x_1,x_2,y)$。

  - [19:42–19:46] **Compute partial derivatives (gradient components)**
     **English**
     Because the inside of the square is a linear function of $\theta$, the derivatives are straightforward.
     For squared loss, each term looks like $(\hat y_i-y_i)^2$. By chain rule,
    $$
    \frac{\partial}{\partial \theta_j}(\hat y_i-y_i)^2 = 2(\hat y_i-y_i)\cdot \frac{\partial \hat y_i}{\partial \theta_j}
    $$
    And since
    $$
    \hat y_i=\theta_0+\theta_1 x_{i1}+\theta_2 x_{i2}
    $$
    we have:
    $$
    \frac{\partial \hat y_i}{\partial \theta_0}=1,\quad
    \frac{\partial \hat y_i}{\partial \theta_1}=x_{i1},\quad
    \frac{\partial \hat y_i}{\partial \theta_2}=x_{i2}
    $$
    So the gradient equations become sums over the data points, with shared terms collected together.

    **Chinese**
     因为平方项内部是关于 $\theta$ 的线性函数，所以求导很直接。
     平方损失每一项是 $(\hat y_i-y_i)^2$。用链式法则：
    $$
    \frac{\partial}{\partial \theta_j}(\hat y_i-y_i)^2 = 2(\hat y_i-y_i)\cdot \frac{\partial \hat y_i}{\partial \theta_j}
    $$
    而
    $$
    \hat y_i=\theta_0+\theta_1 x_{i1}+\theta_2 x_{i2}
    $$
    因此：
    $$
    \frac{\partial \hat y_i}{\partial \theta_0}=1,\quad
    \frac{\partial \hat y_i}{\partial \theta_1}=x_{i1},\quad
    \frac{\partial \hat y_i}{\partial \theta_2}=x_{i2}
    $$
    所以最后得到的是对所有数据点求和的方程，并把公共项合并整理。

  - [19:46–19:48] **Set gradient to zero → normal equations**
     **English**
     At the minimum, each partial derivative equals zero:
    $$
    \frac{\partial L}{\partial \theta_0}=0,\quad
    \frac{\partial L}{\partial \theta_1}=0,\quad
    \frac{\partial L}{\partial \theta_2}=0
    $$
    With three data points and three unknowns, we obtain a system of linear equations.
     This is why, for linear regression with squared loss, we can solve parameters by solving linear equations (the “normal equation” approach), rather than iterative search.

    **Chinese**
     在最小点处，每个方向的偏导都等于 0：
    $$
    \frac{\partial L}{\partial \theta_0}=0,\quad
    \frac{\partial L}{\partial \theta_1}=0,\quad
    \frac{\partial L}{\partial \theta_2}=0
    $$
    三个数据点、三个未知参数，会得到一个线性方程组。
     这就是为什么在线性回归 + 平方损失的情况下，我们往往可以直接解线性方程（正规方程），而不一定要用迭代搜索。

  - [19:48–19:52] **Perfect fit vs generalization**
     **English**
     In this small example, you may get parameters that predict these three training points very well—possibly almost perfectly.
     But fitting the training points extremely well does not guarantee good prediction on unseen data. This is the generalization issue, which we will discuss later (overfitting).

    Even for a simple linear model, the mathematics can be done exactly by solving the linear system. That is the “math behind linear regression.”

    **Chinese**
     在这个很小的例子里，算出来的参数可能会让你对这三个训练点预测得非常好，甚至几乎完全贴合。
     但训练集拟合得好，并不等于对没见过的新数据预测也好。这就是泛化问题，后面会讲到（过拟合）。

    即使线性模型很简单，它背后的数学也可以通过解线性方程组得到精确解。这就是线性回归的“数学基础”。

  - [19:52–19:57] **Why numerical computation is used in real problems**
     **English**
     For real datasets, we usually cannot solve by hand. Instead, we use numerical computation.
     Conceptually, the data table has feature columns (e.g., bedrooms, area) and a label column (price).

    We then run a computational procedure to estimate the solution and obtain the parameters.
     In modern practice, this is implemented in libraries, so once you format the data correctly, solving linear regression is straightforward.

    **Chinese**
     在真实数据集中，我们通常不可能手算解出来，因此会使用数值计算。
     从概念上看，数据表包含若干特征列（如卧室数、面积等）以及一个标签列（价格）。

    然后通过计算过程来估计最优解，得到参数。
     现在一般都由现成库实现，只要把数据格式整理好，线性回归求解就很方便。

  - [20:34–20:36] **Introducing regularization and the role of λ**
     **English**
     Now we introduce a regularization term into the loss function.
     We try different values of the regularization parameter $\lambda$, such as very small numbers (e.g. 0.0001, 0.01), moderate values, and larger values like 1 or even 100.

    When $\lambda$ is very small, the regularization effect is weak. The model mainly focuses on minimizing the training loss.
     When $\lambda$ is larger, the regularization term becomes more important and prevents parameters from becoming too large.

    **Chinese**
     现在我们在损失函数中加入正则化项。
     我们会尝试不同的正则化参数 $\lambda$，比如非常小的数（0.0001、0.01），以及较大的数（如 1、100）。

    当 $\lambda$ 很小时，正则化作用很弱，模型主要是在最小化训练损失。
     当 $\lambda$ 变大时，正则化项的影响增强，会限制参数不能变得过大。

  - [20:36–20:37] **Trade-off between fitting and model complexity**
     **English**
     With a very small $\lambda$, the model can fit the training data extremely well, but the parameter values may become very large.
     This often leads to overfitting: the model explains training data well but performs poorly on new data.

    With a moderate $\lambda$, the parameters are smaller, the training loss may increase slightly, but the model is more stable and generalizes better.

    **Chinese**
     当 $\lambda$ 非常小时，模型可以把训练数据拟合得非常好，但参数值可能会变得很大。
     这通常会导致过拟合：模型在训练集上表现很好，但在新数据上效果差。

    当 $\lambda$ 取中等值时，参数变小，训练误差可能略有上升，但模型更加稳定，泛化能力更好。

  - [20:37–20:39] **Why minimizing training loss is not enough**
     **English**
     A very small loss value only tells us that the model fits the training data well.
     It does not guarantee good prediction on unseen data.

    A good model should give reasonably good predictions on every data point, including those not used for training.
     This is exactly why we add a regularization term to the loss function.

    **Chinese**
     训练损失很小，只说明模型对训练数据拟合得好。
     这并不能保证它在没见过的数据上预测也好。

    一个好的模型，应该对所有数据点（包括未见过的数据）都有较好的预测表现。
     这正是我们在损失函数中加入正则化项的原因。

  - [20:39–20:41] **Regularized vs non-regularized optimization**
     **English**
     Without regularization, we choose parameters $\theta$ that minimize the empirical loss only.
     With regularization, we choose parameters that do not necessarily minimize training loss, but are less sensitive to small changes in the data.

    As a result, the solution from the regularized version is expected to perform better on new data points.

    **Chinese**
     没有正则化时，我们选择的是使经验损失最小的参数 $\theta$。
     加入正则化后，我们选择的参数不一定让训练损失最小，但对数据的小变化不那么敏感。

    因此，正则化版本得到的解，通常在新数据点上表现更好。

  - [20:41–20:43] **Training, validation, and test intuition**
     **English**
     In practice, we use training data to learn the model parameters.
     We then evaluate the model on validation data to decide which model or parameter setting is better.

    The goal is not to perform best on training data, but to perform well on future, unseen data.

    **Chinese**
     在实践中，我们用训练数据来学习模型参数。
     然后用验证数据来评估不同模型或不同参数设置的效果。

    目标不是在训练集上做到最好，而是在未来、未见过的数据上表现良好。

  - [20:43–20:44] **Geometric intuition: fitting lines**
     **English**
     Consider fitting a line using blue points (training data).
     If the validation points lie close to the fitted line, then the model generalizes well.

    This is the type of model we want: not necessarily perfect on training data, but good overall.

    **Chinese**
     想象用蓝色点（训练数据）去拟合一条直线。
     如果验证点也接近这条直线，说明模型的泛化能力很好。

    这正是我们想要的模型：不一定完美贴合训练数据，但整体表现良好。

  - [20:44–20:47] **Underfitting and overfitting with polynomial models**
     **English**
     If we use a very simple model, such as a linear function, it may underfit the data.
     If we use a very complex model, such as a high-degree polynomial, it may overfit the data.

    For example, a quadratic model might still not fit well, while a cubic model may fit training data extremely well but behave wildly outside the observed range.

    **Chinese**
     如果模型太简单，比如线性模型，可能会出现欠拟合。
     如果模型太复杂，比如高阶多项式，则可能出现过拟合。

    例如，二次模型可能仍然拟合不好，而三次模型可能在训练数据上贴得很紧，但在区间外表现非常不稳定。

  - [20:47–20:49] **Choosing the best model**
     **English**
     The best model is not the one with the smallest training error, but the one with small errors on both training and validation data.
     This indicates a good balance between bias and variance.

    **Chinese**
     最好的模型，不是训练误差最小的模型，而是在训练集和验证集上误差都较小的模型。
     这表明模型在偏差和方差之间取得了良好的平衡。

  - [20:49–20:52] **Final takeaway**
     **English**
     In summary, model selection involves considering all possible models and choosing one that generalizes well.
     Complex models can easily overfit unless you have enough data or proper regularization.

    This principle applies not only to linear regression, but also to modern machine learning models with millions of parameters.

    **Chinese**
     总结来说，模型选择的关键是：在所有可能的模型中，选一个泛化能力好的。
     复杂模型如果没有足够的数据或合适的正则化，很容易过拟合。

    这一原则不仅适用于线性回归，也适用于拥有上百万参数的现代机器学习模型。
***
