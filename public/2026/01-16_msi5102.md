# ___MSI5102 Essentials of Machine Learning___
***

# Tips
## Week 1 Summary
  - **Course Logistics and Tools**

    The course utilizes an interactive format using **Google Colab**, a cloud-based platform built on Jupyter Notebook technology, which allows students to collaborate on the same code simultaneously. Students are required to have a Google account to access it. The first quiz is scheduled for **Week 5**, will be one hour long, consists of approximately 15 multiple-choice questions (no fill-in-the-blank), and is closed-book with no AI tools allowed.

    本周讲座采用了互动的形式，使用 **Google Colab**，这是一个基于 Jupyter Notebook 技术的云端平台，允许学生同时协作处理相同的代码。学生需要拥有 Google 账号才能访问它。第一次测验安排在**第 5 周**，时长一小时，包含大约 15 道选择题（没有填空题），并且是闭卷考试，不允许使用 AI 工具。

  - **AI Landscape and Machine Learning Fundamentals**

    We discussed "Moravec's Paradox": reasoning is hard for humans but easy for machines, while perception (driving, recognizing faces) is easy for humans but historically hard for machines.  However, Deep Learning has bridged this gap, handling tasks like translation and face recognition effectively. Machine Learning is the dominant approach to AI, solving problems via mathematical modeling, primarily through **Classification** (Supervised Learning) and **Clustering** (Unsupervised Learning).

    我们讨论了“莫拉维克悖论”：推理对人类来说很难，但对机器来说很容易；而感知（驾驶、识别人脸）对人类来说很容易，但历史上对机器来说却很难。 然而，深度学习已经弥合了这一差距，有效地处理了翻译和人脸识别等任务。机器学习是 AI 的主流方法，通过数学建模解决问题，主要包括**分类**（监督学习）和**聚类**（无监督学习）。

  - **Feature Engineering and Data Representation**

    To use mathematics, raw data must be mapped to real vectors.

    - **Categorical Data:** Nominal data (e.g., fruit types) uses "One-hot embedding" to ensure zero similarity between distinct categories.  Ordinal data (e.g., rankings) is mapped to consecutive integers to preserve order.
    - **Text & Images:** Techniques like **Word2vec** map words with similar contexts to nearby points in vector space.
    - **Standardization:** Features with large ranges (like house prices) are normalized using **z-scores** or **Logarithm Transforms** to make the learning process more accurate.

    为了使用数学方法，原始数据必须映射为实向量。

    - **分类数据：** 名义型数据（如水果类型）使用“独热嵌入”来确保不同类别之间的零相似性。 有序型数据（如排名）映射为连续整数以保留顺序。
    - **文本与图像：** 像 **Word2vec** 这样的技术将语境相似的词映射到向量空间中邻近的点。
    - **标准化：** 具有大范围特征（如房价）的数据通过 **z-score** 或**对数变换**进行归一化，以使学习过程更准确。

  - **Essential Calculus for Machine Learning**

    Machine Learning is fundamentally an **optimization** problem: minimizing a loss function by adjusting parameters.

    - **Functions:** We deal with linear, polynomial, exponential (Gaussian), and piecewise linear functions (like the absolute value function, which relates to ReLU in deep learning).
    - **Derivatives & Gradients:** The derivative measures the rate of change; to find a minimum, we look for where the gradient (vector of derivatives) is zero. We typically use **Gradient Descent** to solve this numerically rather than analytically.
    - **Rules:** You must master the Sum Rule, Product Rule, and the **Chain Rule**, which is critical for calculating derivatives in composite functions found in neural networks.

    机器学习本质上是一个**优化**问题：通过调整参数来最小化损失函数。

    - **函数：** 我们处理线性、多项式、指数（高斯）以及分段线性函数（如绝对值函数，这与深度学习中的 ReLU 有关）。
    - **导数与梯度：** 导数衡量变化率；为了找到极小值，我们要寻找梯度（导数向量）为零的地方。我们通常使用**梯度下降**来进行数值求解，而不是解析求解。
    - **法则：** 你必须掌握和法则、积法则以及**链式法则**，这对于计算神经网络中复合函数的导数至关重要。
## Week 1 Learning Tips
  - **Don't Reinvent the Wheel:** For the group project, focus on **model reproduction**. Find published work on sites like GitHub and verify their code and results rather than building a model from scratch.

    **不要重复造轮子：** 对于小组项目，专注于**模型复现**。在 GitHub 等网站上找到已发表的作品，验证他们的代码和结果，而不是从头开始构建模型。

  - **Brush Up on Calculus Rules:** The "Chain Rule" is not just abstract math; it is the mechanism behind **backpropagation** in neural networks. Ensure you can manually calculate derivatives using the Product Rule and Chain Rule as shown in the lecture examples.

    **复习微积分法则：** “链式法则”不仅仅是抽象的数学；它是神经网络中**反向传播**背后的机制。确保你能像讲座示例中那样，使用积法则和链式法则手动计算导数。

  - **Understand Data Mapping:** When prepping data, ask yourself if the features are "Nominal" or "Ordinal". Using the wrong embedding (e.g., assigning an order to fruit types) can confuse the model. Remember to standardize data with large variances using Log Transforms or Z-scores.

    **理解数据映射：** 在准备数据时，问问自己特征是“名义型”还是“有序型”。使用错误的嵌入（例如给水果类型分配顺序）会混淆模型。记住要使用对数变换或 Z-scores 来标准化具有大方差的数据。
## Week 2 Summary: Linear Models & Generalization
  - This week, we established the fundamental framework for Supervised Learning, focusing on **Linear Models** for regression. We began by discussing **Feature Mapping**, converting raw data (like house attributes) into numerical vectors that mathematical functions can process. We defined the **Linear Predictor** (f(x) = θ · x) and explored how to measure its performance using various **Loss Functions**, including L1 (Absolute) Loss, Relative Loss, and the widely used L2 (Square) Loss.
  - The core training methodology introduced was **Empirical Risk Minimization (ERM)**, where we find the optimal parameters (θ) by minimizing the average loss over the training set. We demonstrated this mathematically by calculating partial derivatives to solve for θ analytically. We also contrasted this parametric approach with **Non-Parametric methods** like **k-Nearest Neighbors (k-NN)**, which rely on "lazy learning" from stored data rather than learning a fixed function.
  - Finally, we addressed the critical challenge of **Generalization**. We used polynomial regression to illustrate **Overfitting**: while complex models (high-degree polynomials) fit training data perfectly, they often fail on unseen test data due to high sensitivity. To solve this, we introduced **Regularization**. By adding a penalty term to the loss function (penalizing large weights θ), we can constrain the model's complexity, making it less sensitive to noise and improving its ability to predict new data.
  - 本周我们建立了监督学习的基本框架，重点关注用于回归任务的**线性模型**。我们首先讨论了**特征映射**，即将原始数据（如房屋属性）转换为数学函数可以处理的数值向量。我们定义了**线性预测器** (f(x) = θ · x)，并探讨了如何使用各种**损失函数**来衡量其性能，包括 L1（绝对）损失、相对损失以及广泛使用的 L2（平方）损失。
  - 我们介绍的核心训练方法是**经验风险最小化 (ERM)**，即通过最小化训练集上的平均损失来寻找最佳参数 (θ)。我们通过计算偏导数解析地求解 θ，在数学上演示了这一过程。我们还将这种参数化方法与**非参数方法**（如 **k-最近邻 (k-NN)**）进行了对比，后者依赖于从存储数据中进行“懒惰学习”，而不是学习一个固定的函数。
  - 最后，我们解决了**泛化**这一关键挑战。我们利用多项式回归展示了**过拟合**现象：虽然复杂的模型（高次多项式）能完美拟合训练数据，但由于其高敏感性，往往在未见的测试数据上表现不佳。为了解决这个问题，我们引入了**正则化**。通过在损失函数中添加惩罚项（惩罚较大的权重 θ），我们可以限制模型的复杂度，降低其对噪声的敏感度，从而提高其预测新数据的能力。
## Week 2 Learning Tips
  - 1. **Derive, Don't Just Memorize:** When studying Linear Regression, try to derive the partial derivatives for the Square Loss function yourself. Understanding how the chain rule applies here is crucial for understanding how gradients are calculated, which is the foundation for training more complex models like Neural Networks later.
  - 2. **Visualize the Bias-Variance Tradeoff:** Use the polynomial curve examples to build a mental image of "Overfitting." Remember that a "wiggly" line that hits every training point is usually worse than a smooth line that misses a few. Understand that the goal of Machine Learning is not 0% training error, but low test error.
  - 3. **Understand the "Why" of Regularization:** Don't just memorize the formula with λ. Focus on the intuition: Why do large parameters (θ) make a model "sensitive"? (Hint: Large weights mean a small change in input x creates a huge change in output y). Regularization forces the model to be "calm" and stable.
  - 4. **Compare Parametric vs. Non-Parametric:** Create a comparison table between Linear Regression and k-NN. Note the differences in training time (Linear is slow to train, fast to predict; k-NN is zero training, slow to predict) and interpretability.
  - 1. **推导，而不仅仅是记忆：** 在学习线性回归时，试着自己推导平方损失函数的偏导数。理解链式法则在这里如何应用，对于理解梯度的计算至关重要，这也是后续学习神经网络等复杂模型的基础。
  - 2. **可视化偏差-方差权衡：** 利用多项式曲线的例子在脑海中构建“过拟合”的图像。要记住，一条穿过每个训练点的“扭曲”曲线，通常比一条稍微错过几个点的平滑曲线要差。要明白机器学习的目标不是追求 0% 的训练误差，而是追求低的测试误差。
  - 3. **理解正则化的“为什么”：** 不要只死记带有 λ 的公式。要专注于直觉：为什么大的参数 (θ) 会让模型变得“敏感”？（提示：大的权重意味着输入 x 的微小变化会导致输出 y 的巨大变化）。正则化强制模型保持“冷静”和稳定。
  - 4. **对比参数化与非参数化方法：** 制作一个对比表来比较线性回归和 k-NN。注意它们在训练时间上的差异（线性回归训练慢、预测快；k-NN 零训练、预测慢）以及可解释性上的不同。
## Week 3 Summary
  - **Linear Models and the Evolution of Loss Functions**

    In Lecture 2, we established the foundation of using linear models for binary classification. A critical insight was understanding why the standard **Square Loss** is flawed for classification tasks: it penalizes predictions that are "too correct" (i.e., far from the decision boundary on the correct side). To address this, we introduced **Logistic Loss** (used in Logistic Regression) which dampens this penalty, and **Hinge Loss** (used in SVM) which eliminates the penalty entirely for correct confident predictions. We also discussed **Regularization** as the primary tool to constrain model complexity and prevent overfitting.

    在第 2 讲中，我们奠定了使用线性模型进行二分类的基础。一个关键的见解是理解为什么标准的**平方损失**在分类任务中存在缺陷：它会惩罚那些“过于正确”的预测（即在正确一侧远离决策边界的预测）。为了解决这个问题，我们介绍了**Logistic 损失**（用于逻辑回归），它减轻了这种惩罚；以及 **Hinge 损失**（用于 SVM），它完全消除了对正确且确信预测的惩罚。我们还讨论了**正则化**作为限制模型复杂度和防止过拟合的主要工具。

  - **Evaluation Metrics for Imbalanced Data**

    We moved beyond simple "Accuracy," which can be misleading when data is imbalanced (e.g., predicting rare events like earthquakes). We introduced a suite of more robust metrics: **Sensitivity (Recall)**, **Specificity**, and **Precision**. We also learned to visualize model performance using the **AUROC** (Area Under the Receiver Operating Characteristic) curve. The goal of a good classifier is to balance sensitivity and specificity effectively.

    我们超越了简单的“准确率”，因为在数据不平衡时（例如预测地震等罕见事件），准确率可能会产生误导。我们介绍了一套更稳健的指标：**灵敏度（召回率）**、**特异性**和**精确率**。我们还学习了使用 **AUROC**（受试者工作特征曲线下面积）来可视化模型性能。一个好的分类器的目标是有效地平衡灵敏度和特异性。

  - **Probabilistic Predictors and Entropy**

    In Lecture 3, we shifted from **Point Predictors** (which give a single answer) to **Probabilistic Predictors** (which output a distribution of chances, e.g., 80% Rain, 20% Sun). This approach is more robust for ambiguous data. We learned to evaluate these predictors using **Likelihood** and **Negative Log-Likelihood** (to handle small number multiplication). Finally, we introduced **Entropy** and **Cross-Entropy** from Information Theory as ways to measure the distance between the predicted probability distribution and the true distribution.

    在第 3 讲中，我们从**点预测器**（只给出一个答案）转向了**概率预测器**（输出概率分布，例如 80% 雨，20% 晴）。这种方法对于模棱两可的数据更为稳健。我们学习了使用**似然**和**负对数似然**（为了处理小数相乘的问题）来评估这些预测器。最后，我们介绍了信息论中的**熵**和**交叉熵**，以此来衡量预测概率分布与真实分布之间的距离。
## Week 3 Learning Tips
  - **Focus on Concepts, Not Calculation**

    The professor explicitly stated regarding the quiz: "No calculations." Do not waste time memorizing complex arithmetic procedures for updating weights or manually calculating gradients. Instead, focus on the **qualitative properties**:

    - Why do we prefer Hinge Loss over Square Loss for classification?
    - What does it mean if Entropy is high vs. low?
    - What is the relationship between overfitting and model complexity?
    - Why do we take the "Negative Log" of likelihood? (To turn a tiny product into a manageable sum).

    教授关于测验明确指出：“没有计算。”不要浪费时间背诵更新权重的复杂算术过程或手动计算梯度。相反，应专注于**定性属性**：

    - 为什么在分类中我们更喜欢 Hinge 损失而不是平方损失？
    - 熵高与熵低意味着什么？
    - 过拟合与模型复杂度之间有什么关系？
    - 为什么我们要取似然的“负对数”？（为了将极小的乘积转化为可处理的和）。

  - **Prepare for the "Australian Rain" Project**

    This optional project is a perfect sandbox to test your understanding of the full pipeline discussed in Lecture 2. Pay attention to the data preprocessing steps mentioned: removing missing values (NA) and dropping irrelevant columns like "Sunshine" or "WindGustDir." This project forces you to apply the theoretical **Regularized Logistic Regression** to a real-world messy dataset.

    这个可选项目是测试你对第 2 讲中讨论的完整流程理解的完美沙盒。注意提到的数据预处理步骤：删除缺失值（NA）和丢弃如“日照”或“阵风风向”等无关列。这个项目迫使你将理论上的**正则化逻辑回归**应用于现实世界杂乱的数据集。

  - **Clarify the "Point" vs. "Probabilistic" Distinction**

    Ensure you understand that a Point Predictor is just a special case of a Probabilistic Predictor (where one probability is 1 and the rest are 0). This is a favorite conceptual question. Understand that KNN (K-Nearest Neighbors) can be easily adapted into a probabilistic predictor by simply looking at the ratio of labels among the neighbors (e.g., 3 red, 3 blue = 0.5 probability).

    确保你理解点预测器只是概率预测器的一个特例（其中一个概率为 1，其余为 0）。这是一个常见的概念性问题。要理解 KNN（K-最近邻）可以通过简单地查看邻居中标签的比例（例如，3 红，3 蓝 = 0.5 概率）轻松适应为概率预测器。
## Week 4 Summary
  - **1. Decision Trees: The Foundation 决策树：基础**
    - **Concept:** A decision tree is a tree-structured model that mimics human reasoning using a set of "if-then-else" rules. It splits data into branches to classify samples, making it highly interpretable and suitable for disjoint classes.

      **概念：** 决策树是一种模仿人类推理的树状模型，使用一组“如果-那么-否则”规则 。它将数据分裂成分支以对样本进行分类，这使得它具有高度的可解释性，非常适合处理互斥的类别 。
    - **Construction (Entropy & Gain):** To build the tree, we use **Entropy** to measure the disorder or complexity of the data. We select the split feature that provides the maximum **Information Gain**, which corresponds to the greatest reduction in entropy.

      **构建（熵与增益）：** 为了构建树，我们使用**熵**来衡量数据的无序度或复杂性 。我们选择能够提供最大**信息增益**的分裂特征，这意味着熵的减少量最大 。
    - **Example:** In the "Tennis Play" example, attributes like "Outlook" (Sunny, Overcast, Rain) are used to split the dataset until the leaf nodes are pure (Entropy = 0).

      **示例：** 在“打网球”的例子中，我们使用像“天气状况”（晴朗、阴天、雨天）这样的属性来分裂数据集，直到叶节点变纯（熵=0） 。
  - **2. Regression Trees: Predicting Numbers 回归树：预测数值**
    - **Difference:** Unlike classification trees which output classes, regression trees output a numeric value, which is typically the **average** of the target values for the samples in that leaf node.

      **区别：** 与输出类别的分类树不同，回归树输出一个数值，通常是该叶节点中样本目标值的**平均值** 。
    - **Splitting Metric (Squared Loss):** Instead of entropy, we use **Squared Loss** (Sum of Squared Residuals). We search for a split point (e.g., Dosage < 14.5) that minimizes the sum of squared differences between the actual values and the average value in each resulting subset.

      **分裂度量（平方损失）：** 我们不使用熵，而是使用**平方损失**（残差平方和）。我们寻找一个分裂点（例如：剂量 < 14.5），使得每个结果子集中的实际值与平均值之间的平方差之和最小 。
    - **Dosage Example:** The lecture illustrated this with drug effectiveness data. The tree partitioned the dosage range into intervals (e.g., Low, Medium, High dosage), fitting a constant value (step function) for each interval to capture non-linear relationships.

      **剂量示例：** 讲座用药物疗效数据阐述了这一点。树将剂量范围划分为若干区间（例如：低、中、高剂量），并为每个区间拟合一个常数值（阶梯函数），以捕捉非线性关系 。

  - **3. Random Forest: Bagging & Randomness 随机森林：Bagging与随机性**
    - **Bagging (Bootstrap Aggregating):** Random Forest improves upon single trees by creating an ensemble. It generates multiple datasets by **sampling with replacement** (Bagging) and trains a tree on each.

      **Bagging（自助汇聚法）：** 随机森林通过创建一个集成模型来改进单棵树。它通过**有放回抽样**（Bagging）生成多个数据集，并在每个数据集上训练一棵树 。
    - **Feature Randomness:** To prevent trees from being too similar (correlated), Random Forest selects a **random subset of features** (size $m$) at each split instead of checking all features. This diversity makes the model robust against overfitting.

      **特征随机性：** 为了防止树过于相似（相关），随机森林在每次分裂时选择一个**随机特征子集**（大小为 $m$），而不是检查所有特征 。这种多样性使得模型对过拟合具有鲁棒性 。
    - **Validation:** It uses **Out-of-Bag (OOB)** samples—data points left out during bootstrapping—to estimate accuracy without needing a separate validation set.

      **验证：** 它使用**袋外（OOB）**样本——即自助抽样过程中未被选中的数据点——来估计准确性，而无需单独的验证集 。

  - **4. XGBoost: Sequential Correction XGBoost：顺序修正**
    - **Boosting Concept:** Unlike Random Forest (parallel), XGBoost builds trees **sequentially**. Each new tree is trained to predict the **residuals (errors)** of the previous ensemble, not the original target values.

      **Boosting概念：** 与随机森林（并行）不同，XGBoost是**顺序**构建树的。每一棵新树都被训练来预测前一个集成模型的**残差（误差）**，而不是原始目标值 。
    - **Regularization & Learning Rate:** It adds new trees with a small **Learning Rate** (e.g., 0.1) to ensure gradual improvement and prevent overfitting. It also includes regularization terms in its objective function.

      **正则化与学习率：** 它以较小的**学习率**（例如 0.1）加入新树，以确保逐步改进并防止过拟合 。它还在目标函数中包含了正则化项 。
    - **Status:** It is a scalable, state-of-the-art system widely used in competitions (Kaggle) and industry for structured data.

      **地位：** 它是一个可扩展的、最先进的系统，广泛应用于竞赛（Kaggle）和工业界的结构化数据处理 。
## Week 4 Learning Tips
  - **1. Distinguish the "Split Criteria" (分类标准 vs. 回归标准)**
    - **Concept:** Remember that Classification Trees use **Entropy/Gini** (measuring purity), while Regression Trees use **Squared Error** (measuring variance/distance).
    - **Action:** When reviewing the "Drug Dosage" example, manually calculate the squared error for one split (e.g., split at dosage=3) to feel how the "average" minimizes the error on each side.
    - **概念：** 记住分类树使用**熵/基尼系数**（衡量纯度），而回归树使用**平方误差**（衡量方差/距离）。
    - **行动：** 在复习“药物剂量”示例  时，手动计算一个分裂点（例如在剂量=3处分裂）的平方误差，去感受“平均值”是如何最小化两侧误差的。

  - **2. Understand the "Evolution" of Tree Models (模型演进)**
    - **Concept:** Visualize the progression:
      - **Single Tree:** Easy to interpret, but overfits (High Variance).
      - **Random Forest:** Many independent trees averaged together (Lowers Variance).
      - **XGBoost:** Many trees correcting each other's mistakes sequentially (Lowers Bias & Variance).
    - **Action:** Draw a flowchart comparing "Bagging" (Parallel) vs. "Boosting" (Sequential) to solidify the architectural difference.
    - **概念：**通过可视化来理解演进过程：
      - **单棵树：** 易于解释，但容易过拟合（高方差）。
      - **随机森林：** 许多独立树的平均（降低方差）。
      - **XGBoost：** 许多树顺序修正彼此的错误（降低偏差和方差）。
    - **行动：** 画一个流程图对比“Bagging”（并行）与“Boosting”（串行），以巩固架构上的差异。

  - **3. Master the "Residual" Logic in XGBoost (掌握XGBoost的残差逻辑)**
    - **Concept:** The most critical takeaway for XGBoost is that Tree 2 predicts `(Actual - Tree 1 Prediction)`, not `Actual`. This is why we add the predictions together.
    - **Action:** Re-watch the part where we calculated `88 - 71.2 = 16.8`. Try to create a dummy dataset of 3 points and simulate 2 rounds of XGBoost updates on paper.
    - **概念：** XGBoost最关键的知识点是：树2预测的是 `(真实值 - 树1的预测值)`，而不是 `真实值`。这就是为什么我们将预测值相加。
    - **行动：** 重看我们要计算 `88 - 71.2 = 16.8` 的部分 。尝试在纸上创建一个包含3个点的虚拟数据集，并模拟2轮XGBoost的更新过程。

  - **4. Memorize Key Hyperparameters (熟记关键超参数)**
    - **Concept:** For practical application (like your Kaggle projects), you need to know what knobs to turn.
      - `n_estimators`: Too few = underfitting; Too many = diminishing returns.
      - `max_features`: The knob for controlling correlation between trees.
    - **Action:** Review the table on slide 39  regarding default `max_features` (`√p` for classification vs `p/3` for regression, where `p` represents the total number of features).
    - **概念：** 对于实际应用（比如你的Kaggle项目），你需要知道调节哪些旋钮。
      - `n_estimators`（树的数量）：太少=欠拟合；太多=收益递减 。
      - `max_features`（最大特征数）：控制树之间相关性的旋钮 。
    - **行动：** 复习幻灯片39页上的表格 ，关注默认的 `max_features` （分类问题用 √p，回归问题用 p/3，p 代表特征数量）。
## Week 5 Summary: From Decision Trees to XGBoost
  - **1. Decision and Regression Trees (The Foundation)**

    The lecture began with **Decision Trees**, which classify data by splitting samples based on attributes that maximize **Information Gain** (or minimize Entropy). For continuous variables (like age or blood pressure), the algorithm tests various "cutoff" points to find the best split. **Regression Trees** operate similarly but predict continuous numeric values instead of classes. Instead of entropy, they use **Squared Loss** (minimizing the variance of residuals). A key takeaway is that the prediction value at a leaf node is simply the **average** of the target values of the samples in that leaf.

    讲座从**决策树**（Decision Trees）开始，它通过最大化**信息增益**（Information Gain）或最小化熵（Entropy）来根据属性划分样本，从而对数据进行分类。对于连续变量（如年龄或血压），算法会测试各种“截断”点以找到最佳划分。**回归树**（Regression Trees）的运作方式类似，但它预测的是连续的数值而不是类别。它不使用熵，而是使用**平方损失**（Squared Loss，即最小化残差的方差）。一个关键点是，叶节点的预测值仅仅是该叶节点中样本目标值的**平均值**。

  - **2. Random Forests (Bagging & Ensembling)**

    To improve upon single trees, which are prone to overfitting, we introduced **Random Forests**. This method uses **Bagging** (Bootstrap Aggregating), where multiple trees are trained on different subsets of data sampled with replacement. Crucially, Random Forests add an extra layer of randomness: at each node split, the algorithm only considers a random subset of features rather than all available features. This decorrelates the trees, reducing the variance of the final model.

    为了改进容易过拟合的单棵树，我们介绍了**随机森林**（Random Forests）。该方法使用**Bagging**（自助聚合），即在有放回抽样的数据子集上训练多棵树。关键在于，随机森林增加了一层额外的随机性：在每个节点分裂时，算法只考虑特征的一个随机子集，而不是所有可用特征。这降低了树之间的相关性，从而减少了最终模型的方差。

  - **3. XGBoost (Gradient Boosting)**

    The core of the lecture focused on **XGBoost**, a scalable tree boosting system. Unlike Random Forests, which build independent trees, XGBoost builds trees sequentially. Each new tree attempts to predict the **residuals** (errors) of the previous ensemble. The model uses a specific "Node Score" formula derived from the gradients (G) and Hessians (H) of the loss function: `Score = G^2 / (H + λ)`. This formula incorporates **Regularization** (λ) directly into the tree-building process to control complexity. For classification, the model predicts **log-odds**, which are then converted to probabilities using the Sigmoid function.

    讲座的核心重点是 **XGBoost**，这是一个可扩展的树提升系统。与构建独立树的随机森林不同，XGBoost 是按顺序构建树的。每一棵新树都试图预测前一个集成模型的**残差**（错误）。该模型使用一个特定的“节点分数”公式，该公式源自损失函数的梯度（G）和海森矩阵（H）：`Score = G^2 / (H + λ)`。这个公式将**正则化**（λ）直接融入到建树过程中以控制复杂度。在分类问题中，模型预测的是**对数几率**（log-odds），然后使用 Sigmoid 函数将其转换为概率。
## Week 5 Learning Tips: How to Master This Week's Content
  - **1. Master the "Node Score" Formula**

    Do not just memorize the XGBoost score formula; understand what the components represent. The numerator, `(Sum of Residuals)^2`, rewards nodes that group large errors together (so they can be corrected). The denominator, `Number of Samples + λ`, penalizes nodes with very few samples. This is a built-in mechanism to prevent overfitting. If you understand that `λ` acts as the "brake" on splitting small groups, you understand the core of XGBoost regularization.

    不要死记硬背 XGBoost 的分数公式，要理解各个组成部分代表什么。分子 `(残差之和)^2` 奖励那些将大误差聚集在一起的节点（以便修正它们）。分母 `样本数量 + λ` 惩罚那些样本极少的节点。这是一个防止过拟合的内置机制。如果你能理解 `λ` 起到了“刹车”的作用，阻止对小群体进行分裂，你就掌握了 XGBoost 正则化的核心。

  - **2. Distinguish Between Regression and Classification Trees**

    It is easy to confuse the outputs of these two. Remember:

    - **Regression Trees:** The leaf output is a direct numeric value (the average of residuals).
    - **Classification Trees:** The leaf output is a **log-odds** score (weight), NOT a probability. You must apply the **Sigmoid function** (`1 / (1 + e^-w)`) to transform this score into a probability between 0 and 1.

    很容易混淆这两者的输出。请记住：

    - **回归树：** 叶子输出是直接的数值（残差的平均值）。
    - **分类树：** 叶子输出是一个**对数几率**分数（权重），**不是**概率。你必须应用 **Sigmoid 函数** (`1 / (1 + e^-w)`) 将此分数转换为 0 到 1 之间的概率。

  - **3. Understand the Hyperparameters Trade-off**

    The lecture highlighted `λ` (Lambda) and `γ` (Gamma). Create a mental model of their effects:

    - **High `λ` or `γ`:** The model is conservative. It requires a massive gain in accuracy to justify making a split. This leads to **shallower trees** and potential underfitting.

    - **Low `λ` or `γ`:** The model is aggressive. It will split even for small gains. This leads to **deeper trees** and potential overfitting.

      Tuning these parameters is about finding the "Goldilocks" zone.

    讲座强调了 `λ` (Lambda) 和 `γ` (Gamma)。建立一个关于它们效果的思维模型：

    - **高 `λ` 或 `γ`：** 模型是保守的。它需要巨大的准确率提升来证明分裂的合理性。这会导致**较浅的树**和潜在的欠拟合。

    - **低 `λ` 或 `γ`：** 模型是激进的。即使增益很小它也会分裂。这会导致**较深的树**和潜在的过拟合。

      调整这些参数就是为了找到“恰到好处”的平衡点。
***

# Lectures
## Week 1
  - [19:05 - 19:09] **Course Format and Google Colab Platform**

    In terms of the class format, I want to make it easy for everyone. Unlike a traditional lecture, we will have interactive components. Now, for the second half of the session, I will discuss the core technical content. We will be using a programming platform that allows you to work on the same code from different places. This is Google Colab. It is built on Jupyter Notebook technology and runs entirely in the cloud. This means multiple people can work on the same code at the same time, which is no problem and very convenient for your projects. However, this does mean you will need to have a Google account to access it. Does anyone have questions about this plan? If you really want to try it out, we can proceed.

    关于课程形式，我希望让大家都感到轻松。不同于传统的讲座，我们会有互动的环节。现在进入后半部分，我将讨论核心的技术内容。我们将使用一个允许你们从不同地点处理相同代码的编程平台，这就是 Google Colab。它基于 Jupyter Notebook 技术，完全在云端运行。这意味着多人可以同时处理同一份代码，这没有任何问题，对你们的项目也非常方便。不过，这意味着你们需要一个 Google 账号来使用它。关于这个计划大家有什么问题吗？如果你们想尝试一下，那我们就开始吧。

  - [19:09 - 19:30] **AI vs. Machine Learning: The "Moravec's Paradox" and Industry Trends**

    Let's start discussing the learning material. The first question usually is: "Why Machine Learning?". AI is a buzzword right now in industry and society. AI is defined as the science and engineering of making intelligent machines. When we talk about "General Intelligence", humans can perform many different intelligent tasks: we can drive a car, recognize a friend's face in an image, use language, accumulate knowledge, reason, do mathematics, and play games like Chess or Go.

    Interestingly, the difficulty level for machines is the reverse of what it is for humans. For humans, reasoning and calculus are hard, but perception is easy. For machines, "Classical AI" found reasoning and calculus easy, but general intelligence and perception were very hard. However, AI technology is developing at unprecedented speed. We now use Deep Learning to allow machines to do things that were previously difficult, like translating languages and recognizing faces. I recall reviewing your applications last year; some of you might remember using automated gates at the airport where the machine scans your passport and face—Deep Learning works very well for these tasks.

    Because of this success, there are numerous opportunities in the AI industry. For example, in January 2026, Meta bought the AI startup "Manus" for more than $2 billion. Manus develops LLM-powered AI agents. This is a huge success story. On the flip side, this challenges young professionals. There was an article about an ex-Google executive, Jad Tarifi, who suggested that traditional degrees might be a waste of time because AI will solve those fields by the time you graduate. Even in education, I feel less like a teacher and more like an examiner now, because you can find all materials online.

    Years ago in China, if you traveled to a new region, you were considered smart if you could navigate the bus system and book a hotel efficiently. It was a complex processing problem. Now, with tools like Google Maps, you can go anywhere without asking a single question. The success of AI relies heavily on Machine Learning, which is the dominant approach. Machine Learning solves AI problems through mathematical modeling. We have two main tasks: Classification (Supervised Learning), where we use labeled examples to predict the label of an unseen object—similar to how biology classifies species—and Clustering (Unsupervised Learning).

    让我们开始讨论学习材料。通常第一个问题是：“为什么要学习机器学习？”。AI（人工智能）目前在工业界和社会上都是一个热门词汇。AI 被定义为制造智能机器的科学和工程。当我们谈论“通用智能” 时，人类可以执行许多不同的智能任务：我们可以开车、在图像中认出朋友的脸、使用语言、积累知识、推理、做数学题，以及玩国际象棋 或围棋。

    有趣的是，机器的困难层级与人类恰恰相反。对人类来说，推理和微积分很难，但感知很容易。对机器而言，“古典 AI”发现推理和微积分很容易，但通用智能和感知却非常难。然而，AI 技术正以前所未有的速度发展。我们现在使用深度学习 让机器能够完成以前很难的事情，比如翻译语言和识别人脸。我记得去年审查你们的申请材料时——你们中有些人可能记得在机场使用自动闸机，机器扫描你的护照和人脸——深度学习在这些任务上表现得非常好。

    正因为这种成功，AI 行业出现了无数的机会。例如，在 2026 年 1 月，Meta 以超过 20 亿美元的价格收购了 AI 初创公司“Manus”。Manus 开发基于大语言模型的 AI 代理。这是一个巨大的成功故事。但另一方面，这也给年轻人带来了挑战。有一篇文章提到前谷歌高管 Jad Tarifi 建议传统的学位可能是浪费时间，因为等你毕业时 AI 可能已经解决了那些领域的问题。即使在教育领域，我现在感觉自己更像是一个考官而不是老师，因为你们可以在网上找到所有资料。

    几年前在中国，如果你去一个新的地区旅行，能搞清楚公交系统并高效预订酒店会被认为是很聪明的。那是一个复杂的信息处理问题。现在，有了像谷歌地图这样的工具，你不需要问任何人就可以去任何地方。AI 的成功很大程度上依赖于机器学习，这是目前的主流方法。机器学习通过数学建模来解决 AI 问题。我们要解决两个主要任务：分类（监督学习），即利用标记的样本预测未见对象的标签——类似于生物学对物种的分类——以及聚类（无监督学习）。

  - [19:30 - 19:52] **Deep Learning vs. Classical Methods and Feature Engineering Formulation**

    In this course, I will focus on classical methods like Linear Regression and Decision Trees before moving to Deep Learning. In classical methods, we map data examples into a structured vector space and learn a mathematical function for prediction. Deep Learning is slightly different; it allows the machine to automatically extract informative features from the data. This is inspired by the neural networks in our brain. Our brain has huge neural circuits consisting of numerous neurons. Each neuron has a simple function: it receives signals, accumulates them, and if a threshold is crossed, it fires an outgoing signal. Deep learning emulates this with layers of simple computing units that, when combined, become very powerful. However, Deep Learning requires a lot of data. If you don't have enough data, you must go back to classical models.

    Now, let's look at "Feature Engineering for Modeling". The core question is: How do we convert learning problems into algorithmic problems where mathematics can be used?. Consider a medical diagnosis task: predicting if a patient has cancer based on health records. The input might be 8 Boolean values (symptoms), and the output is a Boolean value (Cancer Yes/No). Or consider digit recognition, where the input is a 28x28 pixel image.

    Formally, a data point has two parts: the feature part `x` (properties) and the label part `y` (the answer). We try to learn a predicting function `f` from the feature space `X` to the label space `Y`. In supervised learning, the label space `Y` is not empty. For regression, `Y` is a real number (or real vector). For binary classification, `Y` is `{0, 1}`. The difficulty lies in non-numeric features. Real-world data, like the "House Sale" example, contains text (address), images (photos), and categories (Bungalow vs. Apartment). Health records contain dates, text notes, and binary flags. We must map these records into real vectors.

    This mapping has two purposes: to convert learning into numerical computing, and to put different data types on an equal footing. If one feature dominates the process just because of its scale, you won't get good results. We map components independently. For images, we map each pixel's RGB values to a vector. We want "Faithful Embeddings", meaning if two data samples `u` and `v` are similar in reality, their vector representations `φ(u)` and `φ(v)` should be close in the vector space (e.g., small Euclidean distance).

    在本课程中，由于深度学习之前的铺垫，我将首先通过线性回归和决策树 等经典方法展开讨论。在经典方法中，我们将数据样本映射到结构化的向量空间，并学习一个数学函数来进行预测。深度学习稍有不同；它允许机器自动从数据中提取有用的特征。这是受到人脑神经网络启发的。我们的大脑拥有巨大的神经回路，由无数神经元组成。每个神经元的功能很简单：它接收信号，积累信号，如果超过阈值，它就发射输出信号。深度学习通过层层简单的计算单元模拟了这一点，当它们组合在一起时，就会变得非常强大。但是，深度学习需要大量数据。如果你没有足够的数据，你就必须回到经典模型。

    现在，我们来看看“建模的特征工程”。核心问题是：我们如何将学习问题转化为可以使用数学的算法问题？。考虑一个医疗诊断任务：根据健康记录预测患者是否患有癌症。输入可能是 8 个布尔值（症状），输出是一个布尔值（癌症 是/否）。或者考虑数字识别，输入是 28x28 像素的图像。

    形式上，一个数据点有两部分：特征部分 `x`（属性）和标签部分 `y`（答案）。我们试图学习一个从特征空间 `X` 到标签空间 `Y` 的预测函数 `f`。在监督学习中，标签空间 `Y` 不是空的。对于回归，`Y` 是实数（或实向量）。对于二分类，`Y` 是 `{0, 1}`。困难在于非数值特征。现实世界的数据，比如“房屋销售”的例子，包含文本（地址）、图像（照片）和类别（平房与公寓）。健康记录包含日期、文本注释和二进制标记。我们必须将这些记录映射为实向量。

    这种映射有两个目的：将学习转化为数值计算，并将不同的数据类型置于平等的地位。如果某个特征仅仅因为它的尺度大而主导了过程，你就得不到好的结果。我们独立地映射每个分量。对于图像，我们将每个像素的 RGB 值映射为一个向量。我们需要“忠实的嵌入（Faithful Embeddings）”，这意味着如果两个数据样本 `u` and `v` 在现实中是相似的，它们的向量表示 `φ(u)` 和 `φ(v)` 在向量空间中也应该是接近的（例如，欧几里得距离很小）。

  - [19:52 - 20:21] **Handling Categorical Data, Word Embeddings, and Standardization**

    For categorical data, we have two types: Nominal and Ordinal. Nominal data, like product types or colors, has no inherent order. For these, we use "One-hot embedding". If we have `k` elements, we map them to the standard basis vectors of `R^k` (the k-dimensional real space). For example, if we have three fruits—Apple, Orange, Banana—we map Apple to `(1, 0, 0)`, Orange to `(0, 1, 0)`, and Banana to `(0, 0, 1)`. In this vector space, the inner product between any two different vectors is zero, implying zero similarity, which is appropriate as they are distinct categories. If you want to reduce the dimension by one, you can map the last element (Banana) to the zero vector `(0, 0)`. This is called "Reduced one-hot embedding." However, mapping Banana to `(0, 0)` might induce an implicit ordering (magnitude) compared to the others, but it is still linearly independent.

    For ordinal data, like Hotel Ranks (1-star to 5-stars) or Likert scales, there is an order. We can map these `k` elements to consecutive integers, preserving the order (e.g., 1, 2, 3, 4, 5).

    For text, a very important technique is "Word2vec". Developed by Google News from 100 billion words, it captures semantic meaning. As shown in the slide, words like "annoyed," "scared," and "frustrated" are mapped to points close together in the vector space because they share similar contexts. Similarly, for images, we use Deep Learning methods like VGG16. This maps images to vectors where, for instance, the distance between the vector for a "Car" and a "Bus" is smaller than the distance between a "Car" and something unrelated.

    Finally, we must discuss Standardization. We need to map values into a small region, usually centered around 0 with a standard deviation of 1. Some features, like house prices or areas, can have very large ranges. We calculate the mean `μ` and standard deviation `σ` and compute the z-score: `x = (u - μ) / σ`. For variables with wide scales (like house prices), we often apply a Logarithm Transform (`log u`) first to reduce the range, and then standardize. This makes the learning process easier and more accurate.

    This concludes the section on feature engineering. I will stop here for a break. If you have difficulties, please email me. Regarding the group project, I can assign you to a group if needed; the group size is 4 to 6 students.

    对于分类数据，我们要处理两种类型：名义型（Nominal）和有序型（Ordinal）。名义型数据，如产品类型或颜色，没有内在顺序。对于这些，我们使用“独热嵌入（One-hot embedding）”。如果我们有 `k` 个元素，我们将它们映射到 `R^k`（k 维实空间）的标准基向量。例如，如果我们有三种水果——苹果、橙子、香蕉——我们将苹果映射为 `(1, 0, 0)`，橙子为 `(0, 1, 0)`，香蕉为 `(0, 0, 1)`。在这个向量空间中，任意两个不同向量之间的内积为零，意味着零相似性，这很合适，因为它们是不同的类别。如果你想减少一个维度，你可以将最后一个元素（香蕉）映射为零向量 `(0, 0)`。这被称为“简化独热嵌入”。不过，将香蕉映射到 `(0, 0)` 可能会引入与其他向量相比的隐式排序（大小），但它仍然是线性无关的。

    对于有序型数据，如酒店星级（1 星到 5 星）或李克特量表，存在顺序。我们可以将这 `k` 个元素映射为连续的整数，以保留顺序（例如 1, 2, 3, 4, 5）。

    对于文本，一个非常重要的技术是“Word2vec”。它由 Google News 基于 1000 亿个单词开发，能够捕捉语义。如幻灯片所示，像“annoyed（恼怒）”、“scared（害怕）”和“frustrated（沮丧）”这样的词在向量空间中被映射到靠得很近的点，因为它们共享相似的语境。同样，对于图像，我们使用像 VGG16 这样的深度学习方法。这将图像映射为向量，例如，“汽车”和“公共汽车”向量之间的距离会小于“汽车”与不相关物体之间的距离。

    最后，我们必须讨论标准化。我们需要将数值映射到一个小区域，通常是以 0 为中心，标准差为 1。某些特征，如房价或面积，可能具有非常大的范围。我们要计算均值 `μ` 和标准差 `σ` 并计算 z-score：`x = (u - μ) / σ`。对于具有大跨度的变量（如房价），我们要先应用对数变换（`log u`）来缩小范围，然后再进行标准化。这使得学习过程更容易、更准确。

    特征工程部分就讲到这里。我们先休息一下。如果你有困难，请给我发邮件。关于小组项目，如果有需要，我可以把你分配到一个小组；小组规模是 4 到 6 名学生。

  - [20:22 - 20:25] **Project Evaluation and Code Reproducibility**

    Let's discuss how the group project is marked and evaluated. Nowadays, in the field of Machine Learning, when researchers publish their work, they almost always upload the corresponding data and code to a public website, such as GitHub. This transparency makes it very easy for others to verify the code and run it themselves. With modern AI assistant tools, running this existing code has become even simpler. I will show you examples of this later. So, this is the core of the project: you do not need to invent everything from scratch. You will find a published work, download their code—what we call "model reproduction"—and run it. Your task is to verify if you can get the same results they claim. You might just run their code to see whether you can reproduce their result or not. For example, you might perform some extra validations on their model. It is not just about comparing numbers; it is about understanding the implementation. With the test data in the correct format, building or running these models is often very easy. In the second half of the lecture, I will show you some samples of code where executing a complex model takes just one line.

    我们来谈谈项目是如何评分的。如今在机器学习领域，研究人员发表论文时，通常会将数据和代码上传到像 GitHub 这样的公共网站。这种透明度使得其他人很容易验证并运行他们的代码。有了现在的 AI 辅助工具，运行这些现有的代码变得更加简单。稍后我会给你们展示相关的例子。所以，这个项目的核心在于：你们不需要从头开始发明所有东西。你们需要找到一篇已发表的论文，下载他们的代码——也就是我们所说的“模型复现”——然后运行它。你们的任务是验证能否得到他们声称的结果。你们可能只需要运行代码，看看能否复现结果，或者对他们的模型进行一些额外的验证。这不仅仅是比较数字，更是为了理解其实现过程。只要测试数据的格式正确，构建或运行这些模型通常非常容易。在课程的后半部分，我会向你们展示一些代码示例，有时候执行一个复杂的模型只需要一行代码。

  - [20:25 - 20:26] **Project Components, Topics, and Custom Proposals**

    Let's look at the second part of the assessment, which involves improving the model or conducting an extended comparative analysis, followed by an oral presentation. The grading rubric is broken down into these specific components. In Week 6, I will announce a list of suggested works for the project. These will be standard Deep Learning topics appropriate for this course. However, I know that most of you are part-time students, so you may have workplace commitments or specific industry interests. If you feel you want to do a different project—perhaps something more relevant to your job—that is absolutely fine. Please just discuss it with me in advance so I can approve it. Is that clear? The coding part itself should be quite manageable.

    我们来看看评估的第二部分，这涉及到改进模型或进行扩展的比较分析，随后是口头报告。评分标准是按照这些具体部分细分的。在第 6 周，我会公布一份建议的项目列表。这些将是适合本课程的标准深度学习课题。不过，我知道你们大多数人是在职学生，可能有工作任务或特定的行业兴趣。如果你觉得想做一个不同的项目——也许是与你的工作更相关的——那是完全没问题的。请提前和我讨论，以便我批准。这部分清楚了吗？代码部分本身应该是很容易上手的。

  - [20:27 - 20:29] **Scheduling the First Quiz**

    Regarding the schedule for the first quiz: originally, I planned to hold it at a specific time, but a few of you came up to me during the break and mentioned you would be away. It turns out that Week 5 seems to be a good time for most of you, but we need to be sure there are no remaining conflicts. Does anyone else have a problem with Week 5? We need to settle this because Chinese New Year falls in Week 6, and we definitely want to avoid scheduling anything during that holiday week.

    关于第一次测验的时间安排：我原本计划在一个特定的时间举行，但在休息期间有几位同学过来跟我说他们那时候不在。目前看来，第 5 周对大多数人来说似乎是个不错的时间，但我们需要确定没有其他的冲突。还有人对第 5 周有意见吗？我们需要把这事定下来，因为第 6 周是农历新年，我们肯定要避免在那一周安排任何考试。

  - [20:29 - 20:30] **Quiz Format and Rules**

    There is another important rule for the quiz: you cannot use AI tools. Regarding the materials, you cannot open PDFs during the quiz; it is closed-book. A student asked about the length and format. The first quiz will be one hour long. It consists of roughly 15 multiple-choice questions. There are no fill-in-the-blank questions for this quiz, only multiple-choice.

    关于测验还有一条重要的规则：你们不能使用 AI 工具。关于资料，测验期间不能打开 PDF 文件，这是闭卷考试。有同学问到了考试时长和形式。第一次测验时长为一小时。它包含大约 15 道选择题。这次测验没有填空题，只有选择题。

  - [20:40 - 20:41] **Optimization and Loss Functions**

    Machine learning is fundamentally about optimization. The goal is to find the model parameters that minimize a "loss function." I think you likely already know this concept, even if you are not yet familiar with the specific mathematical definition of "loss" in this context. Essentially, the computer or "calculator" provides the tools, but your objective is to find the specific parameter values that result in the minimum loss. This process is the same whether you are training a simple regression model or a complex neural network. You adjust the weights (parameters) until the error is minimized.

    机器学习本质上就是优化。我们的目标是找到能使“损失函数”最小化的模型参数。我想你们可能已经知道了这一概念，即使你们通过这门课才刚刚接触“损失”的具体数学定义 。基本上，计算机提供了工具，但你们的任务是找出能让损失函数达到最小值的特定参数值 。无论你是在训练简单的回归模型还是复杂的神经网络，这个过程都是一样的 。你需要不断调整权重（参数），直到误差降到最低。

  - [20:41 - 20:44] **Function Complexity: From Linear to High-Degree Polynomials**

    Let's look at the types of functions we deal with. First, we have linear functions, which are very easy to understand. If you plot a linear function with one variable, it forms a line; with two variables, it forms a flat plane or "hyperplane" in a high-dimensional space. Intuition works well here. However, beyond linear models, things get harder. For example, a quadratic function forms a curve (a parabola) or a curved surface. While it is harder than a line, we still understand it well—it has a clear minimum or maximum point, like the bottom of a bowl. But when we go beyond degree 2—to cubic or quartic functions—it becomes mathematically very difficult to find simple analytical solutions for the minimum points. We also encounter exponential functions, such as the Gaussian function (the "Bell curve"), which is widely used in probability and statistics. These functions exist in high-dimensional spaces in machine learning, making them difficult to visualize.

    我们来看看我们要处理的函数类型。首先是线性函数，这非常容易理解 。如果你绘制一个单变量线性函数，它是一条直线；如果是双变量，它在高维空间中形成一个平面或“超平面” 。直觉在这里很管用。然而，除此之外，情况就变得复杂了。例如，二次函数会形成一条曲线（抛物线）或曲面 。虽然比直线难，但我们仍然很了解它——它有一个清晰的极小值或极大值点，就像碗底一样. 但是，当我们处理超过二次的函数——比如三次或四次函数时——要找到简单的解析解来确定极值点在数学上变得非常困难 。我们还会遇到指数函数，比如在概率统计中广泛使用的的高斯函数（“钟形曲线”）。在机器学习中，这些函数存在于高维空间中，这使得它们很难可视化。

  - [20:44 - 20:47] **Piecewise Linear Functions**

    Sometimes a single linear model cannot fit the data effectively. In these cases, we use what is called a "piecewise linear function". A classic example is the absolute value function, `f(x) = |x|`. This function makes everything positive: if `x` is positive, the value is `x`; if `x` is negative, we "flip" it to become `-x`. In the slides, we see a function defined in three distinct pieces. For `x < -2`, it follows one line; between `-2` and `0`, it follows another; and for `x > 0`, it follows a third. This approach is basically "divide and conquer." We split the domain into intervals and use a different simple linear expression for each interval. In deep learning, functions like ReLU (Rectified Linear Unit) are based on this exact piecewise linear concept.

    有时候，单一的线性模型无法有效地拟合数据。在这种情况下，我们会使用所谓的“分段线性函数” 。一个典型的例子是绝对值函数 `f(x) = |x|` 。这个函数让所有值都变为正数：如果 `x` 是正数，值就是 `x`；如果 `x` 是负数，我们就把它“翻转”成 `-x` 。在幻灯片中，我们可以看到一个由三部分组成的函数 。当 `x < -2` 时，它遵循一条直线；在 `-2` 和 `0` 之间，它遵循另一条；当 `x > 0` 时，它遵循第三条 。这种方法基本上就是“分而治之”。我们将定义域分成不同的区间，并为每个区间使用不同的简单线性表达式。在深度学习中，像 ReLU（修正线性单元）这样的函数正是基于这种分段线性的概念。

  - [20:48 - 20:51] **Composite Functions**

    We can combine simple functions to form very complicated ones. Mathematically, we call this a "composite function". This is formed by applying one function to the result of another. For example, if you have `z = sin(y)` and `y = x^3`, you can combine them to get `z = sin(x^3)`. In machine learning, this corresponds to mapping sample points through multiple layers of feature spaces before finally mapping them to a label. You apply the first function, get a value, and then apply the next function to that value. Deep learning essentially relies on this idea of nesting many functions (layers) together to model complex, non-linear relationships.

    我们可以组合简单的函数来形成非常复杂的函数。在数学上，我们称之为“复合函数” 。这是通过将一个函数应用于另一个函数的结果而形成的 。例如，如果你有 `z = sin(y)` 和 `y = x^3`，你可以将它们组合成 `z = sin(x^3)` 。在机器学习中，这对应于将样本点通过多层特征空间映射，最后映射到标签 。你应用第一个函数，得到一个值，然后将下一个函数应用于该值。深度学习本质上依赖于这种将许多函数（层）嵌套在一起的想法，以模拟复杂的非线性关系 。

  - [20:52 - 20:58] **Derivatives and Gradients**

    How do we find the minimum value of these functions? We use derivatives. The derivative measures the rate of change. For a continuous function, we know that at a minimum or maximum point, the slope (derivative) must be zero. Think of it as an indicator: if the derivative is positive, the function is increasing, so the minimum is to your left (you should move back). If the derivative is negative, the function is decreasing, so the minimum is ahead (move forward). In high-dimensional spaces with multiple variables, we consider the "gradient," which is the vector of partial derivatives in every direction (`x1`, `x2`, etc.). At the minimum point, this entire gradient vector must be zero. Since we often cannot solve for this point analytically, we do it numerically. We calculate the gradient at our current point and use it to guide us step-by-step toward the minimum. This is the foundation of "Gradient Descent".

    我们要如何找到这些函数的最小值呢？我们使用导数 。导数衡量的是变化率 。对于连续函数，我们知道在极小值或极大值点，斜率（导数）必须为零 。把它看作一个指示器：如果导数为正，函数正在增加，所以最小值在你的左边（你应该往回移）。如果导数为负，函数正在减少，所以最小值在前方（向前移）。在具有多个变量的高维空间中，我们考虑“梯度”，即各个方向（`x1`, `x2` 等）偏导数组成的向量 。在极小值点，整个梯度向量必须为零 。由于我们通常无法通过解析方法解出这个点，我们采用数值方法。我们计算当前点的梯度，并利用它一步步引导我们要向最小值移动。这就是“梯度下降”的基础 。

  - [20:58 - 21:02] **Derivative Rules: Sum and Product**

    To calculate derivatives, we rely on a few specific rules. The "Sum Rule" is straightforward, like elementary school math: the derivative of a sum is just the sum of the derivatives. The "Product Rule" is slightly more difficult. If you have a function `h(x) = f(x)g(x)`, the derivative is `f'(x)g(x) + f(x)g'(x)`. For example, if you have `x^2 * sin(x)`, you treat `x^2` as the first function and `sin(x)` as the second. Applying the rule: the derivative of the first (`2x`) times the second (`sin(x)`), plus the first (`x^2`) times the derivative of the second (`cos(x)`). This gives `2x*sin(x) + x^2*cos(x)`. You must know these rules to derive gradients for training.

    为了计算导数，我们依赖几条特定的规则。“和法则”很直观，就像小学数学一样：和的导数就是导数的和 。“积法则”稍微难一点。如果你有一个函数 `h(x) = f(x)g(x)`，其导数是 `f'(x)g(x) + f(x)g'(x)` 。例如，如果你有 `x^2 * sin(x)` ，你把 `x^2` 看作第一个函数，`sin(x)` 看作第二个。应用规则：第一个函数的导数（`2x`）乘以第二个函数（`sin(x)`），加上第一个函数（`x^2`）乘以第二个函数的导数（`cos(x)`）。结果是 `2x*sin(x) + x^2*cos(x)` 。你们必须掌握这些规则才能推导用于训练的梯度。

  - [21:03 - 21:09] **The Chain Rule: A Detailed Example**

    Now, let's look at the Chain Rule, which is crucial for neural networks. Let's walk through the specific example on the slide. We have `z = x^2y`, where `x` and `y` are themselves functions of `s` and `t`: `x = s + t` and `y = s - t`. We want to find the partial derivative of `z` with respect to `s` (`∂z/∂s`). According to the Chain Rule formula: `∂z/∂s = (∂z/∂x)(∂x/∂s) + (∂z/∂y)(∂y/∂s)`.

    1. `∂z/∂x` is `2xy` (since `y` is constant here).
    2. `∂x/∂s` is `1` (derivative of `s+t` wrt `s`).
    3. `∂z/∂y` is `x^2` (since `x` is constant here).
    4. `∂y/∂s` is `1` (derivative of `s-t` wrt `s`). Combining these: `(2xy * 1) + (x^2 * 1) = 2xy + x^2`. If we substitute back `x` and `y`, we get `2(s+t)(s-t) + (s+t)^2`. Similarly, for `∂z/∂t`, notice that `∂y/∂t` is `-1`. So the result becomes `2xy - x^2`. This process of propagating derivatives through nested functions is exactly what happens during backpropagation in neural networks.

    现在，我们来看看对神经网络至关重要的“链式法则” 。我们来详细过一遍幻灯片上的例子 。假设 `z = x^2y`，其中 `x` 和 `y` 本身又是 `s` 和 `t` 的函数：`x = s + t` 且 `y = s - t` 。我们要找出 `z` 对 `s` 的偏导数（`∂z/∂s`）。 根据链式法则公式：`∂z/∂s = (∂z/∂x)(∂x/∂s) + (∂z/∂y)(∂y/∂s)` 。

    1. `∂z/∂x` 是 `2xy`（因为这里 `y` 是常数）。
    2. `∂x/∂s` 是 `1`（`s+t` 对 `s` 的导数）。
    3. `∂z/∂y` 是 `x^2`（因为这里 `x` 是常数）。
    4. `∂y/∂s` 是 `1`（`s-t` 对 `s` 的导数）。 组合起来：`(2xy * 1) + (x^2 * 1) = 2xy + x^2` 。如果我们把 `x` 和 `y` 代回原式，就得到 `2(s+t)(s-t) + (s+t)^2` 。 同样地，对于 `∂z/∂t`，注意 `∂y/∂t` 是 `-1`。所以结果变成 `2xy - x^2` 。这种通过嵌套函数传播导数的过程，正是神经网络中反向传播所发生的过程 。

  - [21:10 - 21:14] **Solving for Parameters: The Numerical Approach**

    To summarize, training a model means minimizing the loss function. We calculate the derivative of the loss function with respect to the parameters and set it to zero to find the minimum. This gives us a system of equations. However, for complex models, we often cannot solve these equations analytically (using algebra) to get a simple formula for the parameters. Instead, we solve them numerically using Gradient Descent. We iteratively adjust the parameters in the direction of the negative gradient. Even if the loss function is complicated or piecewise (like in deep learning), as long as it is differentiable in pieces, we can apply this method to find the optimal solution. That is all for today's lecture.

    总结一下，训练模型意味着最小化损失函数。我们要计算损失函数对参数的导数，并令其为零以找到极小值 。这会给我们一个方程组。然而，对于复杂的模型，我们通常无法通过解析方法（代数）解出这些方程来得到参数的简单公式。相反，我们使用梯度下降法进行数值求解 。我们在负梯度的方向上迭代调整参数。即使损失函数很复杂或是分段的（如在深度学习中），只要它在各分段内是可微的，我们就可以应用这种方法找到最优解。今天的课就到这里。
## Week 2
  - [19:10 - 19:12] **Feature Mapping and Recap**

    The price is just a number. That is the function, okay? This is the concept we discovered in the last lecture. The main issue is that you have an original feature space, which is normally not suitable for machine learning directly. Therefore, we have to convert these original features into a real vector space. Once that is done, you can learn a mathematical function. Okay? For instance, predicting movie sales works well with this approach. It is an unknown function, so sometimes you need to try different ways to model it. This is what we discussed in the last selection.

    价格只是一个数字。这就是那个函数，好吗？这是我们在上一讲中探讨过的概念。主要的问题在于，原本的特征空间通常不适合直接用于机器学习。因此，我们必须把这些原始特征转换成实向量空间。一旦完成了这一步，你就可以学习一个数学函数了。好吗？例如，预测电影票房就用这种方法效果很好。这是一个未知的函数，所以有时你需要尝试不同的方法来建模。这就是我们上一节讨论的内容。

  - [19:12 - 19:16] **Vector Notation and Linear Predictors**

    We have a dataset of `k` sales numbers. Basically, we denote the features of the first house as `x`. Here, for `k` houses, this represents the features of the first house, and then we have the price. In mathematics, the way we write this bold stuff—like **x**—means it is not just a single number; it is a vector, a group of numbers. For the house sale, the price is just a scalar number. So, you will see the input becomes a normalized vector. The question is: Can we try to learn a linear function?

    Rather than just looking at features, we look at the label space. For the average house price, this is a regression problem. The red dot in our visualization represents one data sample. Since the output is one dimension, we try to learn a linear function to estimate the value. For this example, the feature vector has components like the number of bedrooms and the area. Therefore, what we learn is a linear function of the form: `f(x) = θ_0 + θ_1 x_1 + θ_2 x_2`. Now you see, the price is just one number, making this a standard setting. The binary classification problem is actually just a special case where the labels are just 0 or 1. We will discuss binary classification later, but for now, this is a regression problem. Is that clear? We just try to use a matrix method to find out what `θ_0` is, what `θ_1` is, and what `θ_2` is.

    我们有一组 `k` 个销售数据。基本上，我们将第一栋房子的特征表示为 `x`。在这里，对于 `k` 栋房子，这代表了第一栋房子的特征，然后我们有价格。在数学中，当我们写这种黑体字——比如 **x**——时，意味着它不仅仅是一个数字，它是一个向量，是一组数字。对于房屋销售来说，价格只是一个标量。所以，你会看到输入变成了标准化的向量。问题是：我们能不能尝试学习一个线性函数？

    我们不只是看特征，还要看标签空间。对于平均房价，这是一个回归问题。我们可视化中的红点代表一个数据样本。因为输出是一维的，我们尝试学习一个线性函数来估计这个值。在这个例子中，特征向量包含了像卧室数量和面积这样的分量。因此，我们要学习的是这种形式的线性函数：`f(x) = θ_0 + θ_1 x_1 + θ_2 x_2`。现在你看，价格只是一个数字，这使它成为一个标准的设置。二分类问题实际上只是一个特例，其标签只有 0 或 1。我们要稍后讨论二分类，但现在，这是一个回归问题。清楚了吗？我们只是尝试用矩阵方法来找出 `θ_0`、`θ_1` 和 `θ_2` 是什么。

  - [19:17 - 19:20] **The Fitting Process**

    Basically, this process of estimation is called the fitting process. What we do is take this sample, okay? We assume the label is the price, denoted as `y_1`, `y_2`, ..., `y_k`. We have already created a heat map or visualization. Here, we have the `y`, which is the real price—the price in the actual cycle. But we also have the estimate for each of these. Therefore, for the first house, we have an estimate, let's call it `y_hat_1` (`ŷ_1`). This is the predictive value based on the property's features.

    What we want to do is find the best parameters to give the best prediction. You are like a doctor trying to find the best treatment. We use this logic: for one direction, the predicted value should be equal to the true value. We look at the difference between the predictive value and the real value. Sometimes the real value is underestimated, and sometimes it is overestimated. We want to find the parameters `θ` to minimize this difference as much as possible for every data point.

    基本上，这个估计的过程被称为拟合过程。我们要做的就是利用这些样本，好吗？我们假设标签是价格，表示为 `y_1`、`y_2`，一直到 `y_k`。我们已经创建了一个热力图或可视化图表。在这里，我们有 `y`，这是真实价格——实际周期中的价格。但我们也有对每一个样本的估计值。因此，对于第一栋房子，我们要有一个估计值，我们称之为 `y_hat_1` (`ŷ_1`)。这是基于房产特征得出的预测值。

    我们要做的就是找到最佳参数来给出最好的预测。你就像医生试图找到最佳治疗方案一样。我们使用这样的逻辑：在一个方向上，预测值应该等于真实值。我们要看预测值和真实值之间的差异。有时真实值被低估了，有时被高估了。我们的目标是找到参数 `θ`，使得每一个数据点的这种差异尽可能地小。

  - [19:20 - 19:23] **Empirical Risk Minimization (ERM) and Loss Functions**

    So, we will use an algorithm called ERM (Empirical Risk Minimization). This is a typical approach. The algorithm uses a defined loss function. There are many possible functions you can define. Ideally, if you predict the real price exactly, the loss is zero. Also, the loss should always be a positive function. Dealing with negative functions would be difficult, so that's not what we want. The loss function must satisfy this property: even if the loss value is small, it implies that this is a good guess.

    因此，我们将使用一种称为 ERM（经验风险最小化）的算法。这是一种典型的方法。该算法使用定义的损失函数。你可以定义许多可能的函数。理想情况下，如果你准确预测了真实价格，损失就是零。此外，损失应该始终是一个正函数。处理负函数会很困难，所以那不是我们要的。损失函数必须满足这个属性：即使损失值很小，也意味着这是一个很好的猜测。

  - [19:23 - 19:27] **L1 Loss (Absolute Loss)**

    Let's look at what equations we use in practice. The first one is what we call the L1 loss or absolute loss. In vector form, if you have a plan, what is the distance? This function is exactly equal to the sum of the two physical distances. Think about it: when you walk on the street, you cannot just take a shortcut (the diagonal); sometimes you have to follow the street grid. This is the meaning of this loss. One part corresponds to the x-direction difference, and another part corresponds to the y-direction difference. So, this is the sum of the absolute differences between the corresponding coordinates.

    For example, if you have point (1, 1) and (3, 2/3), we calculate the "street distance" or Manhattan distance. You are blocked from going diagonally, so you must sum the absolute differences.

    让我们看看在实践中我们使用什么方程。第一个是我们所说的 L1 损失或绝对损失。在向量形式中，如果你有一个平面图，距离是多少？这个函数正好等于两个物理距离的总和。试想一下：当你在街上走路时，你不能直接抄近路（走对角线）；有时你必须沿着街道网格走。这就是这个损失的含义。一部分对应 x 方向的差异，另一部分对应 y 方向的差异。所以，这是对应坐标之间绝对差值的总和。

    例如，如果你有点 (1, 1) 和 (3, 2/3)，我们计算“街道距离”或曼哈顿距离。你无法走对角线，所以必须将绝对差值相加。

  - [19:27 - 19:33] **Relative Loss**

    Some people also use what we call relative loss. This is only good for one dimension, say for house prices. If you have a predicted value, sometimes you overestimate the real value. One term is positive, but if it is perfectly predicted, it is zero. If it is underestimated, one term becomes negative, so we define the loss using the maximum of the two relative ratios to keep it positive. There are two possible terms for any prediction, and we just take the positive one.

    I will give you an example. If the real price is 2.5 million, and your prediction is bigger, say 3 million. This ratio is bigger than one. So the loss is `3 / 2.5 - 1`, which is positive. This is the relative rate of overestimation. It is like 20%. But if the real price is 2.5 million and you predict 2 million, then you use the inverse: `2.5 / 2 - 1`. This gives you the loss for underestimation. This is very meaningful and is something we use.

    有些人也使用我们所说的相对损失。这只适用于一维数据，比如房价。如果你有一个预测值，有时你会高估真实值。其中一项是正的，但如果预测完美，它就是零。如果被低估了，一项会变成负数，所以我们使用两个相对比率的最大值来定义损失，以保持其为正。任何预测都有两个可能的项，我们只取正的那一项。

    我举个例子。如果真实价格是 250 万，而你的预测值更大，比如 300 万。这个比率大于 1。所以损失是 `3 / 2.5 - 1`，这是正数。这是高估的相对比率。就像是 20%。但如果真实价格是 250 万，而你预测 200 万，那么你就要用倒数：`2.5 / 2 - 1`。这给了你低估时的损失。这是非常有意义的，也是我们使用的一种方法。

  - [19:33 - 19:35] **Square Loss and Empirical Risk**

    In machine learning, people use square loss a lot because it is a nice function compared to the others. Although absolute loss is simple, we don't really have good mathematical tools to differentiate it easily at all points. We have a multiple target optimization problem, and reaching the best situation for every point is not easy. We don't really have the math tools to solve for all targets simultaneously. That is why we consider "massaging" all the losses together. We sum them all together and take the average to define the **Empirical Risk** of the model.

    If the total loss is small, then the loss for every data point is likely small. Conversely, if the error for each point is small, the total empirical risk is small. This is easy to do mathematically. So that is why we have this empirical risk of the model. We try to work on minimizing this aggregate loss function for estimation problems.

    在机器学习中，人们经常使用平方损失，因为与其他函数相比，它是一个很好的函数。虽然绝对损失很简单，但我们并没有好的数学工具在所有点上轻松地对其求导。我们面临的是一个多目标优化问题，要让每个点都达到最佳情况并不容易。我们确实没有数学工具能同时解出所有目标。这就是为什么我们要把所有的损失“揉”在一起。我们将它们加在一起并取平均值，从而定义模型的**经验风险**。

    如果总损失很小，那么每个数据点的损失很可能也很小。反之，如果每个点的误差都很小，总经验风险也会很小。这在数学上很容易处理。这就是为什么我们要引入模型的经验风险。我们在估计问题中致力于最小化这个聚合损失函数。

  - [19:35 - 19:40] **Worked Example: Fitting a Linear Model**

    Is that clear? If not, let's work through this example with three data points. Suppose there are only three houses sold, and we use this data to get the parameters `θ_0`, `θ_1`, and `θ_2`. We use the square loss. The empirical risk `L(θ)` is the sum of squared errors. `y_hat` is the predicted value on one data point, and `y` is the real price.

    We need to find out what `θ_0` and `θ_1` are. If we look at the function, a parabola like `x^2`, we know it has a minimum point. When we add these functions together, the resulting function is still a quadratic bowl shape. Since it is differentiable, the derivative exists everywhere. Therefore, we basically just calculate the partial derivative for every direction: for `θ_0`, for `θ_1`, and for `θ_2`. Then we solve these equations.

    Let's look at the data. The first data point: number of bedrooms is 1, area size is 0.5 (in units of 1000 sq ft), and the price is 491k. This gives us the predicted value for the first house versus the real price. The second house has 2 bedrooms and 1 unit of area, with a price of 975k. The third one follows the same logic. We try to find `θ_0`, `θ_1`, `θ_2` to minimize the error.

    清楚了吗？如果不清楚，让我们来看这个有三个数据点的例子。假设只卖了三栋房子，我们用这些数据来求参数 `θ_0`、`θ_1` 和 `θ_2`。我们使用平方损失。经验风险 `L(θ)` 是平方误差的总和。`y_hat` 是对一个数据点的预测值，`y` 是真实价格。

    我们需要找出 `θ_0` 和 `θ_1` 是什么。如果我们看这个函数，像 `x^2` 这样的抛物线，我们知道它有一个极小值点。当我们把这些函数加在一起时，结果函数仍然是一个二次碗状。因为它是可微的，导数处处存在。因此，我们基本上只需要计算每个方向的偏导数：针对 `θ_0`、`θ_1` 和 `θ_2`。然后我们解这些方程。

    让我们看看数据。第一个数据点：卧室数量是 1，面积大小是 0.5（单位是 1000 平方英尺），价格是 491k。这给出了第一栋房子的预测值与真实价格的对比。第二栋房子有 2 间卧室和 1 单位面积，价格是 975k。第三个以此类推。我们试图找到 `θ_0`、`θ_1`、`θ_2` 来最小化误差。

  - [19:40 - 19:47] **Calculating Partial Derivatives**

    It is very easy to calculate the partial derivative for this function. For the first part—the partial derivative with respect to `θ_0`—the term inside the square is a linear function. The coefficient in front of `θ_0` is 1. So when we differentiate the squared term `(θ_0 + ...)^2`, we use the chain rule: it becomes `2 * (expression) * 1`. We do this for all three data points and sum them up.

    When we do the partial derivative for the `θ_1` direction, something similar happens, but there is a constant factor coming from the coefficient of `θ_1`. For the first house, the coefficient of `θ_1` is 1 (bedroom). For the second house, it is 2. For the third house, it is 3. So when you differentiate, you multiply by these coefficients: 1, 2, and 3 respectively. You put all the common terms together, and you get a linear equation.

    Then we go to `θ_2`. Again, we have coefficients: 0.5 for the first house, 1 for the second, and 2 for the third. These come from the area feature `x_2`.

    How do we find the minimum? We know at a minimum point, the derivative must be zero in every direction. So, we have to solve this system of linear equations to get the values of `θ_0`, `θ_1`, and `θ_2`. Luckily, this is a system of linear equations. We have 3 equations and 3 variables, so we can solve this. For this example, we get `θ_0 = 7`, `θ_1 = 483`, and `θ_2 = 2`.

    计算这个函数的偏导数非常容易。对于第一部分——关于 `θ_0` 的偏导数——平方项内部是一个线性函数。`θ_0` 前面的系数是 1。所以当我们对平方项 `(θ_0 + ...)^2` 求导时，我们使用链式法则：它变成 `2 * (表达式) * 1`。我们对所有三个数据点都这样做并把它们加起来。

    当我们对 `θ_1` 方向求偏导时，情况类似，但会有一个来自 `θ_1` 系数的常数因子。对于第一栋房子，`θ_1` 的系数是 1（卧室数）。对于第二栋房子，它是 2。对于第三栋房子，它是 3。所以当你求导时，你要分别乘以这些系数：1、2 和 3。你把所有同类项放在一起，就得到一个线性方程。

    然后我们看 `θ_2`。同样，我们有系数：第一栋房子是 0.5，第二栋是 1，第三栋是 2。这些来自面积特征 `x_2`。

    我们如何找到最小值？我们知道在极小值点，导数在每个方向上必须为零。所以，我们需要解这个线性方程组来得到 `θ_0`、`θ_1` 和 `θ_2` 的值。幸运的是，这是一个线性方程组。我们有 3 个方程和 3 个变量，所以我们可以解出来。在这个例子中，我们得到 `θ_0 = 7`，`θ_1 = 483`，`θ_2 = 2`。

  - [19:47 - 19:51] **Reviewing the Result and Generalization**

    We get the result: `θ_0 = 7`, `θ_1 = 483`, `θ_2 = 2`. If you plug these back in, for the first data point, the prediction is `7 + 483(1) + 2(0.5) = 491`. It matches exactly. For the second point, `7 + 483(2) + 2(1) = 975`. Also an exact match. It predicts these three values perfectly. However, there is no guarantee of generalization to new data, which is something we will discuss later.

    Anyway, this is the whole process. You see, the linear model is very easy; you just solve this linear equation. This method has been used for many years. Is that clear? This is the mathematics behind the linear model.

    我们得到了结果：`θ_0 = 7`，`θ_1 = 483`，`θ_2 = 2`。如果你把这些值代回，对于第一个数据点，预测值是 `7 + 483(1) + 2(0.5) = 491`。完全吻合。对于第二个点，`7 + 483(2) + 2(1) = 975`。也是完全吻合。它完美地预测了这三个值。然而，这不能保证对新数据的泛化能力，这是我们稍后会讨论的内容。

    不管怎样，这就是全过程。你看，线性模型非常简单；你只需要解这个线性方程。这种方法已经使用了许多年。清楚了吗？这就是线性模型背后的数学原理。

  - [19:52 - 19:57] **Numerical Computation for Other Loss Functions**

    We worked through this example so we know how it works mathematically. However, for other loss functions, like the absolute loss, we don't have a closed-form analytic solution. We can only use numerical computation. Basically, we might just guess 100 values for `θ_0`, guess 100 values for `θ_1`, and calculate the number of errors to find the minimum. This is the fitting process in the feature space.

    We need some technical process here, usually numerical minimization, to generate the solutions. If you want to predict the house price on a new house, you use this function. Nowadays, you just use a library to do this—you don't calculate the inverse matrix by hand. But the underlying solution is what we derived.

    我们通过这个例子了解了它在数学上是如何运作的。然而，对于其他损失函数，比如绝对损失，我们没有封闭形式的解析解。我们只能使用数值计算。基本上，我们可能只是猜测 100 个 `θ_0` 的值，猜测 100 个 `θ_1` 的值，然后计算误差数来找到最小值。这就是特征空间中的拟合过程。

    我们需要一些技术过程，通常是数值最小化，来生成解。如果你想预测新房子的价格，你就使用这个函数。如今，你只需要使用一个库来做这件事——你不需要手算逆矩阵。但其底层的解就是我们推导出来的那个。

  - [20:00 - 20:12] **Nearest Neighbor Predictor

    Now, let's look at a completely different approach. We call this the Nearest Neighbor Predictor. It is very intuitive. Imagine you have a new data point x in the feature space—say, a house you want to sell—and you want to guess its label, the price. Instead of training a complex formula, you simply look at your database and find the "nearest neighbor." You find the sample x^(j) that is most similar to your new house. Then, you just take that neighbor's label y^(j) as your prediction. It is that simple.

    This method requires almost trivial training. In fact, we often call it "lazy learning" because you don't really calculate parameters beforehand; you just store the data. If you look at the graph for a 1-Nearest Neighbor model, the function g(x) becomes a piecewise linear function. It looks like a step function. For any x in a certain range, the prediction stays constant until you get closer to a different data point, and then it jumps. It is simple, but it can be very sensitive to noise.

    现在，让我们看一种完全不同的方法。我们称之为最近邻预测器。这非常直观。想象一下，你在特征空间中有一个新的数据点 x——比如你想卖的一栋房子——你想猜测它的标签，也就是价格。与其训练一个复杂的公式，你只需查看数据库并找到“最近的邻居”。你找到与你的新房子最相似的样本 x^(j)。然后，你直接取那个邻居的标签 y^(j) 作为你的预测值。就是这么简单。

    这种方法几乎不需要训练。事实上，我们常称之为“懒惰学习”，因为你实际上不需要预先计算参数；你只是存储数据。如果你看 1-最近邻模型的图表，函数 g(x) 变成了一个分段线性函数。它看起来像一个阶梯函数。对于特定范围内的任何 x，预测值保持不变，直到你更接近另一个数据点，然后它会发生跳变。这很简单，但它可能对噪声非常敏感。

  - [20:12 - 20:22] **k-Nearest Neighbor (k-NN) Regression**

    To fix the issue of just relying on one neighbor, we use the k-Nearest Neighbor Predictor. Instead of finding just one, given an input x, we find its k nearest neighbors. For example, if k=3, we identify the three closest data points. Then, we take the average of their labels—sum them up and divide by k—to get our predicted value ŷ.

    Sometimes, you might want to be more sophisticated. You can use a weighted average, where closer neighbors have a bigger influence on the prediction than those further away. This smoothing helps reduce the variance. If one neighbor is an outlier with a weird price, averaging it with others helps correct the error. So, k-NN is a very flexible, non-parametric method.

    为了解决仅仅依赖一个邻居的问题，我们使用 k-最近邻预测器。给定输入 x，我们不再只找一个，而是找到它的 k 个最近邻居。例如，如果 k=3，我们识别出三个最近的数据点。然后，我们取它们标签的平均值——把它们加起来除以 k——来得到我们的预测值 ŷ。

    有时，你可能想做得更精细一些。你可以使用加权平均，距离更近的邻居比远的邻居对预测有更大的影响。这种平滑处理有助于减少方差。如果一个邻居是价格离谱的异常值，将其与其他邻居平均可以帮助修正误差。所以，k-NN 是一种非常灵活的非参数方法。

  - [20:22 - 20:34] **Model Sensitivity and The Need for Regularization**

    Now, we need to discuss a very important property for any good model: Sensitivity. Or rather, we want our model to be "insensitive." Think about it: if you have two feature vectors u and v that are very similar—maybe two houses that differ only slightly in square footage—the model's predictions f(u) and f(v) should be close to each other. If a tiny change in input causes a huge jump in the output, the model is unstable. Insensitive predictors generally perform better on new, unseen data because they don't overreact to noise.

    Let's look at this mathematically for a linear model. The difference between predictions is |f(u) - f(v)|. Since it is linear, this equals the absolute value of the sum of θ_i times the difference (u_i - v_i). By using the Cauchy-Schwarz inequality, we know this is less than or equal to the magnitude of the parameter vector θ times the magnitude of the data difference.

    This gives us a crucial insight: The upper bound of the prediction difference is controlled by the size of θ. If the sum of squared parameters (∑ θ_i^2) is small, the model is guaranteed to be less sensitive. This motivates us to modify our training. We don't just want to minimize the error; we also want to keep θ small. This brings us to Regularized Linear Regression. We add a penalty term—a "regularizer" r(θ)—to our loss function.

    现在，我们需要讨论任何好模型都必须具备的一个非常重要的属性：敏感性。或者更确切地说，我们希望我们的模型是“不敏感的”。试想一下：如果你有两个非常相似的特征向量 u 和 v——也许是两栋面积仅略有不同的房子——模型的预测值 f(u) 和 f(v) 应该彼此接近。如果输入的微小变化导致输出的巨大跳变，那么这个模型是不稳定的。不敏感的预测器通常在新的、未见过的数据上表现更好，因为它们不会对噪声反应过度。

    让我们用数学方法在线性模型中看看这一点。预测值之间的差异是 |f(u) - f(v)|。因为它是线性的，这等于 θ_i 乘以差异 (u_i - v_i) 之和的绝对值。通过使用柯西-施瓦茨不等式，我们知道这小于或等于参数向量 θ 的模乘以数据差异的模。

    这给了我们一个至关重要的启示：预测差异的上限是由 θ 的大小控制的。如果参数的平方和 (∑ θ_i^2) 很小，模型就能保证不那么敏感。这促使我们修改训练方法。我们不仅要最小化误差，还要保持 θ 很小。这就引出了正则化线性回归。我们在损失函数中添加一个惩罚项——一个“正则化项” r(θ)。

  - [20:34 - 20:37] **Search Method for Regularization Hyperparameter (λ)**

    So, how do we choose this hyperparameter λ? You typically try a set of values, usually log-spaced. You will try λ equal to a very small number, like 0.0001, 0.01, 1, and then maybe a large number like 100. For each λ, you get a different model, and each model has a specific loss value. Remember, the regularized loss function has two terms: the first term is the empirical loss (how well we fit the data), and the second term is the penalty on the parameters. If you choose a case where λ is very small, say close to 0, the model focuses almost entirely on the first term. It fits the data very well, but the parameters might become very big, leading to overfitting issues. On the other hand, if λ is very large, say 100, the second term dominates. The parameters are forced to be small, but the empirical loss might increase because the model is too constrained to fit the data. Ideally, we want to find a λ value such that the empirical loss is not too big, and the parameter size is also not too big. This balance is the exact purpose of adding this regularization term.

    那么，我们如何选择这个超参数 λ 呢？你通常会尝试一组数值，一般是对数间距的。你会尝试 λ 等于一个非常小的数字，比如 0.0001、0.01、1，然后可能是一个大数字，比如 100。对于每一个 λ，你会得到一个不同的模型，每个模型都有特定的损失值。记住，正则化损失函数有两项：第一项是经验损失（我们拟合数据的程度），第二项是对参数的惩罚。如果你选择 λ 非常小的情况，比如接近 0，模型几乎完全专注于第一项。它非常贴合数据，但参数可能会变得非常大，导致过拟合问题。另一方面，如果 λ 非常大，比如 100，第二项就会占主导地位。参数会被迫变小，但经验损失可能会增加，因为模型受限太大，无法拟合数据。理想情况下，我们希望找到一个 λ 值，使得经验损失不太大，同时参数大小也不太大。这种平衡正是我们添加这个正则化项的目的。

  - [20:37 - 20:41] **The Trade-off: Fitting vs. Generalization**

    This leads us to the comparison between the normal version (Empirical Risk Minimization, ERM) and the regularized version. In the normal version, you choose the parameters θ simply to minimize the loss function. This means the loss value is very small, and you fit the training data very well. However, fitting the training data perfectly does not mean you give a good prediction for data you don't see (unseen data). A good model means you give a good prediction on every data point, not just the ones you trained on. This is why we have this regularization term. In the regularized version, we choose a model that does *not* strictly minimize the empirical loss function. Instead, it is less sensitive to the specific training samples. We expect that the solution from the regularized version will give better predictions on new data points. Basically, we use the training data to build the model, but we care about its performance on unseen data.

    这就引出了普通版本（经验风险最小化，ERM）与正则化版本之间的比较。在普通版本中，你选择参数 θ 仅仅是为了最小化损失函数。这意味着损失值非常小，你对训练数据拟合得非常好。然而，完美拟合训练数据并不意味着你对未见过的数据（不可见数据）也能给出好的预测。一个好的模型意味着你对每一个数据点都能给出好的预测，而不仅仅是你训练过的那些。这就是为什么我们要有这个正则化项。在正则化版本中，我们选择的模型*并不*严格最小化经验损失函数。相反，它对特定的训练样本不那么敏感。我们期望正则化版本的解能在新数据点上给出更好的预测。基本上，我们使用训练数据来建立模型，但我们关心的是它在未见数据上的表现。

  - [20:41 - 20:46] **Visualizing Generalization: Training vs. Validation Points**

    Let's illustrate this with an example. Suppose we use the blue points to derive a linear model. These blue points are our training data. Then we have some red points, which are our validation points. As you can see in the linear model example, the line is not bad; the predicted values are very close to the validation points. This means it is a real model—a robust model we want to have. Now, look at the other models. We trained three models: linear, quadratic, and cubic polynomial. The quadratic model (degree 2) is better; it fits the curve closer. But look at the cubic model (degree 3). It goes up, then down, then throws up again. This "wiggly" shape fits the training points (blue) very well, but look at the red validation points—it misses them significantly.

    让我们用一个例子来说明这一点。假设我们使用蓝点来推导一个线性模型。这些蓝点是我们的训练数据。然后我们有一些红点，那是我们的验证点。正如你在线性模型示例中看到的，这条线还不错；预测值与验证点非常接近。这意味着它是一个真正的模型——一个我们想要拥有的稳健模型。现在，看看其他模型。我们训练了三个模型：线性、二次和三次多项式。二次模型（2 次）更好；它更贴合曲线。但看看三次模型（3 次）。它先上升，然后下降，再剧烈上升。这种“扭曲”的形状非常贴合训练点（蓝色），但看看红色的验证点——它严重错过了它们。

  - [20:46 - 20:50] **Model Selection Using RMSE Curves**

    Let's look at the performance metric, the Root Mean Square Error (RMSE), as we increase the complexity of the model. We have 48 training points and 12 validation points.

    - **Linear Model (Degree 1):** The model underfits the data. The error is high for both training and test sets. This is not a good choice.

    - **Degree 2 to 4:** As we increase the degree, the model improves. The RMSE decreases.

    - **Degree 5:** This seems to be the sweet spot. The parameter values improve the fit significantly, and the test error is minimized.

    - **High Degrees (e.g., Degree 14):** As we continue to increase the degree, the training error keeps going down (fitting the blue points perfectly). However, on the new data (test data), the performance becomes much worse.

      This separation—where training error is low but test error is high—is called **overfitting**. Therefore, when we choose a model, we should choose the one that has a small error on *both* the training and test data. This is how you select the best model.

    让我们看看随着模型复杂度增加，性能指标——均方根误差（RMSE）的变化。我们有 48 个训练点和 12 个验证点。

    - **线性模型（1 次）：** 模型对数据欠拟合。训练集和测试集的误差都很高。这不是一个好的选择。

    - **2 次到 4 次：** 随着次数增加，模型在改进。RMSE 下降。

    - **5 次：** 这似乎是最佳点。参数值显著改善了拟合，且测试误差最小。

    - **高次数（例如 14 次）：** 当我们要继续增加次数，训练误差持续下降（完美拟合蓝点）。然而，在新数据（测试数据）上，性能变得更差。

      这种分离——训练误差低但测试误差高——被称为**过拟合**。因此，当我们选择模型时，我们应该选择在训练和测试数据上误差都*很小*的那个。这就是你选择最佳模型的方法。

  - [20:50 - 20:52] **Conclusion on Model Complexity**

  Finally, regarding this slide on model complexity: You can consider all possible models. Simple models (low complexity) are safer but might not capture the pattern. On the other hand, Deep Learning models have millions of parameters. If you have such a complicated model, it is very easy to overfit the data unless you have a massive amount of data to train the model. If you have limited data, a simpler model or a regularized model is often better.

  最后，关于这张模型复杂度的幻灯片：你可以考虑所有可能的模型。简单模型（低复杂度）更安全，但可能无法捕捉到模式。另一方面，深度学习模型有数百万个参数。如果你有这样一个复杂的模型，除非你有海量的数据来训练它，否则非常容易过拟合。如果你数据有限，更简单的模型或正则化的模型通常更好。
## Week 3
  - [19:03 - 19:07] **Binary Classification and Linear Separability**

    There are some data points—let's call them "blue lights" or dots—scattered among others. We essentially build a linear model to separate them. This is the core case where we attempt to split the feature space into two distinct regions. You can imagine this visually: in one dimension, it is very clear; in higher dimensions, we are trying to find a line (or hyperplane) that divides the positive examples from the negative examples.

    The property of a linear function is that if you define this dividing line, points on one side will yield a positive value (function output > 0), and points on the other side will yield a negative value. This is the fundamental property we leverage. We want to find a function where one side represents the positive class and the other the negative class. If we cannot make a prediction or if the separation is too complex, we might fall back to a simpler model. As we discussed, a simple model is often better because it avoids overfitting. Recall that binary classification is actually a special case of the general regression problem, so the methods we use here are consistent with the general regression framework.

    有一些数据点——我们可以称之为“蓝光”或点——散落在其他数据中间。我们基本上是构建一个线性模型来区分它们。这是我们试图将特征空间划分为两个不同区域的核心案例。你可以想象一下：在一维空间中这非常清晰；在更高维空间中，我们试图找到一条线（或超平面）将正样本和负样本分开。

    线性函数的一个特性是，如果你定义了这条分界线，一侧的点将产生正值（函数输出大于0），另一侧的点将产生负值。这就是我们要利用的基本属性。我们希望找到一个函数，使得一侧代表正类，另一侧代表负类。如果我们无法做出预测，或者分隔太复杂，我们可能会退回到一个更简单的模型。正如我们讨论过的，简单的模型往往更好，因为它避免了过拟合。回想一下，二分类实际上是广义回归问题的一个特例，因此我们在这里使用的方法与广义回归框架是一致的。

  - [19:07 - 19:11] **Model Validation: The Neyman-Pearson Loss**

    To build this predictor, we need to select a loss function and a regularizer, using the Regularized Empirical Risk Minimization (ERM) framework. Once trained, we must validate the model. Here, we introduce a specific validation metric called the Neyman-Pearson loss. The formula is `K · r_fn + r_fp`, where `r_fn` is the false negative rate and `r_fp` is the false positive rate.

    Let's define these. The false negative rate asks: "Among all actual positives, what fraction did the model fail to detect?". If we have positive samples (e.g., patients with cancer), but the prediction falls into the error region giving a negative result, that is a false negative. We naturally want this rate to be as small as possible. Conversely, the false positive rate measures how good the predictor is on negative samples. If a sample is actually negative (e.g., a healthy patient), but the model predicts it as positive, that is a false positive.

    The variable `K` acts as a hyperparameter. This parameter weights the importance of false negative errors relative to false positives. In certain applications, like cancer prediction, false negatives are critical errors—we absolutely do not want to predict a sick patient as healthy. In such cases, we set `K` to a large value (greater than 1) to heavily penalize false negatives. If, for some reason, we wanted to avoid false positives more, we would set `K` to be less than 1. Essentially, `K` is the relative penalty or weight we assign to different types of errors.

    为了构建这个预测器，我们需要选择一个损失函数和一个正则化项，并使用正则化经验风险最小化（ERM）框架。模型训练完成后，我们要对其进行验证。这里我们介绍一种特定的验证指标，称为 Neyman-Pearson 损失。公式为 `K · r_fn + r_fp`，其中 `r_fn` 是假阴性率，`r_fp` 是假阳性率。

    我们要定义一下这些概念。假阴性率是指：“在所有实际为正的样本中，模型未能检测出的比例是多少？”。如果我们有正样本（例如癌症患者），但预测结果错误地给出了阴性，这就是假阴性。我们自然希望这个比率越小越好。相反，假阳性率衡量的是预测器在负样本上的表现。如果样本实际上是负的（例如健康的人），但模型将其预测为阳性，这就是假阳性。

    变量 `K` 作为一个超参数。这个参数用来加权假阴性错误相对于假阳性错误的重要性。在某些应用中，比如癌症预测，假阴性是致命的错误——我们要绝对避免把生病的患者预测为健康的。在这种情况下，我们将 `K` 设为一个较大的值（大于1），以严厉惩罚假阴性。如果出于某种原因，我们更想避免假阳性，我们会将 `K` 设为小于1。本质上，`K` 就是我们分配给不同类型错误的相对惩罚或权重。

  - [19:11 - 19:14] **Regularization and Model Combinations (SVM, Logistic Regression)**

    Now, how do we choose the regularizer? Recall that regularization basically tries to find a linear function with small parameter coefficients, `θ` (theta). A linear model with small parameters is unlikely to suffer from overfitting. Conversely, if `θ` is very large, the model likely has poor predictive power on new data because it is over-fitting the training noise. Therefore, we use a regularizer to force the machine learning algorithm to find a model with small coefficients. Regularization is our primary tool to avoid the overfitting problem.

    We have choices for both the loss function and the regularizer. Looking at the different combinations:

    1. If we use **Square Loss** with a **Square Regularizer**, this is called the Least Square Classifier.
    2. If we use **Logistic Loss** with an **`l₁` Regularizer** (or square), this is called the Logistic Regression Classifier.
    3. If we use **Hinge Loss** with a **Square Regularizer**, this forms the Support Vector Machine (SVM).

    SVM was historically a very popular model. While deep learning is dominant now, SVM is theoretically very sound. In a simple binary case, you could draw many lines to separate the data, but the Support Vector Machine finds the "best" one—the one right in the middle that maximizes the margin.

    现在，我们要如何选择正则化项？回想一下，正则化基本上是试图找到一个参数系数 `θ`（Theta）较小的线性函数。参数较小的线性模型不太可能出现过拟合。相反，如果 `θ` 非常大，模型在新数据上的预测能力可能很差，因为它过度拟合了训练噪声。因此，我们使用正则化器来强制机器学习算法找到系数较小的模型。正则化是我们避免过拟合问题的主要工具。

    对于损失函数和正则化项，我们都有选择。看看不同的组合：

    1. 如果我们使用**平方损失**配合**平方正则化**，这被称为最小二乘分类器。
    2. 如果我们使用**Logistic损失**配合**`l₁`正则化**（或平方），这被称为逻辑回归分类器。
    3. 如果我们使用**Hinge损失**配合**平方正则化**，这就构成了支持向量机（SVM）。

    SVM 在历史上曾是一个非常流行的模型。虽然现在深度学习占据主导地位，但 SVM 在理论上非常完善。在简单的二分类情况中，你可以画很多条线来分隔数据，但支持向量机能找到“最好”的那一条——即位于中间、能最大化间隔的那一条。

  - [19:14 - 19:28] **Critique of Square Loss for Classification**

    Let's review the loss functions in detail. First, the **Square Loss**. Imagine we have many samples, predicted values, and true labels. The loss is calculated as `(ŷ - y)²`.

    Consider the regularizer term `r(θ) = Σ θᵢ²`. If this sum is small, it implies every individual `θᵢ` must be small, effectively constraining the model complexity.

    Now, looking specifically at the Square Loss function on a negative sample where the true label is -1. The loss is `(ŷ - (-1))² = (ŷ + 1)²`. Ideally, if we have a perfect prediction, the loss is 0. In a 2D space, the loss relates to the distance from the separating line.

    However, Square Loss has a major weakness in classification. Suppose we have a negative sample, and our function outputs a value of -2.5. Since the label is -1 (negative), a prediction of -2.5 is actually a *very confident* and correct classification—it is far on the negative side. But if you plug -2.5 into the Square Loss formula, you get `(-2.5 - (-1))² = (-1.5)² = 2.25`. The loss function gives a large penalty!

    This means the model is being "overcharged" or penalized for being "too correct" in the right direction. This large penalty for confident correct predictions can mislead the training process, causing the algorithm to adjust parameters unnecessarily to reduce this "error," even though the classification was correct. This is the fundamental flaw of using Square Loss for classification: it penalizes correct predictions that lie far from the margin. The same logic applies symmetrically to positive samples.

    让我们详细回顾一下损失函数。首先是**平方损失**。假设我们要处理许多样本、预测值和真实标签。损失计算为 `(ŷ - y)²`。

    考虑正则化项 `r(θ) = Σ θᵢ²`。如果这个和很小，意味着每个单独的 `θᵢ` 都必须很小，从而有效地限制了模型的复杂度。

    现在具体看看负样本（真实标签为 -1）上的平方损失。损失为 `(ŷ - (-1))² = (ŷ + 1)²`。理想情况下，如果预测完美，损失应为 0。在二维空间中，损失与到分界线的距离有关。

    然而，平方损失在分类中有一个主要弱点。假设我们有一个负样本，我们的函数输出值为 -2.5。因为标签是 -1（负类），-2.5 的预测实际上是一个*非常确信*且正确的分类——它在负的一侧很远。但如果你把 -2.5 代入平方损失公式，你会得到 `(-2.5 - (-1))² = (-1.5)² = 2.25`。损失函数竟然给出了一个很大的惩罚！

    这意味着模型因为“过于正确”而受到了“过度惩罚”。这种对确信的正确预测的惩罚会误导训练过程，导致算法为了减少这种“误差”而不必要地调整参数，即使分类本身是正确的。这就是在分类中使用平方损失的根本缺陷：它惩罚了那些远离边界的正确预测。同样的逻辑也对称地适用于正样本。

  - [19:28 - 19:35] **Logistic Loss and Hinge Loss (SVM)**

    Because of the problems with Square Loss, we propose more meaningful loss functions. The **Logistic Loss** is one such alternative. It dampens the penalty for correct classifications that are far from the boundary. On the negative side, the loss is `log(1 + e^ŷ)`, and for positive samples, it is `log(1 + e^(-ŷ))`. It is differentiable and convex, which is mathematically convenient for optimization.

    However, Logistic Loss is still not perfect. Even if you give a perfect prediction, you still incur a small positive loss—it never truly hits zero. Ideally, a perfect prediction should have zero penalty.

    This leads us to the **Hinge Loss**, used in Support Vector Machines (SVM). This function is very reasonable. If the true label is -1 and you predict -1, the loss is 0. If you predict -2 or -3 (very confident negative), the loss is *also* 0. It does not penalize you for being "extra" correct. The formula uses the "positive part" notation `(·)₊`, which simply means `max(0, ·)`.

    - For negative samples: `ℓ(ŷ, -1) = max(0, ŷ + 1)`. If `ŷ ≤ -1`, loss is 0.
    - For positive samples: `ℓ(ŷ, 1) = max(0, 1 - ŷ)`. If `ŷ ≥ 1`, loss is 0.

    Mathematically, this function is convex, though not differentiable everywhere (specifically at the "hinge" point), but we can handle that. We balance the differentiability of Logistic Loss with the zero-penalty property of Hinge Loss. These are the three popular loss functions you need to know.

    由于平方损失存在的问题，我们提出了更有意义的损失函数。**Logistic损失**（逻辑损失） 就是这样一个替代方案。它减轻了对远离边界的正确分类的惩罚。在负样本侧，损失是 `log(1 + e^ŷ)`，在正样本侧是 `log(1 + e^(-ŷ))`。它是可微且凸的，这在数学上便于优化。

    然而，Logistic 损失仍然不是完美的。即使你给出了完美的预测，你仍然会有一个很小的正损失——它永远不会真正达到零。理想情况下，完美的预测应该有零惩罚。

    这引出了支持向量机（SVM）中使用的 **Hinge 损失**。这个函数非常合理。如果真实标签是 -1 而你预测 -1，损失是 0。如果你预测 -2 或 -3（非常确信的负值），损失*也是* 0。它不会因为你“额外”正确而惩罚你。公式使用了“正部”符号 `(·)₊`，意思就是 `max(0, ·)`。

    - 对于负样本：`ℓ(ŷ, -1) = max(0, ŷ + 1)`。如果 `ŷ ≤ -1`，损失为 0。
    - 对于正样本：`ℓ(ŷ, 1) = max(0, 1 - ŷ)`。如果 `ŷ ≥ 1`，损失为 0。

    在数学上，这个函数是凸的，虽然不是处处可微（特别是在“铰链”点），但我们可以处理这个问题。我们在 Logistic 损失的可微性和 Hinge 损失的零惩罚属性之间进行权衡。这些是你需要了解的三种流行损失函数。

  - [19:35 - 19:42] **Accuracy Evaluation: Confusion Matrix and AUROC**

    We evaluate these models using a **Confusion Matrix**. This matrix breaks down results into True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). From these, we define **Accuracy**, which is simply the total correct predictions divided by the total cases.

    However, if you have **imbalanced data**, accuracy is misleading. For example, if you are predicting earthquakes, they happen very rarely. If you build a model that predicts "No Earthquake" every single day, your accuracy might be 99.9%, but the model is useless because it has zero predictive power for the event of interest. Therefore, accuracy is not a good indicator for imbalanced datasets. In such cases, we consider **Precision**, **Specificity**, and **Sensitivity (Recall)**. Machine learning is essentially finding the best model to balance these two competing indicators: sensitivity and specificity. We want a predictor that performs well on both negative and positive samples.

    To visualize this balance, we use the **AUROC** (Area Under the Receiver Operating Characteristic Curve). The name comes from signal processing fields. Essentially, if you have a random classifier (guessing blindly), the curve is a diagonal line, and the area under it is 0.5. This is the baseline—the worst useful classifier. A good predictor will have a curve that bows outward, approaching 1.0. People often claim their predictors have AUROC values of 0.90, 0.95, or 0.99. Similarly, we have the **AUPRC** (Area Under the Precision-Recall Curve), which is also useful for imbalanced data.

    我们使用**混淆矩阵** 来评估这些模型。该矩阵将结果细分为真阳性（TP）、假阳性（FP）、真阴性（TN）和假阴性（FN）。由此，我们可以定义**准确率**，即正确的预测总数除以总案例数。

    然而，如果你有**不平衡数据**，准确率会产生误导。例如，如果你预测地震，地震非常罕见。如果你建立一个模型，每天都预测“无地震”，你的准确率可能高达 99.9%，但这个模型是无用的，因为它对我们要关注的事件没有任何预测能力。因此，对于不平衡数据集，准确率不是一个好的指标。在这种情况下，我们要考虑**精确率**、**特异性**和**灵敏度（召回率）**。机器学习本质上是找到最佳模型来平衡这两个相互竞争的指标：灵敏度和特异性。我们需要一个在负样本和正样本上表现都很好的预测器。

    为了可视化这种平衡，我们使用 **AUROC**（受试者工作特征曲线下面积）。这个名字来源于信号处理领域。本质上，如果你有一个随机分类器（盲猜），曲线是一条对角线，其下面积为 0.5。这是基线——最有用的最差分类器。一个好的预测器的曲线会向外凸出，接近 1.0。人们经常声称他们的预测器 AUROC 值为 0.90、0.95 或 0.99。同样，我们还有 **AUPRC**（精确率-召回率曲线下面积），这对不平衡数据也很有用。

  - [19:42 - 19:45] **Warm-Up Project: Australian Weather Prediction**

    I have provided an optional warm-up project. You can download the data from Kaggle or Canvas. The task is to "Predict Next-Day Rain with Australian Weather Data". The dataset contains over 100,000 records from various cities like Sydney and Melbourne. The objective is to use today's weather measurements to predict rainfall tomorrow.

    For the project:

    1. Write a Python program to extract records specifically for **Melbourne**.

    2. Remove rows with missing values (N/A).

    3. Drop specific columns like Date, Sunshine, WindGustDir, etc., as they are not needed for this exercise.

    4. Map non-numerical features to a real vector space and split the data into training (90%) and validation (10%) sets.

    5. Implement a linear model using **Logistic Loss** and a **Square Regularizer**.

       This is a simple project to practice the full pipeline.

    我提供了一个可选的热身项目。你可以从 Kaggle 或 Canvas 下载数据。任务是“利用澳大利亚天气数据预测第二天的降雨”。数据集包含来自悉尼和墨尔本等城市的 10 万多条记录。目标是利用当天的天气测量值来预测第二天的降雨量。

    项目要求：

    1. 编写 Python 程序，专门提取**墨尔本**的记录。

    2. 删除包含缺失值（N/A）的行。

    3. 删除日期、日照、阵风风向等特定列，因为本练习不需要这些列。

    4. 将非数值特征映射到实向量空间，并将数据分为训练集（90%）和验证集（10%）。

    5. 使用 **Logistic损失** 和 **平方正则化** 实现线性模型。

       这是一个练习完整流程的简单项目。

  - [19:45 - 19:48] **Summary of Part 1**

    To summarize Part 1: I introduced the linear model and used it to demonstrate the entire machine learning pipeline.

    - We choose a model class (Linear Predictor).
    - We select a Loss Function (Empirical Risk).
    - We add Regularization for model complexity control to prevent overfitting.
    - We use an optimization/training algorithm.
    - We evaluate using performance metrics (Sensitivity, Specificity, AUROC).
    - We perform validation by splitting data into Training, Validation, and Test sets, or using Cross-Validation.

    We also briefly mentioned Nearest-Neighbor predictors, which we will return to when we discuss decision trees later.

    总结第一部分：我介绍了线性模型，并用它来演示整个机器学习流程。

    - 我们选择一个模型类（线性预测器）。
    - 我们选择一个损失函数（经验风险）。
    - 我们添加正则化来进行模型复杂度控制，以防止过拟合。
    - 我们使用优化/训练算法。
    - 我们使用性能指标（灵敏度、特异性、AUROC）进行评估。
    - 我们通过将数据分为训练集、验证集和测试集，或使用交叉验证来进行验证。

    我们也简要提到了最近邻预测器，我们将在稍后讨论决策树时回到这个话题。

  - [19:48 - 19:50] **Student Q&A: AUROC vs. AUPRC**

    **Student Question:** When do you use AUROC versus AUPRC?

    **Professor's Answer:** Honestly, I am not a statistics guy, so I don't pay excessive attention to the subtle theoretical distinctions between every metric. However, for classical machine learning, my practical suggestion is: use 5 or 6 different performance metrics. If your model beats other models on the majority of these metrics, then your model is good. That is essentially how you write a research paper—you show robustness across multiple indicators.

    For this course, you should understand the concepts—why we have these formulas—but you don't need to memorize every single formula if you don't use it constantly. The key takeaway is understanding *why* we prefer simple linear models over complicated ones: overfitting. You don't need a complex model that captures 99% of the training data if it fails to generalize. A linear model with simple non-linear components often yields a very differentiable and effective approach.

    **学生提问：** 什么时候使用 AUROC，什么时候使用 AUPRC？

    **教授回答：** 说实话，我不是搞统计学的，所以我不会过分关注每个指标之间细微的理论区别。但是，对于经典机器学习，我的实用建议是：使用 5 或 6 种不同的性能指标。如果你的模型在大多数指标上都击败了其他模型，那么你的模型就是好的。这基本上就是写研究论文的方法——你要展示在多个指标上的稳健性。

    对于这门课，你应该理解这些概念——为什么我们有这些公式——但如果你不经常使用，就不需要死记硬背每一个公式。核心要点是理解*为什么*我们要选择简单的线性模型而不是复杂的模型：因为过拟合。如果一个复杂的模型在训练数据上达到了 99% 的准确率却无法泛化，那你是并不需要它的。线性模型加上简单的非线性组件，通常能产生一种非常可微且有效的方法。

  - [19:50 - 19:51] **Final Remarks on Part 1**

    Important concepts to retain are: Regularizers, Loss Functions, and specifically the distinction between Square Loss (and why it fails for classification), Logistic Loss, and Hinge Loss. That concludes the first part of today's lecture. Next, we will move to Lecture 3: Probabilistic Predictors.

    需要记住的重要概念是：正则化项、损失函数，特别是平方损失（以及它为何在分类中失效）、Logistic 损失和 Hinge 损失之间的区别。今天讲座的第一部分到此结束。接下来，我们将进入第 3 讲：概率预测器。

  - [20:03 - 20:07] **Quiz Logistics and Administrative Details**

    Including the lecture notes, the quiz will be very easy. It is designed to take about 40 minutes, but you have one hour. There are absolutely no calculations involved. Okay, so no calculations, just simple multiple-choice questions. We have a special arrangement regarding the schedule; while the topics covered are a weak constraint, the oral presentation pieces can be arranged in the same week or after that.

    Important note: You have to come here physically to the classroom. You cannot access this quiz online because there is a password required, and I will only give the password inside this room. This assessment involves me directly.

    包括讲义在内，这次测验会非常简单。它设计为 40 分钟完成，但你们有一个小时的时间。绝对不涉及计算。好的，没有计算，只是简单的选择题。关于时间表我们有一个特别安排；虽然涵盖的主题是一个软性约束，但口头报告的部分可以安排在同一周或之后进行。

    重要提示：你们必须亲自来到教室。你们无法在线访问这个测验，因为需要密码，而我只会在这个房间里提供密码。这次评估需要我在场。

  - [20:07 - 20:12] **Introduction to Probabilistic and List Predictors**

    In the second part of today's lecture, we discuss the **Probabilistic Predictor**. This is a different type of learning concept. So far, we have only learned the linear model. The linear model simply returns one single value `ŷ` for each data point `x`. We call this a **Point Predictor**. However, for some problems, it is better if we return a list of answers. For example, we might have a first guess, a second guess, and a third guess. The customer is often happier with this approach, particularly in recommendation systems.

    Sometimes, naturally, we don't really know exactly which one is the correct answer, or the answer is ambiguous. In these cases, we use probability. For example, for a weather forecast, we might say: "There is a 5% chance of rain tomorrow, and an 80% chance of clear sky sunshine." The former type—returning a ranked list—is called a **List Predictor** (like a Google search giving top 10 results). The latter is called a **Probabilistic Predictor**. In a recommendation system, since we don't really know what the user wants to buy, giving multiple weighted answers allows the user to check which one is reasonable.

    在今天讲座的第二部分，我们讨论**概率预测器**。这是一个不同的学习概念。到目前为止，我们只学习了线性模型。线性模型对于每个数据点 `x` 只返回一个单一的值 `ŷ`。我们称之为**点预测器**。然而，对于某些问题，如果我们返回一系列答案会更好。例如，我们可能有第一猜测、第二猜测和第三猜测。客户通常更喜欢这种方式，特别是在推荐系统中。

    有时，本质上我们并不知道确切的正确答案，或者答案是模棱两可的。在这些情况下，我们使用概率。例如，对于天气预报，我们可能会说：“明天有 5% 的几率下雨，80% 的几率是晴天。”前者——返回排名列表——被称为**列表预测器**（就像 Google 搜索给出前 10 个结果）。后者被称为**概率预测器**。在推荐系统中，因为我们要么不知道用户真正想买什么，给出多个加权答案可以让用户自己检查哪个是合理的。

  - [20:12 - 20:16] **Defining the Probabilistic Predictor**

    Let's define the probabilistic predictor formally. We have a label space. For cancer prediction, it is very easy: we have "cancer" or "benign"—only two possible answers. For a weather forecast, say we have three possible answers: {Cloud, Rain, Shine}. For each answer, we assign a probability, representing the chance of that label occurring. Ideally, the sum of these chances is 1.

    So, the predictor is actually a function that takes an input `x` and returns a **distribution function** (or a table). It does not return a single scalar value. For example, for today's condition, it might output: 10% (0.1) chance of Rain, 5% (0.05) chance of Cloud, and 85% (0.85) chance of Sunshine. It returns a table where each possible answer is associated with a number representing its likelihood. This is distinct from a point predictor which just gives you one decision. Of course, you can always convert this back to a point prediction if you want, but the probabilistic output is more informative.

    让我们正式定义概率预测器。我们有一个标签空间。对于癌症预测，这很简单：我们有“癌症”或“良性”——只有两个可能的答案。对于天气预报，假设我们有三个可能的答案：{多云，雨，晴}。对于每个答案，我们分配一个概率，代表该标签发生的几率。理想情况下，这些几率之和为 1。

    因此，预测器实际上是一个函数，它接收输入 `x` 并返回一个**分布函数**（或一张表）。它不返回单个标量值。例如，对于今天的情况，它可能输出：10%（0.1）的几率下雨，5%（0.05）的几率多云，85%（0.85）的几率晴天。它返回一张表，其中每个可能的答案都关联着一个代表其可能性的数字。这与只给出一个决定的点预测器不同。当然，如果你愿意，你总是可以将其转换回点预测，但概率输出包含更多信息。

  - [20:16 - 20:20] **Ambiguity and Point Predictors as Special Cases**

    Why do we need this? Sometimes the situation is ambiguous. A point predictor might say "Rain," but ignores the fact that "Sunshine" was also very likely. A probabilistic predictor tells you that "everything is possible, and everyone has a chance."

    A Point Predictor is actually a **special case** of a Probabilistic Predictor. If you are extremely confident in one answer, you can assign that answer a probability of 1, and all other answers a probability of 0. For example, if a normal predictor says "Sunny," it is mathematically equivalent to a probability table where P(Sunny) = 1 and P(Rain) = 0. However, usually, outputting a probability distribution is more meaningful because point predictors may perform poorly in uncertain scenarios. If we give probabilities, the customer is happier, and we are happier. Neural networks, for instance, typically output a probabilistic predictor (using Softmax), which we then convert to a point prediction by selecting the label with the highest probability.

    为什么我们需要这个？有时情况是模棱两可的。点预测器可能会说“雨”，但忽略了“晴天”也很有可能的事实。概率预测器告诉你“一切皆有可能，每个选项都有机会”。

    点预测器实际上是概率预测器的**特例**。如果你对某个答案非常有信心，你可以给该答案分配概率 1，而所有其他答案的概率为 0。例如，如果一个普通预测器说“晴天”，这在数学上等同于一个概率表，其中 P(晴天) = 1，P(雨) = 0。然而，通常情况下，输出概率分布更有意义，因为点预测器在不确定场景中可能表现不佳。如果我们给出概率，客户会更满意，我们也更满意。例如，神经网络通常输出概率预测器（使用 Softmax），然后我们通过选择概率最高的标签将其转换为点预测。

  - [20:20 - 20:25] **K-Nearest Neighbor (KNN) Probabilistic Predictor**

    This method is theoretically more robust. Let's look at the **K-Nearest Neighbor Probabilistic Predictor**. Suppose we set `k = 6`. For any given data point in the feature space, how do we make a prediction?

    We find the 6 closest neighbors to this point. Imagine a circle around the point that contains exactly these 6 neighbors.

    1. **Clear Case:** If for a specific data point, all 6 neighbors are "blue" (negative/benign), then the probability for "blue" is 1 (or 100%), and "red" is 0. This gives a very certain prediction.
    2. **Ambiguous Case:** However, consider a data point in a "troublesome region" (the boundary). If you draw the circle for the 6 nearest neighbors, you might get 3 blue points and 3 red points. In this case, the predictor returns a table: P(Blue) = 0.5, P(Red) = 0.5. It basically says "No specific answer" or "I don't know."

    This is how KNN works as a probabilistic predictor. It gives you a distribution, like 0.8 for blue and 0.2 for red, rather than a forced binary choice. The only challenge is defining a distance function to find these neighbors.

    这种方法在理论上更稳健。让我们看看**K-最近邻概率预测器**。假设我们将 `k` 设为 6。对于特征空间中的任何给定数据点，我们如何进行预测？

    我们找到距离该点最近的 6 个邻居。想象一下该点周围的一个圆，刚好包含这 6 个邻居。

    1. **清晰案例：** 如果对于某个特定数据点，所有 6 个邻居都是“蓝色”（负类/良性），那么“蓝色”的概率就是 1（或 100%），“红色”是 0。这给出了一个非常确定的预测。
    2. **模糊案例：** 然而，考虑一个位于“麻烦区域”（边界）的数据点。如果你画出包含 6 个最近邻的圆，你可能会得到 3 个蓝点和 3 个红点。在这种情况下，预测器返回一个表：P(蓝) = 0.5，P(红) = 0.5。它基本上是在说“没有确切答案”或“我不知道”。

    这就是 KNN 作为概率预测器的工作原理。它给你一个分布，比如蓝色 0.8，红色 0.2，而不是强制的二元选择。唯一的挑战是定义一个距离函数来找到这些邻居。

  - [20:25 - 20:32] **Assessment: Likelihood**

    How do we say a prediction is "good"? We use the concept of **Likelihood**. Let's use the weather forecast example. You have today's condition and the actual outcome (label) for tomorrow (e.g., "Sunny").

    - **Intuition:** For the true label (what actually happened), the model should have predicted a high probability. If it actually rained, and your model gave P(Rain) a high value, that's good. If it was sunny, and your model gave P(Sunny) a high value, that's also good.

    We formalize this intuition. Suppose we have samples `(x⁽¹⁾, y⁽¹⁾), (x⁽²⁾, y⁽²⁾)...` and for each sample, our model produces a probability table. We look at the probability assigned *specifically* to the true label `y`.

    - Sample 1 (Sunny): Model predicted P(Sunny) = 0.91.
    - Sample 2 (Overcast): Model predicted P(Overcast) = 0.85.
    - Sample 3 (Rain): Model predicted P(Rain) = 0.82.

    The **Likelihood** is the **product** of these probabilities: `0.91 × 0.85 × 0.82 ≈ 0.634`. This single number measures how well the predicted distribution matches the actual data.

    我们如何判断一个预测是“好的”？我们使用**似然**（Likelihood）的概念。让我们使用天气预报的例子。你有今天的情况和明天的实际结果（标签）（例如“晴天”）。

    - **直觉：** 对于真实标签（实际发生的情况），模型应该预测出高概率。如果实际下雨了，而你的模型给出的 P(雨) 值很高，那就很好。如果是晴天，而你的模型给出的 P(晴) 值很高，那也很好。

    我们将这种直觉形式化。假设我们有样本 `(x⁽¹⁾, y⁽¹⁾), (x⁽²⁾, y⁽²⁾)...`，对于每个样本，我们的模型都会生成一个概率表。我们查看分配给*特定*真实标签 `y` 的概率。

    - 样本 1（晴）：模型预测 P(晴) = 0.91。
    - 样本 2（多云）：模型预测 P(多云) = 0.85。
    - 样本 3（雨）：模型预测 P(雨) = 0.82。

    **似然**就是这些概率的**乘积**：`0.91 × 0.85 × 0.82 ≈ 0.634`。这一个数字衡量了预测分布与实际数据的匹配程度。

  - [20:32 - 20:37] **Negative Log-Likelihood**

    There is a mathematical problem with raw Likelihood. If you multiply many small numbers (probabilities < 1) together—say, 400 samples—the result becomes a tiny, tiny number close to zero (underflow). It becomes hard to work with.

    Therefore, we calculate the **Negative Log-Likelihood**.

    1. Take the Logarithm of the likelihood. Since `log(0.x)` is negative (e.g., log(0.00034) might be -4 or -5), this converts the product into a sum of negative numbers.
    2. Put a **negative sign** in front of it to make the result positive. `-(log 0.91 + log 0.85 + log 0.82)`.

    For example, taking the negative log of our previous likelihood (0.634) gives roughly 0.4557. This number is much more manageable. Finally, to compare datasets of different sizes (e.g., a test set of 100 vs 200), we take the **average** (divide by `n`). This metric allows us to compare two different probabilistic predictors; the one with the *lower* negative log-likelihood is better.

    原始似然存在一个数学问题。如果你将许多小数（概率 < 1）相乘——比如说 400 个样本——结果会变成一个接近于零的极小数值（下溢）。这变得很难处理。

    因此，我们计算**负对数似然**（Negative Log-Likelihood）。

    1. 取似然的对数。因为 `log(0.x)` 是负数（例如 log(0.00034) 可能是 -4 或 -5），这将乘积转换为负数之和。
    2. 在前面加一个**负号**，使结果变为正数。`-(log 0.91 + log 0.85 + log 0.82)`。

    例如，对我们要之前的似然（0.634）取负对数大约得到 0.4557。这个数字更容易处理。最后，为了比较不同大小的数据集（例如 100 个样本的测试集与 200 个样本的测试集），我们要取**平均值**（除以 `n`）。这个指标允许我们比较两个不同的概率预测器；负对数似然*越低*的那个越好。

  - [20:38 - 20:45] **Information Theory: Entropy**

    How do we compare two probability distributions theoretically? We use **Entropy**, a concept from Information Theory.

    Consider a coin flipping game.

    - **High Information/Complexity:** If the coin is fair (Heads 0.5, Tails 0.5), it is hard to predict the outcome. In Information Theory, we say this system has "high information" or high entropy.
    - **Low Information/Simplicity:** If the coin is heavily biased (e.g., Heads 0.99, Tails 0.01), it is a very simple game. You almost always get Heads. This system has "low information" or low entropy because it is easy to predict.

    The formula for Entropy `H(p)` involves summing `-p · log(p)`. If you have a distribution like `1/7, 1/7... 1/7` (uniform, everything is equally likely), the entropy is maximized—you know nothing. If you have a distribution like `[1, 0, 0...]` (certainty), the entropy is zero. We use this to indicate whether a predictor "knows something" or is just guessing. If every possible label has a high probability (flat distribution), your prediction is no good.

    我们如何在理论上比较两个概率分布？我们使用**熵**（Entropy），这是一个来自信息论的概念。

    考虑一个抛硬币游戏。

    - **高信息/复杂性：** 如果硬币是公平的（正面 0.5，反面 0.5），很难预测结果。在信息论中，我们说这个系统具有“高信息量”或高熵。
    - **低信息/简单性：** 如果硬币严重有偏差（例如，正面 0.99，反面 0.01），这就是一个非常简单的游戏。你几乎总是得到正面。这个系统具有“低信息量”或低熵，因为它很容易预测。

    熵 `H(p)` 的公式涉及对 `-p · log(p)` 求和。如果你有一个像 `1/7, 1/7... 1/7` 这样的分布（均匀分布，一切都有可能），熵是最大的——你什么都不知道。如果你有一个像 `[1, 0, 0...]` 这样的分布（确定性），熵为零。我们用它来表示预测器是“知道些什么”还是仅仅在猜测。如果每个可能的标签都有很高的概率（平坦分布），你的预测就不好。

  - [20:45 - 20:50] **Cross-Entropy and KL Divergence**

    We extend this to **Cross-Entropy**. It measures the cost of using a predicted distribution `Q` to encode data that actually comes from the true distribution `P`.

    - Formula: `H(P, Q) = -Σ P(y) · log Q(y)`.
    - **Interpretation:** If your prediction `Q` is very different from the truth `P` (e.g., pointing in the wrong direction), the Cross-Entropy is high. It represents the "extra work" needed to correct the wrong direction. If `Q` is close to `P`, the value is small.

    This is closely related to the **Kullback-Leibler (KL) Divergence**, denoted as `d_kl(P || Q)`.

    - Formula: `d_kl(P || Q) = H(P, Q) - H(P)`.
    - KL Divergence measures the "distance" or divergence between two distributions. Since the entropy of the true distribution `H(P)` is usually fixed (we can't change the ground truth), minimizing Cross-Entropy is mathematically the same as minimizing KL Divergence. This is why Cross-Entropy is widely used as a loss function in neural networks.

    我们将此扩展到**交叉熵**。它衡量的是使用预测分布 `Q` 来编码实际来自真实分布 `P` 的数据的成本。

    - 公式：`H(P, Q) = -Σ P(y) · log Q(y)`。
    - **解释：** 如果你的预测 `Q` 与真实情况 `P` 非常不同（例如，指向错误的方向），交叉熵就会很高。它代表了纠正错误方向所需的“额外工作”。如果 `Q` 接近 `P`，该值就很小。

    这与 **Kullback-Leibler (KL) 散度**密切相关，表示为 `d_kl(P || Q)`。

    - 公式：`d_kl(P || Q) = H(P, Q) - H(P)`。
    - KL 散度衡量两个分布之间的“距离”或散度。由于真实分布的熵 `H(P)` 通常是固定的（我们无法改变基本事实），因此最小化交叉熵在数学上等同于最小化 KL 散度。这就是为什么交叉熵被广泛用作神经网络中的损失函数。
## Week 4
  - [19:02 - 19:06] **Introduction to Decision Trees and Historical Context**

    As you can see from the slide, we are looking at the origins of this method. This refers to the classic book written in 1984 by Breiman, Friedman, Olshen, and Stone. At that time, these statisticians worked together to formalize this model. The core idea is very simple: intelligence can be captured in a set of "if-then-else" rules that provide branching for classification. For example, consider a very simple decision tree used for animal recognition. We look at features like body size or color. If we ask "Is it large?" or "Is it gray?", we branch left or right.

    从幻灯片中可以看到，我们正在回顾这种方法的起源。这指的是Breiman、Friedman、Olshen和Stone在1984年合著的经典著作。当时，这些统计学家共同致力于该模型的规范化。其核心思想非常简单：智能可以通过一组提供分类分支的“if-then-else”（如果-那么-否则）规则来捕捉。例如，考虑一个用于动物识别的简单决策树。我们观察体型或颜色等特征。如果我们问“它很大吗？”或“它是灰色的吗？”，我们就会向左或向右分支。

  - [19:06 - 19:10] **Mathematical Formulation and Model Interpretability**

    Mathematically, we look at a data point and check its feature X. We compare X against a value, say "a". If X is greater than or equal to "a", we go one way; otherwise, we go the other. We might then check if a second feature Y is greater than "b". This effectively splits the feature space into partitions—left parts and right parts. The decision tree is a tree-structured model for prediction where internal nodes test whether a feature possesses an attribute, branches represent the possible values or outcomes of that test, and leaves output the final predicted class. Why do we choose Decision Trees? They are highly interpretable. Unlike "black box" models where you might not understand the internal logic, a decision tree mimics human reasoning. It is suitable when you have small data, classes are disjoint, and you need a model that is easy to view and explain.

    在数学上，我们观察一个数据点并检查其特征X。我们将X与某个值（比如“a”）进行比较。如果X大于或等于“a”，我们走一条路；否则走另一条路。接着我们可以检查第二个特征Y是否大于“b”。这有效地将特征空间划分为不同的部分——左边部分和右边部分。决策树是一种树状预测模型，其中内部节点测试特征是否具有某种属性，分支代表该测试的可能值或结果，叶节点输出最终的预测类别。为什么我们选择决策树？因为它们具有高度的可解释性。与你可能无法理解内部逻辑的“黑盒”模型不同，决策树模仿了人类的推理过程。它适用于数据量较小、类别互斥、且你需要一个易于查看和解释的模型的情况。

  - [19:10 - 19:17] **Information Theory Foundation: Entropy and Complexity**

    However, we cannot just build the tree arbitrarily; we need a mathematical foundation to select the most important attributes for splitting. We frame this as a binary classification problem where we have a set of training samples, some positive and some negative. From Information Theory, we use "Entropy" to quantify the difficulty or complexity of the problem. Entropy represents the expected number of bits required to encode the class information string. If a problem is very complex—meaning the classes are mixed and difficult to distinguish—the entropy will be high. You cannot represent a complex class with a string shorter than its entropy. This gives us a lower bound on the complexity of the decision tree. If we can split the data such that the resulting subsets have lower entropy (are more pure), we have gained information. This concept guides our selection of features: we choose the split that maximally reduces the complexity.

    然而，我们要如何构建这棵树呢？我们不能随意构建，我们需要一个数学基础来选择最重要的属性进行分裂。我们将此设定为一个二分类问题，我们有一组训练样本，其中包含正样本和负样本。根据信息论，我们使用“熵”（Entropy）来量化问题的难度或复杂性。熵代表编码类别信息字符串所需的期望比特数。如果一个问题非常复杂——意味着类别混合在一起难以区分——熵就会很高。你无法用短于其熵的字符串来表示一个复杂的类别。这为决策树的复杂性提供了一个下界。如果我们能对数据进行分裂，使得结果子集的熵更低（更纯），我们就获得了信息。这个概念指导我们对特征的选择：我们选择能够最大程度降低复杂性的分裂方式。

  - [19:17 - 19:22] **Information Gain and Feature Selection Strategy**

    So, we look at the entropy of the original dataset versus the entropy after splitting. The original data has a certain complexity. When we split it into branches based on a feature, we calculate the weighted average entropy of the new subsets. If the new weighted entropy is significantly lower than the original entropy, it means we have a better understanding of the problem; the classification has become easier. The difference between the original entropy and the weighted average entropy of the branches is called "Information Gain." We iterate through all unused features and select the one that provides the maximum Information Gain to be our splitting node.

    因此，我们比较原始数据集的熵与分裂后的熵。原始数据具有一定的复杂性。当我们根据某个特征将其分裂为分支时，我们计算新子集的加权平均熵。如果新的加权熵显著低于原始熵，这意味着我们对问题有了更好的理解；分类变得更容易了。原始熵与分支加权平均熵之间的差值称为“信息增益”。我们遍历所有未使用的特征，并选择能提供最大信息增益的那个特征作为我们的分裂节点。

  - [19:22 - 19:27] **The Tennis Club Example: Calculating Entropy**

    Let's apply this to an example. Consider a dataset for a Tennis Club that records whether tennis players come to play or not based on weather conditions. We have four attributes: Outlook, Temperature, Humidity, and Wind. Let's look at "Outlook." It has three possible values: Sunny, Overcast, and Rain.

    First, calculate the entropy of the whole dataset. Out of 14 days, 9 are "Yes" (Play) and 5 are "No" (Don't Play). The entropy is calculated as `-p_pos * log(p_pos) - p_neg * log(p_neg)`, which gives us 0.94.

    Now, split by Outlook:

    1. **Sunny:** We have 5 samples. 2 are "Yes", 3 are "No". This is mixed, so the entropy is high, calculated as 0.97.
    2. **Overcast:** We have 4 samples. All 4 are "Yes". Since this is purely one class, there is no disorder, so the entropy is exactly 0.
    3. **Rain:** We have 5 samples. 3 are "Yes", 2 are "No". Similar to Sunny, the entropy is 0.97.

    The weighted average entropy for the Outlook split is `(5/14)*0.97 + (4/14)*0 + (5/14)*0.97`, which equals 0.693. The Gain is `0.94 - 0.693 = 0.246`. (Note: In the lecture, I mentioned 0.029 for Temperature, but for Outlook the gain is higher).

    让我们把这个应用到一个例子中。考虑一个网球俱乐部的数据集，它根据天气情况记录网球运动员是否来打球。我们有四个属性：天气状况（Outlook）、温度、湿度和风。让我们看看“天气状况”。它有三个可能的值：晴朗（Sunny）、阴天（Overcast）和雨天（Rain）。

    首先，计算整个数据集的熵。在14天中，9天是“Yes”（打球），5天是“No”（不打球）。熵计算为 `-p_pos * log(p_pos) - p_neg * log(p_neg)`，结果是0.94。

    现在，按天气状况分裂：

    1. **晴朗：** 我们有5个样本。2个“Yes”，3个“No”。这是混合的，所以熵很高，计算结果为0.97。
    2. **阴天：** 我们有4个样本。全部4个都是“Yes”。因为这纯粹是同一类别，没有无序性，所以熵正好是0。
    3. **雨天：** 我们有5个样本。3个“Yes”，2个“No”。与晴朗类似，熵为0.97。

    天气状况分裂的加权平均熵是 `(5/14)*0.97 + (4/14)*0 + (5/14)*0.97`，等于0.693。增益是 `0.94 - 0.693 = 0.246`。（注：课上我提到了温度的增益是0.029，但天气状况的增益更高）。

  - [19:27 - 19:34] **Building the Tree Recursively**

    Based on the calculation, "Outlook" provides a good split. The "Overcast" branch is pure (Entropy = 0), so we make it a leaf node predicting "Play". However, for the "Sunny" branch, we still have mixed results (2 Yes, 3 No). We need to partition further. We look at the remaining attributes (Temperature, Humidity, Wind) for just these 5 sunny days and calculate the gain again to find the next best split. Eventually, we might split "Sunny" by "Humidity" and "Rain" by "Wind". We continue this recursive process until all leaf nodes are pure (Entropy = 0) or we run out of attributes to use.

    基于计算，“天气状况”提供了一个很好的分裂。“阴天”分支是纯的（熵=0），所以我们将它设为预测“Play”的叶节点。然而，对于“晴朗”分支，我们仍然有混合结果（2 Yes，3 No）。我们需要进一步划分。我们仅针对这5个晴天，查看剩余的属性（温度、湿度、风），并再次计算增益以找到下一个最佳分裂。最终，我们可能会通过“湿度”来分裂“晴朗”，通过“风”来分裂“雨天”。我们继续这个递归过程，直到所有叶节点都是纯的（熵=0），或者我们用尽了所有属性。

  - [19:34 - 19:38] **Introduction to Regression Trees: Drug Effectiveness Example**

    Now let's consider a different problem. Suppose we have a new drug that can kill a virus. A doctor wants to determine the optimal dosage or treatment plan for individual patients. Unlike the tennis example where the output was a class (Yes/No), here the drug effectiveness is measured by numbers (e.g., percentage of effectiveness or IC50 values). This makes it a **Regression Problem**.

    If we look at the data—plotting Drug Effectiveness against Dosage—we see it is not a straight line. Effectiveness might be low at low dosage, spike at a certain range, and drop again if the dosage is too high (toxicity). A simple linear regression model cannot fit this data well. We need a model that can handle these non-linear relationships.

    现在让我们考虑一个不同的问题。假设我们要用一种新药来杀灭病毒。医生想要确定针对个体患者的最佳剂量或治疗方案。与网球例子中输出是类别（是/否）不同，这里的药物疗效是用数字衡量的（例如，有效性百分比或IC50值）。这使它成为一个**回归问题**。

    如果我们观察数据——绘制药物疗效与剂量的关系图——我们会发现它不是一条直线。疗效可能在低剂量时很低，在某个范围内激增，如果剂量太高（毒性）又会下降。简单的线性回归模型无法很好地拟合这种数据。我们需要一个能够处理这些非线性关系的模型。

  - [19:38 - 19:42] **Regression Tree Structure and Partitioning**

    The Regression Tree is very similar to the Decision Tree, but the output at each leaf is a numeric value. Imagine the tree splitting the feature space. If we have one feature (e.g., Dosage), the tree partitions the x-axis into distinct regions (intervals). For example, one region might be "Dosage ≤ 1.75".

    In each resulting region (leaf node), the model predicts a constant value. This value is typically the **average** of the target values of the training samples falling into that region. So, instead of a smooth curve, the regression tree models the function as a step function. Each branch of the tree corresponds to one of these flat regions on the graph.

    回归树与决策树非常相似，但每个叶节点的输出是一个数值。想象一下树在分割特征空间。如果我们有一个特征（例如剂量），树会将x轴划分为不同的区域（区间）。例如，一个区域可能是“剂量 ≤ 1.75”。

    在每个结果区域（叶节点）中，模型预测一个常量值。这个值通常是落入该区域的训练样本目标值的**平均值**。因此，回归树不是用平滑曲线，而是将函数建模为阶梯函数。树的每个分支对应图表上的这些平坦区域之一。

  - [19:42 - 19:46] **2D Partitioning and Optimization Task**

    If we have two features (e.g., X1 and X2), the tree partitions the 2D plane into rectangular regions. For example, first split by X2 ≥ 0.3, then split the bottom part by X1 ≥ 0.5. In each rectangular region, we simply calculate the average of the data points inside it to get our prediction.

    Constructing this tree is an optimization task. In machine learning, we usually choose a function class and a loss function, then minimize the loss. Here, our parameters are the **split dimension** (which feature to cut) and the **split value** (where to cut). We iterate through possible splits to find the one that minimizes our error.

    如果我们有两个特征（例如X1和X2），树会将二维平面划分为矩形区域。例如，首先按 X2 ≥ 0.3 分裂，然后按 X1 ≥ 0.5 分裂下半部分。在每个矩形区域中，我们只需计算其中数据点的平均值即可得到我们的预测。

    构建这棵树是一项优化任务。在机器学习中，我们通常选择一个函数类和一个损失函数，然后最小化损失。在这里，我们的参数是**分裂维度**（切割哪个特征）和**分裂值**（在哪里切割）。我们遍历可能的分裂，找到能最小化我们误差的那个。

  - [19:46 - 19:54] **Loss Function and Stopping Criteria (Regularization)**

    For regression trees, we typically use **Squared Loss** (Sum of Squared Residuals) as our metric. We want to find a split that makes the data in the child nodes as "tight" (low variance) as possible.

    However, we must prevent overfitting. A tree that is too deep (one leaf per sample) fits the training data perfectly but fails on new data. To control the size of the tree, we introduce a parameter **k** (minimum samples per leaf).

    The algorithm is:

    1. Check if the number of samples in the current node `|I|` is less than or equal to `k`.
    2. If yes, stop. Create a leaf node and output the average `y` value of those samples. (The average is used because it minimizes the squared loss for a set of numbers).
    3. If no, continue to search for the best split.

    对于回归树，我们通常使用**平方损失**（残差平方和）作为我们的度量标准。我们希望找到一种分裂，使得子节点中的数据尽可能“紧凑”（低方差）。

    但是，我们要防止过拟合。过深的树（每个样本一个叶子）能完美拟合训练数据，但在新数据上会失效。为了控制树的大小，我们引入参数**k**（每个叶节点的最小样本数）。

    算法如下：

    1. 检查当前节点的样本数 `|I|` 是否小于或等于 `k`。
    2. 如果是，停止。创建一个叶节点并输出这些样本的平均 `y` 值。（使用平均值是因为它能最小化一组数值的平方损失）。
    3. 如果否，继续寻找最佳分裂。

  - [19:54 - 19:59] **Calculating the Split: Dosage Example**

    Let's look at the specific calculation for the Drug dataset. We have 19 data points.

    First, consider the **Dosage** dimension. We sort the data by dosage.

    We only need to consider split points between adjacent data values. For 19 points, there are 18 possible gaps (candidate thresholds) to test.

    For each candidate threshold `s`:

    1. Split the data into Left Set (Dosage < s) and Right Set (Dosage ≥ s).
    2. Calculate the average effectiveness for the Left Set and the Right Set independently.
    3. Calculate the Sum of Squared Residuals (SSR) for both sides.

    For example, let's test a split at `s = 3`.

    - **Left:** Only 1 sample (Value=0). Average=0. Residual=`(0-0)^2 = 0`.
    - **Right:** 18 samples. Calculate their average (which is 38.8). Calculate the sum of squared differences from 38.8 for all 18 points.
    - **Total Loss:** Sum of Left SSR + Right SSR = 27,468.5.

    让我们看看药物数据集的具体计算。我们有19个数据点。

    首先，考虑**剂量**维度。我们按剂量对数据进行排序。

    我们只需要考虑相邻数据值之间的分裂点。对于19个点，有18个可能的间隙（候选阈值）需要测试。

    对于每个候选阈值 `s`：

    1. 将数据分为左集合（剂量 < s）和右集合（剂量 ≥ s）。
    2. 分别计算左集合和右集合的平均疗效。
    3. 计算两侧的残差平方和（SSR）。

    例如，让我们测试 `s = 3` 的分裂。

    - **左边：** 只有1个样本（值=0）。平均值=0。残差=`(0-0)^2 = 0`。
    - **右边：** 18个样本。计算它们的平均值（即38.8）。计算所有18个点与38.8的平方差之和。
    - **总损失：** 左SSR + 右SSR之和 = 27,468.5。

  - [19:59 - 20:03] **Selecting the Best Dosage Split**

    **(Student interaction about calculation specifics)**: Yes, if you look at the slide, the "38.8" is the average of the 18 samples on the right side. We calculate the squared residuals for each of those 18 points against 38.8 and sum them up.

    We repeat this calculation for all 18 possible thresholds in the Dosage dimension. It turns out the best split is at **Dosage = 14.5**.

    This split separates the data into:

    - **Left (Dosage < 14.5):** 6 samples (Low effectiveness).

    - **Right (Dosage ≥ 14.5):** 13 samples.

      This gives us a specific SSR value.

    **（关于计算细节的学生互动）**：是的，如果你看幻灯片，“38.8”是右侧18个样本的平均值。我们计算这18个点中每个点与38.8的残差平方，并将它们加起来。

    我们对剂量维度中所有18个可能的阈值重复此计算。结果表明，最佳分裂点是 **剂量 = 14.5**。

    这个分裂将数据分为：

    - **左边（剂量 < 14.5）：** 6个样本（低疗效）。

    - **右边（剂量 ≥ 14.5）：** 13个样本。

      这给了我们一个具体的SSR值。

  - [20:03 - 20:09] **Comparing Across Dimensions and Choosing the Root**

    We are not done. We must also check the other dimensions.

    - **Age:** We check all possible splits for Age. (e.g., Age < 50 vs Age ≥ 50).
    - **Sex:** Since it's binary (Male/Female), there is only one split to check.

    We compare the minimum SSR obtained from the best Dosage split, the best Age split, and the Sex split.

    In this example, splitting by **Age > 50** results in the lowest total Squared Loss (SSR = 12,017, as seen on the slide). Therefore, **Age** is chosen as the root node of our tree.

    One branch has 6 samples, and the other has 13. We then recursively apply the same process to these subsets. For the subset of 16 samples (Wait, the slide says 6 and 13, let's stick to the recursive logic), we check Dosage and Sex again to split further. We stop when the number of samples is small (e.g., k=6 or 7).

    OK, we will take a break here.

    我们还没做完。我们必须检查其他维度。

    - **年龄：** 我们检查年龄的所有可能分裂。（例如，年龄 < 50 对比 年龄 ≥ 50）。
    - **性别：** 因为它是二元的（男/女），只有一个分裂需要检查。

    我们比较从最佳剂量分裂、最佳年龄分裂和性别分裂中获得的最小SSR。

    在这个例子中，按 **年龄 > 50** 分裂导致了最低的总平方损失（SSR = 12,017，如幻灯片所示）。因此，**年龄**被选为我们树的根节点。

    一个分支有6个样本，另一个有13个。然后我们对这些子集递归地应用相同的过程。对于16个样本的子集（等等，幻灯片说是6和13，我们要遵循递归逻辑），我们会再次检查剂量和性别以进行进一步分裂。当样本数量很少（例如k=6或7）时，我们就停止。

    好的，我们在这里休息一下。

  - [20:32 - 20:35] **Bagging: Sampling with Replacement**

    Now, look at the data tables on the slide. You will see something interesting: the first data point in the new dataset is identical to the last two. Some samples from the original dataset are missing, while others appear multiple times. This is exactly what I mean by "Sampling with Replacement." This technique is called **Bagging** (Bootstrap Aggregating). The idea is simple:

    1. **Bootstrap:** You produce multiple datasets (e.g., $D_1, D_2, \dots, D_k$) from the original data $D$. Each new dataset is the same size as the original but is created by randomly picking samples one by one, putting them back (replacement), and picking again. This creates variations in the training data.
    2. **Aggregating:** You build a model for each bootstrapped dataset. Then, you combine their predictions. For regression problems, you take the **average** of all model outputs. For classification, you take a **majority vote** (like voting 0 or 1).

    现在，请看幻灯片上的数据表。你会发现一些有趣的现象：新数据集中的第一个数据点与最后两个完全相同。原始数据集中的某些样本缺失了，而其他样本则出现了多次。这就是我所说的“有放回抽样”（Sampling with Replacement）。这种技术称为**Bagging**（Bootstrap Aggregating，自助汇聚法）。其思想很简单：

    1. **Bootstrap（自助法）：** 你从原始数据 $D$ 中生成多个数据集（例如 $D_1, D_2, \dots, D_k$）。每个新数据集的大小与原始数据集相同，但通过逐个随机抽取样本、放回（置换）、再抽取的方式创建。这在训练数据中创造了变异性。
    2. **Aggregating（汇聚）：** 你为每个自助数据集构建一个模型。然后，结合它们的预测结果。对于回归问题，你取所有模型输出的**平均值**。对于分类问题，你取**多数投票**（就像投0或1一样）。

  - [20:35 - 20:41] **Random Forest: Adding Extra Randomness**

    Random Forest is essentially Bagging applied to decision trees, but with an extra trick to make it better. The trick is how we build the trees. If we just used standard decision trees on bootstrapped data, the trees might still be very similar (correlated) because they would all pick the same strong features at the top.

    To solve this, Random Forest introduces **feature randomness**. At each node splitting step, instead of looking at *all* available features to find the best split, we randomly select a subset of **m** features (where $m < total\ features$). We pick the best split *only* from this random subset.

    Why is this good? It forces the trees to be different. It "decorrelates" them. Imagine a committee where everyone has the exact same information and bias; their "average" opinion adds no value. But if committee members (trees) look at different aspects (features) of the problem, their combined decision is much wiser and more robust. By choosing a random subset of features, we ensure a high degree of independence between the trees.

    随机森林本质上是应用于决策树的Bagging，但增加了一个额外的技巧使其变得更好。这个技巧在于我们如何构建树。如果我们仅在自助数据上使用标准决策树，树与树之间可能仍然非常相似（相关），因为它们都会在顶部选择相同的强特征。

    为了解决这个问题，随机森林引入了**特征随机性**。在每个节点的分类步骤中，我们不是查看*所有*可用特征来寻找最佳分裂，而是随机选择**m**个特征的子集（其中 $m < 总特征数$）。我们*仅*在这个随机子集中选择最佳分裂。

    为什么这很好？它迫使树变得不同。它使它们“去相关”。想象一个委员会，如果每个人都拥有完全相同的信息和偏见，他们的“平均”意见就没有任何附加价值。但是，如果委员会成员（树）观察问题的不同方面（特征），他们的综合决策就会更加明智和稳健。通过选择随机特征子集，我们确保了树之间的高度独立性。

  - [20:41 - 20:44] **Validation: Out-of-Bag (OOB) Accuracy**

    How do we estimate the accuracy of a Random Forest? We don't necessarily need a separate validation set because of the bagging process. Recall that when we create a bootstrap sample, some original data points are left out. These are called **Out-of-Bag (OOB)** samples.

    For each tree in the forest, there are specific samples that were *not* used to train that specific tree. We can use these "leftover" samples to test that tree's performance. By averaging the prediction errors on the OOB samples across the entire forest, we get a reliable estimate of the model's accuracy without wasting training data on a hold-out set.

    我们如何估计随机森林的准确性？由于Bagging过程，我们不一定需要单独的验证集。回想一下，当我们创建一个自助样本时，一些原始数据点被留在了外面。这些被称为**袋外（Out-of-Bag, OOB）**样本。

    对于森林中的每棵树，都有一些特定的样本*没有*被用来训练这棵特定的树。我们可以使用这些“剩余”的样本来测试那棵树的性能。通过计算整个森林在OOB样本上的平均预测误差，我们可以获得模型准确性的可靠估计，而无需将训练数据浪费在留出集上。

  - [20:44 - 20:49] **Hyperparameters: n_estimators and max_features**

    The performance of a Random Forest depends on key hyperparameters.

    1. **n_estimators (Number of Trees):** How many trees should we build? If you use too few trees (e.g., 3 or 4), the model is unstable and has high variance. As you add more trees, performance improves. Eventually, the performance plateaus—adding more trees after a certain point (e.g., 50 vs 100) doesn't gain much accuracy, it just costs more computation. You want to find that stable point.
    2. **max_features (Size of random subset):** How many features ($m$) do we check at each split?
       - For **Classification**, a common heuristic (default) is $m = \sqrt{p}$, where $p$ is the total number of features.
       - For **Regression**, the default is often $m = p/3$.

    Smaller `max_features` leads to more diversity (less correlation) among trees but individual trees might be weaker. Larger `max_features` makes trees stronger individually but more correlated. This is a trade-off you tune through validation.

    随机森林的性能取决于关键的超参数。

    1. **n_estimators（树的数量）：** 我们应该构建多少棵树？如果你使用的树太少（例如3或4棵），模型会不稳定且方差很高。随着你增加更多的树，性能会提高。最终，性能会趋于平稳——在某一点之后增加更多的树（例如50对比100）并不会增加太多准确性，只会消耗更多的计算资源。你需要找到那个稳定点。
    2. **max_features（随机子集的大小）：** 我们在每次分裂时检查多少个特征（$m$）？
       - 对于**分类**，一个常见的启发式（默认值）是 $m = \sqrt{p}$，其中 $p$ 是总特征数。
       - 对于**回归**，默认值通常是 $m = p/3$。

    较小的 `max_features` 会导致树之间更多的多样性（相关性更低），但单棵树可能会更弱。较大的 `max_features` 使单棵树更强，但相关性更高。这是一个你需要通过验证来调整的权衡。

  - [20:53 - 20:57] **XGBoost: The Iterative Boosting Concept**

    Now we move to XGBoost. This is a very effective model, but the core idea is actually very simple. Unlike Random Forest, where we build trees in parallel, here we repeatedly build new tree models one by one and add them into an ensemble. Mathematically, this is an additive process. You start with a base model, and then you add a second tree, then a third tree, and so on. But here is the key difference: when you build the second model, you do not build it on the original target values. Instead, you build it based on the *errors* of the previous model. We call these "residuals." If the current model has a prediction error on a data point, the next tree's job is to predict that specific error. You repeat this process—calculating residuals, fitting a new tree to them, and adding it to the ensemble—until you get a good final prediction.

    现在我们讲到XGBoost。这是一个非常有效的模型，但其核心思想实际上非常简单。与并行构建树的随机森林不同，这里我们反复地逐一构建新的树模型，并将它们加入到集成模型中。在数学上，这是一个加法过程。你从一个基础模型开始，然后加入第二棵树，再加入第三棵树，以此类推。但这里的关键区别在于：当你构建第二个模型时，你不是基于原始目标值构建的。相反，你是基于前一个模型的*误差*来构建的。我们称这些为“残差”（residuals）。如果当前模型在一个数据点上有预测误差，下一棵树的任务就是预测那个特定的误差。你重复这个过程——计算残差，拟合一棵新树来修正它们，并将其加入到集成模型中——直到你获得一个好的最终预测。

  - [20:57 - 20:59] **Hyperparameters: Learning Rate and Regularization**

    When we add these new trees into the model, we don't just add them directly. We introduce a hyperparameter called the "Learning Rate" (often denoted as eta). This value is usually very small, like 0.1 or 0.3. Why? Because we don't want any single tree to change the model too drastically. If a tree is bad, we don't want it to cause too much damage. By multiplying the new tree's output by a small learning rate, we ensure that each tree only makes a small contribution or correction. This means we need a large number of trees to reach the final solution, but the process is much more stable and avoids overfitting. While standard Gradient Boosting optimizes the variance, XGBoost is famous because it also includes unique regularization terms in its objective function to further prevent overfitting. It balances bias and variance very well.

    当我们把这些新树加入模型时，我们不会直接加入。我们引入了一个称为“学习率”（通常表示为eta）的超参数。这个值通常很小，比如0.1或0.3。为什么？因为我们不希望任何单独的一棵树过激地改变模型。如果一棵树很差，我们不希望它造成太大的破坏。通过将新树的输出乘以一个小的学习率，我们确保每棵树只做出微小的贡献或修正。这意味着我们需要大量的树来达到最终解，但这个过程要稳定得多，并且能避免过拟合。虽然标准的梯度提升（Gradient Boosting）优化了方差，但XGBoost之所以出名，是因为它在目标函数中包含了独特的正则化项，以进一步防止过拟合。它很好地平衡了偏差和方差。

  - [20:59 - 21:03] **Step-by-Step Example: Initial Prediction and Residuals**

    Let's walk through a concrete example using the dataset on the slide. We have features like Height, Color, and Gender, and we want to predict a person's Weight. This is a regression problem.

    **Step 1:** We create an initial "naive" model. The simplest possible guess is just the average of all target values. In this dataset, the average weight is 71.2 kg. So, our first model (F0) predicts 71.2 for everyone.

    **Step 2:** Now we calculate the **Residuals** for each data point. The residual is simply the Real Value minus the Predicted Value.

    Take the first data point: Ideally, the weight is 88 kg. Our model predicted 71.2 kg.

    The residual is `88 - 71.2 = 16.8`.

    This "16.8" represents the error—the part of the weight our first model failed to explain. We do this for all data points (e.g., the second point has a residual of 4.8).

    让我们用幻灯片上的数据集来做一个具体的例子。我们有身高、颜色和性别等特征，我们想要预测一个人的体重。这是一个回归问题。

    **第一步：** 我们创建一个初始的“朴素”模型。最简单的猜测就是所有目标值的平均值。在这个数据集中，平均体重是71.2公斤。所以，我们的第一个模型（F0）对所有人的预测都是71.2。

    **第二步：** 现在我们要为每个数据点计算**残差**。残差就是“真实值”减去“预测值”。

    以第一个数据点为例：理想情况下，体重是88公斤。我们的模型预测是71.2公斤。

    残差是 `88 - 71.2 = 16.8`。

    这个“16.8”代表了误差——即我们的第一个模型未能解释的那部分体重。我们对所有数据点都这样做（例如，第二个点的残差是4.8）。

  - [21:03 - 21:07] **Boosting Iteration: Updating with the Second Tree**

    **Step 3:** Now we build the second model (Tree 1). Crucially, this tree does *not* try to predict the weight (88 kg). It tries to predict the *residual* (16.8). We use the features (Height, Color, Gender) to build a decision tree that fits these residual values.

    Looking at the tree on the slide, let's trace the path for that first data point (Male, Height 1.6, Blue).

    - Is it Female? No.

    - Is Height < 1.6? No.

    - Is it not Blue? No (it is Blue).

      This path leads to a leaf node that outputs a prediction close to the residual, let's say 16.8.

      **Step 4:** We update our ensemble prediction. We take the initial guess (71.2) and add the new tree's prediction scaled by the learning rate (0.1).

      The adjustment is `0.1 * 16.8 = 1.68`.

      So the new prediction is `71.2 + 1.68 = 72.88`.

      Finally, we calculate the **Updated Residual**: `88 - 72.88 = 15.1`.

      You can see the residual has dropped from 16.8 to 15.1. The error got smaller! We then repeat this process, building a third tree to fit the new residual of 15.1, and so on.

    **第三步：** 现在我们要构建第二个模型（树1）。关键在于，这棵树*不*尝试预测体重（88公斤）。它尝试预测*残差*（16.8）。我们利用特征（身高、颜色、性别）来构建一棵决策树，去拟合这些残差值。

    看幻灯片上的树，让我们追踪第一个数据点（男性，身高1.6，蓝色）的路径。

    - 是女性吗？否。

    - 身高 < 1.6吗？否。

    - 不是蓝色吗？否（它是蓝色）。

      这条路径通向一个叶节点，输出一个接近残差的预测值，假设是16.8。

      **第四步：** 我们更新集成模型的预测。我们取初始猜测（71.2），加上乘以学习率（0.1）后的新树预测值。

      调整量是 `0.1 * 16.8 = 1.68`。

      所以新的预测是 `71.2 + 1.68 = 72.88`。

      最后，我们计算**更新后的残差**：`88 - 72.88 = 15.1`。

      你可以看到残差从16.8下降到了15.1。误差变小了！然后我们重复这个过程，构建第三棵树来拟合15.1这个新残差，以此类推。

  - [21:11 - 21:14] **The Role of Learning Rate and Regularization**

    So, why do we do this? Why do we multiply the tree's output by 0.1? This 0.1 is the **Learning Rate** (or shrinkage). We never want to fully trust a single tree because it might be overfitting the specific residuals it saw. If we simply added the full tree, the model would change too drastically. Therefore, we multiply the prediction by this small factor (0.1) so that this specific tree only contributes a small part to the final ensemble. It forces the model to learn slowly and incrementally. Additionally, XGBoost is special because it includes a **regularization term** in its objective function. This penalizes complex trees, further ensuring that no single tree dominates or overfits the data.

    那么，我们为什么要这样做？我们为什么要将树的输出乘以0.1？这个0.1就是**学习率**（Learning Rate，或称为收缩步长）。我们从不希望完全信任单独的一棵树，因为它可能会过拟合它所看到的特定残差。如果我们简单地加上整棵树，模型的变化就会过于剧烈。因此，我们将预测值乘以这个小因子（0.1），使得这棵特定的树只对最终的集成模型做出很小的贡献。这迫使模型缓慢且增量地学习。此外，XGBoost的特殊之处在于它在目标函数中包含了一个**正则化项** 。这会惩罚复杂的树，进一步确保没有单独的一棵树能主导模型或过拟合数据。

  - [21:14 - 21:15] **Iterative Model Refinement**

    So this is the process: You have the first model (the initial average). Then you build the second tree to fit the errors. Now you have a combined model: `Initial + (0.1 * Tree1)`. Is this complete? No, it is still not perfect. There is still an error (e.g., the residual dropped to 15.1, but it's not zero). So, we do this again. Another step, and another step. We determine the hyperparameters—specifically **how many trees** (n_estimators) to build and what learning rate to use. You repeat this loop until the error is sufficiently small or you reach the maximum number of trees.

    这就是整个过程：你有第一个模型（初始平均值）。然后你构建第二棵树来拟合误差。现在你有了一个组合模型：`初始值 + (0.1 * 树1)`。这完成了吗？不，它仍然不完美。仍然存在误差（例如，残差降到了15.1，但不是零）。所以，我们再做一次。一步接一步。我们需要确定超参数——具体是**构建多少棵树**（n_estimators）以及使用什么学习率。你重复这个循环，直到误差足够小或者达到了树的最大数量。

  - [21:15 - 21:16] **XGBoost's Industry Status**

    This is a very popular model. It is widely used in competitions like Kaggle and the Netflix Prize. In fact, it is so effective that even modern Deep Learning papers focusing on tabular data are required to compare their results against XGBoost. It remains the state-of-the-art baseline for structured data problems. It is a powerful tool that you should know.

    这是一个非常流行的模型。它广泛应用于Kaggle和Netflix Prize等竞赛中 。事实上，它非常有效，以至于即使是现代专注于表格数据的深度学习论文，也被要求将结果与XGBoost进行比较 。它仍然是结构化数据问题的最先进基准。这是一个你应该掌握的强大工具。
## Week 5
  - [20:19 - 20:20] **Introduction to Tree Construction and Residuals**

    Presently, we focus on the performance based on the residuals. Remember, we built a first model, and then we build a second one. The combination of the first and second models acts as our new prediction. So, based on this additive prediction approach, today's lecture will show you exactly how to build these trees. You will see that we use a specific method. Here we choose the tree structure. The idea is the same as we discussed before, but there are some differences in the details. So now, let's see how to build the tree in each stage. We also have some controls on the size of the tree.

    目前，我们关注基于残差（residuals）的性能。记得吗，我们建立了第一个模型，然后建立第二个模型。第一个和第二个模型的组合就成为了我们要的新预测结果。所以，基于这种加法预测的方法，今天的课程将向你们展示究竟如何构建这些树。你们会看到我们使用了一种特定的方法。在这里我们选择树结构。这背后的核心理念和我们之前讨论的是一样的，但在细节上有一些不同。现在，让我们来看看如何在每个阶段构建树。我们对树的大小也有一些控制机制。

  - [20:20 - 20:21] **Calculating the Node Score using Negative Residuals**

    When we build a tree, we start from the root and proceed node by node. I assume you know the basics of that process. Key symbols here involve the "negative residuals." We define `g_i` as `ŷ_i - y_i` (the predicted value minus the actual value). These are what we call the active or negative residuals because we are fitting the tree to them. We then calculate a "Node Score" from these residuals using a specific formula. For each example, we have a residual. We sum them all together, square that sum, and then divide by the number of samples plus Lambda (`λ`). Lambda here is a hyperparameter.

    当我们构建一棵树时，我们从根节点开始，逐个节点地进行。我假设你们已经了解了这个过程的基础知识。这里的关键符号涉及“负残差”（negative residuals）。我们将 `g_i` 定义为 `ŷ_i - y_i`（预测值减去实际值）。这些被称为负残差，因为我们要用树来拟合它们。然后，我们使用特定的公式根据这些残差计算“节点分数”（Node Score）。对于每个样本，我们都有一个残差。我们将它们全部加在一起，对总和进行平方，然后除以样本数量加上 Lambda (`λ`)。这里的 Lambda 是一个超参数。

  - [20:21 - 20:23] **Strategy for Splitting Nodes**

    We will see how to obtain the values of this hyperparameter later. Now, you see, in the original dataset, we have many features. Here, we want to test every possible feature to find the best partition or split in the node. Remember, we build the tree from the root, then we split and split again. For example, when we reach a specific node, we calculate the score. Then we need to find out what is the best partition—the left and right partition. We have a number of features available, so we have to try every possible feature. We try every possible partition split according to that feature because we don't know beforehand which one is best.

    我们将稍后讨论如何获取这个超参数的值。现在，你们看，在原始数据集中，我们有很多特征。在这里，我们要测试每一个可能的特征，以找到节点中最好的划分或分裂点。记得吗，我们从根节点开始建树，然后不断地分裂。例如，当我们到达一个特定的节点时，我们计算分数。然后我们需要找出什么是最好的划分——即左边和右边的划分。我们要利用所有可用的特征，所以我们必须尝试每一个可能的特征。我们尝试根据该特征进行的每一个可能的划分，因为我们事先不知道哪一个是最好的。

  - [20:23 - 20:25] **Numeric Example: Splitting on Feature X**

    Now, let's consider one dimension, one feature; we call it `J`. There is a value we use to split the data into two parts. Say, for example, looking at the data on the slide, if we use a value of 6 as a cutoff, then we split these samples into two parts. The first part, let's call it `A`, contains all the samples with a value greater than or equal to 6. In this feature `X`, we have already sorted the samples according to their values. So, "greater than or equal to 6" means it contains these two specific samples (where X is 7 and 9). If we use 6 as a cutoff, these samples go here. Then we calculate the score for this new node `A` in the same manner. But when we calculate this score, we just use the residuals for the samples in `A`.

    现在，让我们考虑一个维度，一个特征；我们称之为 `J`。我们使用一个值将数据分成两部分。比如，看幻灯片上的数据，如果我们使用 6 作为截断值（cutoff），那么我们将这些样本分成两部分。第一部分，我们称之为 `A`，包含所有值大于或等于 6 的样本。在这个特征 `X` 中，我们已经根据它们的值对样本进行了排序。所以，“大于或等于 6”意味着它包含这两个特定的样本（X 为 7 和 9 的样本）。如果我们使用 6 作为截断值，这些样本就归到这里。然后我们以同样的方式计算这个新节点 `A` 的分数。但是当我们计算这个分数时，我们只使用 `A` 中样本的残差。

  - [20:25 - 20:26] **Calculating Gain with Lambda (`λ`) and Gamma (`γ`)**

    When we calculate the score for the other part, `B` (samples where X < 6), we just use the residuals in `B`. So, for the score formula, the numerator is the square of the sum of residuals in that group. The denominator is the number of samples in that group (for example, 2 for group `A`) plus Lambda. This Lambda is the same hyperparameter used everywhere. Then we consider the "Gain" of the split. The Gain is calculated as the sum of the scores in the two parts (Score A + Score B), minus the score of the parent node (Score before split), and then minus another parameter, Gamma (`γ`). Gamma is another hyperparameter.

    当我们计算另一部分 `B`（X < 6 的样本）的分数时，我们只使用 `B` 中的残差。所以，对于分数公式，分子是该组残差之和的平方。分母是该组中的样本数量（例如组 `A` 是 2）加上 Lambda。这个 Lambda 是到处使用的同一个超参数。然后我们考虑这次分裂的“增益”（Gain）。增益的计算方法是两部分的分数之和（分数 A + 分数 B），减去父节点的分数（分裂前的分数），然后再减去另一个参数，Gamma (`γ`)。Gamma 是另一个超参数。

  - [20:26 - 20:28] **Decision Rule: To Split or Not to Split**

    We consider every possible feature and every possible cutoff, and then we choose the best one. We want the one that gives the largest Gain. This corresponds to the best partition of the samples. If this Gain is positive, then we perform the split. If the Gain is not positive (negative), then we just stop there. We treat this node as a leaf node. The value for this leaf node is calculated this way: the sum of residuals divided by the number of samples `K` plus Lambda `λ`, and there is a negative sign in front. This is how we determine whether to split in each node or establish a leaf value.

    我们考虑每一个可能的特征和每一个可能的截断值，然后我们选择最好的一个。我们想要那个能产生最大增益（Gain）的划分。这对应于样本的最佳划分。如果这个增益是正的，那么我们就进行分裂。如果增益不是正的（是负的），那么我们就在那里停止。我们将这个节点视为叶节点。叶节点的值是这样计算的：残差之和除以样本数量 `K` 加上 Lambda `λ`，并且前面有一个负号。这就是我们如何决定在每个节点是否分裂或确立叶子值的方法。

  - [20:28 - 20:30] **Walkthrough of the Numeric Example (Slide 51)**

    Let's look at this example with 5 samples. We focus on this feature `X`. These are the true values `y` and the current predicted values `ŷ`. The negative residual is equal to the predicted value minus the true value (`ŷ - y`). So, for the first example, it is -0.5 (2.5 - 3). For the second one, it is 0.5 (2.5 - 2). For the third one, it is -3.5 (2.5 - 6). Is this clear how to calculate this negative residual? Now, we will calculate the score for this parent node. You will see we just sum all the negative residuals. This one corresponds to this, this one to that... we square the sum. There are 5 samples. Lambda is a hyperparameter, and for investigation purposes here, we set Lambda to be 1. So therefore, we get this number: the score is 30.375.

    让我们来看看这个有 5 个样本的例子。我们关注特征 `X`。这是真实值 `y` 和当前的预测值 `ŷ`。负残差等于预测值减去真实值（`ŷ - y`）。所以，对于第一个例子，它是 -0.5（2.5 - 3）。对于第二个，它是 0.5（2.5 - 2）。对于第三个，它是 -3.5（2.5 - 6）。如何计算这个负残差清楚了吗？现在，我们将计算这个父节点的分数。你们会看到我们只是将所有负残差相加。这个对应这个，那个对应那个……我们将总和平方。这里有 5 个样本。Lambda 是一个超参数，为了便于研究，这里我们将 Lambda 设为 1。因此，我们得到这个数字：分数是 30.375。

  - [20:30 - 20:32] **Identifying Possible Split Points**

    Is this clear? We focus on one feature, and we calculate the score for the whole dataset (or the samples contained in the node). In the very beginning, this node contains all samples. But after a split, a node will contain only some samples. This example focuses on these 5 samples, so this is the score. Now we consider the split. For this 5-sample dataset, how many possible splits are there? The samples are sorted according to the values of feature `X`: 2, 3, 5, 7, 9. So if we split between 2 and 3, we get one possible split. Then we have another possible split between 3 and 5. If we choose, say, 3.5, we get another one. And another one between 5 and 7, and another between 7 and 9. So that is four possible splits for this feature.

    这清楚吗？我们专注于一个特征，并计算整个数据集（或节点中包含的样本）的分数。最开始，这个节点包含所有样本。但在分裂之后，一个节点将只包含部分样本。这个例子关注这 5 个样本，这就是它的分数。现在我们考虑分裂。对于这 5 个样本的数据集，有多少种可能的分裂？样本是根据特征 `X` 的值排序的：2、3、5、7、9。所以如果我们在 2 和 3 之间分裂，我们得到一种可能的分裂。在 3 和 5 之间我们有另一种可能的分裂。如果我们选择，比如说 3.5，我们得到另一种。在 5 和 7 之间还有一种，在 7 和 9 之间也有一种。所以对于这个特征，有四种可能的分裂。

  - [20:32 - 20:35] **Calculating Scores for the Left and Right Children**

    Now, we just use 6 as a cutoff for this example. I use this one to explain. So `A` is the set of samples with `X` greater than or equal to 6. This refers to the last two samples (7 and 9). `B` is the samples with `X` less than 6, which are the first three samples. This split is clear: we use 6 as a cutoff value. Then we calculate the score for part `A`. Part `A` contains the last two samples. The residuals are -5.5 and -4.5. So the sum is -10. -10 squared is 100, right? The Lambda now is 1. The number of samples is 2. So the denominator is 2 + 1 = 3. The result is 100 divided by 3, which is 33.33. Similarly, `B` contains the first three samples, so we use these three residuals: -0.5, 0.5, and -3.5. We sum them, square them, and divide by 3 (samples) plus Lambda (1). The value is 3.06.

    现在，在这个例子中我们只使用 6 作为截断值。我用这个来解释。所以 `A` 是 `X` 大于或等于 6 的样本集合。这指的是最后两个样本（7 和 9）。`B` 是 `X` 小于 6 的样本，也就是前三个样本。这个分裂很清楚：我们使用 6 作为截断值。然后我们计算 `A` 部分的分数。`A` 部分包含最后两个样本。残差是 -5.5 和 -4.5。所以总和是 -10。-10 的平方是 100，对吧？Lambda 现在是 1。样本数量是 2。所以分母是 2 + 1 = 3。结果是 100 除以 3，即 33.33。同样地，`B` 包含前三个样本，所以我们使用这三个残差：-0.5、0.5 和 -3.5。我们将它们相加，平方，然后除以 3（样本数）加上 Lambda（1）。这个值是 3.06。

  - [20:35 - 20:37] **Gain Calculation and Split Decision**

    Therefore, the Gain for this partition is 0.5 multiplied by (33.33 + 3.06 minus the parent score 30.375), and then minus Gamma (`γ`). Gamma is another hyperparameter, and we decided to set this to 0. Since the result is positive, it means it is a good split. If the Gain is negative, then we should stop. This is the condition to determine whether to split or not split based on the Gain value. A positive Gain implies a split. We want to increase the Gain, so positive means good. It implies we should split.

    因此，这个划分的增益（Gain）是 0.5 乘以（33.33 + 3.06 减去父节点分数 30.375），然后再减去 Gamma (`γ`)。Gamma 是另一个超参数，我们决定将其设为 0。因为结果是正的，这意味着这是一个好的分裂。如果增益是负的，那么我们就应该停止。这是根据增益值决定是否分裂的条件。正的增益意味着分裂。我们想要增加增益，所以正值意味着好。它意味着我们应该进行分裂。

  - [20:37 - 20:39] **Theoretical Basis and Leaf Values**

    In fact, this condition is controlled by this hyperparameter. There is some theory behind this formula. Basically, it controls the balance. If we stop splitting, we need to output a value in the node. We want to get this model such that it minimizes the error. If the tree gives a wrong prediction error, we want to fix it. We build many trees. Lambda controls the size or the complexity. Is the formula clear? We have these residuals, and this is how we compute this.

    事实上，这个条件是由这个超参数控制的。这个公式背后有一些理论依据。基本上，它控制着平衡。如果我们停止分裂，我们需要在节点中输出一个值。我们希望得到的模型能最小化误差。如果树给出了错误的预测误差，我们希望修正它。我们要建立很多棵树。Lambda 控制大小或复杂度。公式清楚了吗？我们有这些残差，这就是我们计算它的方式。

  - [20:39 - 20:42] **Calculating Leaf Output Values (Weights)**

    So now you see after this, we split the five samples into two nodes. One node has 2 samples (where X >= 6). The other has 3 samples (where X < 6). These two nodes, if we don't need to split them more, become leaves. Therefore, we need to calculate the optimal output value (prediction value) for each node. The calculation is also very straightforward. For node `B` (the left part), it has 3 samples. We just take the sum of residuals for samples in `B`: -0.5 + 0.5 + -3.5. Then we divide by the number of samples (3) plus Lambda (1), and apply a negative sign. This gives you the leaf value. That's why the leaf value is 7/8 (or 0.875). For the other node with 2 samples, we calculate the sum of residuals (-4.5 - 5.5 = -10), divide by (2+1), and apply the negative sign. So the value is 10/3.

    现在你们看到，在这之后，我们将五个样本分成了两个节点。一个节点有 2 个样本（X >= 6）。另一个有 3 个样本（X < 6）。这两个节点，如果我们不需要进一步分裂，就成为叶子节点。因此，我们需要计算每个节点的最佳输出值（预测值）。计算也非常简单。对于节点 `B`（左边部分），它有 3 个样本。我们只需取 `B` 中样本残差的总和：-0.5 + 0.5 + -3.5。然后我们除以样本数（3）加上 Lambda（1），并加上一个负号。这给出了叶子值。这就是为什么叶子值是 7/8（或 0.875）。对于另一个有 2 个样本的节点，我们计算残差之和（-4.5 - 5.5 = -10），除以（2+1），并加上负号。所以值是 10/3。

  - [20:42 - 20:44] **Scaling to Multiple Features and Full Trees**

    Is this video decision tree process clear? Here we just considered one feature, but if you have one million dimensions or many features, you just do this for every feature. You choose the best split from all possible splits across all features. You consider all possible splits and choose the best one. We want large Gains. When we choose the largest Gain, we then determine whether to split or not based on whether it's positive (after subtracting Gamma).

    这个视频决策树的过程清楚了吗？这里我们只考虑了一个特征，但如果你有一百万个维度或很多特征，你就对每个特征都这样做。你从所有特征的所有可能分裂中选择最好的一个。你考虑所有可能的分裂并选择最好的。我们想要大的增益。当我们选择了最大增益后，我们根据它是否为正（减去 Gamma 后）来决定是否分裂。

  - [20:44 - 20:46] **Hyperparameters: Trees, Learning Rate, Lambda, Gamma**

    There are key hyperparameters for building a tree. The first one, and most important, is how many trees you want. This one we cannot treat automatically; we have to assign it. We have to try different numbers. Also, there is a learning rate in front of each tree. The default is 0.3, but you also need to find the best weight for performance. Lambda is for regularization; a larger Lambda means stronger regularization, which means a simpler model. The purpose is to avoid overfitting. Gamma is another hyperparameter; a large Gamma implies fewer splits because if Gamma is big, the Gain needs to be more positive to split. This builds a shallower tree.

    构建树有一些关键的超参数。第一个，也是最重要的，是你想要多少棵树。这个我们无法自动处理；我们必须指定它。我们必须尝试不同的数量。此外，每棵树前面都有一个学习率（learning rate）。默认值是 0.3，但你也需要找到性能最佳的权重。Lambda 用于正则化；较大的 Lambda 意味着更强的正则化，也就意味着更简单的模型。目的是为了避免过拟合。Gamma 是另一个超参数；较大的 Gamma 意味着更少的分裂，因为如果 Gamma 很大，增益需要非常正才能进行分裂。这会构建出一棵较浅的树。

  - [20:46 - 20:51] **Hyperparameter Tuning Methods**

    How do we determine these? There is no cheap method for this. The straightforward method is to search over all possible parameter values. For example, for the learning rate, can you try these five possibilities: 0.1, 0.3, 0.6, 1.0? For the max depth (longest path from root to leaf), you can try 1, 3, 6, 10. This is the standard method for building XGBoost models—what we call Grid Search or Randomized Search. The principle is to build many trees and put them together.

    我们如何确定这些参数呢？这没有捷径。直接的方法是搜索所有可能的参数值。例如，对于学习率，你可以尝试这五种可能：0.1、0.3、0.6、1.0？对于最大深度（从根到叶的最长路径），你可以尝试 1、3、6、10。这是构建 XGBoost 模型的标准方法——我们称之为网格搜索（Grid Search）或随机搜索。其原则是构建许多树并将它们组合在一起。

  - [20:51 - 20:53] **XGBoost for Classification: Introduction**

    The only different thing now is when we use a different objective, like classification. It can be very dangerous if misunderstood. So, in the very beginning, we might again pick a base value like 0.5. We build a tree with one node outputting 0.5. I will explain this. Then we build the first tree, the second one, and so forth. The procedure is the same, only the condition is different. Let's see this simple dataset with only four samples for binary classification. There are two classes. One class has label 1, and the other class has label 0.

    现在唯一不同的是当我们使用不同的目标时，比如分类。如果理解错了可能会很危险。所以，在一开始，我们可能再次选择一个基准值，比如 0.5。我们建立一棵只有一个节点输出 0.5 的树。我会解释这一点。然后我们建立第一棵树，第二棵树，依此类推。过程是一样的，只是条件不同。让我们看看这个只有四个样本的简单数据集，用于二分类。有两个类别。一个类别的标签是 1，另一个类别的标签是 0。

  - [20:53 - 20:55] **Interpreting Labels as Probabilities**

    We can consider the dataset as providing probabilities. For example, being in Class 1 means the probability is 1. So Label 1 is a sample in Class 1 with probability 1. Label 0 is a sample with probability 0 for Class 1. Therefore, we interpret these labels as probability values. Basically, we train an XGBoost classifier to predict the probability that a sample belongs to Class 1. Everyone reports probabilities. For a sample in the first class, the model output probability should be very close to 1. For the other class, the model output probability is small, close to 0. If we can do this, the model is good. Is that clear? We don't really want to output just 0 or 1; that is hard. The model gives you a confidence.

    我们可以将数据集视为提供概率。例如，属于类别 1 意味着概率为 1。所以标签 1 是一个概率为 1 的类别 1 的样本。标签 0 是一个类别 1 概率为 0 的样本。因此，我们将这些标签解释为概率值。基本上，我们训练一个 XGBoost 分类器来预测样本属于类别 1 的概率。每个人都报告概率。对于第一类中的样本，模型输出的概率应该非常接近 1。对于另一类，模型输出的概率很小，接近 0。如果我们能做到这一点，模型就是好的。清楚了吗？我们并不真的想只输出 0 或 1；那太硬了。模型给你的是一个置信度。

  - [20:55 - 20:57] **Initial Prediction and Classification Residuals**

    In the very beginning, we choose 0.5 because, looking here, there are two samples in each class. The average value is 0.5. It's reasonable. If there is a large number of samples in Class 1, we might choose a different value, but 0.5 is a very reasonable average value. This is the prediction probability from the first base model. Then, to build the second tree, we use the negative residual. The negative residual is `p - y` (prediction minus label). Wait, actually, typically it is `y - p`, but let's look at the negative residual values. For the first sample (Class 1), it gives -0.5. For another, -0.5. For the third one in the other class (Class 0), the negative residual is 0.5. Then we can build the second tree using these values. This is similar to the regression tree, just playing with these small numbers.

    在一开始，我们选择 0.5，因为看这里，每个类别有两个样本。平均值是 0.5。这是合理的。如果类别 1 中有大量样本，我们可能会选择不同的值，但 0.5 是一个非常合理的平均值。这是第一个基础模型的预测概率。然后，为了构建第二棵树，我们使用负残差。负残差是 `p - y`（预测值减去标签）。对于第一个样本（类别 1），它给出 -0.5。对于另一个，也是 -0.5。对于另一类（类别 0）的第三个样本，负残差是 0.5。然后我们可以使用这些值来构建第二棵树。这与回归树类似，只是在处理这些小数字。

  - [20:58 - 21:01] **Calculating Node Scores for Classification (Gradient and Hessian)**

    Now, how do we calculate the score for this classification problem? The formula is slightly different. The top part (numerator) is the sum of residuals squared (`G^2`), which is the same as regression. But the bottom is very different. The bottom uses `H`. `H` is `p * (1 - p)`. Since our initial `p` is 0.5, `p * (1 - p)` is `0.5 * 0.5 = 0.25`. Since the first model is very simple and just outputs 0.5 for everyone, `H` is 0.25 for all terms. Thank you for pointing that out. So `G^2` is on top, and the bottom involves the sum of `H` plus Lambda. Lambda is set to 1 here.

    现在，我们要如何计算这个分类问题的分数呢？公式稍微有些不同。顶部部分（分子）是残差之和的平方（`G^2`），这与回归相同。但底部非常不同。底部使用的是 `H`。`H` 是 `p * (1 - p)`。因为我们要初始 `p` 是 0.5，`p * (1 - p)` 就是 `0.5 * 0.5 = 0.25`。因为第一个模型非常简单，对所有人只输出 0.5，所以所有项的 `H` 都是 0.25。谢谢你的指出。所以 `G^2` 在上面，底部涉及 `H` 之和加上 Lambda。这里 Lambda 设为 1。

  - [21:01 - 21:03] **Classification Split Example**

    Then we consider a split. There are 3 possible splits for this case. This samples are `X` smaller than 5. So 5 is here. Therefore, the first two samples go to one side, and the last two samples go to the other. This split is very natural because the top ones have small values for feature `X`, and the bottom ones have larger values. So we just cut at the bottom part. Then we calculate the score in the same way, just on a small scale. We calculate `G_A` (sum of residuals for A). For the first group, residuals are -0.5 and -0.5, so the sum is -1. `H_A` is sum of `p(1-p)`, so it is 0.25 + 0.25 = 0.5.

    然后我们考虑分裂。这种情况下有 3 种可能的分裂。这些样本的 `X` 小于 5。所以 5 在这里。因此，前两个样本去一边，后两个样本去另一边。这个分裂很自然，因为上面的样本特征 `X` 值较小，下面的值较大。所以我们就从下面切开。然后我们以同样的方式计算分数，只是规模变小了。我们计算 `G_A`（A 组的残差之和）。对于第一组，残差是 -0.5 和 -0.5，所以总和是 -1。`H_A` 是 `p(1-p)` 的总和，所以它是 0.25 + 0.25 = 0.5。

  - [21:03 - 21:06] **Score and Gain Calculation for Classification**

    So the score for part `A` is `(-1)^2` divided by `(0.5 + 1)`. The `+1` is Lambda. This gives `1 / 1.5` or `2/3`. For part `B`, the residuals are 0.5 and 0.5, sum is 1. `H` sum is also 0.5. So the score is also `1^2 / (0.5 + 1) = 2/3`. The Gain is the score of `A` plus score of `B` minus the score of the parent, divided by 2 (Note: the factor 1/2 appears in the Gain formula in the slide), minus Gamma (0). Since the result is positive, we perform some split. We consider all possible features and cutoffs, and find the best one.

    所以 `A` 部分的分数是 `(-1)^2` 除以 `(0.5 + 1)`。`+1` 是 Lambda。这得出 `1 / 1.5` 或 `2/3`。对于 `B` 部分，残差是 0.5 和 0.5，和是 1。`H` 的和也是 0.5。所以分数也是 `1^2 / (0.5 + 1) = 2/3`。增益是 `A` 的分数加上 `B` 的分数减去父节点的分数，除以 2（注：幻灯片中的增益公式里有 1/2 的系数），再减去 Gamma（0）。因为结果是正的，我们执行某种分裂。我们考虑所有可能的特征和截断值，并找到最好的一个。

  - [21:06 - 21:08] **Converting Leaf Weight to Probability (Sigmoid)**

    In this case, there are no further splits. So what is the output value? It means probability. How do we calculate the probability? We first calculate the leaf weight value `w` using the formula `w = -Sum(G) / (Sum(H) + Lambda)`. For the left leaf, this gives `2/3`. Then we compute the probability using the Sigmoid formula: `p = 1 / (1 + e^(-w))`. This gives `0.661`. Why do we use this complicated formula to get the probability? Because the theory works on the "log-odds" value. The optimization isn't directly on the probability `p`, but on the log-odds.

    在这种情况下，没有进一步的分裂。那么输出值是什么呢？它意味着概率。我们如何计算概率？我们首先使用公式 `w = -Sum(G) / (Sum(H) + Lambda)` 计算叶子权重值 `w`。对于左叶子，这得出 `2/3`。然后我们使用 Sigmoid 公式计算概率：`p = 1 / (1 + e^(-w))`。这得出 `0.661`。为什么我们要用这个复杂的公式来得到概率？因为理论是基于“对数几率”（log-odds）值的。优化不是直接针对概率 `p` 进行的，而是针对对数几率。

  - [21:08 - 21:12] **Log-Odds and Deep Learning Connection**

    If you take the log of `p / (1-p)`, that is the log-odds. The whole optimization theory works on this number, not directly on the probability. That's why we need to convert this log-odds value back to this probability. This is also a popular transformation function in deep learning and neural networks. We don't really work on the probability directly; we work on the log-odds because it is easier to work on mathematically. So if `log(p/(1-p)) = x`, then `p = 1 / (1 + e^(-x))`. This is the formula we use here. It converts the leaf score to probability.

    如果你取 `p / (1-p)` 的对数，那就是对数几率。整个优化理论是基于这个数字工作的，而不是直接基于概率。这就是为什么我们需要将这个对数几率值转换回概率。这也是深度学习和神经网络中一种流行的变换函数。我们并不真的直接处理概率；我们处理对数几率，因为这在数学上更容易处理。所以如果 `log(p/(1-p)) = x`，那么 `p = 1 / (1 + e^(-x))`。这就是我们在这里使用的公式。它将叶子分数转换为概率。

  - [21:12 - 21:16] **Conclusion and Advantages of XGBoost**

    So, building the tree for classification is the same as regression; you calculate gradients (residuals). The only difference is the leaf score formula involving `H` and the final conversion to probability using Sigmoid. In summary, XGBoost works well and can be used for very large datasets. It is based on good theory, has a regularization mechanism, and efficient split finding. It is also parallelizable, so you can work on large data using multiple processors. Although it has been around for 10 years, it is still a very good method for tabular data and competitive modeling. I think that's enough for today. Thank you.

    所以，构建分类树与回归树是一样的；你计算梯度（残差）。唯一的区别是涉及 `H` 的叶子分数公式以及最后使用 Sigmoid 转换为概率。总之，XGBoost 效果很好，可以用于非常大的数据集。它基于良好的理论，有正则化机制，以及高效的分裂查找。它也是可并行的，所以你可以使用多处理器处理大数据。虽然它已经出现了 10 年，但对于表格数据和竞技建模来说，它仍然是一个非常好的方法。我想今天就讲到这里。谢谢。
***

## Syllabus
  - **Course Description** This course offers an accessible introduction to machine learning, tailored for individuals from diverse backgrounds. As part of the 'AI and Innovation' program, it equips students with the fundamental knowledge and skills needed to effectively deploy AI-based solutions for tasks in industry, business, healthcare and daily life.  Emphasizing foundational concepts and intuitive understanding, the course ensures that even those with limited quantitative skills can grasp the core ideas. Through hands-on exercises and real-world case studies, students will become adept at utilizing machine learning tools, empowering them to contribute effectively to AI-driven initiatives in their respective fields.
  - **Workload hours**
    | Activity                         | Total Duration | Remarks |
    | -------------------------------- | -------------- | ------- |
    | Lecture                          | 3              |         |
    | Tutorial                         | 0              |         |
    | Laboratory                       | 0              |         |
    | Project/Assignment               | 3              |         |
    | Workshop/Seminar/Fieldwork       | 0              |         |
    | Others, include preparatory work | 4              |         |
  - **Learning outcomes** By the end of this course, participants will be able to:
    - Understand the foundational concepts of AI and machine learning and their various applications.
    - Utilize AI and machine learning tools and frameworks to analyze and interpret data.
    - Design and implement basic AI and machine learning models.
    - Critically evaluate the performance of AI and machine learning models.
    - Apply AI and machine learning techniques to innovative projects within their respective fields.
  - **Week 1** Introduction to AI and Machine Learning
    - Overview of AI: History and Impact
    - Basic Concepts and Definitions in Machine Learning
    - Real-World Applications of AI and Machine Learning
  - **Week 2** Understanding and Preparing Data for AI
    - Types of Data and Their Uses in AI
    - Ensuring Data Quality for AI Applications
  - **Week 3** Basic Techniques in Supervised Learning for AI
    - Understanding Regression and Classification in AI
    - Practical Examples with User-Friendly AI Tools
    - Hands-On Exercises: Building Simple AI Models
  - **Week 4** Exploring Unsupervised Learning in AI
    - Introduction to Clustering and Grouping Data in AI
    - Practical Applications of Unsupervised Learning in AI
  - **Week 5** Evaluating and Validating AI Models
    - Easy-to-Understand Performance Metrics for AI
    - Methods to Test and Improve AI Model Accuracy
    - Understanding Bias and Fairness in AI
  - **Assessment components**
    | CA Component                                 | % Weightage | Remarks |
    | -------------------------------------------- | ----------- | ------- |
    | Class Participation                          | 0%          |         |
    | Essays                                       | 0%          |         |
    | Project/Group Project                        | 30%         |         |
    | Quizzes/Tests                                | 30%         |         |
    | Laboratory Tests                             | 0%          |         |
    | Mid-term Test                                | 0%          |         |
    | Others 1 (if applicable & describe in notes) | 0%          |         |
    | Others 2 (if applicable & describe in notes) | 0%          |         |
    | Others 3 (if applicable & describe in notes) | 0%          |         |
    | Final Exam                                   | 40%         |         |
***
