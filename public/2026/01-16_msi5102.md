# ___MSI5102 Essentials of Machine Learning___
***

# Tips
## Week 3 Summary
  - **Linear Models and the Evolution of Loss Functions**

    In Lecture 2, we established the foundation of using linear models for binary classification. A critical insight was understanding why the standard **Square Loss** is flawed for classification tasks: it penalizes predictions that are "too correct" (i.e., far from the decision boundary on the correct side). To address this, we introduced **Logistic Loss** (used in Logistic Regression) which dampens this penalty, and **Hinge Loss** (used in SVM) which eliminates the penalty entirely for correct confident predictions. We also discussed **Regularization** as the primary tool to constrain model complexity and prevent overfitting.

    在第 2 讲中，我们奠定了使用线性模型进行二分类的基础。一个关键的见解是理解为什么标准的**平方损失**在分类任务中存在缺陷：它会惩罚那些“过于正确”的预测（即在正确一侧远离决策边界的预测）。为了解决这个问题，我们介绍了**Logistic 损失**（用于逻辑回归），它减轻了这种惩罚；以及 **Hinge 损失**（用于 SVM），它完全消除了对正确且确信预测的惩罚。我们还讨论了**正则化**作为限制模型复杂度和防止过拟合的主要工具。

  - **Evaluation Metrics for Imbalanced Data**

    We moved beyond simple "Accuracy," which can be misleading when data is imbalanced (e.g., predicting rare events like earthquakes). We introduced a suite of more robust metrics: **Sensitivity (Recall)**, **Specificity**, and **Precision**. We also learned to visualize model performance using the **AUROC** (Area Under the Receiver Operating Characteristic) curve. The goal of a good classifier is to balance sensitivity and specificity effectively.

    我们超越了简单的“准确率”，因为在数据不平衡时（例如预测地震等罕见事件），准确率可能会产生误导。我们介绍了一套更稳健的指标：**灵敏度（召回率）**、**特异性**和**精确率**。我们还学习了使用 **AUROC**（受试者工作特征曲线下面积）来可视化模型性能。一个好的分类器的目标是有效地平衡灵敏度和特异性。

  - **Probabilistic Predictors and Entropy**

    In Lecture 3, we shifted from **Point Predictors** (which give a single answer) to **Probabilistic Predictors** (which output a distribution of chances, e.g., 80% Rain, 20% Sun). This approach is more robust for ambiguous data. We learned to evaluate these predictors using **Likelihood** and **Negative Log-Likelihood** (to handle small number multiplication). Finally, we introduced **Entropy** and **Cross-Entropy** from Information Theory as ways to measure the distance between the predicted probability distribution and the true distribution.

    在第 3 讲中，我们从**点预测器**（只给出一个答案）转向了**概率预测器**（输出概率分布，例如 80% 雨，20% 晴）。这种方法对于模棱两可的数据更为稳健。我们学习了使用**似然**和**负对数似然**（为了处理小数相乘的问题）来评估这些预测器。最后，我们介绍了信息论中的**熵**和**交叉熵**，以此来衡量预测概率分布与真实分布之间的距离。
## Week 3 Tips
  - **Focus on Concepts, Not Calculation**

    The professor explicitly stated regarding the quiz: "No calculations." Do not waste time memorizing complex arithmetic procedures for updating weights or manually calculating gradients. Instead, focus on the **qualitative properties**:

    - Why do we prefer Hinge Loss over Square Loss for classification?
    - What does it mean if Entropy is high vs. low?
    - What is the relationship between overfitting and model complexity?
    - Why do we take the "Negative Log" of likelihood? (To turn a tiny product into a manageable sum).

    教授关于测验明确指出：“没有计算。”不要浪费时间背诵更新权重的复杂算术过程或手动计算梯度。相反，应专注于**定性属性**：

    - 为什么在分类中我们更喜欢 Hinge 损失而不是平方损失？
    - 熵高与熵低意味着什么？
    - 过拟合与模型复杂度之间有什么关系？
    - 为什么我们要取似然的“负对数”？（为了将极小的乘积转化为可处理的和）。

  - **Prepare for the "Australian Rain" Project**

    This optional project is a perfect sandbox to test your understanding of the full pipeline discussed in Lecture 2. Pay attention to the data preprocessing steps mentioned: removing missing values (NA) and dropping irrelevant columns like "Sunshine" or "WindGustDir." This project forces you to apply the theoretical **Regularized Logistic Regression** to a real-world messy dataset.

    这个可选项目是测试你对第 2 讲中讨论的完整流程理解的完美沙盒。注意提到的数据预处理步骤：删除缺失值（NA）和丢弃如“日照”或“阵风风向”等无关列。这个项目迫使你将理论上的**正则化逻辑回归**应用于现实世界杂乱的数据集。

  - **Clarify the "Point" vs. "Probabilistic" Distinction**

    Ensure you understand that a Point Predictor is just a special case of a Probabilistic Predictor (where one probability is 1 and the rest are 0). This is a favorite conceptual question. Understand that KNN (K-Nearest Neighbors) can be easily adapted into a probabilistic predictor by simply looking at the ratio of labels among the neighbors (e.g., 3 red, 3 blue = 0.5 probability).

    确保你理解点预测器只是概率预测器的一个特例（其中一个概率为 1，其余为 0）。这是一个常见的概念性问题。要理解 KNN（K-最近邻）可以通过简单地查看邻居中标签的比例（例如，3 红，3 蓝 = 0.5 概率）轻松适应为概率预测器。
## Week 4 Summary
  - **1. Decision Trees: The Foundation 决策树：基础**
    - **Concept:** A decision tree is a tree-structured model that mimics human reasoning using a set of "if-then-else" rules. It splits data into branches to classify samples, making it highly interpretable and suitable for disjoint classes.

      **概念：** 决策树是一种模仿人类推理的树状模型，使用一组“如果-那么-否则”规则 。它将数据分裂成分支以对样本进行分类，这使得它具有高度的可解释性，非常适合处理互斥的类别 。
    - **Construction (Entropy & Gain):** To build the tree, we use **Entropy** to measure the disorder or complexity of the data. We select the split feature that provides the maximum **Information Gain**, which corresponds to the greatest reduction in entropy.

      **构建（熵与增益）：** 为了构建树，我们使用**熵**来衡量数据的无序度或复杂性 。我们选择能够提供最大**信息增益**的分裂特征，这意味着熵的减少量最大 。
    - **Example:** In the "Tennis Play" example, attributes like "Outlook" (Sunny, Overcast, Rain) are used to split the dataset until the leaf nodes are pure (Entropy = 0).

      **示例：** 在“打网球”的例子中，我们使用像“天气状况”（晴朗、阴天、雨天）这样的属性来分裂数据集，直到叶节点变纯（熵=0） 。
  - **2. Regression Trees: Predicting Numbers 回归树：预测数值**
    - **Difference:** Unlike classification trees which output classes, regression trees output a numeric value, which is typically the **average** of the target values for the samples in that leaf node.

      **区别：** 与输出类别的分类树不同，回归树输出一个数值，通常是该叶节点中样本目标值的**平均值** 。
    - **Splitting Metric (Squared Loss):** Instead of entropy, we use **Squared Loss** (Sum of Squared Residuals). We search for a split point (e.g., Dosage < 14.5) that minimizes the sum of squared differences between the actual values and the average value in each resulting subset.

      **分裂度量（平方损失）：** 我们不使用熵，而是使用**平方损失**（残差平方和）。我们寻找一个分裂点（例如：剂量 < 14.5），使得每个结果子集中的实际值与平均值之间的平方差之和最小 。
    - **Dosage Example:** The lecture illustrated this with drug effectiveness data. The tree partitioned the dosage range into intervals (e.g., Low, Medium, High dosage), fitting a constant value (step function) for each interval to capture non-linear relationships.

      **剂量示例：** 讲座用药物疗效数据阐述了这一点。树将剂量范围划分为若干区间（例如：低、中、高剂量），并为每个区间拟合一个常数值（阶梯函数），以捕捉非线性关系 。

  - **3. Random Forest: Bagging & Randomness 随机森林：Bagging与随机性**
    - **Bagging (Bootstrap Aggregating):** Random Forest improves upon single trees by creating an ensemble. It generates multiple datasets by **sampling with replacement** (Bagging) and trains a tree on each.

      **Bagging（自助汇聚法）：** 随机森林通过创建一个集成模型来改进单棵树。它通过**有放回抽样**（Bagging）生成多个数据集，并在每个数据集上训练一棵树 。
    - **Feature Randomness:** To prevent trees from being too similar (correlated), Random Forest selects a **random subset of features** (size $m$) at each split instead of checking all features. This diversity makes the model robust against overfitting.

      **特征随机性：** 为了防止树过于相似（相关），随机森林在每次分裂时选择一个**随机特征子集**（大小为 $m$），而不是检查所有特征 。这种多样性使得模型对过拟合具有鲁棒性 。
    - **Validation:** It uses **Out-of-Bag (OOB)** samples—data points left out during bootstrapping—to estimate accuracy without needing a separate validation set.

      **验证：** 它使用**袋外（OOB）**样本——即自助抽样过程中未被选中的数据点——来估计准确性，而无需单独的验证集 。

  - **4. XGBoost: Sequential Correction XGBoost：顺序修正**
    - **Boosting Concept:** Unlike Random Forest (parallel), XGBoost builds trees **sequentially**. Each new tree is trained to predict the **residuals (errors)** of the previous ensemble, not the original target values.

      **Boosting概念：** 与随机森林（并行）不同，XGBoost是**顺序**构建树的。每一棵新树都被训练来预测前一个集成模型的**残差（误差）**，而不是原始目标值 。
    - **Regularization & Learning Rate:** It adds new trees with a small **Learning Rate** (e.g., 0.1) to ensure gradual improvement and prevent overfitting. It also includes regularization terms in its objective function.

      **正则化与学习率：** 它以较小的**学习率**（例如 0.1）加入新树，以确保逐步改进并防止过拟合 。它还在目标函数中包含了正则化项 。
    - **Status:** It is a scalable, state-of-the-art system widely used in competitions (Kaggle) and industry for structured data.

      **地位：** 它是一个可扩展的、最先进的系统，广泛应用于竞赛（Kaggle）和工业界的结构化数据处理 。
## Week 4 Tips
  - **1. Distinguish the "Split Criteria" (分类标准 vs. 回归标准)**
    - **Concept:** Remember that Classification Trees use **Entropy/Gini** (measuring purity), while Regression Trees use **Squared Error** (measuring variance/distance).
    - **Action:** When reviewing the "Drug Dosage" example, manually calculate the squared error for one split (e.g., split at dosage=3) to feel how the "average" minimizes the error on each side.
    - **概念：** 记住分类树使用**熵/基尼系数**（衡量纯度），而回归树使用**平方误差**（衡量方差/距离）。
    - **行动：** 在复习“药物剂量”示例  时，手动计算一个分裂点（例如在剂量=3处分裂）的平方误差，去感受“平均值”是如何最小化两侧误差的。

  - **2. Understand the "Evolution" of Tree Models (模型演进)**
    - **Concept:** Visualize the progression:
      - **Single Tree:** Easy to interpret, but overfits (High Variance).
      - **Random Forest:** Many independent trees averaged together (Lowers Variance).
      - **XGBoost:** Many trees correcting each other's mistakes sequentially (Lowers Bias & Variance).
    - **Action:** Draw a flowchart comparing "Bagging" (Parallel) vs. "Boosting" (Sequential) to solidify the architectural difference.
    - **概念：**通过可视化来理解演进过程：
      - **单棵树：** 易于解释，但容易过拟合（高方差）。
      - **随机森林：** 许多独立树的平均（降低方差）。
      - **XGBoost：** 许多树顺序修正彼此的错误（降低偏差和方差）。
    - **行动：** 画一个流程图对比“Bagging”（并行）与“Boosting”（串行），以巩固架构上的差异。

  - **3. Master the "Residual" Logic in XGBoost (掌握XGBoost的残差逻辑)**
    - **Concept:** The most critical takeaway for XGBoost is that Tree 2 predicts `(Actual - Tree 1 Prediction)`, not `Actual`. This is why we add the predictions together.
    - **Action:** Re-watch the part where we calculated `88 - 71.2 = 16.8`. Try to create a dummy dataset of 3 points and simulate 2 rounds of XGBoost updates on paper.
    - **概念：** XGBoost最关键的知识点是：树2预测的是 `(真实值 - 树1的预测值)`，而不是 `真实值`。这就是为什么我们将预测值相加。
    - **行动：** 重看我们要计算 `88 - 71.2 = 16.8` 的部分 。尝试在纸上创建一个包含3个点的虚拟数据集，并模拟2轮XGBoost的更新过程。

  - **4. Memorize Key Hyperparameters (熟记关键超参数)**
    - **Concept:** For practical application (like your Kaggle projects), you need to know what knobs to turn.
      - `n_estimators`: Too few = underfitting; Too many = diminishing returns.
      - `max_features`: The knob for controlling correlation between trees.
    - **Action:** Review the table on slide 39  regarding default `max_features` (`√p` for classification vs `p/3` for regression, where `p` represents the total number of features).
    - **概念：** 对于实际应用（比如你的Kaggle项目），你需要知道调节哪些旋钮。
      - `n_estimators`（树的数量）：太少=欠拟合；太多=收益递减 。
      - `max_features`（最大特征数）：控制树之间相关性的旋钮 。
    - **行动：** 复习幻灯片39页上的表格 ，关注默认的 `max_features` （分类问题用 √p，回归问题用 p/3，p 代表特征数量）。
***

# Lectures
## Week 1
  * [19:05–19:09] **Class format and collaboration setup**

    The class will not be run like a traditional one-way lecture. Instead, the format is designed to be more flexible and interactive, so that everyone can follow more easily.

    In the second half of the class, we will focus more on discussion and deeper understanding. You will also work with shared code bases, so that multiple people can work on the same code at the same time. This is mainly for convenience and collaboration.

    Before moving on, are there any questions about the course plan or the way the class will run?

    这门课不会采用传统的“老师一直讲、学生一直听”的方式，而是一个相对灵活、互动性更强的课堂形式，方便大家理解和参与。

    在课堂后半部分，我们会更多进行讨论和深入讲解。另外，你们会使用共享的代码仓库，允许多人同时在同一套代码上协作，这主要是为了提高效率和方便学习。

    在正式开始之前，大家对课程安排或上课方式有问题吗？

  * [19:09–19:18] **Why machine learning? What is AI?**

    Let us now start with the main topic: machine learning.

    People often ask why machine learning is important. One reason is that AI has become a very popular term in industry and society. Artificial intelligence is about making intelligent machines.

    Intelligence includes many abilities: driving a car, recognizing images and faces, understanding language, accumulating knowledge, reasoning, doing mathematics, and playing games like chess and Go.

    For humans, some tasks like image recognition are easy, while others like complex calculations may be harder. For machines, the difficulty is often reversed. Tasks that are easy for humans can be very difficult for machines.

    现在我们正式进入课程的核心内容：机器学习。

    很多人会问，为什么要学习机器学习？一个很现实的原因是，AI 已经成为工业界和社会中的热门话题。人工智能的目标，是让机器具备“智能”。

    智能包含很多能力，比如自动驾驶、图像和人脸识别、语言理解、知识积累、推理、数学计算，以及下棋、下围棋等。

    对人来说，图像识别这类任务很容易，而复杂计算可能较难；但对机器而言，情况往往正好相反。

  * [19:18–19:24] **General intelligence vs machine intelligence**

    By general intelligence, we mean the ability to learn, solve problems, and adapt solutions to very different environments. This kind of intelligence is still very hard for machines.

    However, AI technology is advancing at an unprecedented speed. With deep learning, machines can now translate languages, recognize faces, and even drive cars reasonably well.

    These successes create many opportunities for AI-based industries, including startups and applied research.

    所谓“通用智能”，指的是能够学习、解决问题，并把解决方案迁移到不同环境中的能力。这种能力对机器来说仍然非常困难。

    但与此同时，AI 技术正以前所未有的速度发展。借助深度学习，机器已经可以很好地完成语言翻译、人脸识别，甚至在一定条件下实现自动驾驶。

    这些突破也带来了大量 AI 相关产业和创业机会。

  * [19:24–19:30] **AI, education, and social impact**

    On the other hand, AI also brings serious challenges. In some areas, machines may outperform humans, which creates pressure on traditional professions and education systems.

    Today, much learning material is freely available online. Students can often learn knowledge without relying solely on teachers. This changes the role of education.

    For example, in the past, traveling required human assistance for booking hotels and planning routes. Today, online platforms and navigation systems make this easy.

    但另一方面，AI 也带来了很大的挑战。在某些领域，机器可能比人做得更好，这对传统职业和教育体系产生了冲击。

    现在，学习资料大量存在于网络上，学生不再只能依赖老师获取知识，教育的角色正在发生变化。

    过去出行时，需要向人询问路线、预订酒店；而现在，搜索引擎和导航系统几乎可以解决所有问题。

  * [19:30–19:38] **AI vs Machine Learning**

    AI is a very broad field and includes many different approaches. Machine learning is one of the most important and dominant approaches.

    Machine learning solves AI problems through mathematical modeling. Instead of explicitly programming rules, we let machines learn patterns from data.

    人工智能是一个非常宽广的领域，包含多种方法。机器学习是其中最重要、也最主流的一种。

    机器学习的核心思想是：用数学模型来解决 AI 问题，而不是人为写出所有规则，让机器从数据中学习规律。

  * [19:38–19:45] **Basic machine learning tasks**

    There are two main types of machine learning tasks.

    The first is supervised learning, such as classification and regression. In classification, we learn rules to assign objects to different categories. This idea is widely used, for example, in biology when classifying species.

    The second is unsupervised learning, where we do not have labels and instead try to discover structure or patterns in data.

    机器学习的基本任务主要分为两类。

    第一类是监督学习，包括分类和回归。分类的目标是把对象分到不同类别中，比如生物学中的物种分类。

    第二类是无监督学习，在这种情况下没有标签，模型需要从数据中自行发现结构或规律。

  * [19:45–19:52] **Traditional ML vs Deep Learning**

    In traditional machine learning, we manually design features and then learn a prediction function.

    In deep learning, feature extraction is done automatically by multiple layers of neural networks. Each layer extracts higher-level features from the previous one.

    This idea is inspired by how the human brain works, where simple neurons combine to form powerful networks.

    在传统机器学习中，特征通常由人手工设计，然后模型学习预测函数。

    在深度学习中，特征由神经网络自动提取，多层结构逐层学习更高级的特征。

    这种结构受到人脑工作方式的启发：大量简单神经元组合在一起，形成强大的计算能力。

  * [19:52–20:05] **Feature engineering and data representation**

    To apply machine learning, we must convert real-world data into numerical vectors. This process is called feature engineering.

    Data can be numerical, categorical, text, images, or audio. Each type needs to be mapped into real-valued vectors so that mathematical models can be applied.

    For example, categorical data without order uses one-hot encoding, while ordered data can be mapped to integers.

    要使用机器学习，必须把现实世界中的数据转换成数值向量，这个过程叫做特征工程。

    数据可以是数值、类别、文本、图像或音频，不同类型需要不同的映射方式，最终都转成实数向量，才能进行数学计算。

    比如无序类别数据用 one-hot 编码，有序数据可以直接映射成整数。

  * [20:05–20:15] **Standardization and log transformation**

    Different features may have very different scales. Standardization ensures each feature has mean zero and unit variance.

    For features with very large ranges, such as house prices, we often apply a logarithmic transformation before standardization. This helps the model focus on relative differences rather than absolute values.

    不同特征的取值范围可能差异很大，因此需要做标准化，使其均值为 0、方差为 1。

    对于跨度特别大的特征（如房价），通常先进行对数变换，再标准化，这样模型更关注相对差异而不是绝对大小。

  * [20:15–20:21] **Wrap-up and next steps**

    Feature engineering helps improve model performance, but it must be validated experimentally.

    Remaining challenges include choosing evaluation metrics, selecting and tuning models, and avoiding overfitting or underfitting. These topics will be discussed later in the course.

    If you have difficulties forming groups or following the material, please email me. Each group should have four to six students.

    特征工程是否有效，需要通过实验验证。

    后续我们还将讨论模型评估指标、模型选择与调参，以及如何避免过拟合和欠拟合。

    如果在分组或学习过程中遇到困难，可以给我发邮件。每组人数为 4 到 6 人。

  * [20:22–20:27] **Project marking and reproducibility**
    **English**
    OK. The project score is evaluated like this—this is how the project will be marked.
    The project is based on some published work. Nowadays, in machine learning, authors usually upload their data and code to a public website, so it is easy for others to verify the results and run the code.
    With AI assistant tools, running code and setting up experiments has become much easier.
    In most cases, the first step is simply: run their code and see whether you can reproduce their reported results.
    But that is not the whole story. Often you also need extra validation—for example, compare methods properly, or test on the same test data in the correct format. If the test data is prepared correctly, building a baseline model can be very easy. I will show you some simple code in the second half—sometimes it’s just one line.
    So that is the second part of the project. There is also a presentation component—basically you present what you have done.
    I will upload a list of candidate works around Week 6. Most of them are deep learning papers—fairly standard.
    Since many of you are part-time students, you may have time constraints. Any questions about the project? It should be clearer now.

    **Chinese**
    好，项目的分数大概是这样评的——这就是项目评分方式。
    这个项目是基于一篇已经发表的工作来做。现在机器学习领域通常会把数据和代码上传到公开网站，方便别人验证结果、复现代码。
    另外，现在有各种 AI 辅助工具，跑代码、搭环境都变得更容易了。
    所以大多数情况下，第一步就是：把对方的代码跑起来，看你能不能复现出论文里报告的结果。
    但这还不够，很多时候还需要做额外验证，比如更规范地做对比实验，或者保证大家都用同一份测试数据、同一种正确格式。只要测试数据准备对，搭一个基线模型其实很简单。我会在后半部分给你们看一些例子代码，有时候真的就一行。
    这就是项目的第二块。还有展示部分——基本就是把你做了什么讲清楚。
    我会在第 6 周左右放出一批可选的论文题目，大部分是深度学习相关的，比较标准。
    考虑到你们很多是兼职学生，时间上可能会有压力。关于项目还有问题吗？现在应该更清楚了。

  * [20:27–20:30] **Scheduling, quiz logistics, and AI-tool restriction**
    **English**
    For scheduling: originally I planned to place something at a certain time. If you have issues, come during the break and tell me.
    It turns out Week 5 works for most of you, but there may still be problems. Does anyone have a conflict?
    Also, Chinese New Year is in Week 6, right?
    One more thing: you cannot use AI tools during the quiz. (For example, do not use AI tools to “make the PDF” / generate answers.)
    OK, no problem. The first quiz is one hour. It is about 15 questions, multiple-choice only—no fill-in-the-blank.

    **Chinese**
    关于时间安排：我一开始是想把某些安排放在某个时间点。如果你有问题，休息时间过来跟我说。
    结果是第 5 周对大多数人来说比较合适，但可能仍然有人有冲突。有谁有时间问题吗？
    另外，第 6 周是春节对吧？
    还有一点：测验时不能使用 AI 工具。（比如用 AI 来生成答案、做成 PDF 之类的都不可以。）
    好的。第一次测验一小时，大概 15 题，全是选择题，没有填空。

  * [20:40–20:41] **Why calculus: learning as optimization**
    **English**
    Machine learning is essentially optimization: you find a model by minimizing a loss function. You may already know this idea, but many people do not really understand what the “loss” is.
    Calculus provides tools to find parameter values that minimize the loss function. This is used for regression models and for training neural networks.

    **Chinese**
    机器学习本质上就是优化：通过最小化一个损失函数来找到模型。你可能听过这个说法，但很多人其实并不真正理解“损失”到底是什么。
    微积分提供的工具可以帮我们找到使损失函数最小的参数值。这在回归模型和神经网络训练中都会用到。

  * [20:41–20:44] **Types of functions and why minima/maxima are hard**
    **English**
    We start with easy functions. Linear functions are easy to understand: the values form a line; in higher dimensions, they form a plane or hyperplane.
    But beyond that, it becomes harder. For example, quadratic functions are curves, and it is not always obvious where the minimum or maximum is, or whether a maximum exists.
    Exponential-type functions can be even harder to visualize—sometimes you see a bell-curve-like shape, but in higher dimensions.

    **Chinese**
    我们先从简单函数开始。线性函数比较好理解：在二维里像一条直线；维度更高时就是平面或超平面。
    但再往后就难了。比如二次函数是曲线，最小点、最大点在哪里并不总是直观，甚至有时你还要判断到底有没有最大值。
    指数类函数更难想象。有时你看到像“钟形曲线”那种形状，但在高维里就更复杂。

  * [20:45–20:47] **Piecewise linear functions (decision trees intuition)**
    **English**
    If your data has a pattern like this, there is no way a single linear function can fit it. What you can do is split the input space into different regions and use different linear functions in different regions—combine them.
    This is what we call a piecewise linear function.
    For example, the absolute value function makes everything nonnegative: when (x \ge 0), it is (x); when (x < 0), it is (-x).
    Another example is a three-piece linear function: one line for (x < -2), another for (-2 \le x \le 0), and another for (x > 0).
    Decision tree models try to learn functions of this “piecewise” type.

    **Chinese**
    如果数据分布是那种“弯来弯去”的形状，用一个线性函数是不可能拟合好的。你能做的是把输入空间分成不同区域，在每个区域用不同的线性表达式，再把它们拼起来。
    这就叫分段线性函数（piecewise linear）。
    比如绝对值函数会把数变成非负：(x \ge 0) 时就是 (x)，(x < 0) 时就是 (-x)。
    还有一种三段式的线性函数：(x < -2) 用一条线，(-2 \le x \le 0) 用一条线，(x > 0) 再用一条线。
    决策树这类模型，本质上就是在学习这种“分段”的函数结构。

  * [20:48–20:52] **Composite functions and deep learning intuition**
    **English**
    Another important idea is that we can form complicated functions by composing simpler functions. A composite function means you apply one function to the output of another.
    In machine learning, you can think of mapping a sample to another feature space, then mapping again to the label space—multiple levels—so naturally we get composite functions.
    Example: let (y = g(x) = x^3), and (z = f(y) = \sin(y)). Then (z = \sin(x^3)) is a composite function (apply (g) first, then (f)).
    Deep learning is built from stacking many simple nonlinear functions; together they can represent very complicated functions.

    **Chinese**
    另一个重要概念是“复合函数”：把一个函数作用在另一个函数的输出上，用简单函数组合出复杂函数。
    在机器学习里，你可以理解为：先把样本映射到某个特征空间，再映射到标签空间，一层一层做，所以天然会出现复合函数。
    例如 (y=g(x)=x^3)，再令 (z=f(y)=\sin(y))，那么 (z=\sin(x^3)) 就是复合函数（先算 (x^3)，再取正弦）。
    深度学习就是把很多简单的非线性函数叠起来，最终得到非常复杂的表达能力。

  * [20:52–20:58] **Derivatives: locating minima/maxima; multi-variable case**
    **English**
    For a continuous function, we talk about minima and maxima. Derivatives tell us the rate of increase or decrease.
    At a minimum point, the derivative is zero. If the derivative at a point is positive, you know you are on an increasing side; if it is negative, you are on a decreasing side—this helps you move toward a minimum.
    In practice, we may not have a closed-form expression for the minimizer, but we can find it numerically by checking derivatives and moving step by step.
    For multivariable functions, you need derivatives in every direction: partial derivatives with respect to (x_1, x_2, \dots, x_m). At a local optimum, the gradient vector is the zero vector.
    The derivative (or gradient) also serves as an indicator: if it is small, you may be close to a minimum.

    **Chinese**
    对连续函数来说，我们讨论最小值和最大值。导数反映函数上升或下降的速度。
    在最小点处，导数等于 0。如果某点导数为正，说明函数在那边是上升的；如果为负，说明在下降——这能指导你往更可能接近最小值的方向移动。
    实际上很多函数没有解析解（没法写出一个明确公式直接给你最小点），但可以用数值方法：不断看导数，逐步迭代逼近最小点。
    多元函数要考虑各个方向的变化，所以需要对 (x_1,x_2,\dots,x_m) 分别求偏导。在局部最优点，梯度向量为零向量。
    导数/梯度也可以当“指示器”：越小通常表示越接近最小点。

  * [20:58–21:01] **Two derivative rules: sum rule and product rule**
    **English**
    To compute derivatives, we mainly use a few rules. First, the derivative of a sum is the sum of derivatives.
    The product rule is slightly more complicated: derivative of (f(x)g(x)) is (f'(x)g(x) + f(x)g'(x)).
    If you do not know this rule, it is hard to differentiate many expressions. But once you know the rule, you apply it systematically.
    For example, if a function is written as a product like (x^2 \cdot (\text{something in } x)), treat it as the first function times the second function and apply the rule.

    **Chinese**
    计算导数主要依靠一些基本法则。第一条：和的导数等于导数的和。
    乘积法则稍微复杂：((f(x)g(x))' = f'(x)g(x) + f(x)g'(x))。
    如果不知道这个法则，很多函数就很难求导；知道之后就可以机械套用。
    比如一个函数写成乘积形式 (x^2 \cdot (\text{某个关于}x\text{的式子}))，就把它当成“第一个函数×第二个函数”，直接用乘积法则。

  * [21:03–21:05] **Chain rule (composite function derivative)**
    **English**
    For composite functions, we use the chain rule. If (z = f(y)) and (y = g(x)), then
    (\dfrac{dz}{dx} = \dfrac{dz}{dy}\dfrac{dy}{dx}).
    For example, if (y = x^3), then (\dfrac{dy}{dx} = 3x^2). You multiply this with (\dfrac{dz}{dy}) from the outer function to get (\dfrac{dz}{dx}).
    This is exactly the kind of calculation behind training neural networks.

    **Chinese**
    复合函数求导用链式法则：如果 (z=f(y))，且 (y=g(x))，那么
    (\dfrac{dz}{dx} = \dfrac{dz}{dy}\dfrac{dy}{dx})。
    例如 (y=x^3)，则 (\dfrac{dy}{dx}=3x^2)。再把外层函数的 (\dfrac{dz}{dy}) 乘上去，就得到 (\dfrac{dz}{dx})。
    这类计算就是神经网络训练（反向传播）背后的数学基础之一。

  * [21:05–21:09] **Multivariable example: partial derivatives and substitution**
    **English**
    For multivariable cases, it is not always clear at first, so let me give an example. Suppose (z = x^2y).
    If you take partial derivative with respect to (x), treat (y) as constant: (\frac{\partial z}{\partial x} = 2xy).
    If you take partial derivative with respect to (y), treat (x^2) as constant: (\frac{\partial z}{\partial y} = x^2).
    Then if you further define variables like (x = s + t), (y = s - t), you can substitute them in, and the derivative becomes a function of (s,t).
    This is the kind of rule-based computation we use.

    **Chinese**
    多变量情况一开始可能不太直观，所以举个例子。假设 (z=x^2y)。
    对 (x) 求偏导时，把 (y) 当常数：(\frac{\partial z}{\partial x}=2xy)。
    对 (y) 求偏导时，把 (x^2) 当常数：(\frac{\partial z}{\partial y}=x^2)。
    如果再定义 (x=s+t)，(y=s-t)，就可以把它们代入，导数会变成关于 (s,t) 的函数。
    这类计算就是按规则一步步推出来的。

  * [21:10–21:14] **Optimization workflow and gradient descent; closing**
    **English**
    So the logic is: you want to optimize the loss function—minimize it. You compute derivatives of the loss function with respect to parameters, set them to zero (in simple cases), and solve equations to get the minimizer.
    In machine learning models, the loss depends on many parameters, so you take derivatives with respect to those parameters.
    Numerically, we solve this using gradient descent. Loss functions can be complicated; sometimes they are not continuous or not differentiable. But most are piecewise differentiable, so we can still apply these methods.
    OK, I think that’s all for today’s lecture.

    **Chinese**
    所以整体逻辑是：你要优化损失函数，也就是把它最小化。简单情况下，你对损失函数关于参数求导，令导数为 0，然后解方程得到最小点。
    机器学习模型里，损失函数通常依赖很多参数，所以你要对这些参数分别求导。
    数值上，我们通常用梯度下降来做。损失函数可能很复杂；有些不连续，或不可导。但大多数是“分段可导”的，所以仍然可以用这些方法。
    好的，我想今天的课就到这里。
## Week 2
  - [19:10–19:13] **From raw features to a learnable function**
     **English**
     Price itself is just a number. What really matters is how we model the relationship between input features and that price.
     In practice, the original feature space is often not suitable for machine learning directly. So we first convert raw features into a numerical feature representation. Once this mapping is done, we try to learn a mathematical function from features to the output.

    This function is unknown in advance, so in practice we often try different functional forms and see which one works well.

    **Chinese**
     房价本身只是一个数字，真正重要的是我们如何刻画“特征”和“价格”之间的关系。
     在实际中，原始特征空间通常并不适合直接用于机器学习，因此我们需要先把原始数据转换成数值特征表示。完成这个映射之后，再去学习一个从特征到输出的数学函数。

    这个函数事先并不知道，所以通常需要尝试不同形式的函数，看看哪一种效果更好。

  - [19:13–19:15] **Feature vectors and labels (house price example)**
     **English**
     Consider a dataset of house sales. For each house, we have a feature vector, such as size, number of bedrooms, and other attributes.
     Mathematically, we write features as a bold symbol, meaning it is a vector rather than a single number.

    The price, on the other hand, is just a scalar value. So the input is a vector, while the output (label) is a single real number.
     This naturally leads us to ask: can we learn a linear function that maps feature vectors to prices?

    **Chinese**
     以房屋销售数据为例，每一套房子都有一组特征，比如面积、卧室数量等。
     在数学中，我们用加粗符号表示特征，这是因为它不是一个数，而是一组数构成的向量。

    房价本身是一个标量值。因此，输入是向量，输出（标签）是一个实数。
     这就引出了一个问题：我们能否学习一个线性函数，把特征向量映射到房价？

  - [19:15–19:16] **Linear regression model**
     **English**
     A typical linear regression model has the form
    $$
    \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots
    $$
    Here, $\theta_0$ is the intercept, and the other $\theta_i$ are coefficients for different features.

    The predicted price $\hat{y}$ is still a single number, even though the input has multiple dimensions. This is the general regression setting.

    **Chinese**
     一个典型的线性回归模型可以写成
    $$
    \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots
    $$
    其中 $\theta_0$ 是截距，其余的 $\theta_i$ 是各个特征对应的系数。

    虽然输入是多维特征，但预测结果 $\hat{y}$ 仍然是一个数。这就是回归问题的一般形式。

  - [19:16–19:17] **Regression vs. classification**
     **English**
     In regression problems, the label is a real number.
     Binary classification is a special case where labels are restricted to 0 or 1.

    In this lecture, we focus on regression, especially linear regression, and later we will discuss classification models separately.

    **Chinese**
     在回归问题中，标签是一个实数。
     二分类问题是一个特殊情况，标签只能取 0 或 1。

    本节课主要讨论回归问题，特别是线性回归，分类模型会在之后单独介绍。

  - [19:17–19:19] **Predicted values and errors**
     **English**
     For each house, we have a real price $y$ and a predicted price $\hat{y}$.
     The difference between these two is the error. Sometimes the model underestimates the real value, and sometimes it overestimates it.

    Our goal is to find parameters $\theta$ such that these errors are as small as possible across all data points.

    **Chinese**
     对于每一套房子，都有一个真实价格 $y$ 和一个预测价格 $\hat{y}$。
     两者之间的差值就是误差。有时模型会低估真实价格，有时会高估。

    我们的目标是找到一组参数 $\theta$，使得在所有数据点上的误差都尽可能小。

  - [19:19–19:21] **Loss function: why we need it**
     **English**
     To formalize “how small the error is,” we introduce a loss function.
     A loss function measures how bad a prediction is for a single data point.

    There are many possible loss functions. If the predicted value equals the real value exactly, the loss should be zero. Otherwise, the loss should be positive.

    **Chinese**
     为了定量描述“误差有多大”，我们引入损失函数。
     损失函数用于衡量单个数据点上的预测有多糟。

    损失函数的形式有很多种。如果预测值与真实值完全一致，损失应为 0；否则，损失应为正数。

  - [19:21–19:23] **Absolute loss (L1 loss / Manhattan distance)**
     **English**
     One simple loss function is the absolute loss, which measures the absolute difference between predicted and real values.
     Geometrically, this corresponds to the Manhattan distance: like walking along streets rather than cutting straight across blocks.

    In one-dimensional problems such as house price prediction, absolute loss can be used directly.

    **Chinese**
     一种简单的损失函数是绝对损失，即预测值与真实值之差的绝对值。
     在几何上，它对应“曼哈顿距离”，类似在城市街道上行走，而不是直接斜穿街区。

    在房价预测这种一维输出的问题中，可以直接使用绝对损失。

  - [19:23–19:29] **Relative loss (percentage error)**
     **English**
     Sometimes we care more about relative error than absolute error.
     Relative loss measures how much the prediction deviates proportionally from the true value.

    For example, if the real price is 2.5 million and the prediction is 3 million, the relative error is about 20%.
     If the prediction is 2 million instead, the relative error is again about 20%, but in the opposite direction.

    **Chinese**
     有时我们更关心相对误差，而不是绝对误差。
     相对损失衡量的是预测值相对于真实值偏离了多少比例。

    例如，真实价格是 250 万，而预测为 300 万，相对误差约为 20%。
     如果预测为 200 万，相对误差同样约为 20%，只是方向相反。

  - [19:29–19:35] **Squared loss and empirical risk**
     **English**
     In practice, squared loss is used very frequently because it has nice mathematical properties.
     For a dataset with many data points, we do not look at the loss of a single point, but sum up all losses and take the average.

    This average loss over the dataset is called the empirical risk.
     Minimizing empirical risk means the model fits the training data well overall.

    **Chinese**
     在实际中，平方损失被大量使用，因为它在数学上非常方便处理。
     当数据点很多时，我们不会只看单个样本的损失，而是把所有损失加起来，再取平均。

    这个平均损失称为经验风险。
     最小化经验风险，意味着模型在整体上对训练数据拟合得较好。

  - [19:35–19:39] **Learning as optimization**
     **English**
     The key idea of machine learning here is optimization.
     We want to find parameter values that minimize the empirical risk.

    For squared loss, the resulting objective function is differentiable, which allows us to use calculus tools such as derivatives and gradients to find the minimum.

    **Chinese**
     这里机器学习的核心思想是优化。
     我们的目标是找到一组参数，使经验风险最小。

    对于平方损失，目标函数是可微的，因此可以使用微积分工具，如导数和梯度，来寻找最小值。

  - [19:35–19:37] **From single-point loss to an optimization problem**
     **English**
     Suppose we only have three training data points (three houses sold). We use these three records to determine the parameters $\theta_0, \theta_1, \theta_2$.

    If we use squared loss, then the empirical risk is the average (or sum) of the squared prediction errors across these three points:
    $$
    L(\theta)=\frac{1}{3}\sum_{i=1}^{3}\big(\hat y_i-y_i\big)^2
    $$
    Here, $\hat y_i = f(x_i;\theta)$ is the predicted price of the $i$-th house and $y_i$ is the real price.

    **Chinese**
     假设我们只有三个训练样本（三套房子的成交记录），我们要用这三条数据来确定参数 $\theta_0,\theta_1,\theta_2$。

    如果采用平方损失，那么经验风险就是这三个点上预测误差平方的平均（或总和）：
    $$
    L(\theta)=\frac{1}{3}\sum_{i=1}^{3}\big(\hat y_i-y_i\big)^2
    $$
    其中 $\hat y_i=f(x_i;\theta)$ 是第 $i$ 套房子的预测价格，$y_i$ 是真实价格。

  - [19:37–19:39] **Why calculus appears: differentiable objective**
     **English**
     To find the best parameters, we want the minimum point of $L(\theta)$.
     For squared loss, $L(\theta)$ is differentiable, so we can compute its partial derivatives with respect to each parameter:
    $$
    \frac{\partial L}{\partial \theta_0},\quad \frac{\partial L}{\partial \theta_1},\quad \frac{\partial L}{\partial \theta_2}
    $$
    At a minimum point (for a smooth function), the gradient must be zero in every direction.

    **Chinese**
     为了找到最优参数，我们需要找到 $L(\theta)$ 的最小点。
     平方损失的好处是 $L(\theta)$ 可微，因此我们可以对每个参数求偏导：
    $$
    \frac{\partial L}{\partial \theta_0},\quad \frac{\partial L}{\partial \theta_1},\quad \frac{\partial L}{\partial \theta_2}
    $$
    对于光滑可微函数，在最小点处，梯度在每个方向都应当为 0。

  - [19:40–19:42] **Concrete example: the 3-house dataset**
     **English**
     Now we plug in the three house records. For each record, we have two features (for instance):

    - $x_1$: number of bedrooms
    - $x_2$: standardized area (e.g., 0.5 corresponds to 500 square feet in some normalized unit)

    The model for each house is:
    $$
    \hat y=\theta_0+\theta_1 x_1+\theta_2 x_2
    $$
    So the first house might have $x_1=1$ bedroom and $x_2=0.5$, with real price $y_1\approx 491$ (in some simplified unit, e.g., “thousands”).
     The second and third houses have their own $(x_1,x_2,y)$ values.

    **Chinese**
     接下来把三条房屋记录代入。每条记录有两个特征（例如）：

    - $x_1$：卧室数量
    - $x_2$：标准化后的面积（比如 0.5 对应 500 平方英尺这一类归一化单位）

    模型对每套房子都写成：
    $$
    \hat y=\theta_0+\theta_1 x_1+\theta_2 x_2
    $$
    比如第一套房可能是 $x_1=1$、$x_2=0.5$，真实价格 $y_1\approx 491$（为方便讲解，把单位做了简化，比如“千”为单位）。
     第二套、第三套同理，各有自己的 $(x_1,x_2,y)$。

  - [19:42–19:46] **Compute partial derivatives (gradient components)**
     **English**
     Because the inside of the square is a linear function of $\theta$, the derivatives are straightforward.
     For squared loss, each term looks like $(\hat y_i-y_i)^2$. By chain rule,
    $$
    \frac{\partial}{\partial \theta_j}(\hat y_i-y_i)^2 = 2(\hat y_i-y_i)\cdot \frac{\partial \hat y_i}{\partial \theta_j}
    $$
    And since
    $$
    \hat y_i=\theta_0+\theta_1 x_{i1}+\theta_2 x_{i2}
    $$
    we have:
    $$
    \frac{\partial \hat y_i}{\partial \theta_0}=1,\quad
    \frac{\partial \hat y_i}{\partial \theta_1}=x_{i1},\quad
    \frac{\partial \hat y_i}{\partial \theta_2}=x_{i2}
    $$
    So the gradient equations become sums over the data points, with shared terms collected together.

    **Chinese**
     因为平方项内部是关于 $\theta$ 的线性函数，所以求导很直接。
     平方损失每一项是 $(\hat y_i-y_i)^2$。用链式法则：
    $$
    \frac{\partial}{\partial \theta_j}(\hat y_i-y_i)^2 = 2(\hat y_i-y_i)\cdot \frac{\partial \hat y_i}{\partial \theta_j}
    $$
    而
    $$
    \hat y_i=\theta_0+\theta_1 x_{i1}+\theta_2 x_{i2}
    $$
    因此：
    $$
    \frac{\partial \hat y_i}{\partial \theta_0}=1,\quad
    \frac{\partial \hat y_i}{\partial \theta_1}=x_{i1},\quad
    \frac{\partial \hat y_i}{\partial \theta_2}=x_{i2}
    $$
    所以最后得到的是对所有数据点求和的方程，并把公共项合并整理。

  - [19:46–19:48] **Set gradient to zero → normal equations**
     **English**
     At the minimum, each partial derivative equals zero:
    $$
    \frac{\partial L}{\partial \theta_0}=0,\quad
    \frac{\partial L}{\partial \theta_1}=0,\quad
    \frac{\partial L}{\partial \theta_2}=0
    $$
    With three data points and three unknowns, we obtain a system of linear equations.
     This is why, for linear regression with squared loss, we can solve parameters by solving linear equations (the “normal equation” approach), rather than iterative search.

    **Chinese**
     在最小点处，每个方向的偏导都等于 0：
    $$
    \frac{\partial L}{\partial \theta_0}=0,\quad
    \frac{\partial L}{\partial \theta_1}=0,\quad
    \frac{\partial L}{\partial \theta_2}=0
    $$
    三个数据点、三个未知参数，会得到一个线性方程组。
     这就是为什么在线性回归 + 平方损失的情况下，我们往往可以直接解线性方程（正规方程），而不一定要用迭代搜索。

  - [19:48–19:52] **Perfect fit vs generalization**
     **English**
     In this small example, you may get parameters that predict these three training points very well—possibly almost perfectly.
     But fitting the training points extremely well does not guarantee good prediction on unseen data. This is the generalization issue, which we will discuss later (overfitting).

    Even for a simple linear model, the mathematics can be done exactly by solving the linear system. That is the “math behind linear regression.”

    **Chinese**
     在这个很小的例子里，算出来的参数可能会让你对这三个训练点预测得非常好，甚至几乎完全贴合。
     但训练集拟合得好，并不等于对没见过的新数据预测也好。这就是泛化问题，后面会讲到（过拟合）。

    即使线性模型很简单，它背后的数学也可以通过解线性方程组得到精确解。这就是线性回归的“数学基础”。

  - [19:52–19:57] **Why numerical computation is used in real problems**
     **English**
     For real datasets, we usually cannot solve by hand. Instead, we use numerical computation.
     Conceptually, the data table has feature columns (e.g., bedrooms, area) and a label column (price).

    We then run a computational procedure to estimate the solution and obtain the parameters.
     In modern practice, this is implemented in libraries, so once you format the data correctly, solving linear regression is straightforward.

    **Chinese**
     在真实数据集中，我们通常不可能手算解出来，因此会使用数值计算。
     从概念上看，数据表包含若干特征列（如卧室数、面积等）以及一个标签列（价格）。

    然后通过计算过程来估计最优解，得到参数。
     现在一般都由现成库实现，只要把数据格式整理好，线性回归求解就很方便。

  - [20:34–20:36] **Introducing regularization and the role of λ**
     **English**
     Now we introduce a regularization term into the loss function.
     We try different values of the regularization parameter $\lambda$, such as very small numbers (e.g. 0.0001, 0.01), moderate values, and larger values like 1 or even 100.

    When $\lambda$ is very small, the regularization effect is weak. The model mainly focuses on minimizing the training loss.
     When $\lambda$ is larger, the regularization term becomes more important and prevents parameters from becoming too large.

    **Chinese**
     现在我们在损失函数中加入正则化项。
     我们会尝试不同的正则化参数 $\lambda$，比如非常小的数（0.0001、0.01），以及较大的数（如 1、100）。

    当 $\lambda$ 很小时，正则化作用很弱，模型主要是在最小化训练损失。
     当 $\lambda$ 变大时，正则化项的影响增强，会限制参数不能变得过大。

  - [20:36–20:37] **Trade-off between fitting and model complexity**
     **English**
     With a very small $\lambda$, the model can fit the training data extremely well, but the parameter values may become very large.
     This often leads to overfitting: the model explains training data well but performs poorly on new data.

    With a moderate $\lambda$, the parameters are smaller, the training loss may increase slightly, but the model is more stable and generalizes better.

    **Chinese**
     当 $\lambda$ 非常小时，模型可以把训练数据拟合得非常好，但参数值可能会变得很大。
     这通常会导致过拟合：模型在训练集上表现很好，但在新数据上效果差。

    当 $\lambda$ 取中等值时，参数变小，训练误差可能略有上升，但模型更加稳定，泛化能力更好。

  - [20:37–20:39] **Why minimizing training loss is not enough**
     **English**
     A very small loss value only tells us that the model fits the training data well.
     It does not guarantee good prediction on unseen data.

    A good model should give reasonably good predictions on every data point, including those not used for training.
     This is exactly why we add a regularization term to the loss function.

    **Chinese**
     训练损失很小，只说明模型对训练数据拟合得好。
     这并不能保证它在没见过的数据上预测也好。

    一个好的模型，应该对所有数据点（包括未见过的数据）都有较好的预测表现。
     这正是我们在损失函数中加入正则化项的原因。

  - [20:39–20:41] **Regularized vs non-regularized optimization**
     **English**
     Without regularization, we choose parameters $\theta$ that minimize the empirical loss only.
     With regularization, we choose parameters that do not necessarily minimize training loss, but are less sensitive to small changes in the data.

    As a result, the solution from the regularized version is expected to perform better on new data points.

    **Chinese**
     没有正则化时，我们选择的是使经验损失最小的参数 $\theta$。
     加入正则化后，我们选择的参数不一定让训练损失最小，但对数据的小变化不那么敏感。

    因此，正则化版本得到的解，通常在新数据点上表现更好。

  - [20:41–20:43] **Training, validation, and test intuition**
     **English**
     In practice, we use training data to learn the model parameters.
     We then evaluate the model on validation data to decide which model or parameter setting is better.

    The goal is not to perform best on training data, but to perform well on future, unseen data.

    **Chinese**
     在实践中，我们用训练数据来学习模型参数。
     然后用验证数据来评估不同模型或不同参数设置的效果。

    目标不是在训练集上做到最好，而是在未来、未见过的数据上表现良好。

  - [20:43–20:44] **Geometric intuition: fitting lines**
     **English**
     Consider fitting a line using blue points (training data).
     If the validation points lie close to the fitted line, then the model generalizes well.

    This is the type of model we want: not necessarily perfect on training data, but good overall.

    **Chinese**
     想象用蓝色点（训练数据）去拟合一条直线。
     如果验证点也接近这条直线，说明模型的泛化能力很好。

    这正是我们想要的模型：不一定完美贴合训练数据，但整体表现良好。

  - [20:44–20:47] **Underfitting and overfitting with polynomial models**
     **English**
     If we use a very simple model, such as a linear function, it may underfit the data.
     If we use a very complex model, such as a high-degree polynomial, it may overfit the data.

    For example, a quadratic model might still not fit well, while a cubic model may fit training data extremely well but behave wildly outside the observed range.

    **Chinese**
     如果模型太简单，比如线性模型，可能会出现欠拟合。
     如果模型太复杂，比如高阶多项式，则可能出现过拟合。

    例如，二次模型可能仍然拟合不好，而三次模型可能在训练数据上贴得很紧，但在区间外表现非常不稳定。

  - [20:47–20:49] **Choosing the best model**
     **English**
     The best model is not the one with the smallest training error, but the one with small errors on both training and validation data.
     This indicates a good balance between bias and variance.

    **Chinese**
     最好的模型，不是训练误差最小的模型，而是在训练集和验证集上误差都较小的模型。
     这表明模型在偏差和方差之间取得了良好的平衡。

  - [20:49–20:52] **Final takeaway**
     **English**
     In summary, model selection involves considering all possible models and choosing one that generalizes well.
     Complex models can easily overfit unless you have enough data or proper regularization.

    This principle applies not only to linear regression, but also to modern machine learning models with millions of parameters.

    **Chinese**
     总结来说，模型选择的关键是：在所有可能的模型中，选一个泛化能力好的。
     复杂模型如果没有足够的数据或合适的正则化，很容易过拟合。

    这一原则不仅适用于线性回归，也适用于拥有上百万参数的现代机器学习模型。
## Week 3
  - [19:03 - 19:07] **Binary Classification and Linear Separability**

    There are some data points—let's call them "blue lights" or dots—scattered among others. We essentially build a linear model to separate them. This is the core case where we attempt to split the feature space into two distinct regions. You can imagine this visually: in one dimension, it is very clear; in higher dimensions, we are trying to find a line (or hyperplane) that divides the positive examples from the negative examples.

    The property of a linear function is that if you define this dividing line, points on one side will yield a positive value (function output > 0), and points on the other side will yield a negative value. This is the fundamental property we leverage. We want to find a function where one side represents the positive class and the other the negative class. If we cannot make a prediction or if the separation is too complex, we might fall back to a simpler model. As we discussed, a simple model is often better because it avoids overfitting. Recall that binary classification is actually a special case of the general regression problem, so the methods we use here are consistent with the general regression framework.

    有一些数据点——我们可以称之为“蓝光”或点——散落在其他数据中间。我们基本上是构建一个线性模型来区分它们。这是我们试图将特征空间划分为两个不同区域的核心案例。你可以想象一下：在一维空间中这非常清晰；在更高维空间中，我们试图找到一条线（或超平面）将正样本和负样本分开。

    线性函数的一个特性是，如果你定义了这条分界线，一侧的点将产生正值（函数输出大于0），另一侧的点将产生负值。这就是我们要利用的基本属性。我们希望找到一个函数，使得一侧代表正类，另一侧代表负类。如果我们无法做出预测，或者分隔太复杂，我们可能会退回到一个更简单的模型。正如我们讨论过的，简单的模型往往更好，因为它避免了过拟合。回想一下，二分类实际上是广义回归问题的一个特例，因此我们在这里使用的方法与广义回归框架是一致的。

  - [19:07 - 19:11] **Model Validation: The Neyman-Pearson Loss**

    To build this predictor, we need to select a loss function and a regularizer, using the Regularized Empirical Risk Minimization (ERM) framework. Once trained, we must validate the model. Here, we introduce a specific validation metric called the Neyman-Pearson loss. The formula is `K · r_fn + r_fp`, where `r_fn` is the false negative rate and `r_fp` is the false positive rate.

    Let's define these. The false negative rate asks: "Among all actual positives, what fraction did the model fail to detect?". If we have positive samples (e.g., patients with cancer), but the prediction falls into the error region giving a negative result, that is a false negative. We naturally want this rate to be as small as possible. Conversely, the false positive rate measures how good the predictor is on negative samples. If a sample is actually negative (e.g., a healthy patient), but the model predicts it as positive, that is a false positive.

    The variable `K` acts as a hyperparameter. This parameter weights the importance of false negative errors relative to false positives. In certain applications, like cancer prediction, false negatives are critical errors—we absolutely do not want to predict a sick patient as healthy. In such cases, we set `K` to a large value (greater than 1) to heavily penalize false negatives. If, for some reason, we wanted to avoid false positives more, we would set `K` to be less than 1. Essentially, `K` is the relative penalty or weight we assign to different types of errors.

    为了构建这个预测器，我们需要选择一个损失函数和一个正则化项，并使用正则化经验风险最小化（ERM）框架。模型训练完成后，我们要对其进行验证。这里我们介绍一种特定的验证指标，称为 Neyman-Pearson 损失。公式为 `K · r_fn + r_fp`，其中 `r_fn` 是假阴性率，`r_fp` 是假阳性率。

    我们要定义一下这些概念。假阴性率是指：“在所有实际为正的样本中，模型未能检测出的比例是多少？”。如果我们有正样本（例如癌症患者），但预测结果错误地给出了阴性，这就是假阴性。我们自然希望这个比率越小越好。相反，假阳性率衡量的是预测器在负样本上的表现。如果样本实际上是负的（例如健康的人），但模型将其预测为阳性，这就是假阳性。

    变量 `K` 作为一个超参数。这个参数用来加权假阴性错误相对于假阳性错误的重要性。在某些应用中，比如癌症预测，假阴性是致命的错误——我们要绝对避免把生病的患者预测为健康的。在这种情况下，我们将 `K` 设为一个较大的值（大于1），以严厉惩罚假阴性。如果出于某种原因，我们更想避免假阳性，我们会将 `K` 设为小于1。本质上，`K` 就是我们分配给不同类型错误的相对惩罚或权重。

  - [19:11 - 19:14] **Regularization and Model Combinations (SVM, Logistic Regression)**

    Now, how do we choose the regularizer? Recall that regularization basically tries to find a linear function with small parameter coefficients, `θ` (theta). A linear model with small parameters is unlikely to suffer from overfitting. Conversely, if `θ` is very large, the model likely has poor predictive power on new data because it is over-fitting the training noise. Therefore, we use a regularizer to force the machine learning algorithm to find a model with small coefficients. Regularization is our primary tool to avoid the overfitting problem.

    We have choices for both the loss function and the regularizer. Looking at the different combinations:

    1. If we use **Square Loss** with a **Square Regularizer**, this is called the Least Square Classifier.
    2. If we use **Logistic Loss** with an **`l₁` Regularizer** (or square), this is called the Logistic Regression Classifier.
    3. If we use **Hinge Loss** with a **Square Regularizer**, this forms the Support Vector Machine (SVM).

    SVM was historically a very popular model. While deep learning is dominant now, SVM is theoretically very sound. In a simple binary case, you could draw many lines to separate the data, but the Support Vector Machine finds the "best" one—the one right in the middle that maximizes the margin.

    现在，我们要如何选择正则化项？回想一下，正则化基本上是试图找到一个参数系数 `θ`（Theta）较小的线性函数。参数较小的线性模型不太可能出现过拟合。相反，如果 `θ` 非常大，模型在新数据上的预测能力可能很差，因为它过度拟合了训练噪声。因此，我们使用正则化器来强制机器学习算法找到系数较小的模型。正则化是我们避免过拟合问题的主要工具。

    对于损失函数和正则化项，我们都有选择。看看不同的组合：

    1. 如果我们使用**平方损失**配合**平方正则化**，这被称为最小二乘分类器。
    2. 如果我们使用**Logistic损失**配合**`l₁`正则化**（或平方），这被称为逻辑回归分类器。
    3. 如果我们使用**Hinge损失**配合**平方正则化**，这就构成了支持向量机（SVM）。

    SVM 在历史上曾是一个非常流行的模型。虽然现在深度学习占据主导地位，但 SVM 在理论上非常完善。在简单的二分类情况中，你可以画很多条线来分隔数据，但支持向量机能找到“最好”的那一条——即位于中间、能最大化间隔的那一条。

  - [19:14 - 19:28] **Critique of Square Loss for Classification**

    Let's review the loss functions in detail. First, the **Square Loss**. Imagine we have many samples, predicted values, and true labels. The loss is calculated as `(ŷ - y)²`.

    Consider the regularizer term `r(θ) = Σ θᵢ²`. If this sum is small, it implies every individual `θᵢ` must be small, effectively constraining the model complexity.

    Now, looking specifically at the Square Loss function on a negative sample where the true label is -1. The loss is `(ŷ - (-1))² = (ŷ + 1)²`. Ideally, if we have a perfect prediction, the loss is 0. In a 2D space, the loss relates to the distance from the separating line.

    However, Square Loss has a major weakness in classification. Suppose we have a negative sample, and our function outputs a value of -2.5. Since the label is -1 (negative), a prediction of -2.5 is actually a *very confident* and correct classification—it is far on the negative side. But if you plug -2.5 into the Square Loss formula, you get `(-2.5 - (-1))² = (-1.5)² = 2.25`. The loss function gives a large penalty!

    This means the model is being "overcharged" or penalized for being "too correct" in the right direction. This large penalty for confident correct predictions can mislead the training process, causing the algorithm to adjust parameters unnecessarily to reduce this "error," even though the classification was correct. This is the fundamental flaw of using Square Loss for classification: it penalizes correct predictions that lie far from the margin. The same logic applies symmetrically to positive samples.

    让我们详细回顾一下损失函数。首先是**平方损失**。假设我们要处理许多样本、预测值和真实标签。损失计算为 `(ŷ - y)²`。

    考虑正则化项 `r(θ) = Σ θᵢ²`。如果这个和很小，意味着每个单独的 `θᵢ` 都必须很小，从而有效地限制了模型的复杂度。

    现在具体看看负样本（真实标签为 -1）上的平方损失。损失为 `(ŷ - (-1))² = (ŷ + 1)²`。理想情况下，如果预测完美，损失应为 0。在二维空间中，损失与到分界线的距离有关。

    然而，平方损失在分类中有一个主要弱点。假设我们有一个负样本，我们的函数输出值为 -2.5。因为标签是 -1（负类），-2.5 的预测实际上是一个*非常确信*且正确的分类——它在负的一侧很远。但如果你把 -2.5 代入平方损失公式，你会得到 `(-2.5 - (-1))² = (-1.5)² = 2.25`。损失函数竟然给出了一个很大的惩罚！

    这意味着模型因为“过于正确”而受到了“过度惩罚”。这种对确信的正确预测的惩罚会误导训练过程，导致算法为了减少这种“误差”而不必要地调整参数，即使分类本身是正确的。这就是在分类中使用平方损失的根本缺陷：它惩罚了那些远离边界的正确预测。同样的逻辑也对称地适用于正样本。

  - [19:28 - 19:35] **Logistic Loss and Hinge Loss (SVM)**

    Because of the problems with Square Loss, we propose more meaningful loss functions. The **Logistic Loss** is one such alternative. It dampens the penalty for correct classifications that are far from the boundary. On the negative side, the loss is `log(1 + e^ŷ)`, and for positive samples, it is `log(1 + e^(-ŷ))`. It is differentiable and convex, which is mathematically convenient for optimization.

    However, Logistic Loss is still not perfect. Even if you give a perfect prediction, you still incur a small positive loss—it never truly hits zero. Ideally, a perfect prediction should have zero penalty.

    This leads us to the **Hinge Loss**, used in Support Vector Machines (SVM). This function is very reasonable. If the true label is -1 and you predict -1, the loss is 0. If you predict -2 or -3 (very confident negative), the loss is *also* 0. It does not penalize you for being "extra" correct. The formula uses the "positive part" notation `(·)₊`, which simply means `max(0, ·)`.

    - For negative samples: `ℓ(ŷ, -1) = max(0, ŷ + 1)`. If `ŷ ≤ -1`, loss is 0.
    - For positive samples: `ℓ(ŷ, 1) = max(0, 1 - ŷ)`. If `ŷ ≥ 1`, loss is 0.

    Mathematically, this function is convex, though not differentiable everywhere (specifically at the "hinge" point), but we can handle that. We balance the differentiability of Logistic Loss with the zero-penalty property of Hinge Loss. These are the three popular loss functions you need to know.

    由于平方损失存在的问题，我们提出了更有意义的损失函数。**Logistic损失**（逻辑损失） 就是这样一个替代方案。它减轻了对远离边界的正确分类的惩罚。在负样本侧，损失是 `log(1 + e^ŷ)`，在正样本侧是 `log(1 + e^(-ŷ))`。它是可微且凸的，这在数学上便于优化。

    然而，Logistic 损失仍然不是完美的。即使你给出了完美的预测，你仍然会有一个很小的正损失——它永远不会真正达到零。理想情况下，完美的预测应该有零惩罚。

    这引出了支持向量机（SVM）中使用的 **Hinge 损失**。这个函数非常合理。如果真实标签是 -1 而你预测 -1，损失是 0。如果你预测 -2 或 -3（非常确信的负值），损失*也是* 0。它不会因为你“额外”正确而惩罚你。公式使用了“正部”符号 `(·)₊`，意思就是 `max(0, ·)`。

    - 对于负样本：`ℓ(ŷ, -1) = max(0, ŷ + 1)`。如果 `ŷ ≤ -1`，损失为 0。
    - 对于正样本：`ℓ(ŷ, 1) = max(0, 1 - ŷ)`。如果 `ŷ ≥ 1`，损失为 0。

    在数学上，这个函数是凸的，虽然不是处处可微（特别是在“铰链”点），但我们可以处理这个问题。我们在 Logistic 损失的可微性和 Hinge 损失的零惩罚属性之间进行权衡。这些是你需要了解的三种流行损失函数。

  - [19:35 - 19:42] **Accuracy Evaluation: Confusion Matrix and AUROC**

    We evaluate these models using a **Confusion Matrix**. This matrix breaks down results into True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). From these, we define **Accuracy**, which is simply the total correct predictions divided by the total cases.

    However, if you have **imbalanced data**, accuracy is misleading. For example, if you are predicting earthquakes, they happen very rarely. If you build a model that predicts "No Earthquake" every single day, your accuracy might be 99.9%, but the model is useless because it has zero predictive power for the event of interest. Therefore, accuracy is not a good indicator for imbalanced datasets. In such cases, we consider **Precision**, **Specificity**, and **Sensitivity (Recall)**. Machine learning is essentially finding the best model to balance these two competing indicators: sensitivity and specificity. We want a predictor that performs well on both negative and positive samples.

    To visualize this balance, we use the **AUROC** (Area Under the Receiver Operating Characteristic Curve). The name comes from signal processing fields. Essentially, if you have a random classifier (guessing blindly), the curve is a diagonal line, and the area under it is 0.5. This is the baseline—the worst useful classifier. A good predictor will have a curve that bows outward, approaching 1.0. People often claim their predictors have AUROC values of 0.90, 0.95, or 0.99. Similarly, we have the **AUPRC** (Area Under the Precision-Recall Curve), which is also useful for imbalanced data.

    我们使用**混淆矩阵** 来评估这些模型。该矩阵将结果细分为真阳性（TP）、假阳性（FP）、真阴性（TN）和假阴性（FN）。由此，我们可以定义**准确率**，即正确的预测总数除以总案例数。

    然而，如果你有**不平衡数据**，准确率会产生误导。例如，如果你预测地震，地震非常罕见。如果你建立一个模型，每天都预测“无地震”，你的准确率可能高达 99.9%，但这个模型是无用的，因为它对我们要关注的事件没有任何预测能力。因此，对于不平衡数据集，准确率不是一个好的指标。在这种情况下，我们要考虑**精确率**、**特异性**和**灵敏度（召回率）**。机器学习本质上是找到最佳模型来平衡这两个相互竞争的指标：灵敏度和特异性。我们需要一个在负样本和正样本上表现都很好的预测器。

    为了可视化这种平衡，我们使用 **AUROC**（受试者工作特征曲线下面积）。这个名字来源于信号处理领域。本质上，如果你有一个随机分类器（盲猜），曲线是一条对角线，其下面积为 0.5。这是基线——最有用的最差分类器。一个好的预测器的曲线会向外凸出，接近 1.0。人们经常声称他们的预测器 AUROC 值为 0.90、0.95 或 0.99。同样，我们还有 **AUPRC**（精确率-召回率曲线下面积），这对不平衡数据也很有用。

  - [19:42 - 19:45] **Warm-Up Project: Australian Weather Prediction**

    I have provided an optional warm-up project. You can download the data from Kaggle or Canvas. The task is to "Predict Next-Day Rain with Australian Weather Data". The dataset contains over 100,000 records from various cities like Sydney and Melbourne. The objective is to use today's weather measurements to predict rainfall tomorrow.

    For the project:

    1. Write a Python program to extract records specifically for **Melbourne**.

    2. Remove rows with missing values (N/A).

    3. Drop specific columns like Date, Sunshine, WindGustDir, etc., as they are not needed for this exercise.

    4. Map non-numerical features to a real vector space and split the data into training (90%) and validation (10%) sets.

    5. Implement a linear model using **Logistic Loss** and a **Square Regularizer**.

       This is a simple project to practice the full pipeline.

    我提供了一个可选的热身项目。你可以从 Kaggle 或 Canvas 下载数据。任务是“利用澳大利亚天气数据预测第二天的降雨”。数据集包含来自悉尼和墨尔本等城市的 10 万多条记录。目标是利用当天的天气测量值来预测第二天的降雨量。

    项目要求：

    1. 编写 Python 程序，专门提取**墨尔本**的记录。

    2. 删除包含缺失值（N/A）的行。

    3. 删除日期、日照、阵风风向等特定列，因为本练习不需要这些列。

    4. 将非数值特征映射到实向量空间，并将数据分为训练集（90%）和验证集（10%）。

    5. 使用 **Logistic损失** 和 **平方正则化** 实现线性模型。

       这是一个练习完整流程的简单项目。

  - [19:45 - 19:48] **Summary of Part 1**

    To summarize Part 1: I introduced the linear model and used it to demonstrate the entire machine learning pipeline.

    - We choose a model class (Linear Predictor).
    - We select a Loss Function (Empirical Risk).
    - We add Regularization for model complexity control to prevent overfitting.
    - We use an optimization/training algorithm.
    - We evaluate using performance metrics (Sensitivity, Specificity, AUROC).
    - We perform validation by splitting data into Training, Validation, and Test sets, or using Cross-Validation.

    We also briefly mentioned Nearest-Neighbor predictors, which we will return to when we discuss decision trees later.

    总结第一部分：我介绍了线性模型，并用它来演示整个机器学习流程。

    - 我们选择一个模型类（线性预测器）。
    - 我们选择一个损失函数（经验风险）。
    - 我们添加正则化来进行模型复杂度控制，以防止过拟合。
    - 我们使用优化/训练算法。
    - 我们使用性能指标（灵敏度、特异性、AUROC）进行评估。
    - 我们通过将数据分为训练集、验证集和测试集，或使用交叉验证来进行验证。

    我们也简要提到了最近邻预测器，我们将在稍后讨论决策树时回到这个话题。

  - [19:48 - 19:50] **Student Q&A: AUROC vs. AUPRC**

    **Student Question:** When do you use AUROC versus AUPRC?

    **Professor's Answer:** Honestly, I am not a statistics guy, so I don't pay excessive attention to the subtle theoretical distinctions between every metric. However, for classical machine learning, my practical suggestion is: use 5 or 6 different performance metrics. If your model beats other models on the majority of these metrics, then your model is good. That is essentially how you write a research paper—you show robustness across multiple indicators.

    For this course, you should understand the concepts—why we have these formulas—but you don't need to memorize every single formula if you don't use it constantly. The key takeaway is understanding *why* we prefer simple linear models over complicated ones: overfitting. You don't need a complex model that captures 99% of the training data if it fails to generalize. A linear model with simple non-linear components often yields a very differentiable and effective approach.

    **学生提问：** 什么时候使用 AUROC，什么时候使用 AUPRC？

    **教授回答：** 说实话，我不是搞统计学的，所以我不会过分关注每个指标之间细微的理论区别。但是，对于经典机器学习，我的实用建议是：使用 5 或 6 种不同的性能指标。如果你的模型在大多数指标上都击败了其他模型，那么你的模型就是好的。这基本上就是写研究论文的方法——你要展示在多个指标上的稳健性。

    对于这门课，你应该理解这些概念——为什么我们有这些公式——但如果你不经常使用，就不需要死记硬背每一个公式。核心要点是理解*为什么*我们要选择简单的线性模型而不是复杂的模型：因为过拟合。如果一个复杂的模型在训练数据上达到了 99% 的准确率却无法泛化，那你是并不需要它的。线性模型加上简单的非线性组件，通常能产生一种非常可微且有效的方法。

  - [19:50 - 19:51] **Final Remarks on Part 1**

    Important concepts to retain are: Regularizers, Loss Functions, and specifically the distinction between Square Loss (and why it fails for classification), Logistic Loss, and Hinge Loss. That concludes the first part of today's lecture. Next, we will move to Lecture 3: Probabilistic Predictors.

    需要记住的重要概念是：正则化项、损失函数，特别是平方损失（以及它为何在分类中失效）、Logistic 损失和 Hinge 损失之间的区别。今天讲座的第一部分到此结束。接下来，我们将进入第 3 讲：概率预测器。

  - [20:03 - 20:07] **Quiz Logistics and Administrative Details**

    Including the lecture notes, the quiz will be very easy. It is designed to take about 40 minutes, but you have one hour. There are absolutely no calculations involved. Okay, so no calculations, just simple multiple-choice questions. We have a special arrangement regarding the schedule; while the topics covered are a weak constraint, the oral presentation pieces can be arranged in the same week or after that.

    Important note: You have to come here physically to the classroom. You cannot access this quiz online because there is a password required, and I will only give the password inside this room. This assessment involves me directly.

    包括讲义在内，这次测验会非常简单。它设计为 40 分钟完成，但你们有一个小时的时间。绝对不涉及计算。好的，没有计算，只是简单的选择题。关于时间表我们有一个特别安排；虽然涵盖的主题是一个软性约束，但口头报告的部分可以安排在同一周或之后进行。

    重要提示：你们必须亲自来到教室。你们无法在线访问这个测验，因为需要密码，而我只会在这个房间里提供密码。这次评估需要我在场。

  - [20:07 - 20:12] **Introduction to Probabilistic and List Predictors**

    In the second part of today's lecture, we discuss the **Probabilistic Predictor**. This is a different type of learning concept. So far, we have only learned the linear model. The linear model simply returns one single value `ŷ` for each data point `x`. We call this a **Point Predictor**. However, for some problems, it is better if we return a list of answers. For example, we might have a first guess, a second guess, and a third guess. The customer is often happier with this approach, particularly in recommendation systems.

    Sometimes, naturally, we don't really know exactly which one is the correct answer, or the answer is ambiguous. In these cases, we use probability. For example, for a weather forecast, we might say: "There is a 5% chance of rain tomorrow, and an 80% chance of clear sky sunshine." The former type—returning a ranked list—is called a **List Predictor** (like a Google search giving top 10 results). The latter is called a **Probabilistic Predictor**. In a recommendation system, since we don't really know what the user wants to buy, giving multiple weighted answers allows the user to check which one is reasonable.

    在今天讲座的第二部分，我们讨论**概率预测器**。这是一个不同的学习概念。到目前为止，我们只学习了线性模型。线性模型对于每个数据点 `x` 只返回一个单一的值 `ŷ`。我们称之为**点预测器**。然而，对于某些问题，如果我们返回一系列答案会更好。例如，我们可能有第一猜测、第二猜测和第三猜测。客户通常更喜欢这种方式，特别是在推荐系统中。

    有时，本质上我们并不知道确切的正确答案，或者答案是模棱两可的。在这些情况下，我们使用概率。例如，对于天气预报，我们可能会说：“明天有 5% 的几率下雨，80% 的几率是晴天。”前者——返回排名列表——被称为**列表预测器**（就像 Google 搜索给出前 10 个结果）。后者被称为**概率预测器**。在推荐系统中，因为我们要么不知道用户真正想买什么，给出多个加权答案可以让用户自己检查哪个是合理的。

  - [20:12 - 20:16] **Defining the Probabilistic Predictor**

    Let's define the probabilistic predictor formally. We have a label space. For cancer prediction, it is very easy: we have "cancer" or "benign"—only two possible answers. For a weather forecast, say we have three possible answers: {Cloud, Rain, Shine}. For each answer, we assign a probability, representing the chance of that label occurring. Ideally, the sum of these chances is 1.

    So, the predictor is actually a function that takes an input `x` and returns a **distribution function** (or a table). It does not return a single scalar value. For example, for today's condition, it might output: 10% (0.1) chance of Rain, 5% (0.05) chance of Cloud, and 85% (0.85) chance of Sunshine. It returns a table where each possible answer is associated with a number representing its likelihood. This is distinct from a point predictor which just gives you one decision. Of course, you can always convert this back to a point prediction if you want, but the probabilistic output is more informative.

    让我们正式定义概率预测器。我们有一个标签空间。对于癌症预测，这很简单：我们有“癌症”或“良性”——只有两个可能的答案。对于天气预报，假设我们有三个可能的答案：{多云，雨，晴}。对于每个答案，我们分配一个概率，代表该标签发生的几率。理想情况下，这些几率之和为 1。

    因此，预测器实际上是一个函数，它接收输入 `x` 并返回一个**分布函数**（或一张表）。它不返回单个标量值。例如，对于今天的情况，它可能输出：10%（0.1）的几率下雨，5%（0.05）的几率多云，85%（0.85）的几率晴天。它返回一张表，其中每个可能的答案都关联着一个代表其可能性的数字。这与只给出一个决定的点预测器不同。当然，如果你愿意，你总是可以将其转换回点预测，但概率输出包含更多信息。

  - [20:16 - 20:20] **Ambiguity and Point Predictors as Special Cases**

    Why do we need this? Sometimes the situation is ambiguous. A point predictor might say "Rain," but ignores the fact that "Sunshine" was also very likely. A probabilistic predictor tells you that "everything is possible, and everyone has a chance."

    A Point Predictor is actually a **special case** of a Probabilistic Predictor. If you are extremely confident in one answer, you can assign that answer a probability of 1, and all other answers a probability of 0. For example, if a normal predictor says "Sunny," it is mathematically equivalent to a probability table where P(Sunny) = 1 and P(Rain) = 0. However, usually, outputting a probability distribution is more meaningful because point predictors may perform poorly in uncertain scenarios. If we give probabilities, the customer is happier, and we are happier. Neural networks, for instance, typically output a probabilistic predictor (using Softmax), which we then convert to a point prediction by selecting the label with the highest probability.

    为什么我们需要这个？有时情况是模棱两可的。点预测器可能会说“雨”，但忽略了“晴天”也很有可能的事实。概率预测器告诉你“一切皆有可能，每个选项都有机会”。

    点预测器实际上是概率预测器的**特例**。如果你对某个答案非常有信心，你可以给该答案分配概率 1，而所有其他答案的概率为 0。例如，如果一个普通预测器说“晴天”，这在数学上等同于一个概率表，其中 P(晴天) = 1，P(雨) = 0。然而，通常情况下，输出概率分布更有意义，因为点预测器在不确定场景中可能表现不佳。如果我们给出概率，客户会更满意，我们也更满意。例如，神经网络通常输出概率预测器（使用 Softmax），然后我们通过选择概率最高的标签将其转换为点预测。

  - [20:20 - 20:25] **K-Nearest Neighbor (KNN) Probabilistic Predictor**

    This method is theoretically more robust. Let's look at the **K-Nearest Neighbor Probabilistic Predictor**. Suppose we set `k = 6`. For any given data point in the feature space, how do we make a prediction?

    We find the 6 closest neighbors to this point. Imagine a circle around the point that contains exactly these 6 neighbors.

    1. **Clear Case:** If for a specific data point, all 6 neighbors are "blue" (negative/benign), then the probability for "blue" is 1 (or 100%), and "red" is 0. This gives a very certain prediction.
    2. **Ambiguous Case:** However, consider a data point in a "troublesome region" (the boundary). If you draw the circle for the 6 nearest neighbors, you might get 3 blue points and 3 red points. In this case, the predictor returns a table: P(Blue) = 0.5, P(Red) = 0.5. It basically says "No specific answer" or "I don't know."

    This is how KNN works as a probabilistic predictor. It gives you a distribution, like 0.8 for blue and 0.2 for red, rather than a forced binary choice. The only challenge is defining a distance function to find these neighbors.

    这种方法在理论上更稳健。让我们看看**K-最近邻概率预测器**。假设我们将 `k` 设为 6。对于特征空间中的任何给定数据点，我们如何进行预测？

    我们找到距离该点最近的 6 个邻居。想象一下该点周围的一个圆，刚好包含这 6 个邻居。

    1. **清晰案例：** 如果对于某个特定数据点，所有 6 个邻居都是“蓝色”（负类/良性），那么“蓝色”的概率就是 1（或 100%），“红色”是 0。这给出了一个非常确定的预测。
    2. **模糊案例：** 然而，考虑一个位于“麻烦区域”（边界）的数据点。如果你画出包含 6 个最近邻的圆，你可能会得到 3 个蓝点和 3 个红点。在这种情况下，预测器返回一个表：P(蓝) = 0.5，P(红) = 0.5。它基本上是在说“没有确切答案”或“我不知道”。

    这就是 KNN 作为概率预测器的工作原理。它给你一个分布，比如蓝色 0.8，红色 0.2，而不是强制的二元选择。唯一的挑战是定义一个距离函数来找到这些邻居。

  - [20:25 - 20:32] **Assessment: Likelihood**

    How do we say a prediction is "good"? We use the concept of **Likelihood**. Let's use the weather forecast example. You have today's condition and the actual outcome (label) for tomorrow (e.g., "Sunny").

    - **Intuition:** For the true label (what actually happened), the model should have predicted a high probability. If it actually rained, and your model gave P(Rain) a high value, that's good. If it was sunny, and your model gave P(Sunny) a high value, that's also good.

    We formalize this intuition. Suppose we have samples `(x⁽¹⁾, y⁽¹⁾), (x⁽²⁾, y⁽²⁾)...` and for each sample, our model produces a probability table. We look at the probability assigned *specifically* to the true label `y`.

    - Sample 1 (Sunny): Model predicted P(Sunny) = 0.91.
    - Sample 2 (Overcast): Model predicted P(Overcast) = 0.85.
    - Sample 3 (Rain): Model predicted P(Rain) = 0.82.

    The **Likelihood** is the **product** of these probabilities: `0.91 × 0.85 × 0.82 ≈ 0.634`. This single number measures how well the predicted distribution matches the actual data.

    我们如何判断一个预测是“好的”？我们使用**似然**（Likelihood）的概念。让我们使用天气预报的例子。你有今天的情况和明天的实际结果（标签）（例如“晴天”）。

    - **直觉：** 对于真实标签（实际发生的情况），模型应该预测出高概率。如果实际下雨了，而你的模型给出的 P(雨) 值很高，那就很好。如果是晴天，而你的模型给出的 P(晴) 值很高，那也很好。

    我们将这种直觉形式化。假设我们有样本 `(x⁽¹⁾, y⁽¹⁾), (x⁽²⁾, y⁽²⁾)...`，对于每个样本，我们的模型都会生成一个概率表。我们查看分配给*特定*真实标签 `y` 的概率。

    - 样本 1（晴）：模型预测 P(晴) = 0.91。
    - 样本 2（多云）：模型预测 P(多云) = 0.85。
    - 样本 3（雨）：模型预测 P(雨) = 0.82。

    **似然**就是这些概率的**乘积**：`0.91 × 0.85 × 0.82 ≈ 0.634`。这一个数字衡量了预测分布与实际数据的匹配程度。

  - [20:32 - 20:37] **Negative Log-Likelihood**

    There is a mathematical problem with raw Likelihood. If you multiply many small numbers (probabilities < 1) together—say, 400 samples—the result becomes a tiny, tiny number close to zero (underflow). It becomes hard to work with.

    Therefore, we calculate the **Negative Log-Likelihood**.

    1. Take the Logarithm of the likelihood. Since `log(0.x)` is negative (e.g., log(0.00034) might be -4 or -5), this converts the product into a sum of negative numbers.
    2. Put a **negative sign** in front of it to make the result positive. `-(log 0.91 + log 0.85 + log 0.82)`.

    For example, taking the negative log of our previous likelihood (0.634) gives roughly 0.4557. This number is much more manageable. Finally, to compare datasets of different sizes (e.g., a test set of 100 vs 200), we take the **average** (divide by `n`). This metric allows us to compare two different probabilistic predictors; the one with the *lower* negative log-likelihood is better.

    原始似然存在一个数学问题。如果你将许多小数（概率 < 1）相乘——比如说 400 个样本——结果会变成一个接近于零的极小数值（下溢）。这变得很难处理。

    因此，我们计算**负对数似然**（Negative Log-Likelihood）。

    1. 取似然的对数。因为 `log(0.x)` 是负数（例如 log(0.00034) 可能是 -4 或 -5），这将乘积转换为负数之和。
    2. 在前面加一个**负号**，使结果变为正数。`-(log 0.91 + log 0.85 + log 0.82)`。

    例如，对我们要之前的似然（0.634）取负对数大约得到 0.4557。这个数字更容易处理。最后，为了比较不同大小的数据集（例如 100 个样本的测试集与 200 个样本的测试集），我们要取**平均值**（除以 `n`）。这个指标允许我们比较两个不同的概率预测器；负对数似然*越低*的那个越好。

  - [20:38 - 20:45] **Information Theory: Entropy**

    How do we compare two probability distributions theoretically? We use **Entropy**, a concept from Information Theory.

    Consider a coin flipping game.

    - **High Information/Complexity:** If the coin is fair (Heads 0.5, Tails 0.5), it is hard to predict the outcome. In Information Theory, we say this system has "high information" or high entropy.
    - **Low Information/Simplicity:** If the coin is heavily biased (e.g., Heads 0.99, Tails 0.01), it is a very simple game. You almost always get Heads. This system has "low information" or low entropy because it is easy to predict.

    The formula for Entropy `H(p)` involves summing `-p · log(p)`. If you have a distribution like `1/7, 1/7... 1/7` (uniform, everything is equally likely), the entropy is maximized—you know nothing. If you have a distribution like `[1, 0, 0...]` (certainty), the entropy is zero. We use this to indicate whether a predictor "knows something" or is just guessing. If every possible label has a high probability (flat distribution), your prediction is no good.

    我们如何在理论上比较两个概率分布？我们使用**熵**（Entropy），这是一个来自信息论的概念。

    考虑一个抛硬币游戏。

    - **高信息/复杂性：** 如果硬币是公平的（正面 0.5，反面 0.5），很难预测结果。在信息论中，我们说这个系统具有“高信息量”或高熵。
    - **低信息/简单性：** 如果硬币严重有偏差（例如，正面 0.99，反面 0.01），这就是一个非常简单的游戏。你几乎总是得到正面。这个系统具有“低信息量”或低熵，因为它很容易预测。

    熵 `H(p)` 的公式涉及对 `-p · log(p)` 求和。如果你有一个像 `1/7, 1/7... 1/7` 这样的分布（均匀分布，一切都有可能），熵是最大的——你什么都不知道。如果你有一个像 `[1, 0, 0...]` 这样的分布（确定性），熵为零。我们用它来表示预测器是“知道些什么”还是仅仅在猜测。如果每个可能的标签都有很高的概率（平坦分布），你的预测就不好。

  - [20:45 - 20:50] **Cross-Entropy and KL Divergence**

    We extend this to **Cross-Entropy**. It measures the cost of using a predicted distribution `Q` to encode data that actually comes from the true distribution `P`.

    - Formula: `H(P, Q) = -Σ P(y) · log Q(y)`.
    - **Interpretation:** If your prediction `Q` is very different from the truth `P` (e.g., pointing in the wrong direction), the Cross-Entropy is high. It represents the "extra work" needed to correct the wrong direction. If `Q` is close to `P`, the value is small.

    This is closely related to the **Kullback-Leibler (KL) Divergence**, denoted as `d_kl(P || Q)`.

    - Formula: `d_kl(P || Q) = H(P, Q) - H(P)`.
    - KL Divergence measures the "distance" or divergence between two distributions. Since the entropy of the true distribution `H(P)` is usually fixed (we can't change the ground truth), minimizing Cross-Entropy is mathematically the same as minimizing KL Divergence. This is why Cross-Entropy is widely used as a loss function in neural networks.

    我们将此扩展到**交叉熵**。它衡量的是使用预测分布 `Q` 来编码实际来自真实分布 `P` 的数据的成本。

    - 公式：`H(P, Q) = -Σ P(y) · log Q(y)`。
    - **解释：** 如果你的预测 `Q` 与真实情况 `P` 非常不同（例如，指向错误的方向），交叉熵就会很高。它代表了纠正错误方向所需的“额外工作”。如果 `Q` 接近 `P`，该值就很小。

    这与 **Kullback-Leibler (KL) 散度**密切相关，表示为 `d_kl(P || Q)`。

    - 公式：`d_kl(P || Q) = H(P, Q) - H(P)`。
    - KL 散度衡量两个分布之间的“距离”或散度。由于真实分布的熵 `H(P)` 通常是固定的（我们无法改变基本事实），因此最小化交叉熵在数学上等同于最小化 KL 散度。这就是为什么交叉熵被广泛用作神经网络中的损失函数。
## Week 4
  - [19:02 - 19:06] **Introduction to Decision Trees and Historical Context**

    As you can see from the slide, we are looking at the origins of this method. This refers to the classic book written in 1984 by Breiman, Friedman, Olshen, and Stone. At that time, these statisticians worked together to formalize this model. The core idea is very simple: intelligence can be captured in a set of "if-then-else" rules that provide branching for classification. For example, consider a very simple decision tree used for animal recognition. We look at features like body size or color. If we ask "Is it large?" or "Is it gray?", we branch left or right.

    从幻灯片中可以看到，我们正在回顾这种方法的起源。这指的是Breiman、Friedman、Olshen和Stone在1984年合著的经典著作。当时，这些统计学家共同致力于该模型的规范化。其核心思想非常简单：智能可以通过一组提供分类分支的“if-then-else”（如果-那么-否则）规则来捕捉。例如，考虑一个用于动物识别的简单决策树。我们观察体型或颜色等特征。如果我们问“它很大吗？”或“它是灰色的吗？”，我们就会向左或向右分支。

  - [19:06 - 19:10] **Mathematical Formulation and Model Interpretability**

    Mathematically, we look at a data point and check its feature X. We compare X against a value, say "a". If X is greater than or equal to "a", we go one way; otherwise, we go the other. We might then check if a second feature Y is greater than "b". This effectively splits the feature space into partitions—left parts and right parts. The decision tree is a tree-structured model for prediction where internal nodes test whether a feature possesses an attribute, branches represent the possible values or outcomes of that test, and leaves output the final predicted class. Why do we choose Decision Trees? They are highly interpretable. Unlike "black box" models where you might not understand the internal logic, a decision tree mimics human reasoning. It is suitable when you have small data, classes are disjoint, and you need a model that is easy to view and explain.

    在数学上，我们观察一个数据点并检查其特征X。我们将X与某个值（比如“a”）进行比较。如果X大于或等于“a”，我们走一条路；否则走另一条路。接着我们可以检查第二个特征Y是否大于“b”。这有效地将特征空间划分为不同的部分——左边部分和右边部分。决策树是一种树状预测模型，其中内部节点测试特征是否具有某种属性，分支代表该测试的可能值或结果，叶节点输出最终的预测类别。为什么我们选择决策树？因为它们具有高度的可解释性。与你可能无法理解内部逻辑的“黑盒”模型不同，决策树模仿了人类的推理过程。它适用于数据量较小、类别互斥、且你需要一个易于查看和解释的模型的情况。

  - [19:10 - 19:17] **Information Theory Foundation: Entropy and Complexity**

    However, we cannot just build the tree arbitrarily; we need a mathematical foundation to select the most important attributes for splitting. We frame this as a binary classification problem where we have a set of training samples, some positive and some negative. From Information Theory, we use "Entropy" to quantify the difficulty or complexity of the problem. Entropy represents the expected number of bits required to encode the class information string. If a problem is very complex—meaning the classes are mixed and difficult to distinguish—the entropy will be high. You cannot represent a complex class with a string shorter than its entropy. This gives us a lower bound on the complexity of the decision tree. If we can split the data such that the resulting subsets have lower entropy (are more pure), we have gained information. This concept guides our selection of features: we choose the split that maximally reduces the complexity.

    然而，我们要如何构建这棵树呢？我们不能随意构建，我们需要一个数学基础来选择最重要的属性进行分裂。我们将此设定为一个二分类问题，我们有一组训练样本，其中包含正样本和负样本。根据信息论，我们使用“熵”（Entropy）来量化问题的难度或复杂性。熵代表编码类别信息字符串所需的期望比特数。如果一个问题非常复杂——意味着类别混合在一起难以区分——熵就会很高。你无法用短于其熵的字符串来表示一个复杂的类别。这为决策树的复杂性提供了一个下界。如果我们能对数据进行分裂，使得结果子集的熵更低（更纯），我们就获得了信息。这个概念指导我们对特征的选择：我们选择能够最大程度降低复杂性的分裂方式。

  - [19:17 - 19:22] **Information Gain and Feature Selection Strategy**

    So, we look at the entropy of the original dataset versus the entropy after splitting. The original data has a certain complexity. When we split it into branches based on a feature, we calculate the weighted average entropy of the new subsets. If the new weighted entropy is significantly lower than the original entropy, it means we have a better understanding of the problem; the classification has become easier. The difference between the original entropy and the weighted average entropy of the branches is called "Information Gain." We iterate through all unused features and select the one that provides the maximum Information Gain to be our splitting node.

    因此，我们比较原始数据集的熵与分裂后的熵。原始数据具有一定的复杂性。当我们根据某个特征将其分裂为分支时，我们计算新子集的加权平均熵。如果新的加权熵显著低于原始熵，这意味着我们对问题有了更好的理解；分类变得更容易了。原始熵与分支加权平均熵之间的差值称为“信息增益”。我们遍历所有未使用的特征，并选择能提供最大信息增益的那个特征作为我们的分裂节点。

  - [19:22 - 19:27] **The Tennis Club Example: Calculating Entropy**

    Let's apply this to an example. Consider a dataset for a Tennis Club that records whether tennis players come to play or not based on weather conditions. We have four attributes: Outlook, Temperature, Humidity, and Wind. Let's look at "Outlook." It has three possible values: Sunny, Overcast, and Rain.

    First, calculate the entropy of the whole dataset. Out of 14 days, 9 are "Yes" (Play) and 5 are "No" (Don't Play). The entropy is calculated as `-p_pos * log(p_pos) - p_neg * log(p_neg)`, which gives us 0.94.

    Now, split by Outlook:

    1. **Sunny:** We have 5 samples. 2 are "Yes", 3 are "No". This is mixed, so the entropy is high, calculated as 0.97.
    2. **Overcast:** We have 4 samples. All 4 are "Yes". Since this is purely one class, there is no disorder, so the entropy is exactly 0.
    3. **Rain:** We have 5 samples. 3 are "Yes", 2 are "No". Similar to Sunny, the entropy is 0.97.

    The weighted average entropy for the Outlook split is `(5/14)*0.97 + (4/14)*0 + (5/14)*0.97`, which equals 0.693. The Gain is `0.94 - 0.693 = 0.246`. (Note: In the lecture, I mentioned 0.029 for Temperature, but for Outlook the gain is higher).

    让我们把这个应用到一个例子中。考虑一个网球俱乐部的数据集，它根据天气情况记录网球运动员是否来打球。我们有四个属性：天气状况（Outlook）、温度、湿度和风。让我们看看“天气状况”。它有三个可能的值：晴朗（Sunny）、阴天（Overcast）和雨天（Rain）。

    首先，计算整个数据集的熵。在14天中，9天是“Yes”（打球），5天是“No”（不打球）。熵计算为 `-p_pos * log(p_pos) - p_neg * log(p_neg)`，结果是0.94。

    现在，按天气状况分裂：

    1. **晴朗：** 我们有5个样本。2个“Yes”，3个“No”。这是混合的，所以熵很高，计算结果为0.97。
    2. **阴天：** 我们有4个样本。全部4个都是“Yes”。因为这纯粹是同一类别，没有无序性，所以熵正好是0。
    3. **雨天：** 我们有5个样本。3个“Yes”，2个“No”。与晴朗类似，熵为0.97。

    天气状况分裂的加权平均熵是 `(5/14)*0.97 + (4/14)*0 + (5/14)*0.97`，等于0.693。增益是 `0.94 - 0.693 = 0.246`。（注：课上我提到了温度的增益是0.029，但天气状况的增益更高）。

  - [19:27 - 19:34] **Building the Tree Recursively**

    Based on the calculation, "Outlook" provides a good split. The "Overcast" branch is pure (Entropy = 0), so we make it a leaf node predicting "Play". However, for the "Sunny" branch, we still have mixed results (2 Yes, 3 No). We need to partition further. We look at the remaining attributes (Temperature, Humidity, Wind) for just these 5 sunny days and calculate the gain again to find the next best split. Eventually, we might split "Sunny" by "Humidity" and "Rain" by "Wind". We continue this recursive process until all leaf nodes are pure (Entropy = 0) or we run out of attributes to use.

    基于计算，“天气状况”提供了一个很好的分裂。“阴天”分支是纯的（熵=0），所以我们将它设为预测“Play”的叶节点。然而，对于“晴朗”分支，我们仍然有混合结果（2 Yes，3 No）。我们需要进一步划分。我们仅针对这5个晴天，查看剩余的属性（温度、湿度、风），并再次计算增益以找到下一个最佳分裂。最终，我们可能会通过“湿度”来分裂“晴朗”，通过“风”来分裂“雨天”。我们继续这个递归过程，直到所有叶节点都是纯的（熵=0），或者我们用尽了所有属性。

  - [19:34 - 19:38] **Introduction to Regression Trees: Drug Effectiveness Example**

    Now let's consider a different problem. Suppose we have a new drug that can kill a virus. A doctor wants to determine the optimal dosage or treatment plan for individual patients. Unlike the tennis example where the output was a class (Yes/No), here the drug effectiveness is measured by numbers (e.g., percentage of effectiveness or IC50 values). This makes it a **Regression Problem**.

    If we look at the data—plotting Drug Effectiveness against Dosage—we see it is not a straight line. Effectiveness might be low at low dosage, spike at a certain range, and drop again if the dosage is too high (toxicity). A simple linear regression model cannot fit this data well. We need a model that can handle these non-linear relationships.

    现在让我们考虑一个不同的问题。假设我们要用一种新药来杀灭病毒。医生想要确定针对个体患者的最佳剂量或治疗方案。与网球例子中输出是类别（是/否）不同，这里的药物疗效是用数字衡量的（例如，有效性百分比或IC50值）。这使它成为一个**回归问题**。

    如果我们观察数据——绘制药物疗效与剂量的关系图——我们会发现它不是一条直线。疗效可能在低剂量时很低，在某个范围内激增，如果剂量太高（毒性）又会下降。简单的线性回归模型无法很好地拟合这种数据。我们需要一个能够处理这些非线性关系的模型。

  - [19:38 - 19:42] **Regression Tree Structure and Partitioning**

    The Regression Tree is very similar to the Decision Tree, but the output at each leaf is a numeric value. Imagine the tree splitting the feature space. If we have one feature (e.g., Dosage), the tree partitions the x-axis into distinct regions (intervals). For example, one region might be "Dosage ≤ 1.75".

    In each resulting region (leaf node), the model predicts a constant value. This value is typically the **average** of the target values of the training samples falling into that region. So, instead of a smooth curve, the regression tree models the function as a step function. Each branch of the tree corresponds to one of these flat regions on the graph.

    回归树与决策树非常相似，但每个叶节点的输出是一个数值。想象一下树在分割特征空间。如果我们有一个特征（例如剂量），树会将x轴划分为不同的区域（区间）。例如，一个区域可能是“剂量 ≤ 1.75”。

    在每个结果区域（叶节点）中，模型预测一个常量值。这个值通常是落入该区域的训练样本目标值的**平均值**。因此，回归树不是用平滑曲线，而是将函数建模为阶梯函数。树的每个分支对应图表上的这些平坦区域之一。

  - [19:42 - 19:46] **2D Partitioning and Optimization Task**

    If we have two features (e.g., X1 and X2), the tree partitions the 2D plane into rectangular regions. For example, first split by X2 ≥ 0.3, then split the bottom part by X1 ≥ 0.5. In each rectangular region, we simply calculate the average of the data points inside it to get our prediction.

    Constructing this tree is an optimization task. In machine learning, we usually choose a function class and a loss function, then minimize the loss. Here, our parameters are the **split dimension** (which feature to cut) and the **split value** (where to cut). We iterate through possible splits to find the one that minimizes our error.

    如果我们有两个特征（例如X1和X2），树会将二维平面划分为矩形区域。例如，首先按 X2 ≥ 0.3 分裂，然后按 X1 ≥ 0.5 分裂下半部分。在每个矩形区域中，我们只需计算其中数据点的平均值即可得到我们的预测。

    构建这棵树是一项优化任务。在机器学习中，我们通常选择一个函数类和一个损失函数，然后最小化损失。在这里，我们的参数是**分裂维度**（切割哪个特征）和**分裂值**（在哪里切割）。我们遍历可能的分裂，找到能最小化我们误差的那个。

  - [19:46 - 19:54] **Loss Function and Stopping Criteria (Regularization)**

    For regression trees, we typically use **Squared Loss** (Sum of Squared Residuals) as our metric. We want to find a split that makes the data in the child nodes as "tight" (low variance) as possible.

    However, we must prevent overfitting. A tree that is too deep (one leaf per sample) fits the training data perfectly but fails on new data. To control the size of the tree, we introduce a parameter **k** (minimum samples per leaf).

    The algorithm is:

    1. Check if the number of samples in the current node `|I|` is less than or equal to `k`.
    2. If yes, stop. Create a leaf node and output the average `y` value of those samples. (The average is used because it minimizes the squared loss for a set of numbers).
    3. If no, continue to search for the best split.

    对于回归树，我们通常使用**平方损失**（残差平方和）作为我们的度量标准。我们希望找到一种分裂，使得子节点中的数据尽可能“紧凑”（低方差）。

    但是，我们要防止过拟合。过深的树（每个样本一个叶子）能完美拟合训练数据，但在新数据上会失效。为了控制树的大小，我们引入参数**k**（每个叶节点的最小样本数）。

    算法如下：

    1. 检查当前节点的样本数 `|I|` 是否小于或等于 `k`。
    2. 如果是，停止。创建一个叶节点并输出这些样本的平均 `y` 值。（使用平均值是因为它能最小化一组数值的平方损失）。
    3. 如果否，继续寻找最佳分裂。

  - [19:54 - 19:59] **Calculating the Split: Dosage Example**

    Let's look at the specific calculation for the Drug dataset. We have 19 data points.

    First, consider the **Dosage** dimension. We sort the data by dosage.

    We only need to consider split points between adjacent data values. For 19 points, there are 18 possible gaps (candidate thresholds) to test.

    For each candidate threshold `s`:

    1. Split the data into Left Set (Dosage < s) and Right Set (Dosage ≥ s).
    2. Calculate the average effectiveness for the Left Set and the Right Set independently.
    3. Calculate the Sum of Squared Residuals (SSR) for both sides.

    For example, let's test a split at `s = 3`.

    - **Left:** Only 1 sample (Value=0). Average=0. Residual=`(0-0)^2 = 0`.
    - **Right:** 18 samples. Calculate their average (which is 38.8). Calculate the sum of squared differences from 38.8 for all 18 points.
    - **Total Loss:** Sum of Left SSR + Right SSR = 27,468.5.

    让我们看看药物数据集的具体计算。我们有19个数据点。

    首先，考虑**剂量**维度。我们按剂量对数据进行排序。

    我们只需要考虑相邻数据值之间的分裂点。对于19个点，有18个可能的间隙（候选阈值）需要测试。

    对于每个候选阈值 `s`：

    1. 将数据分为左集合（剂量 < s）和右集合（剂量 ≥ s）。
    2. 分别计算左集合和右集合的平均疗效。
    3. 计算两侧的残差平方和（SSR）。

    例如，让我们测试 `s = 3` 的分裂。

    - **左边：** 只有1个样本（值=0）。平均值=0。残差=`(0-0)^2 = 0`。
    - **右边：** 18个样本。计算它们的平均值（即38.8）。计算所有18个点与38.8的平方差之和。
    - **总损失：** 左SSR + 右SSR之和 = 27,468.5。

  - [19:59 - 20:03] **Selecting the Best Dosage Split**

    **(Student interaction about calculation specifics)**: Yes, if you look at the slide, the "38.8" is the average of the 18 samples on the right side. We calculate the squared residuals for each of those 18 points against 38.8 and sum them up.

    We repeat this calculation for all 18 possible thresholds in the Dosage dimension. It turns out the best split is at **Dosage = 14.5**.

    This split separates the data into:

    - **Left (Dosage < 14.5):** 6 samples (Low effectiveness).

    - **Right (Dosage ≥ 14.5):** 13 samples.

      This gives us a specific SSR value.

    **（关于计算细节的学生互动）**：是的，如果你看幻灯片，“38.8”是右侧18个样本的平均值。我们计算这18个点中每个点与38.8的残差平方，并将它们加起来。

    我们对剂量维度中所有18个可能的阈值重复此计算。结果表明，最佳分裂点是 **剂量 = 14.5**。

    这个分裂将数据分为：

    - **左边（剂量 < 14.5）：** 6个样本（低疗效）。

    - **右边（剂量 ≥ 14.5）：** 13个样本。

      这给了我们一个具体的SSR值。

  - [20:03 - 20:09] **Comparing Across Dimensions and Choosing the Root**

    We are not done. We must also check the other dimensions.

    - **Age:** We check all possible splits for Age. (e.g., Age < 50 vs Age ≥ 50).
    - **Sex:** Since it's binary (Male/Female), there is only one split to check.

    We compare the minimum SSR obtained from the best Dosage split, the best Age split, and the Sex split.

    In this example, splitting by **Age > 50** results in the lowest total Squared Loss (SSR = 12,017, as seen on the slide). Therefore, **Age** is chosen as the root node of our tree.

    One branch has 6 samples, and the other has 13. We then recursively apply the same process to these subsets. For the subset of 16 samples (Wait, the slide says 6 and 13, let's stick to the recursive logic), we check Dosage and Sex again to split further. We stop when the number of samples is small (e.g., k=6 or 7).

    OK, we will take a break here.

    我们还没做完。我们必须检查其他维度。

    - **年龄：** 我们检查年龄的所有可能分裂。（例如，年龄 < 50 对比 年龄 ≥ 50）。
    - **性别：** 因为它是二元的（男/女），只有一个分裂需要检查。

    我们比较从最佳剂量分裂、最佳年龄分裂和性别分裂中获得的最小SSR。

    在这个例子中，按 **年龄 > 50** 分裂导致了最低的总平方损失（SSR = 12,017，如幻灯片所示）。因此，**年龄**被选为我们树的根节点。

    一个分支有6个样本，另一个有13个。然后我们对这些子集递归地应用相同的过程。对于16个样本的子集（等等，幻灯片说是6和13，我们要遵循递归逻辑），我们会再次检查剂量和性别以进行进一步分裂。当样本数量很少（例如k=6或7）时，我们就停止。

    好的，我们在这里休息一下。

  - [20:32 - 20:35] **Bagging: Sampling with Replacement**

    Now, look at the data tables on the slide. You will see something interesting: the first data point in the new dataset is identical to the last two. Some samples from the original dataset are missing, while others appear multiple times. This is exactly what I mean by "Sampling with Replacement." This technique is called **Bagging** (Bootstrap Aggregating). The idea is simple:

    1. **Bootstrap:** You produce multiple datasets (e.g., $D_1, D_2, \dots, D_k$) from the original data $D$. Each new dataset is the same size as the original but is created by randomly picking samples one by one, putting them back (replacement), and picking again. This creates variations in the training data.
    2. **Aggregating:** You build a model for each bootstrapped dataset. Then, you combine their predictions. For regression problems, you take the **average** of all model outputs. For classification, you take a **majority vote** (like voting 0 or 1).

    现在，请看幻灯片上的数据表。你会发现一些有趣的现象：新数据集中的第一个数据点与最后两个完全相同。原始数据集中的某些样本缺失了，而其他样本则出现了多次。这就是我所说的“有放回抽样”（Sampling with Replacement）。这种技术称为**Bagging**（Bootstrap Aggregating，自助汇聚法）。其思想很简单：

    1. **Bootstrap（自助法）：** 你从原始数据 $D$ 中生成多个数据集（例如 $D_1, D_2, \dots, D_k$）。每个新数据集的大小与原始数据集相同，但通过逐个随机抽取样本、放回（置换）、再抽取的方式创建。这在训练数据中创造了变异性。
    2. **Aggregating（汇聚）：** 你为每个自助数据集构建一个模型。然后，结合它们的预测结果。对于回归问题，你取所有模型输出的**平均值**。对于分类问题，你取**多数投票**（就像投0或1一样）。

  - [20:35 - 20:41] **Random Forest: Adding Extra Randomness**

    Random Forest is essentially Bagging applied to decision trees, but with an extra trick to make it better. The trick is how we build the trees. If we just used standard decision trees on bootstrapped data, the trees might still be very similar (correlated) because they would all pick the same strong features at the top.

    To solve this, Random Forest introduces **feature randomness**. At each node splitting step, instead of looking at *all* available features to find the best split, we randomly select a subset of **m** features (where $m < total\ features$). We pick the best split *only* from this random subset.

    Why is this good? It forces the trees to be different. It "decorrelates" them. Imagine a committee where everyone has the exact same information and bias; their "average" opinion adds no value. But if committee members (trees) look at different aspects (features) of the problem, their combined decision is much wiser and more robust. By choosing a random subset of features, we ensure a high degree of independence between the trees.

    随机森林本质上是应用于决策树的Bagging，但增加了一个额外的技巧使其变得更好。这个技巧在于我们如何构建树。如果我们仅在自助数据上使用标准决策树，树与树之间可能仍然非常相似（相关），因为它们都会在顶部选择相同的强特征。

    为了解决这个问题，随机森林引入了**特征随机性**。在每个节点的分类步骤中，我们不是查看*所有*可用特征来寻找最佳分裂，而是随机选择**m**个特征的子集（其中 $m < 总特征数$）。我们*仅*在这个随机子集中选择最佳分裂。

    为什么这很好？它迫使树变得不同。它使它们“去相关”。想象一个委员会，如果每个人都拥有完全相同的信息和偏见，他们的“平均”意见就没有任何附加价值。但是，如果委员会成员（树）观察问题的不同方面（特征），他们的综合决策就会更加明智和稳健。通过选择随机特征子集，我们确保了树之间的高度独立性。

  - [20:41 - 20:44] **Validation: Out-of-Bag (OOB) Accuracy**

    How do we estimate the accuracy of a Random Forest? We don't necessarily need a separate validation set because of the bagging process. Recall that when we create a bootstrap sample, some original data points are left out. These are called **Out-of-Bag (OOB)** samples.

    For each tree in the forest, there are specific samples that were *not* used to train that specific tree. We can use these "leftover" samples to test that tree's performance. By averaging the prediction errors on the OOB samples across the entire forest, we get a reliable estimate of the model's accuracy without wasting training data on a hold-out set.

    我们如何估计随机森林的准确性？由于Bagging过程，我们不一定需要单独的验证集。回想一下，当我们创建一个自助样本时，一些原始数据点被留在了外面。这些被称为**袋外（Out-of-Bag, OOB）**样本。

    对于森林中的每棵树，都有一些特定的样本*没有*被用来训练这棵特定的树。我们可以使用这些“剩余”的样本来测试那棵树的性能。通过计算整个森林在OOB样本上的平均预测误差，我们可以获得模型准确性的可靠估计，而无需将训练数据浪费在留出集上。

  - [20:44 - 20:49] **Hyperparameters: n_estimators and max_features**

    The performance of a Random Forest depends on key hyperparameters.

    1. **n_estimators (Number of Trees):** How many trees should we build? If you use too few trees (e.g., 3 or 4), the model is unstable and has high variance. As you add more trees, performance improves. Eventually, the performance plateaus—adding more trees after a certain point (e.g., 50 vs 100) doesn't gain much accuracy, it just costs more computation. You want to find that stable point.
    2. **max_features (Size of random subset):** How many features ($m$) do we check at each split?
       - For **Classification**, a common heuristic (default) is $m = \sqrt{p}$, where $p$ is the total number of features.
       - For **Regression**, the default is often $m = p/3$.

    Smaller `max_features` leads to more diversity (less correlation) among trees but individual trees might be weaker. Larger `max_features` makes trees stronger individually but more correlated. This is a trade-off you tune through validation.

    随机森林的性能取决于关键的超参数。

    1. **n_estimators（树的数量）：** 我们应该构建多少棵树？如果你使用的树太少（例如3或4棵），模型会不稳定且方差很高。随着你增加更多的树，性能会提高。最终，性能会趋于平稳——在某一点之后增加更多的树（例如50对比100）并不会增加太多准确性，只会消耗更多的计算资源。你需要找到那个稳定点。
    2. **max_features（随机子集的大小）：** 我们在每次分裂时检查多少个特征（$m$）？
       - 对于**分类**，一个常见的启发式（默认值）是 $m = \sqrt{p}$，其中 $p$ 是总特征数。
       - 对于**回归**，默认值通常是 $m = p/3$。

    较小的 `max_features` 会导致树之间更多的多样性（相关性更低），但单棵树可能会更弱。较大的 `max_features` 使单棵树更强，但相关性更高。这是一个你需要通过验证来调整的权衡。

  - [20:53 - 20:57] **XGBoost: The Iterative Boosting Concept**

    Now we move to XGBoost. This is a very effective model, but the core idea is actually very simple. Unlike Random Forest, where we build trees in parallel, here we repeatedly build new tree models one by one and add them into an ensemble. Mathematically, this is an additive process. You start with a base model, and then you add a second tree, then a third tree, and so on. But here is the key difference: when you build the second model, you do not build it on the original target values. Instead, you build it based on the *errors* of the previous model. We call these "residuals." If the current model has a prediction error on a data point, the next tree's job is to predict that specific error. You repeat this process—calculating residuals, fitting a new tree to them, and adding it to the ensemble—until you get a good final prediction.

    现在我们讲到XGBoost。这是一个非常有效的模型，但其核心思想实际上非常简单。与并行构建树的随机森林不同，这里我们反复地逐一构建新的树模型，并将它们加入到集成模型中。在数学上，这是一个加法过程。你从一个基础模型开始，然后加入第二棵树，再加入第三棵树，以此类推。但这里的关键区别在于：当你构建第二个模型时，你不是基于原始目标值构建的。相反，你是基于前一个模型的*误差*来构建的。我们称这些为“残差”（residuals）。如果当前模型在一个数据点上有预测误差，下一棵树的任务就是预测那个特定的误差。你重复这个过程——计算残差，拟合一棵新树来修正它们，并将其加入到集成模型中——直到你获得一个好的最终预测。

  - [20:57 - 20:59] **Hyperparameters: Learning Rate and Regularization**

    When we add these new trees into the model, we don't just add them directly. We introduce a hyperparameter called the "Learning Rate" (often denoted as eta). This value is usually very small, like 0.1 or 0.3. Why? Because we don't want any single tree to change the model too drastically. If a tree is bad, we don't want it to cause too much damage. By multiplying the new tree's output by a small learning rate, we ensure that each tree only makes a small contribution or correction. This means we need a large number of trees to reach the final solution, but the process is much more stable and avoids overfitting. While standard Gradient Boosting optimizes the variance, XGBoost is famous because it also includes unique regularization terms in its objective function to further prevent overfitting. It balances bias and variance very well.

    当我们把这些新树加入模型时，我们不会直接加入。我们引入了一个称为“学习率”（通常表示为eta）的超参数。这个值通常很小，比如0.1或0.3。为什么？因为我们不希望任何单独的一棵树过激地改变模型。如果一棵树很差，我们不希望它造成太大的破坏。通过将新树的输出乘以一个小的学习率，我们确保每棵树只做出微小的贡献或修正。这意味着我们需要大量的树来达到最终解，但这个过程要稳定得多，并且能避免过拟合。虽然标准的梯度提升（Gradient Boosting）优化了方差，但XGBoost之所以出名，是因为它在目标函数中包含了独特的正则化项，以进一步防止过拟合。它很好地平衡了偏差和方差。

  - [20:59 - 21:03] **Step-by-Step Example: Initial Prediction and Residuals**

    Let's walk through a concrete example using the dataset on the slide. We have features like Height, Color, and Gender, and we want to predict a person's Weight. This is a regression problem.

    **Step 1:** We create an initial "naive" model. The simplest possible guess is just the average of all target values. In this dataset, the average weight is 71.2 kg. So, our first model (F0) predicts 71.2 for everyone.

    **Step 2:** Now we calculate the **Residuals** for each data point. The residual is simply the Real Value minus the Predicted Value.

    Take the first data point: Ideally, the weight is 88 kg. Our model predicted 71.2 kg.

    The residual is `88 - 71.2 = 16.8`.

    This "16.8" represents the error—the part of the weight our first model failed to explain. We do this for all data points (e.g., the second point has a residual of 4.8).

    让我们用幻灯片上的数据集来做一个具体的例子。我们有身高、颜色和性别等特征，我们想要预测一个人的体重。这是一个回归问题。

    **第一步：** 我们创建一个初始的“朴素”模型。最简单的猜测就是所有目标值的平均值。在这个数据集中，平均体重是71.2公斤。所以，我们的第一个模型（F0）对所有人的预测都是71.2。

    **第二步：** 现在我们要为每个数据点计算**残差**。残差就是“真实值”减去“预测值”。

    以第一个数据点为例：理想情况下，体重是88公斤。我们的模型预测是71.2公斤。

    残差是 `88 - 71.2 = 16.8`。

    这个“16.8”代表了误差——即我们的第一个模型未能解释的那部分体重。我们对所有数据点都这样做（例如，第二个点的残差是4.8）。

  - [21:03 - 21:07] **Boosting Iteration: Updating with the Second Tree**

    **Step 3:** Now we build the second model (Tree 1). Crucially, this tree does *not* try to predict the weight (88 kg). It tries to predict the *residual* (16.8). We use the features (Height, Color, Gender) to build a decision tree that fits these residual values.

    Looking at the tree on the slide, let's trace the path for that first data point (Male, Height 1.6, Blue).

    - Is it Female? No.

    - Is Height < 1.6? No.

    - Is it not Blue? No (it is Blue).

      This path leads to a leaf node that outputs a prediction close to the residual, let's say 16.8.

      **Step 4:** We update our ensemble prediction. We take the initial guess (71.2) and add the new tree's prediction scaled by the learning rate (0.1).

      The adjustment is `0.1 * 16.8 = 1.68`.

      So the new prediction is `71.2 + 1.68 = 72.88`.

      Finally, we calculate the **Updated Residual**: `88 - 72.88 = 15.1`.

      You can see the residual has dropped from 16.8 to 15.1. The error got smaller! We then repeat this process, building a third tree to fit the new residual of 15.1, and so on.

    **第三步：** 现在我们要构建第二个模型（树1）。关键在于，这棵树*不*尝试预测体重（88公斤）。它尝试预测*残差*（16.8）。我们利用特征（身高、颜色、性别）来构建一棵决策树，去拟合这些残差值。

    看幻灯片上的树，让我们追踪第一个数据点（男性，身高1.6，蓝色）的路径。

    - 是女性吗？否。

    - 身高 < 1.6吗？否。

    - 不是蓝色吗？否（它是蓝色）。

      这条路径通向一个叶节点，输出一个接近残差的预测值，假设是16.8。

      **第四步：** 我们更新集成模型的预测。我们取初始猜测（71.2），加上乘以学习率（0.1）后的新树预测值。

      调整量是 `0.1 * 16.8 = 1.68`。

      所以新的预测是 `71.2 + 1.68 = 72.88`。

      最后，我们计算**更新后的残差**：`88 - 72.88 = 15.1`。

      你可以看到残差从16.8下降到了15.1。误差变小了！然后我们重复这个过程，构建第三棵树来拟合15.1这个新残差，以此类推。

  - [21:11 - 21:14] **The Role of Learning Rate and Regularization**

    So, why do we do this? Why do we multiply the tree's output by 0.1? This 0.1 is the **Learning Rate** (or shrinkage). We never want to fully trust a single tree because it might be overfitting the specific residuals it saw. If we simply added the full tree, the model would change too drastically. Therefore, we multiply the prediction by this small factor (0.1) so that this specific tree only contributes a small part to the final ensemble. It forces the model to learn slowly and incrementally. Additionally, XGBoost is special because it includes a **regularization term** in its objective function. This penalizes complex trees, further ensuring that no single tree dominates or overfits the data.

    那么，我们为什么要这样做？我们为什么要将树的输出乘以0.1？这个0.1就是**学习率**（Learning Rate，或称为收缩步长）。我们从不希望完全信任单独的一棵树，因为它可能会过拟合它所看到的特定残差。如果我们简单地加上整棵树，模型的变化就会过于剧烈。因此，我们将预测值乘以这个小因子（0.1），使得这棵特定的树只对最终的集成模型做出很小的贡献。这迫使模型缓慢且增量地学习。此外，XGBoost的特殊之处在于它在目标函数中包含了一个**正则化项** 。这会惩罚复杂的树，进一步确保没有单独的一棵树能主导模型或过拟合数据。

  - [21:14 - 21:15] **Iterative Model Refinement**

    So this is the process: You have the first model (the initial average). Then you build the second tree to fit the errors. Now you have a combined model: `Initial + (0.1 * Tree1)`. Is this complete? No, it is still not perfect. There is still an error (e.g., the residual dropped to 15.1, but it's not zero). So, we do this again. Another step, and another step. We determine the hyperparameters—specifically **how many trees** (n_estimators) to build and what learning rate to use. You repeat this loop until the error is sufficiently small or you reach the maximum number of trees.

    这就是整个过程：你有第一个模型（初始平均值）。然后你构建第二棵树来拟合误差。现在你有了一个组合模型：`初始值 + (0.1 * 树1)`。这完成了吗？不，它仍然不完美。仍然存在误差（例如，残差降到了15.1，但不是零）。所以，我们再做一次。一步接一步。我们需要确定超参数——具体是**构建多少棵树**（n_estimators）以及使用什么学习率。你重复这个循环，直到误差足够小或者达到了树的最大数量。

  - [21:15 - 21:16] **XGBoost's Industry Status**

    This is a very popular model. It is widely used in competitions like Kaggle and the Netflix Prize. In fact, it is so effective that even modern Deep Learning papers focusing on tabular data are required to compare their results against XGBoost. It remains the state-of-the-art baseline for structured data problems. It is a powerful tool that you should know.

    这是一个非常流行的模型。它广泛应用于Kaggle和Netflix Prize等竞赛中 。事实上，它非常有效，以至于即使是现代专注于表格数据的深度学习论文，也被要求将结果与XGBoost进行比较 。它仍然是结构化数据问题的最先进基准。这是一个你应该掌握的强大工具。
***

## Syllabus
  - **Course Description** This course offers an accessible introduction to machine learning, tailored for individuals from diverse backgrounds. As part of the 'AI and Innovation' program, it equips students with the fundamental knowledge and skills needed to effectively deploy AI-based solutions for tasks in industry, business, healthcare and daily life.  Emphasizing foundational concepts and intuitive understanding, the course ensures that even those with limited quantitative skills can grasp the core ideas. Through hands-on exercises and real-world case studies, students will become adept at utilizing machine learning tools, empowering them to contribute effectively to AI-driven initiatives in their respective fields.
  - **Workload hours**
    | Activity                         | Total Duration | Remarks |
    | -------------------------------- | -------------- | ------- |
    | Lecture                          | 3              |         |
    | Tutorial                         | 0              |         |
    | Laboratory                       | 0              |         |
    | Project/Assignment               | 3              |         |
    | Workshop/Seminar/Fieldwork       | 0              |         |
    | Others, include preparatory work | 4              |         |
  - **Learning outcomes** By the end of this course, participants will be able to:
    - Understand the foundational concepts of AI and machine learning and their various applications.
    - Utilize AI and machine learning tools and frameworks to analyze and interpret data.
    - Design and implement basic AI and machine learning models.
    - Critically evaluate the performance of AI and machine learning models.
    - Apply AI and machine learning techniques to innovative projects within their respective fields.
  - **Week 1** Introduction to AI and Machine Learning
    - Overview of AI: History and Impact
    - Basic Concepts and Definitions in Machine Learning
    - Real-World Applications of AI and Machine Learning
  - **Week 2** Understanding and Preparing Data for AI
    - Types of Data and Their Uses in AI
    - Ensuring Data Quality for AI Applications
  - **Week 3** Basic Techniques in Supervised Learning for AI
    - Understanding Regression and Classification in AI
    - Practical Examples with User-Friendly AI Tools
    - Hands-On Exercises: Building Simple AI Models
  - **Week 4** Exploring Unsupervised Learning in AI
    - Introduction to Clustering and Grouping Data in AI
    - Practical Applications of Unsupervised Learning in AI
  - **Week 5** Evaluating and Validating AI Models
    - Easy-to-Understand Performance Metrics for AI
    - Methods to Test and Improve AI Model Accuracy
    - Understanding Bias and Fairness in AI
  - **Assessment components**
    | CA Component                                 | % Weightage | Remarks |
    | -------------------------------------------- | ----------- | ------- |
    | Class Participation                          | 0%          |         |
    | Essays                                       | 0%          |         |
    | Project/Group Project                        | 30%         |         |
    | Quizzes/Tests                                | 30%         |         |
    | Laboratory Tests                             | 0%          |         |
    | Mid-term Test                                | 0%          |         |
    | Others 1 (if applicable & describe in notes) | 0%          |         |
    | Others 2 (if applicable & describe in notes) | 0%          |         |
    | Others 3 (if applicable & describe in notes) | 0%          |         |
    | Final Exam                                   | 40%         |         |
***
