## Yolov7 torch model test
  ```py
  import torch
  from models.experimental import attempt_load

  model = attempt_load('yolov7.pt', map_location='cpu')
  _ = model.eval()
  _ = model(torch.zeros([1, 3, 224, 224]))

  sys.path.append('../keras_cv_attention_models/')
  from keras_cv_attention_models import download_and_load
  download_and_load.try_save_pth_and_onnx(model)
  ```
  ```py
  sys.path.append('../yolov7')
  import torch
  from models.experimental import Ensemble

  model = Ensemble()
  weight = 'yolov7.pt'
  ckpt = torch.load(weight, map_location="cpu")  # load
  model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().eval())  # FP32 model

  # Compatibility updates
  for m in model.modules():
      if type(m) is torch.nn.Upsample:
          m.recompute_scale_factor = None  # torch 1.11.0 compatibility

  sys.path.append('../keras_cv_attention_models/')
  from keras_cv_attention_models import download_and_load
  download_and_load.try_save_pth_and_onnx(model[-1], save_pth=False, save_name='yolov7')
  ```
## Convert weights
  ```py
  sys.path.append('../yolov7')
  ss = torch.load('yolov7.pt', map_location="cpu")['model'].state_dict()
  # ss = {kk: vv for kk, vv in ss.items() if not(kk.endswith('anchors') or kk.endswith('anchors_grid'))}

  from keras_cv_attention_models import download_and_load
  from keras_cv_attention_models.yolov7 import yolov7
  mm = yolov7.YOLOV7_CSP()

  tail_align_dict = {
    "first_conv": -9, "first_bn": -10,
    "downsample_pool_conv": -3, "downsample_pool_bn": -4,
  }

  full_name_align_dict = {
    "stack4_spp_short_conv": -7, "stack4_spp_short_bn": -8, "stack4_spp_output_bn": -1,
    "pafpn_p4p5_up_conv": -2, "pafpn_p4p5_up_bn": -2, "pafpn_p4p5_first_conv": -9, "pafpn_p4p5_first_bn": -10, "pafpn_p4p5_out_bn": -1,
    "pafpn_p3p4p5_up_conv": -2, "pafpn_p3p4p5_up_bn": -2, "pafpn_p3p4p5_first_conv": -9, "pafpn_p3p4p5_first_bn": -10,
    "pafpn_c3n3_pool_conv": -3, "pafpn_c3n3_pool_bn": -4, "pafpn_c3n3_first_conv": -9, "pafpn_c3n3_first_bn": -10,
    "pafpn_c3n4_pool_conv": -3, "pafpn_c3n4_pool_bn": -4, "pafpn_c3n4_first_conv": -9, "pafpn_c3n4_first_bn": -10,
    "head_1_3x3_bn": -5, "head_1_1x1_bn": -4, "head_2_3x3_bn": -3, "head_2_1x1_bn": -2,
    "head_3_3x3_bn": -1
  }

  skip_weights = ["num_batches_tracked", "anchor_grid", "anchors"]

  download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      skip_weights=skip_weights,
      tail_align_dict=tail_align_dict,
      tail_split_position=1,
      full_name_align_dict=full_name_align_dict,
      save_name=mm.name + "_coco.h5",
      do_convert=True,
  )
  ```
  **Convert bboxes output `[left, top, right, bottom]` -> `top, left, bottom, right`**
  ```py
  from keras_cv_attention_models.yolov7 import yolov7
  mm = yolov7.YOLOV7_CSP(pretrained="yolov7_csp_coco.h5")
  for ii in range(1, 4):
      layer_name = 'head_{}_2_conv'.format(ii)
      print(f">>>> {layer_name = }")
      conv_layer = mm.get_layer(layer_name)
      new_ww = []
      for ww in conv_layer.get_weights():
          ww = np.reshape(ww, [*ww.shape[:-1], 3, 85])[..., [1, 0, 3, 2, *np.arange(5, 85), 4]]
          ww = np.reshape(ww, [*ww.shape[:-2], -1])
          new_ww.append(ww)
      conv_layer.set_weights(new_ww)

  nn = yolor.YOLOR_CSP(pretrained="coco")
  aa = nn(tf.ones([1, *nn.input_shape[1:]]))
  bb = mm(tf.ones([1, *mm.input_shape[1:]]))
  print(np.allclose(aa, bb.numpy()[:, :, [1, 0, 3, 2, 84, *np.arange(4, 84)]]))
  # True
  mm.save(mm.name + "_coco.h5")


  from keras_cv_attention_models import test_images, coco
  imm = test_images.dog_cat()
  preds = mm(mm.preprocess_input(imm))
  bboxs, lables, confidences = mm.decode_predictions(preds)[0]
  coco.show_image_with_bboxes(imm, bboxs, lables, confidences, num_classes=80)
  ```
## Detection
