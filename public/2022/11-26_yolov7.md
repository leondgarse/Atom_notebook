## Yolov7 torch model test
  ```py
  sys.path.append('../yolov7')
  import torch
  from models.experimental import attempt_load
  from utils.general import non_max_suppression

  model = attempt_load('yolov7.pt', map_location='cpu')
  _ = model.eval()

  from skimage.data import chelsea
  from skimage.transform import resize
  imm = resize(chelsea(), [640, 640])  # 测试图片,图片值范围 [0, 1]

  pred = model(torch.from_numpy(imm.astype('float32'))[None].permute([0, 3, 1, 2]))  # 前向
  # print(non_max_suppression(pred))  # 输出
  print(non_max_suppression(pred[0]))  # 输出
  ```
  ```py
  sys.path.append('../yolov7')
  import torch
  from models.experimental import Ensemble

  model = Ensemble()
  weight = 'yolov7.pt'
  ckpt = torch.load(weight, map_location="cpu")  # load
  model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().eval())  # FP32 model

  # Compatibility updates
  for m in model.modules():
      if type(m) is torch.nn.Upsample:
          m.recompute_scale_factor = None  # torch 1.11.0 compatibility

  from keras_cv_attention_models import download_and_load
  download_and_load.try_save_pth_and_onnx(model[-1], input_shape=(1, 3, 640, 640), save_pth=False, save_name='yolov7')
  ```
## Convert weights
  ```py
  sys.path.append('../yolov7')
  import torch
  ss = torch.load('yolov7.pt', map_location="cpu")['model'].state_dict()
  # ss = {kk: vv for kk, vv in ss.items() if not(kk.endswith('anchors') or kk.endswith('anchors_grid'))}

  from keras_cv_attention_models import download_and_load
  from keras_cv_attention_models.yolov7 import yolov7
  mm = yolov7.YOLOV7_CSP()

  tail_align_dict = {"downsample_pool_conv": -3, "downsample_pool_bn": -4}

  full_name_align_dict = {
    "stack1_1_conv": "stack1_2_conv", "stack1_1_bn": "stack1_2_bn", "stack2_1_conv": "stack2_2_conv", "stack2_1_bn": "stack2_2_bn",
    "stack3_1_conv": "stack3_2_conv", "stack3_1_bn": "stack3_2_bn", "stack4_1_conv": "stack4_2_conv", "stack4_1_bn": "stack4_2_bn",
    "stack4_spp_short_conv": "stack4_spp_pre_1_conv", "stack4_spp_short_bn": "stack4_spp_pre_1_bn", "stack4_spp_output_bn": -1,
    "stack5_spp_output_bn": -1, # YOLOV7_W6
    "pafpn_p4p5_up_conv": -2, "pafpn_p4p5_up_bn": -2, "pafpn_p4p5_1_conv": "pafpn_p4p5_2_conv", "pafpn_p4p5_1_bn": "pafpn_p4p5_2_bn", "pafpn_p4p5_out_bn": -1,
    "pafpn_p3p4p5_up_conv": -2, "pafpn_p3p4p5_up_bn": -2, "pafpn_p3p4p5_1_conv": "pafpn_p3p4p5_2_conv", "pafpn_p3p4p5_1_bn": "pafpn_p3p4p5_2_bn",
    "pafpn_c3n3_pool_conv": -3, "pafpn_c3n3_pool_bn": -4, "pafpn_c3n3_1_conv": "pafpn_c3n3_2_conv", "pafpn_c3n3_1_bn": "pafpn_c3n3_2_bn",
    "pafpn_c3n4_pool_conv": -3, "pafpn_c3n4_pool_bn": -4, "pafpn_c3n4_1_conv": "pafpn_c3n4_2_conv", "pafpn_c3n4_1_bn": "pafpn_c3n4_2_bn",
    "head_1_3x3_bn": -5, "head_1_1x1_bn": -4, "head_2_3x3_bn": -3, "head_2_1x1_bn": -2, "head_3_3x3_bn": -1,  # YOLOV7_CSP
    "head_1_1_bn": -2, "head_2_1_bn": -1,  # YOLOV7_X
  }

  skip_weights = ["num_batches_tracked", "anchor_grid", "anchors"]

  download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      skip_weights=skip_weights,
      tail_align_dict=tail_align_dict,
      tail_split_position=1,
      full_name_align_dict=full_name_align_dict,
      save_name=mm.name + "_coco.h5",
      do_convert=True,
  )
  ```
  ```py
  sys.path.append('../yolov7')
  import torch
  ss = torch.load('yolov7.pt', map_location="cpu")['model'].state_dict()
  # ss = {kk: vv for kk, vv in ss.items() if not(kk.endswith('anchors') or kk.endswith('anchors_grid'))}

  from keras_cv_attention_models import download_and_load
  from keras_cv_attention_models.yolov7 import yolov7
  mm = yolov7.YOLOV7_CSP()

  tail_align_dict = {"pafpn": {"up_conv": -2, "up_bn": -2, "1_conv": "2_conv", "1_bn": "2_bn", "pool_conv": -3, "pool_bn": -4}}

  full_name_align_dict = {
    "stack1_1_conv": "stack1_2_conv", "stack1_1_bn": "stack1_2_bn", "stack2_1_conv": "stack2_2_conv", "stack2_1_bn": "stack2_2_bn",
    "stack3_1_conv": "stack3_2_conv", "stack3_1_bn": "stack3_2_bn", "stack4_1_conv": "stack4_2_conv", "stack4_1_bn": "stack4_2_bn",
    "stack5_1_conv": "stack5_2_conv", "stack5_1_bn": "stack5_2_bn",  # YOLOV7_W6
    "stack2_downsample_pool_conv": -3, "stack2_downsample_pool_bn": -4,
    "stack3_downsample_pool_conv": -3, "stack3_downsample_pool_bn": -4,
    "stack4_downsample_pool_conv": -3, "stack4_downsample_pool_bn": -4,
    "stack4_spp_short_conv": -7, "stack4_spp_short_bn": -8, "stack4_spp_output_bn": -1,
    "stack5_spp_short_conv": -7, "stack5_spp_short_bn": -8, "stack5_spp_output_bn": -1,  # YOLOV7_W6
    "stack5_spp_output_bn": -1,  # YOLOV7_W6
    # "pafpn_p4p5_up_conv": -2, "pafpn_p4p5_up_bn": -2, "pafpn_p4p5_1_conv": "pafpn_p4p5_2_conv", "pafpn_p4p5_1_bn": "pafpn_p4p5_2_bn",
    # "pafpn_p3p4p5_up_conv": -2, "pafpn_p3p4p5_up_bn": -2, "pafpn_p3p4p5_1_conv": "pafpn_p3p4p5_2_conv", "pafpn_p3p4p5_1_bn": "pafpn_p3p4p5_2_bn",
    # "pafpn_c3n3_pool_conv": -3, "pafpn_c3n3_pool_bn": -4, "pafpn_c3n3_1_conv": "pafpn_c3n3_2_conv", "pafpn_c3n3_1_bn": "pafpn_c3n3_2_bn",
    # "pafpn_c3n4_pool_conv": -3, "pafpn_c3n4_pool_bn": -4, "pafpn_c3n4_1_conv": "pafpn_c3n4_2_conv", "pafpn_c3n4_1_bn": "pafpn_c3n4_2_bn",
    "pafpn_p4p5_out_bn": -1, "pafpn_p5p6_out_bn": -1, "pafpn_p4p5p6_out_bn": -1,
    "head_1_3x3_bn": -5, "head_1_1x1_bn": -4, "head_2_3x3_bn": -3, "head_2_1x1_bn": -2, "head_3_3x3_bn": -1,  # YOLOV7_CSP
    # "head_1_1_bn": -2, "head_2_1_bn": -1,  # YOLOV7_X [comment off for YOLO_W6]
    "head_1_1_bn": -3, "head_2_1_bn": -2, "head_3_1_bn": -1,  # YOLO_W6 [comment off for YOLOV7_X]

  }

  skip_weights = ["num_batches_tracked", "anchor_grid", "anchors"]

  download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      skip_weights=skip_weights,
      tail_align_dict=tail_align_dict,
      tail_split_position=2,
      full_name_align_dict=full_name_align_dict,
      save_name=mm.name + "_coco.h5",
      do_convert=True,
  )
  ```
  ```py
  sys.path.append('../yolov7')
  import torch
  ss = torch.load('yolov7-e6.pt', map_location="cpu")['model'].state_dict()
  # ss = {kk: vv for kk, vv in ss.items() if not(kk.endswith('anchors') or kk.endswith('anchors_grid'))}

  from keras_cv_attention_models import download_and_load
  from keras_cv_attention_models.yolov7 import yolov7
  mm = yolov7.YOLOV7_E6()

  tail_align_dict = {"pafpn": {"up_conv": -2, "up_bn": -2, "1_conv": "2_conv", "1_bn": "2_bn", "conv_2_bn": -1}}

  full_name_align_dict = {
    "stack1_1_conv": "stack1_2_conv", "stack1_1_bn": "stack1_2_bn", "stack2_1_conv": "stack2_2_conv", "stack2_1_bn": "stack2_2_bn",
    "stack3_1_conv": "stack3_2_conv", "stack3_1_bn": "stack3_2_bn", "stack4_1_conv": "stack4_2_conv", "stack4_1_bn": "stack4_2_bn",
    "stack5_1_conv": "stack5_2_conv", "stack5_1_bn": "stack5_2_bn",  # YOLOV7_W6
    "stack1_downsample_conv_2_bn": -1, "stack2_downsample_conv_2_bn": -1, "stack3_downsample_conv_2_bn": -1, "stack4_downsample_conv_2_bn": -1,  # YOLOV7_E6
    "stack5_downsample_conv_2_bn": -1,  # YOLOV7_E6
    "stack4_spp_short_conv": -7, "stack4_spp_short_bn": -8, "stack4_spp_output_bn": -1,
    "stack5_spp_short_conv": -7, "stack5_spp_short_bn": -8, "stack5_spp_output_bn": -1,  # YOLOV7_W6
    "stack5_spp_output_bn": -1,  # YOLOV7_W6
    # "pafpn_p4p5_up_conv": -2, "pafpn_p4p5_up_bn": -2, "pafpn_p4p5_1_conv": "pafpn_p4p5_2_conv", "pafpn_p4p5_1_bn": "pafpn_p4p5_2_bn",
    # "pafpn_p3p4p5_up_conv": -2, "pafpn_p3p4p5_up_bn": -2, "pafpn_p3p4p5_1_conv": "pafpn_p3p4p5_2_conv", "pafpn_p3p4p5_1_bn": "pafpn_p3p4p5_2_bn",
    # "pafpn_c3n3_pool_conv": -3, "pafpn_c3n3_pool_bn": -4, "pafpn_c3n3_1_conv": "pafpn_c3n3_2_conv", "pafpn_c3n3_1_bn": "pafpn_c3n3_2_bn",
    # "pafpn_c3n4_pool_conv": -3, "pafpn_c3n4_pool_bn": -4, "pafpn_c3n4_1_conv": "pafpn_c3n4_2_conv", "pafpn_c3n4_1_bn": "pafpn_c3n4_2_bn",
    "pafpn_p4p5_out_bn": -1, "pafpn_p5p6_out_bn": -1, "pafpn_p4p5p6_out_bn": -1,
    "head_1_3x3_bn": -5, "head_1_1x1_bn": -4, "head_2_3x3_bn": -3, "head_2_1x1_bn": -2, "head_3_3x3_bn": -1,  # YOLOV7_CSP
    # "head_1_1_bn": -2, "head_2_1_bn": -1,  # YOLOV7_X [comment off for YOLO_W6]
    "head_1_1_bn": -3, "head_2_1_bn": -2, "head_3_1_bn": -1,  # YOLO_W6 [comment off for YOLOV7_X]

  }

  skip_weights = ["num_batches_tracked", "anchor_grid", "anchors"]

  download_and_load.keras_reload_from_torch_model(
      ss,
      mm,
      skip_weights=skip_weights,
      tail_align_dict=tail_align_dict,
      tail_split_position=2,
      full_name_align_dict=full_name_align_dict,
      save_name=mm.name + "_coco.h5",
      do_convert=True,
  )
  ```
  **Convert bboxes output `[left, top, right, bottom]` -> `top, left, bottom, right`**
  ```py
  from keras_cv_attention_models.yolov7 import yolov7
  mm = yolov7.YOLOV7_CSP(pretrained="yolov7_csp_coco.h5")
  heads = [ii.name for ii in mm.layers if ii.name.startswith('head')]
  for ii in range(1, 100):
      layer_name = 'head_{}_2_conv'.format(ii)
      print(f">>>> {layer_name = }")
      if layer_name not in heads:
          break
      conv_layer = mm.get_layer(layer_name)
      new_ww = []
      for ww in conv_layer.get_weights():
          ww = np.reshape(ww, [*ww.shape[:-1], 3, 85])[..., [1, 0, 3, 2, *np.arange(5, 85), 4]]
          ww = np.reshape(ww, [*ww.shape[:-2], -1])
          new_ww.append(ww)
      conv_layer.set_weights(new_ww)

  mm.save(mm.name + "_coco.h5")


  from keras_cv_attention_models import test_images, coco
  imm = test_images.dog_cat()
  preds = mm(mm.preprocess_input(imm))
  bboxs, lables, confidences = mm.decode_predictions(preds)[0]
  coco.show_image_with_bboxes(imm, bboxs, lables, confidences, num_classes=80)
  ```
  **Convert SSP block `[::-1]` concat -> concat weitghs for YOLOV7_Tiny**
  ```py
  aa = mm.get_layer('stack4_spp_post_1_conv')
  ww = aa.get_weights()[0]
  aa.set_weights([np.concatenate([ww[:, :, 768:], ww[:, :, 512:768], ww[:, :, 256:512], ww[:, :, :256]], axis=2)])
  ```
  ```py
  foo = [
      'stem_conv',
      'stem_bn',
      'stack1_downsample_conv_1_conv',
      'stack1_downsample_conv_1_bn',
      'stack1_downsample_conv_2_conv',
      'stack1_downsample_conv_2_bn',
      'stack1_downsample_pool_conv',
      'stack1_downsample_pool_bn',
      'stack1_1_conv',
      'stack1_1_bn',
      'stack1_2_conv',
      'stack1_2_bn',
      'stack1_3_conv',
      'stack1_3_bn',
      'stack1_4_conv',
      'stack1_4_bn',
      'stack1_5_conv',
      'stack1_5_bn',
      'stack1_6_conv',
      'stack1_6_bn',
      'stack1_7_conv',
      'stack1_7_bn',
      'stack1_8_conv',
      'stack1_8_bn',
      'stack1_out_conv',
      'stack1_out_bn',
      'stack1_another_1_conv',
      'stack1_another_1_bn',
      'stack1_another_2_conv',
      'stack1_another_2_bn',
      'stack1_another_3_conv',
      'stack1_another_3_bn',
      'stack1_another_4_conv',
      'stack1_another_4_bn',
      'stack1_another_5_conv',
      'stack1_another_5_bn',
      'stack1_another_6_conv',
      'stack1_another_6_bn',
      'stack1_another_7_conv',
      'stack1_another_7_bn',
      'stack1_another_8_conv',
      'stack1_another_8_bn',
      'stack1_another_out_conv',
      'stack1_another_out_bn',
      'stack2_downsample_conv_1_conv',
      'stack2_downsample_conv_1_bn',
      'stack2_downsample_conv_2_conv',
      'stack2_downsample_conv_2_bn',
      'stack2_downsample_pool_conv',
      'stack2_downsample_pool_bn',
      'stack2_1_conv',
      'stack2_1_bn',
      'stack2_2_conv',
      'stack2_2_bn',
      'stack2_3_conv',
      'stack2_3_bn',
      'stack2_4_conv',
      'stack2_4_bn',
      'stack2_5_conv',
      'stack2_5_bn',
      'stack2_6_conv',
      'stack2_6_bn',
      'stack2_7_conv',
      'stack2_7_bn',
      'stack2_8_conv',
      'stack2_8_bn',
      'stack2_out_conv',
      'stack2_out_bn',
      'stack2_another_1_conv',
      'stack2_another_1_bn',
      'stack2_another_2_conv',
      'stack2_another_2_bn',
      'stack2_another_3_conv',
      'stack2_another_3_bn',
      'stack2_another_4_conv',
      'stack2_another_4_bn',
      'stack2_another_5_conv',
      'stack2_another_5_bn',
      'stack2_another_6_conv',
      'stack2_another_6_bn',
      'stack2_another_7_conv',
      'stack2_another_7_bn',
      'stack2_another_8_conv',
      'stack2_another_8_bn',
      'stack2_another_out_conv',
      'stack2_another_out_bn',
      'stack3_downsample_conv_1_conv',
      'stack3_downsample_conv_1_bn',
      'stack3_downsample_conv_2_conv',
      'stack3_downsample_conv_2_bn',
      'stack3_downsample_pool_conv',
      'stack3_downsample_pool_bn',
      'stack3_1_conv',
      'stack3_1_bn',
      'stack3_2_conv',
      'stack3_2_bn',
      'stack3_3_conv',
      'stack3_3_bn',
      'stack3_4_conv',
      'stack3_4_bn',
      'stack3_5_conv',
      'stack3_5_bn',
      'stack3_6_conv',
      'stack3_6_bn',
      'stack3_7_conv',
      'stack3_7_bn',
      'stack3_8_conv',
      'stack3_8_bn',
      'stack3_out_conv',
      'stack3_out_bn',
      'stack3_another_1_conv',
      'stack3_another_1_bn',
      'stack3_another_2_conv',
      'stack3_another_2_bn',
      'stack3_another_3_conv',
      'stack3_another_3_bn',
      'stack3_another_4_conv',
      'stack3_another_4_bn',
      'stack3_another_5_conv',
      'stack3_another_5_bn',
      'stack3_another_6_conv',
      'stack3_another_6_bn',
      'stack3_another_7_conv',
      'stack3_another_7_bn',
      'stack3_another_8_conv',
      'stack3_another_8_bn',
      'stack3_another_out_conv',
      'stack3_another_out_bn',
      'stack4_downsample_conv_1_conv',
      'stack4_downsample_conv_1_bn',
      'stack4_downsample_conv_2_conv',
      'stack4_downsample_conv_2_bn',
      'stack4_downsample_pool_conv',
      'stack4_downsample_pool_bn',
      'stack4_1_conv',
      'stack4_1_bn',
      'stack4_2_conv',
      'stack4_2_bn',
      'stack4_3_conv',
      'stack4_3_bn',
      'stack4_4_conv',
      'stack4_4_bn',
      'stack4_5_conv',
      'stack4_5_bn',
      'stack4_6_conv',
      'stack4_6_bn',
      'stack4_7_conv',
      'stack4_7_bn',
      'stack4_8_conv',
      'stack4_8_bn',
      'stack4_out_conv',
      'stack4_out_bn',
      'stack4_another_1_conv',
      'stack4_another_1_bn',
      'stack4_another_2_conv',
      'stack4_another_2_bn',
      'stack4_another_3_conv',
      'stack4_another_3_bn',
      'stack4_another_4_conv',
      'stack4_another_4_bn',
      'stack4_another_5_conv',
      'stack4_another_5_bn',
      'stack4_another_6_conv',
      'stack4_another_6_bn',
      'stack4_another_7_conv',
      'stack4_another_7_bn',
      'stack4_another_8_conv',
      'stack4_another_8_bn',
      'stack4_another_out_conv',
      'stack4_another_out_bn',
      'stack5_downsample_conv_1_conv',
      'stack5_downsample_conv_1_bn',
      'stack5_downsample_conv_2_conv',
      'stack5_downsample_conv_2_bn',
      'stack5_downsample_pool_conv',
      'stack5_downsample_pool_bn',
      'stack5_1_conv',
      'stack5_1_bn',
      'stack5_2_conv',
      'stack5_2_bn',
      'stack5_3_conv',
      'stack5_3_bn',
      'stack5_4_conv',
      'stack5_4_bn',
      'stack5_5_conv',
      'stack5_5_bn',
      'stack5_6_conv',
      'stack5_6_bn',
      'stack5_7_conv',
      'stack5_7_bn',
      'stack5_8_conv',
      'stack5_8_bn',
      'stack5_out_conv',
      'stack5_out_bn',
      'stack5_another_1_conv',
      'stack5_another_1_bn',
      'stack5_another_2_conv',
      'stack5_another_2_bn',
      'stack5_another_3_conv',
      'stack5_another_3_bn',
      'stack5_another_4_conv',
      'stack5_another_4_bn',
      'stack5_another_5_conv',
      'stack5_another_5_bn',
      'stack5_another_6_conv',
      'stack5_another_6_bn',
      'stack5_another_7_conv',
      'stack5_another_7_bn',
      'stack5_another_8_conv',
      'stack5_another_8_bn',
      'stack5_another_out_conv',
      'stack5_another_out_bn',
      'stack5_spp_pre_1_conv',
      'stack5_spp_pre_1_bn',
      'stack5_spp_short_conv',
      'stack5_spp_short_bn',
      'stack5_spp_pre_2_conv',
      'stack5_spp_pre_2_bn',
      'stack5_spp_pre_3_conv',
      'stack5_spp_pre_3_bn',
      'stack5_spp_post_1_conv',
      'stack5_spp_post_1_bn',
      'stack5_spp_post_2_conv',
      'stack5_spp_post_2_bn',
      'stack5_spp_output_conv',
      'stack5_spp_output_bn',
      'pafpn_p5p6_up_conv',
      'pafpn_p5p6_up_bn',
      'pafpn_p5_down_conv',
      'pafpn_p5_down_bn',
      'pafpn_p5p6_1_conv',
      'pafpn_p5p6_1_bn',
      'pafpn_p5p6_2_conv',
      'pafpn_p5p6_2_bn',
      'pafpn_p5p6_3_conv',
      'pafpn_p5p6_3_bn',
      'pafpn_p5p6_4_conv',
      'pafpn_p5p6_4_bn',
      'pafpn_p5p6_5_conv',
      'pafpn_p5p6_5_bn',
      'pafpn_p5p6_6_conv',
      'pafpn_p5p6_6_bn',
      'pafpn_p5p6_7_conv',
      'pafpn_p5p6_7_bn',
      'pafpn_p5p6_8_conv',
      'pafpn_p5p6_8_bn',
      'pafpn_p5p6_out_conv',
      'pafpn_p5p6_out_bn',
      'pafpn_p5p6_another_1_conv',
      'pafpn_p5p6_another_1_bn',
      'pafpn_p5p6_another_2_conv',
      'pafpn_p5p6_another_2_bn',
      'pafpn_p5p6_another_3_conv',
      'pafpn_p5p6_another_3_bn',
      'pafpn_p5p6_another_4_conv',
      'pafpn_p5p6_another_4_bn',
      'pafpn_p5p6_another_5_conv',
      'pafpn_p5p6_another_5_bn',
      'pafpn_p5p6_another_6_conv',
      'pafpn_p5p6_another_6_bn',
      'pafpn_p5p6_another_7_conv',
      'pafpn_p5p6_another_7_bn',
      'pafpn_p5p6_another_8_conv',
      'pafpn_p5p6_another_8_bn',
      'pafpn_p5p6_another_out_conv',
      'pafpn_p5p6_another_out_bn',
      'pafpn_p4p5p6_up_conv',
      'pafpn_p4p5p6_up_bn',
      'pafpn_p4_down_conv',
      'pafpn_p4_down_bn',
      'pafpn_p4p5p6_1_conv',
      'pafpn_p4p5p6_1_bn',
      'pafpn_p4p5p6_2_conv',
      'pafpn_p4p5p6_2_bn',
      'pafpn_p4p5p6_3_conv',
      'pafpn_p4p5p6_3_bn',
      'pafpn_p4p5p6_4_conv',
      'pafpn_p4p5p6_4_bn',
      'pafpn_p4p5p6_5_conv',
      'pafpn_p4p5p6_5_bn',
      'pafpn_p4p5p6_6_conv',
      'pafpn_p4p5p6_6_bn',
      'pafpn_p4p5p6_7_conv',
      'pafpn_p4p5p6_7_bn',
      'pafpn_p4p5p6_8_conv',
      'pafpn_p4p5p6_8_bn',
      'pafpn_p4p5p6_out_conv',
      'pafpn_p4p5p6_out_bn',
      'pafpn_p4p5p6_another_1_conv',
      'pafpn_p4p5p6_another_1_bn',
      'pafpn_p4p5p6_another_2_conv',
      'pafpn_p4p5p6_another_2_bn',
      'pafpn_p4p5p6_another_3_conv',
      'pafpn_p4p5p6_another_3_bn',
      'pafpn_p4p5p6_another_4_conv',
      'pafpn_p4p5p6_another_4_bn',
      'pafpn_p4p5p6_another_5_conv',
      'pafpn_p4p5p6_another_5_bn',
      'pafpn_p4p5p6_another_6_conv',
      'pafpn_p4p5p6_another_6_bn',
      'pafpn_p4p5p6_another_7_conv',
      'pafpn_p4p5p6_another_7_bn',
      'pafpn_p4p5p6_another_8_conv',
      'pafpn_p4p5p6_another_8_bn',
      'pafpn_p4p5p6_another_out_conv',
      'pafpn_p4p5p6_another_out_bn',
      'pafpn_p3p4p5p6_up_conv',
      'pafpn_p3p4p5p6_up_bn',
      'pafpn_p3_down_conv',
      'pafpn_p3_down_bn',
      'pafpn_p3p4p5p6_1_conv',
      'pafpn_p3p4p5p6_1_bn',
      'pafpn_p3p4p5p6_2_conv',
      'pafpn_p3p4p5p6_2_bn',
      'pafpn_p3p4p5p6_3_conv',
      'pafpn_p3p4p5p6_3_bn',
      'pafpn_p3p4p5p6_4_conv',
      'pafpn_p3p4p5p6_4_bn',
      'pafpn_p3p4p5p6_5_conv',
      'pafpn_p3p4p5p6_5_bn',
      'pafpn_p3p4p5p6_6_conv',
      'pafpn_p3p4p5p6_6_bn',
      'pafpn_p3p4p5p6_7_conv',
      'pafpn_p3p4p5p6_7_bn',
      'pafpn_p3p4p5p6_8_conv',
      'pafpn_p3p4p5p6_8_bn',
      'pafpn_p3p4p5p6_out_conv',
      'pafpn_p3p4p5p6_out_bn',
      'pafpn_p3p4p5p6_another_1_conv',
      'pafpn_p3p4p5p6_another_1_bn',
      'pafpn_p3p4p5p6_another_2_conv',
      'pafpn_p3p4p5p6_another_2_bn',
      'pafpn_p3p4p5p6_another_3_conv',
      'pafpn_p3p4p5p6_another_3_bn',
      'pafpn_p3p4p5p6_another_4_conv',
      'pafpn_p3p4p5p6_another_4_bn',
      'pafpn_p3p4p5p6_another_5_conv',
      'pafpn_p3p4p5p6_another_5_bn',
      'pafpn_p3p4p5p6_another_6_conv',
      'pafpn_p3p4p5p6_another_6_bn',
      'pafpn_p3p4p5p6_another_7_conv',
      'pafpn_p3p4p5p6_another_7_bn',
      'pafpn_p3p4p5p6_another_8_conv',
      'pafpn_p3p4p5p6_another_8_bn',
      'pafpn_p3p4p5p6_another_out_conv',
      'pafpn_p3p4p5p6_another_out_bn',
      'pafpn_c3n3_conv_1_conv',
      'pafpn_c3n3_conv_1_bn',
      'pafpn_c3n3_conv_2_conv',
      'pafpn_c3n3_conv_2_bn',
      'pafpn_c3n3_pool_conv',
      'pafpn_c3n3_pool_bn',
      'pafpn_c3n3_1_conv',
      'pafpn_c3n3_1_bn',
      'pafpn_c3n3_2_conv',
      'pafpn_c3n3_2_bn',
      'pafpn_c3n3_3_conv',
      'pafpn_c3n3_3_bn',
      'pafpn_c3n3_4_conv',
      'pafpn_c3n3_4_bn',
      'pafpn_c3n3_5_conv',
      'pafpn_c3n3_5_bn',
      'pafpn_c3n3_6_conv',
      'pafpn_c3n3_6_bn',
      'pafpn_c3n3_7_conv',
      'pafpn_c3n3_7_bn',
      'pafpn_c3n3_8_conv',
      'pafpn_c3n3_8_bn',
      'pafpn_c3n3_out_conv',
      'pafpn_c3n3_out_bn',
      'pafpn_c3n3_another_1_conv',
      'pafpn_c3n3_another_1_bn',
      'pafpn_c3n3_another_2_conv',
      'pafpn_c3n3_another_2_bn',
      'pafpn_c3n3_another_3_conv',
      'pafpn_c3n3_another_3_bn',
      'pafpn_c3n3_another_4_conv',
      'pafpn_c3n3_another_4_bn',
      'pafpn_c3n3_another_5_conv',
      'pafpn_c3n3_another_5_bn',
      'pafpn_c3n3_another_6_conv',
      'pafpn_c3n3_another_6_bn',
      'pafpn_c3n3_another_7_conv',
      'pafpn_c3n3_another_7_bn',
      'pafpn_c3n3_another_8_conv',
      'pafpn_c3n3_another_8_bn',
      'pafpn_c3n3_another_out_conv',
      'pafpn_c3n3_another_out_bn',
      'pafpn_c3n4_conv_1_conv',
      'pafpn_c3n4_conv_1_bn',
      'pafpn_c3n4_conv_2_conv',
      'pafpn_c3n4_conv_2_bn',
      'pafpn_c3n4_pool_conv',
      'pafpn_c3n4_pool_bn',
      'pafpn_c3n4_1_conv',
      'pafpn_c3n4_1_bn',
      'pafpn_c3n4_2_conv',
      'pafpn_c3n4_2_bn',
      'pafpn_c3n4_3_conv',
      'pafpn_c3n4_3_bn',
      'pafpn_c3n4_4_conv',
      'pafpn_c3n4_4_bn',
      'pafpn_c3n4_5_conv',
      'pafpn_c3n4_5_bn',
      'pafpn_c3n4_6_conv',
      'pafpn_c3n4_6_bn',
      'pafpn_c3n4_7_conv',
      'pafpn_c3n4_7_bn',
      'pafpn_c3n4_8_conv',
      'pafpn_c3n4_8_bn',
      'pafpn_c3n4_out_conv',
      'pafpn_c3n4_out_bn',
      'pafpn_c3n4_another_1_conv',
      'pafpn_c3n4_another_1_bn',
      'pafpn_c3n4_another_2_conv',
      'pafpn_c3n4_another_2_bn',
      'pafpn_c3n4_another_3_conv',
      'pafpn_c3n4_another_3_bn',
      'pafpn_c3n4_another_4_conv',
      'pafpn_c3n4_another_4_bn',
      'pafpn_c3n4_another_5_conv',
      'pafpn_c3n4_another_5_bn',
      'pafpn_c3n4_another_6_conv',
      'pafpn_c3n4_another_6_bn',
      'pafpn_c3n4_another_7_conv',
      'pafpn_c3n4_another_7_bn',
      'pafpn_c3n4_another_8_conv',
      'pafpn_c3n4_another_8_bn',
      'pafpn_c3n4_another_out_conv',
      'pafpn_c3n4_another_out_bn',
      'pafpn_c3n5_conv_1_conv',
      'pafpn_c3n5_conv_1_bn',
      'pafpn_c3n5_conv_2_conv',
      'pafpn_c3n5_conv_2_bn',
      'pafpn_c3n5_pool_conv',
      'pafpn_c3n5_pool_bn',
      'pafpn_c3n5_1_conv',
      'pafpn_c3n5_1_bn',
      'pafpn_c3n5_2_conv',
      'pafpn_c3n5_2_bn',
      'pafpn_c3n5_3_conv',
      'pafpn_c3n5_3_bn',
      'pafpn_c3n5_4_conv',
      'pafpn_c3n5_4_bn',
      'pafpn_c3n5_5_conv',
      'pafpn_c3n5_5_bn',
      'pafpn_c3n5_6_conv',
      'pafpn_c3n5_6_bn',
      'pafpn_c3n5_7_conv',
      'pafpn_c3n5_7_bn',
      'pafpn_c3n5_8_conv',
      'pafpn_c3n5_8_bn',
      'pafpn_c3n5_out_conv',
      'pafpn_c3n5_out_bn',
      'pafpn_c3n5_another_1_conv',
      'pafpn_c3n5_another_1_bn',
      'pafpn_c3n5_another_2_conv',
      'pafpn_c3n5_another_2_bn',
      'pafpn_c3n5_another_3_conv',
      'pafpn_c3n5_another_3_bn',
      'pafpn_c3n5_another_4_conv',
      'pafpn_c3n5_another_4_bn',
      'pafpn_c3n5_another_5_conv',
      'pafpn_c3n5_another_5_bn',
      'pafpn_c3n5_another_6_conv',
      'pafpn_c3n5_another_6_bn',
      'pafpn_c3n5_another_7_conv',
      'pafpn_c3n5_another_7_bn',
      'pafpn_c3n5_another_8_conv',
      'pafpn_c3n5_another_8_bn',
      'pafpn_c3n5_another_out_conv',
      'pafpn_c3n5_another_out_bn',
      'head_1_1_conv',
      'head_1_1_bn',
      'head_2_1_conv',
      'head_2_1_bn',
      'head_3_1_conv',
      'head_3_1_bn',
      'head_4_1_conv',
      'head_4_1_bn',
      'head_1_2_conv',
      'head_2_2_conv',
      'head_3_2_conv',
      'head_4_2_conv',
  ]

  sys.path.append('../yolov7')
  import torch
  ss = torch.load('yolov7-e6e.pt', map_location="cpu")['model'].state_dict()
  # ss = {kk: vv for kk, vv in ss.items() if not(kk.endswith('anchors') or kk.endswith('anchors_grid'))}

  from keras_cv_attention_models import download_and_load
  from keras_cv_attention_models.yolov7 import yolov7
  mm = yolov7.YOLOV7_E6E()

  skip_weights = ["num_batches_tracked", "anchor_grid", "anchors"]
  download_and_load.keras_reload_from_torch_model(ss, mm, skip_weights=skip_weights, specific_match_func=lambda xx: foo, save_name=mm.name + "_coco.h5")
  ```
***
