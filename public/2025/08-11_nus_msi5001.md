## Midterm
  - **Artificial Intelligence**
    - **Definition**: (human) perceive, think/reason/learn, and act. -> (Machines) Perceive, analyze/cumpute/learn, act.
    - **Subfields**: ML/DL, CV, NLP, Expert Systems, Speech Processing, Robotics (Active Camara Coordination).
    - **Narrow AI**: task-specific (Siri, Translate, Face ID). **General AI**: human-like adaptability (still theoretical).
    - **Landmarks**: Deep Blue (1997, rule based search and evaluation), DARPA Cars (2005, autonomous driving), Watson (2011, Jeopardy), AlphaGo (2016, reinforcement, Go champion), OpenAI Five (2019, esports), GPT-3 (2020, 175B LLM), AlphaFold (2021, protein folding).
    - **Rule-Based vs ML**: Rule-based = human-crafted rules, ML = system learns rules/patterns from data
  - **Types of Machine Learning**
    - **Supervised**: labeled ‚Üí Classification (spam, fraud) & Regression (house price).
    - **Unsupervised**: unlabeled ‚Üí Clustering (K-means, DBSCAN), Dim.Reduction (PCA, t-SNE), Anomaly detection, Association rules (market basket).
    - **Reinforcement**: agent learns by actions + rewards, e.g., game bots, robotics.
    - **Tasks**: Classification, Regression, Clustering, Dim.Reduction, Anomaly Detection.
  - **Data types & Preprocessing**:
    - **Numerical**: Discrete (# of bedrooms), Continuous (temperature). **Categorical**: Nominal (blood type), Ordinal (shirt size).
    - **Structured** (tables), **Unstructured** (images, text), **Semi-structured** (JSON, XML).
    - **Scaling**: **Standardization**: `(x‚ÄìŒº)/œÉ` ‚Üí mean 0, œÉ 1. **Normalization**: `(x‚Äìmin)/(max‚Äìmin)` ‚Üí range 0‚Äì1.
    - **WHY Scaling** The ML algorithms takes only the magnitude of features neglecting the units - there is a chance that higher weightage is given to features with higher magnitude.
    - Without scaling, features with higher magnitudes can dominate distance calculations and optimization.
    - **Euclidean**: `‚àöŒ£(x·µ¢‚Äìy·µ¢)¬≤`. **Manhattan**: `Œ£|x·µ¢‚Äìy·µ¢|`. **Minkowski**: `(Œ£|x·µ¢‚Äìy·µ¢|·µê)¬π·êü·µê`. **Hamming**: count differences (categorical/binary).
  - **KNN Algorithm**
    - **Steps**: choose K ‚Üí compute distances new with all training data ‚Üí sort the distance ‚Üí pick K nearest ‚Üí Determine their class value ‚Üí majority vote (classification) / average (regression)
    - **Sensitivity**: Smaller K = high variance/noisy, Larger K = smoother, less noise
    - **Notes**: no training phase, memory-based (lazy learner). Bad with irrelevant features & high dimension (curse of dimensionality). MUST scale features.
  - **Linear Regression**
    - **Equations**: `≈∑ = w‚ÇÄ + Œ£ w·µ¢x·µ¢`. **Error (cost) (SSE/MSE)**: `E = (1/2N) Œ£(y ‚Äì ≈∑)¬≤`.
    - **Gradient Descent**: `w‚±º ‚Üê w‚±º ‚Äì Œ∑ ‚àÇE/‚àÇw‚±º` = `w‚±º + Œ∑ (1/N) Œ£(y ‚Äì ≈∑)x‚±º`.
    - **Evaluation**: MAE `avg|y‚Äì≈∑|` (easy to interpret), MSE `avg(y‚Äì≈∑)¬≤` (penalizes large errors), RMSE `‚àöMSE` (same units as target), R¬≤ `1-SSE/SST,SSE=sum(y-≈∑)¬≤,SST=sum(y-»≥)¬≤` (explained var, 1->perfect).
    - **Limitations**: assumes linearity, sensitive to outliers, multicollinearity issues.
    - **Learning Rate**: Too Large - overshoot minimum and fail to convergence. Too small - more time to converge. `trial-and-error` to choose a nominal value of ùêø.
  - **Logistic Regression**
    - **Sigmoid (logistic)**: `g(z) = 1/(1 + e^(‚Äìz))`. Output ‚àà (0, 1). **Predict**: `≈∑ = g(W¬∑x·µÄ)` class = 1 if `g(z) ‚â• threshold`.
    - **Log Loss**: `-[y log ≈∑ + (1‚Äìy) log (1‚Äì≈∑)]`. **Gradient Update**: `w‚±º ‚Üê w‚±º + Œ∑ (y ‚Äì ≈∑)x‚±º`.
    - **Applications**: spam detection, fraud detection, medical diagnosis, credit approval.
    - **Why log loss not MSE**: better for probability predictions, smoother optimization, stronger penalty for confident wrong.
  - **Decision Trees (CART)**
    - Flowchart helps make decisions step by step. Used for classification and regression. Intuitive and easy to understand.
    - **Splits**: At each node, choose the best feature and split point - picks the feature that maximally decreases impurity.
    - `Gini=1‚ÄìŒ£p·µ¢¬≤`, `Entropy=-Œ£p·µ¢ log p·µ¢`. **Gini** 0‚Üíall items belong to one class (pure), higher value‚Üímore mix (impure).
    - **Parts**: Root, decision nodes, branches, leaves, sub-tree.
    - **Pros**: Interpretable, handles num+cat, no scaling needed.
    - **Cons**: Overfitting if not limited in depth or size (need depth limit / pruning), high variance and sensitive to Small Changes in Data, greedy/local splits not considering whether it leads to the best overall tree.
    - **Stopping criteria**: max depth reached, min samples per leaf, no improvement in impurity.
  - **Random Forest**
    - Multiple Decision Trees together to make a better decision. Trains many trees on random subsets of data and features.
    - Overfitting from individual trees is smoothed out by the ensemble. many opinions > one biased expert.
    - Each tree becomes a ‚Äúweak learner‚Äù Individually, these trees may overfit or be noisy. But collectively, when combined in the forest, their strengths complement each other.
    - **Steps**:
      1. Bootstrap sampling (with replacement). Randomly pick different subsets of training data.
      2. Construct a Decision Tree on each subset of data.
      3. For each split, only consider a random subset of features (not all).
      4. Predict by majority vote (classification) / average (regression).
    - **Advantage**: reduces overfitting, increases stability, handles missing data better.
    - **Applications**: healthcare: disease risk, Finance: credit scoring / fraud detection, Retail: customer churn, recommend.
  - **K-Means Clustering**
    - It aims to partition the data into K distinct, non-overlapping clusters. Each cluster is defined by its centroid - the average of all points in the cluster.
    - Useful for exploration, preprocessing, segmentation, anomaly detection, pattern discovery, and more.
    - **User cases**: Group customers. Detect unusual behavior or potential intrusions. Reduce the size of images.
    - **Steps**:
      1. Initialize K centroids (random). Sensitive to starting points -> Run K-means multiple times, compute the objective function and choose the run with the lowest value.
      2. **Cluster assignment**: Assign data points to closest cluster center.
      3. **Move centroid**: Change the cluster center to the average of its assigned points.
      4. Repeat 2-3 until convergence (no centroid change).
    - **Optimization Objective**: The squared Euclidean distance between points and centroids: `(1/N) Œ£‚Äñx·µ¢ ‚Äì M·µ¢‚Äñ¬≤`.
    - **Choosing K**: Elbow method (plot WCSSE vs K) ‚Üí optimal K where WCSSE drops slowly.
    - **WCSSE** `Œ£‚Äñx·µ¢ ‚Äì M·µ¢‚Äñ¬≤` (compactness). Lower is better -> indicates compact clusters.
    - **Silhouette score** `(b‚Äìa)/max(a,b), ‚àà [‚Äì1,1]`, `> 0.5 = good clustering`. where `a` means average distance to other points in the same cluster, `b` means average distance to points in the nearest different cluster. (cohesion and separation) comparing the average distance of points to their own and the nearest cluster, visual plots.
    - **Visual inspection** = scatter plots. Look for: clear boundaries between clusters, compact grouping within clusters.
    - **Properties**: needs feature scaling, random init ‚Üí multiple runs, sensitive to outliers, spherical clusters, multiple runs for best result.
  - **Confusion Metrics**
    - TP: Model predicted Positive, and it was correct. TN: Model predicted Negative, and it was correct. FP: Model predicted Positive, but it was wrong. FN: Model predicted Negative, but it was wrong.
    - `Accuracy = (TP+TN)/All`. How often the model is correct overall. In imbalanced datasets, high accuracy can be misleading ‚Üí use Precision/Recall/F1..
    - `Precision = TP/(TP+FP)`. Accuracy of positive predictions.
    - `Recall = TP/(TP+FN)`. Of the actual positives, how many were correctly identified? High Recall ‚Üí few FN.
    - `F1 = 2¬∑(Prec¬∑Recall)/(Prec+Recall)`. Weighted mean of Precision and Recall. Balances both.
    - Confusion matrix = `[[TP, FP],[FN, TN]]`.
    - Precision ‚Üë ‚Üí FP ‚Üì, Recall ‚Üë ‚Üí FN ‚Üì, threshold ‚Üë ‚Üí TP‚ÜìFP‚ÜìFN‚Üë ‚Üí Precision ‚Üë Recall‚Üì
  - **ANN and DL** ANN neuron parts = `weights, bias, activation`.
    - **Activation**: Sigmoid (0‚Äì1), ReLU (max(0,z)), tanh (‚Äì1‚Üí1), Step (binary).
    - Backprop requires differentiable activations. Vanishing gradients: sigmoid/tanh can cause; ReLU alleviates.
    - **Perceptron**: single-layer, step activation, solves linearly separable problems only.
    - **MLP (Multi-layer ANN)**: hidden layers learn hierarchical representations. More layers = capture non-linear patterns.
    - **CNN**: Convolution learns **edges ‚Üí textures ‚Üí shapes ‚Üí objects**. Pooling (max/avg) reduces dimension. Translation invariance. Good for images.
    - **CNN Layers**: Conv ‚Üí ReLU ‚Üí Pool ‚Üí FC. **Tasks**: object detection, segmentation, super-resolution, style transfer.
    - **RNN**: maintains hidden state for sequence data; struggles with long-term dependencies (vanishing gradient).
    - **LSTM**: long-term memory via input/forget/output gates; handles long sequences (text, speech).
    - **DL Advantages**: automatic feature learning, high accuracy on vision/NLP. **Limitations**: large data+compute, difficult interpretability.
    - **Explainability (XAI)**: tools (SHAP/LIME/Grad-CAM) help interpret models but do not remove inherent bias.
  - **GenAI and Modern AI Systems**
    - **Generative AI**: produces new text (ChatGPT), images (DALL¬∑E/Stable Diffusion), audio, code (Copilot). Learns distributions, not rules.
    - **Transformer Architecture**: Attention-based. Parallelized. Replaces recurrence. **Self-attention** weights relevance across tokens. **Paper**: *Attention is All You Need*.
    - **GPT**: autoregressive, predicts next token left‚Üíright.  
      **BERT**: bidirectional, masked language modeling (context both sides).
    - **LLM Issues**: hallucination, bias, lack of grounding, requires huge data, domain generality ‚â† factual accuracy.
    - **RAG (Retrieval-Augmented Generation)**: retrieves relevant documents ‚Üí LLM generates final answer. Reduces hallucination, increases factuality.
    - **Diffusion Models**: **Forward process**: add noise step-by-step ‚Üí pure noise. **Reverse process**: gradually denoise ‚Üí generate new image. **Used in**: DALL¬∑E, Stable Diffusion, Midjourney.
  - **AI System** Problem framing ‚Üí Data collection ‚Üí Preprocessing ‚Üí Model selection ‚Üí Training ‚Üí Validation ‚Üí Evaluation ‚Üí Deployment ‚Üí Monitoring (feedback loop)
    - **Purpose**: structured end-to-end pipeline to move from business need ‚Üí ML solution ‚Üí production system.
    - **Validation vs Evaluation**: validation = tuning; evaluation = final unseen test.
    - **Success Metric Selection**: classification ‚Üí precision/recall/F1; regression ‚Üí MSE/RMSE/R¬≤.
  - **Deployment, Inference & MLOps**
    - **Inference Types**: **Batch**: periodic jobs (e.g., daily risk scoring). **Real-time**: ms latency (fraud detection, recommendations). **Streaming**: continuous flow (IoT sensors, stock feeds).
    - **Deployment**: model exposed via **API endpoint**. Must handle scaling, reliability, latency.
    - **Containers (Docker)**: bundle **model + code + dependencies** ‚Üí consistent runtime across environments.
    - **Edge**: low-latency, offline operation (IoT, mobile). **Cloud**: scalable, centralized computing. **Hybrid**: cloud training + edge inference (common in production).
    - **MLOps Principles**: `CI/CD for models (automated training/testing/deployment)`. `Model/data/feature versioning`. `Automated monitoring, retraining triggers, rollback safety`. `Governance, reproducibility, lineage tracking`.
    - **Prediction Servers**: load-balanced instances handling user queries.
  - **Monitoring, Drift & Reliability**
    - **Model Degradation**: concept drift, data drift, software bugs, API mismatches.
    - **Data Drift**: input feature distribution changes (e.g., users now shop overseas more often). **Concept Drift**: relationship (X ‚Üí y) changes (e.g., fraud patterns evolve).
    - **Drift Signals**: sudden accuracy drop, increased FPs/FNs, confidence ‚Üì, latency ‚Üë, API error spikes.
    - **Actions**: trigger retraining, update training data, recalibrate thresholds, introduce HITL for low-confidence predictions.
    - **HITL (Human-in-the-Loop)**: human reviews borderline or uncertain predictions ‚Üí safety in fraud, healthcare, legal.
    - **Version Control**: track data, model weights, metrics, configs to ensure reproducibility.
    - **Monitoring Tools**: track latency, drift, errors, throughput. **Online APIs**: enable real-time ML predictions integrated into apps or devices.
  - **Others**
    - **Bias Sources**: sampling bias, proxy variables, label bias.
    - **Fairness Metrics**: statistical parity, equalized odds, calibration across groups.
    - **Mitigation**: `Pre-processing: reweighting, resampling`. `In-processing: fairness-aware loss functions`. `Post-processing: adjust decision thresholds`.
    - **Explainability**: helps interpretation but does not eliminate bias
    - **Hyperparameter Tuning**  `Grid Search (combinations)` `Random Search (efficient sampling)` `Bayesian Optimization (probabilistic search)`
    - **Training Modes**: **Batch Training**: periodic full retrains. **Online / Incremental**: update model continuously as new data arrives. **Transfer Learning**: adapt a pre-trained model to a new domain with limited data.

***

## practice quiz 3
- ReLU avoids saturation in the positive domain, reducing vanishing gradient issues.
- Multi-layer networks are highly prone to vanishing gradients when using saturating activations like sigmoid/tanh.
- LSTMs can still face vanishing gradients over very long sequences, despite gating mechanisms designed to mitigate them.
- CNNs with sigmoid/tanh activations can also suffer from vanishing gradients when deep. Modern CNNs with ReLU/variants mitigate this problem but are not fully immune in extremely deep settings.
- CNNs learn features layer by layer: edges ‚Üí lines ‚Üí shapes ‚Üí objects.
- RNNs struggle with vanishing/exploding gradients in long sequences.
- RNNs are designed for sequential or time-dependent data, where past information influences future predictions. For non-sequential data, simpler architectures like CNNs are more efficient and effective.
- LSTM gates handle long-term dependencies.
- embeddings preserve semantic meaning (e.g., ‚Äúking ‚Äì man + woman ‚âà queen‚Äù).
## Week 6
- `Tutorial4_Handout.docx`
## Week 7
- `Tutorial5_Handout-Sol.docx`
- The network needs to predict which digit (0‚Äì9) an image represents. Since there are 10 possible digits, the output layer must have 10 nodes, with each node representing the probability of one digit.
## Week 8
- First layer learns small local patterns like edges, second layer learns larger patterns like shapes and so on, eventually learning the object
## Week 9
- `Tutorial 6_CNN_RNN_Text Representation Answers.docx`
- BERT is bidirectional, great for understanding context.
- Evaluation should be context-aware ‚Äî e.g., a fraud detection model may favor recall (catch more fraud) over precision (less concern for false alarms).
## Week 11
- `Tutorial 8_AI Pipeline.docx`
- deployment means operationalizing a trained model for real use.
- MLOps engineer create CI/CD pipelines to automate model deployment, testing and scaling.
- DevOps teams manage infrastructure and load balancing.
- Batch inference makes predictions on large datasets at fixed times.
- Real-time inference responds within milliseconds to user or system requests.
- Streaming inference handles continuous data flows.
## Week 12 Elearning week, worth 1-2 marks in exam
  1.‚Å† ‚Å†Towards Effective Discrimination Testing for Generative AI: https://arxiv.org/pdf/2412.21052
  2.‚Å† ‚Å†A Critical Survey on Fairness Benefits of Explainable AI: https://arxiv.org/pdf/2310.13007
  3.‚Å† ‚Å†Bias and Discrimination in AI: a cross-disciplinary perspective: https://arxiv.org/pdf/2008.07309
  4.‚Å† ‚Å†Algorithmic Fairness: https://arxiv.org/pdf/2001.09784
## Week 13
- The pipeline includes data collection, preprocessing, model selection, training, evaluation, deployment, and monitoring.
- Data drift = changes in feature distribution (e.g., user behavior shifts).
- Concept drift = when the meaning of features changes relative to the target (e.g., transaction patterns become obsolete).
- MLOps focuses on automation, reproducibility, model versioning, monitoring, and CI/CD.
- Proper tracking of datasets, metrics, and model artifacts is essential for reliable production workflows.
- Problem framing focuses on aligning business goals, user needs, and ML task types before engineering begins.
- Online/incremental learning supports time-sensitive and streaming environments.
- Grid search, random search, and Bayesian optimization are standard approaches to improving model performance.
- Degradation signals include data drift, concept drift, increasing errors, or declining confidence in predictions.
- online APIs support real-time inference in production.
- Data drift occurs when the distribution of input features changes over time (e.g., users now travel/purchase overseas more frequently).
- Continuous monitoring is key to maintaining accuracy.
- HITL improves safety in high-stakes scenarios like fraud detection.
- Updating and retraining models is essential when drift is detected.
- Model evaluation tests the model on unseen data using quantitative metrics (precision, recall, F1).
## Final exam
We will have a midterm exam next week for class MSI5001.
- Here's out guideline:
  - 50 marks, ~15marks calculation based, 90min duration.
  - Calculation until week 7 - feed forward. No back propagation.
  - timeline for model development is not included.
  - Cheat sheet is allowed - only 1 A4 sheet (both sides) is allowed. You can write or print anything within this cheat sheet. I will provide you the rough paper, please bring your own stationery.
  - Topics: All the lecture topics, video recordings, lectures, tutorials, assignments, etc. that are covered up to week 6 (including). No programming questions will be tested. Questions will be MCQ, MRQ, T/F, Fill-ups, numerical problems, short answers, etc. For some questions, you may be given access to spreadsheet within the Examplify software. I have uploaded a practice quiz under Weekly Quiz section.

- Here's some points may be tested:
  - Week 2, 'Week2 - AI and Subfields.pdf' 'Week2 Lecture.pptx', landmark achievements of AI, subfield of AI
  - Week 3, 'Week3 Lecture.pptx', 'Week3 - Machine Learning and KNN.pdf', type of machine learning, type of datasets, KNN,feature scaling (normalization, standardization), distance computation (ED, MD)
  - Week 4, 'Week4 Lecture.pptx', 'Week4 - Linear Regression.pdf', Linear Regression (SV, MV), error function, evaluating regression outputs (MSE, MAE, RMSE, R2)
  - Week 5, 'Week5 Lecture.pptx', 'Week5P1 - Logistic Regression.pdf', 'Week5P2 - Tree Based Models.pdf', Logistic regression, random forest, evaluating classification (accuracy, confusion_matrix, precision, recall, f1-score)
  - Week 6, 'Week6 Lecture.pptx', 'Week6 Slides.pdf', unsupervised learning, kmeans, elbow method, evaluating clustering output (WCSSE, S... score)
  - quizes, 'msi5001_quizes.md', are generally related
