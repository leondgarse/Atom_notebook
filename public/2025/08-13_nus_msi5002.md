# __MSI5002 Management of Technological Innovation with AI__
***

# INFO
  - week 7 homework <- slide this week, 6th 10

  - https://www.youtube.com/watch?v=LBvHI1awWaI&feature=youtu.be
  ***
  Details for week 3 homework and attendance
  Dear all,

  The deadline for Week 3 homework has been extended to September 9, 2025. You may submit it through the link below:

  https://forms.gle/QXxt3NtAbt2t52yx6Links to an external site.

  For this week’s attendance, please approach me during class to have your attendance marked. Please note that any latecomers arriving after 7:15 PM will be marked as late and will receive only partial credit.

  For the upcoming weeks, if you have valid reasons for being late or absent (e.g., business trips, work ending at 7 PM), please inform me via Canvas or Outlook. Please also send me a reminder through either channel in the case I missed your initial message.

  MSI5002: NO CLASS tomorrow September 10th - I've fallen sick - BUT Homework revisions are due end of this week
  Dear class,

  I'm cancelling class tomorrow as I've fallen sick and in no shape to make it. So sorry about that. I'll find a way to stay on track with class over the coming weeks.

  Separately, Shaun has been sending you all your individual feedback on the week 2 homework. I hope it is helpful to see where you should improve, and if the homework was great already, how you could make it even better. I would highly suggest to take this chance to strengthen the starting point of your project.

  Please send me an updated version of your Week 2 homework https://forms.gle/A4eixk7Pm5YKT1Uo9 by no later than September 15th

  ALSO, by Monday September 15th, end of day, you are required to send me the updated homework for week 3 - your 7 lean Canvas elements. I also see only 35 submissions so far, so quite a few of you still need to submit a version.

  https://forms.gle/VJ5LD7ao3dr6eZKR8Links to an external site.

  I'll do better this time and have feedback ready by our next class on September 17th. I'll focus on the Week 3 homework.

  All the best, and see you next week.

  Warm Regards,

  B.

  ***
  - submit again if improve by Sep end https://forms.gle/SdV4hfzRPVsa9TW46
  By now, most of you should have received your feedback via Outlook unless you resubmitted at a later date. The order of feedback is the same as the submission. If you have any questions or if I missed out on any feedback, please feel free to reach out to me.

  Regards,

  Shaun
***

# Assignment 1
## Attempt 1
  - **Problem Tree method**: effects at the top (symptoms and consequences), the focal problem in the middle, and causes at the bottom (root drivers). It looks like a sandglass — broad → narrow → broad.
  - Statement Laddering tool (why ↔ how). Think of it like a zoom lens:
    Middle: The Challenge Statement itself (what we’re really trying to takle).
    Above (WHY): The broader, higher-level motivation. “Why does solving this matter?”
    Below (HOW): The narrower, actionable level. “How could we actually solve it?”
  * Pick a specific Technology
    Retrieval-Augmented Generation (RAG)–powered domain knowledge chatbot

  * Describe the technology
    The technology I have chosen is a domain-specific knowledge-based RAG chatbot. It combines a large language model (LLM) for conversational reasoning with a retrieval layer that searches curated company documents, wikis, and FAQs. This pairing creates an intelligent assistant that can answer recurring technical questions by grounding responses in trusted internal resources. In the context of machine learning engineers and their customers, repeated problems in model training and deployment—such as accuracy drops, hyperparameter tuning, or failed deployments—consume time and increase frustration. Although companies maintain extensive documentation, these resources are often scattered and underused. A RAG chatbot addresses this gap by indexing existing knowledge and making it accessible through natural language queries. For example, a customer facing an accuracy issue could ask the chatbot for help and receive relevant troubleshooting steps, explained in conversational terms, along with links to the underlying sources. Unlike human support staff, the chatbot is always available and can scale to handle large volumes of queries simultaneously. By transforming static documents into an interactive support system, this technology reduces delays in deployment, lowers support costs, and improves customer satisfaction. Its ultimate aim is to turn repetitive Q&A into a driver of efficiency and innovation, freeing engineers to focus on higher-value tasks.

  * Address the focal problem
    * effect:
      Delayed model deployments slow time-to-market.
      High support costs stretch engineering resources and cause burnout.
      Customer frustration leads to poor user experience and increased churn risk.
      Loss of trust damages long-term relationships and brand reputation.
    * Focal problem:
      Repeated technical questions and issues are unresolved efficiently because existing documentation is scattered, hard to navigate, and underutilized.
    * cause:
      No unified channel for accessing troubleshooting knowledge.
      Engineers lack bandwidth to provide consistent real-time support.
      Knowledge sources are fragmented across wikis, FAQs, and support tickets.
      Search tools are limited, forcing customers to dig manually.

  * Address the Challenge Statement
    - why:
      Why: Because unresolved support slows customer workflows and creates friction.
      Why: Because repeated support costs drain engineering resources that could go into innovation.
      Why: Because slow time-to-market erodes competitive advantage.
      Why: Because customers measure value not just by product features, but by how easily they can use them.
      Why: Because in the long run, trust and ease-of-use drive adoption, renewals, and brand loyalty.
      Why: Because organizations that scale knowledge effectively become learning organizations, able to adapt faster than competitors.
    - Challenge statement:
      How might we transform fragmented technical documentation into an intelligent, accessible system that reduces repetitive support burdens and improves customer experience?
    - how:
      How: Add escalation workflows so that if the chatbot can’t resolve an issue, it smoothly hands over to human engineers.
      How: Build a retrieval-augmented chatbot that indexes wikis, FAQs, and troubleshooting logs.
      How: Monitor usage data to identify new recurring problems and feed them back into product improvements.
      How: Continuously update the index from new tickets, so the knowledge base evolves over time.
      How: Train the chatbot to recognize domain-specific terminology and error patterns.
      How: Provide citations and source links to increase transparency and trust in answers.

  * Solution Direction
    The solution is a domain-specific RAG chatbot that turns scattered documentation into an interactive support tool. By combining a language model with a retrieval layer, it gives customers quick, conversational answers drawn from wikis, FAQs, and past troubleshooting logs. This tackles the core problem of repetitive questions and hard-to-find information, while easing the workload on engineers. The chatbot is always available, scales to many users, and learns from new issues over time. In short, it makes knowledge accessible, speeds up problem-solving, and improves the customer experience, all while reducing support costs and delays.

  * Define the level of Innovativeness
    The RAG chatbot is an evolutionary innovation: it builds on existing LLMs and search tools but applies them in a new way to transform static documentation into a real-time, conversational support system. It goes beyond incremental tweaks by reshaping customer support and efficiency, though it is not a radical invention.
## Comment 1
  * Pick a specific Technology
    You selected AI-powered intelligent documentation and support systems. This is a relevant choice given the problem of fragmented knowledge. To make it sharper, you could anchor on one enabling pathway for example, “retrieval-augmented generation (RAG) chatbots for technical support” and then frame other features (escalation, monitoring, indexing) as reinforcing this.

  * Describe the technology
    The description touches on RAG chatbots, domain-specific training, citation-based trust, and escalation workflows. This is good, but still a little high-level. To deepen it, explain how indexing across fragmented sources reduces search friction, how escalation integrates with existing ticketing systems, and how usage telemetry creates feedback loops into product improvements. That way, the description demonstrates technical robustness.

  * Address the focal problem
    The focal problem repeated technical questions remain unresolved because existing documentation is scattered, hard to navigate, and underutilized is strong. The causes (fragmented sources, weak search tools, no unified channel, engineers’ lack of bandwidth) and effects (delayed deployments, high support costs, burnout, poor user experience, churn risk, loss of trust) are valid. However, the tree could go deeper with systemic causes: lack of investment in knowledge management, cultural overreliance on engineers instead of scalable documentation, weak integration across support channels. On the effects side, expand into lower developer productivity, slower adoption of the product, and long-term brand erosion.

  * Address the Challenge Statement
    The challenge statement “How might we transform fragmented technical documentation into an intelligent, accessible system that reduces repetitive support burdens and improves customer experience?” is clear. The “why” laddering (friction slows workflows, support costs drain engineering, slow time-to-market erodes competitiveness, ease-of-use drives adoption and renewals) is strong. The “how” laddering (RAG chatbot, domain-specific training, escalation workflows, continuous updates, usage monitoring, transparency via citations) is relevant but still framed as features. To improve, phrase them as sharper design challenges: “How might we ensure RAG answers stay up to date with constantly evolving technical systems?” or “How might we prevent over-reliance on chatbots without alienating engineers or users?” This would elevate the analysis.

  * Solution Direction
    The solution direction is coherent and aligns well with the problem: RAG chatbot + escalation + feedback loop + transparency. However, it risks being read as a list of promising features rather than a cohesive pathway. To strengthen, highlight one backbone e.g., a unified intelligent support assistant built on RAG and show how other features (monitoring, escalation, citations) reinforce that backbone.

  * Define the level of Innovativeness
    You did not indicate an innovation level. Based on your framing, this project fits best under Sustaining innovation: it enhances existing support systems by making documentation more accessible and reducing manual overhead, but it does not fundamentally change the paradigm of support. If you wanted to argue Radical innovation, you would need to frame it as a paradigm shift toward self-improving, AI-first support ecosystems where documentation evolves autonomously and engineers step back from frontline support entirely. As it stands, Sustaining is the more accurate fit.
## Attempt 2
  * **Question 1: Pick a specific Technology**
    Retrieval-Augmented Generation (RAG) chatbot for technical support with integrated documentation, escalation, and feedback loops
  * **Question 2: Describe the Technology**
    The **Retrieval-Augmented Generation (RAG) chatbot for technical support** is an AI system that enhances a large language model (LLM) by grounding its answers in up-to-date, domain-specific knowledge. It fuses two components: a retrieval layer and a generative LLM. The retrieval layer ingests and indexes fragmented resources—wikis, technical docs, customer logs, and Git issues—using semantic search to match queries by meaning rather than exact wording. This drastically reduces search friction; for example, a query about “training slowdowns” could be linked to documents on “model optimization techniques.”

    Once the retrieval layer surfaces relevant content, the LLM uses it to generate coherent, conversational responses. Unlike a static search engine, the chatbot supports **multi-turn dialogues**: users can refine queries, add details, and get context-aware follow-ups. To build trust, responses can include citations and links back to the original sources.

    Beyond Q\&A, the system integrates with existing support workflows. If confidence is low or a problem remains unresolved, it can escalate directly into **Git issues**, pre-filling a ticket with conversation history so engineers have full context. At the same time, the chatbot collects **usage telemetry**—which questions recur, where answers fall short, and when escalations happen. These signals create a **feedback loop** that improves retrieval quality, highlights documentation gaps, and even informs product improvements.

    By turning static, scattered documents into an interactive and self-improving support system, the RAG chatbot shortens resolution times, reduces engineer workload, and lowers support costs. More importantly, it converts repetitive troubleshooting into a source of organizational learning, freeing engineers to focus on higher-value innovation.
  - **Question 3: Address the focal problem** using the **Problem Tree structure** (effects ↑ focal problem ↑ causes).
    - **Effects (Symptoms & Consequences)**
      * **Delayed time-to-market**: Deployment slowdowns erode competitiveness.
      * **High support costs & burnout**: Engineers waste time on repetitive Q\&A, draining resources from innovation.
      * **Customer frustration & churn risk**: Poor self-service support degrades the user experience.
      * **Slower adoption**: Weak documentation reduces willingness to adopt new features.
      * **Brand erosion**: Long-term trust and reputation suffer.
    - **Focal Problem** Repeated technical questions and issues remain unresolved efficiently because **documentation is scattered, hard to navigate, and underutilized**.
    - **Causes (Root Drivers)**
      * **Fragmented knowledge** spread across wikis, docs, tickets, and chats, with no unified access point.
      * **Weak search tools** that rely on keywords, not context.
      * **Over-reliance on expert engineers**, creating bottlenecks and single points of failure.
      * **Underinvestment in knowledge management**, treating documentation as a byproduct rather than a strategic asset.
      * **Poor integration across support channels**, causing lessons from past issues to be lost.
  - **Question 4: The Challenge Statement and Statement Laddering**
    - **WHY (Strategic Motivation)**
      * **Why?** Because unresolved support and fragmented information create friction that slows down customer workflows and prevents them from getting value from the product.
      * **Why?** Because repetitive support costs drain engineering resources that could instead fuel innovation and new feature development.
      * **Why?** Because slow responses to customer needs erode competitive advantage and delay time-to-market.
      * **Why?** Because customers now measure value not just by features, but by **usability and ease of support**.
      * **Why?** Because in the long run, trust and ease-of-use drive adoption, renewals, and brand loyalty.
      * **Why?** Because organizations that effectively scale their knowledge become **learning organizations**, able to adapt faster than competitors.
    - **Challenge Statement (The Core Problem)**
      *How might we transform fragmented technical documentation into an intelligent, accessible system that reduces repetitive support burdens and improves the customer experience?*
    - **HOW (Actionable Design Challenges)**
      * **How might we build a knowledge system that unifies disparate sources** (wikis, tickets, chat logs) into a single, natural language interface?
      * **How might we ensure the RAG knowledge base stays up to date** as systems evolve and products change?
      * **How might we design seamless escalation workflows** that avoid over-reliance on the chatbot and enable smooth handoffs to engineers?
      * **How might we instill trust in answers** by providing transparency through citations and links to original sources?
      * **How might we leverage usage data as a feedback loop** to improve both chatbot performance and product documentation?
      * **How might we train the system to interpret domain-specific terminology and error patterns** without requiring massive custom datasets?
  - **Question 5: Solution Direction**:
    The proposed solution is a **unified intelligent support assistant built on a Retrieval-Augmented Generation (RAG) backbone**. At its core, the system transforms fragmented documentation into an interactive knowledge layer, where engineers and customers can resolve technical questions through natural language conversation.

    Around this backbone, reinforcing capabilities make the solution robust and scalable:

    * **Escalation workflows** ensure that unresolved cases flow seamlessly into Git issues, preserving context for human engineers.
    * **Usage monitoring and feedback loops** capture recurring queries and failure points, turning support interactions into signals for improving both documentation and product design.
    * **Citation and transparency features** link answers back to original sources, building user trust and accountability.

    By integrating these reinforcing mechanisms around the RAG core, the solution doesn’t just answer questions faster — it **creates a continuously improving support ecosystem**. The result is reduced engineer workload, lower support costs, faster resolution times, and a better customer experience that strengthens adoption and long-term trust.
  - **Question 6: Level of Innovativeness**
    Based on the proposed solution, the RAG chatbot is a **Sustaining Innovation**.

    It's a sustaining innovation because it significantly improves upon existing support systems and knowledge management processes without fundamentally changing the core paradigm of how support is delivered. Instead of inventing a completely new way for customers to get help, it enhances the current model by making it more efficient, accessible, and less reliant on human intervention for repetitive tasks.

    Unlike **Incremental innovation**, which would only add small features to existing search or FAQ systems, this assistant establishes a new backbone that integrates retrieval, conversation, escalation, and feedback loops into one cohesive support pathway. At the same time, it stops short of being **Radical innovation**, since it still relies on human engineers for complex cases and does not fully automate knowledge creation or maintenance.

    Therefore, the RAG chatbot is a **sustaining innovation** that makes the existing support system significantly better and more scalable, leading to a tangible improvement in efficiency and customer experience.
## Your 7 lean canvas
  - **Question**
    For **Week 3 homework**, the professor asked us to start working with the **Lean Canvas**. Specifically,
    instead of filling out all nine blocks of the traditional Business Model Canvas, we should focus on **seven key elements** that matter most at our early project stage :

    1. **Customer (Customer Segments)** – Who are you selling to? Who’s looking to buy? Why?
    2. **Hero Customer** – Who is the very first one likely to buy? Why them?
    3. **Your Edge / Superpower** – What makes you better than anyone else? What’s your key advantage?
    4. **Customer Pain** – What are the focal problems they face? Keep it limited and sharp.
    5. **Competition** – From what angles can competition come? Stay open-minded.
    6. **Customer Needs / Solution** – How are you addressing their problem? What does your solution look like?
    7. **Benefit & Value** – What tangible benefit does your customer gain, and what’s the perceived value of using your solution?

    The slides also emphasized:

    * Answer each of these **critically**. It’s fine not to have all answers yet — that’s why we research and validate with customers.
    * Use GenAI to **reverse-engineer answers** and **stress-test** your Lean Canvas sections.
    * Even after completing the canvas, you must **still talk to real customers** to validate (no skipping fieldwork).
  - **Answer**
    1. **Customer (Customer Segments)**
      - **Primary**: Machine learning engineers and data scientists inside tech companies who face recurring deployment and training issues.
      - **Secondary**: Customer success/support teams who need faster, scalable ways to answer technical queries.
      - **Why**: Both groups waste time searching scattered documentation and repeating known fixes, leading to frustration and inefficiency.
    2. **Hero Customer**
      - The **first adopters** are **mid-size AI/ML product teams** (50–200 engineers) with active customer bases.
      - **Why them?** They have enough documentation and customer queries to feel the pain sharply, but not enough budget for large dedicated support teams. They need leverage, not headcount.
    3. **Your Edge / Superpower**
      - A **unified intelligent assistant** built on RAG that can:
        - Pull knowledge across wikis, Git issues, FAQs, and logs.
        - Provide conversational, multi-turn answers with citations.
        - Escalate seamlessly to engineers when needed.
      - **Advantage**: Not just a chatbot — a continuously learning support ecosystem that integrates directly with existing workflows.
    4. **Customer Pain**
      - Fragmented documentation across wikis, Git issues, and Slack.
      - High support costs and engineer burnout from repetitive troubleshooting.
      - Delayed problem resolution slows down deployments and erodes customer trust.
    5. **Competition**
      - **Direct**: Existing search tools (e.g., Confluence search, ElasticSearch-based systems).
      - **Indirect**: Generic AI chatbots (ChatGPT, Copilot) not tuned for company-specific knowledge.
      - **Alternative**: Hiring more engineers for support (expensive, not scalable).
    6. **Customer Needs / Solution**
      - **Need**: A fast, reliable way to resolve technical issues without digging through fragmented docs or waiting on engineers.
      - **Solution**: RAG chatbot that unifies all knowledge into a conversational, always-available assistant with escalation and feedback loops.
    7. **Benefit & Value**
      - **Tangible Benefit**: Faster issue resolution, lower support costs, reduced engineer burnout.
      - **Perceived Value**: Improved customer satisfaction, stronger trust, smoother adoption of the product.
      - **Long-term Value**: Turns support interactions into continuous learning for both the documentation and the product itself.
***

## Prompt
  - week 4 ppt prompt examples
  ```
  perplexity pro

  Can you fact check the report below, and make sure the prioritisation is right based on the data you have access to. If you see discrepancies, call them out, give the data you have access to, and reprioritise based on you critical assessment.
  ```
  The principle for making prompts from the lecture is really about turning vague questions into structured briefs. Dr. Grobben treats prompting not as “asking AI something,” but as a professional skill that multiplies the quality and speed of your output.

  From the Week 1 slides, he lays out an **8-block prompt system**:

  1. **Intent** — What decision or deliverable do you need, and how will you test success?
  2. **Audience & Voice** — Who is the output for, and what tone is required (business, diplomatic, insight-driven)?
  3. **Context** — What project, constraints, or boundaries matter here (in-scope / out-of-scope)?
  4. **Inputs & Background** — Which trusted sources or factual bases must be included?
  5. **Task & Process** — The steps, gates, or assumptions that guide the AI’s approach.
  6. **Output Format** — Desired form (table, slide, essay, code), structure, and length.
  7. **Quality Bar** — Must-have elements, red lines, rubrics, or evidence rules.
  8. **Next Move** — A specific instruction for the immediate step (outline first, expand later).

  He also emphasizes a shift **from average prompting → expert prompting**:

  * Don’t send one-line vague asks. Instead, use structured, numbered blocks.
  * Don’t ignore audience or tone. Make it explicit.
  * Don’t just accept outputs. Fact-check, refine, and iterate.
  * Don’t dump everything in one ask. Break it into gated stages (outline → expand → polish).

  In practice, this links directly with tools you’ve already touched — like **Problem Tree analysis** (to sharpen *Intent* and *Context*), or **Challenge Statement laddering** (to refine scope and constraints).

  So the principle is: **write prompts like a project brief, not like a casual question.** The clearer your framing (decision, audience, context, constraints, format), the higher the AI’s output quality.

  This turns AI into a professional-grade collaborator — echoing the Human–AI dance: humans provide direction, context, and judgment, while AI delivers speed, breadth, and synthesis.

  Would you like me to show you how to take one of your **Problem Tree nodes** or **Challenge Statement** and rewrite it as a prompt following these 8 blocks? That would give you a concrete example.
***

## Week 2 Study Guide
  1. Startup Survival & Leadership
    * **Runway math**: Small cuts rarely help; you need early, big interventions to extend survival.
    * **Leadership duty**: Plan scenarios, track spending, know who is critical, act early.
    * **Reality check**: Layoffs and cuts look brutal, but often necessary when fundraising slips.
  2. Tools & Models: Picking the Right AI
    * **ChatGPT**: Strength = creativity (writing, coding, brainstorming). Weakness = hallucination, unreliable facts.
    * **Claude**: Strength = reasoning, structured research.
    * **Perplexity**: Strength = fact-checking, references, source retrieval.
    * **Gemini**: Strength = speed, multimodal, integrated with Google.
    * **Ollama / open-source**: Private, offline, fine-tunable with company data; free but needs hardware.
    - **Tip**: Match task → model. Creative work ≠ fact-checking ≠ summarization.
  3. Prompting & Workflow
    * Prompts differ by user context and chat history. Sharing prompts ≠ same outputs.
    * Use **mode-switching**:

      * GPT “Thinking mode” → long, deep analysis.
      * GPT Fast mode → short summaries.
    * Always ask: *Which model? Which setting? Sequence?*
  4. Invention vs Innovation
    * **Invention** = new idea, patent, or breakthrough.
    * **Innovation** = adoption + monetization. Must generate value and revenue.
    * Many scientists fail at innovation because they don’t link invention → business model.
    - **Examples**:
      * *PepsiCo “unpoppable popcorn”* – invention.
      * *Carbon fiber aerogel filters* – innovation (scaled manufacturing + oil/gas use case).
      * *Skin-on-a-chip* – innovation (pivot to animal-free skin testing services).
  5. The Human–AI Dance (Aiden & Harper)
    * **Aiden (AI team member)**: data analysis, pattern-finding, scaling research.
    * **Harper (human team member)**: judgment, ethics, creativity, foresight.
    * Together: faster, better outcomes. Humans bring empathy & consequence-awareness, AI brings speed & scale.
  6. Cognitive Bias & Innovation
    * Humans have \~180 cognitive biases → distort strategy.
    * Teams use cross-checking to reduce bias. Treat AI like a team member with its own biases → fact-check, compare, triangulate.
    * Don’t blame AI’s flaws; humans are just as biased.
  7. Five Innovation Behaviors (From IDEO, Medici Effect, etc.)
    1. **Associating** – Connect patterns across domains (fusion thinking).
    2. **Questioning** – Challenge assumptions, ask “Why? What if?”
    3. **Observing** – Watch compensating behaviors, unmet needs.
    4. **Experimenting** – Test, prototype, iterate.
    5. **Networking** – Expose yourself to diverse people and ideas.
  8. Case Study: Ferrero / Nutella
    * Scarcity → innovation (Napoleon’s cacao blockade → hazelnut blends).
    * Spreadability → rationing but preserved luxury feeling.
    * Gamification → Disney collectibles, Kinder toys.
    * Packaging (gold foil) → emotional design, gift-like ritual.
    * Innovation = connecting emotional, behavioral, and market patterns continuously.
  9. Hidden Value & Secondary Economies
    * **Tesla**: cars → data. Highest-resolution global driving map = insurance & infrastructure leverage.
    * **Uber**: rides → mobility data.
    * **Changi Airport**: real-time passenger demographics → dynamic retail stocking.
    * Innovation isn’t just the product — often the *secondary data economy* is more valuable.
  10. Homework Framework
    1. **Pick a specific technology** (not “drones,” but e.g. “fast-charging lithium battery”).
    2. **Describe it (≤500 words)** — in your own words, not copy-paste.
    3. **Problem Tree**:
       * Focal problem (core issue).
       * Effects (what happens if unresolved).
       * Causes (root drivers, layered).
    4. **Problem Statement**: Clear, negative, urgent, emotionally resonant.
    5. **Challenge Statement**: Forward-looking, “How might we…?”
    6. **Solution Direction**: How technology addresses the problem.
    7. **Level of Innovativeness**: incremental, sustaining, radical, or disruptive (with rationale).
  - **Key Takeaways**
    * **Precision matters**: Be specific in definitions, problem statements, and prompts.
    * **Innovation = invention + adoption + business model.**
    * **AI is not magic**: know which model for which task.
    * **Competitive edge = speed of iteration + human creativity + AI scale.**
## Week 3 – Product Design & Development
  - Main themes: **Level of Innovativeness** + **AI-enhanced work habits**

  **1. Levels of Innovativeness**

    * **Incremental**: small, low-risk tweaks (e.g. adding a razor blade).
    * **Sustaining**: medium upgrades, new features (e.g. fingerprint sensor in iPhone).
    * **Disruptive**: new business models change the game (e.g. Netflix vs. Blockbuster).
    * **Radical**: totally new industries (e.g. Generative AI at launch).
      *Important*: none is “better” — incremental can be more profitable. The point is to classify and justify.

  **2. AI as your co-pilot**

    * Use ChatGPT to unpack slides/terms before class (“anticipate concepts”).
    * Instructor’s example: using Perplexity’s **Comet agent** to auto-summarize research, check email, and deliver daily lit scans → “agentic AI.”
    * Takeaway: AI augments, but human judgment steers.

  **3. Homework feedback**

    * **Tech selection**: 79% good; excellence comes from narrowing to sub-techs.
    * **Descriptions**: need more “how it works” detail, with metrics/examples.
    * **Solution direction**: only \~⅓ showed clear logical flow from problem → tech → solution.
    * **Innovativeness**: students reasoned well even if they didn’t label explicitly.
    * Compared to Tuesday class, our section’s AI-augmented answers were **20–40% more detailed and structured**.
## Week 4 – Product Design & Development (cont’d)
  - Main themes: **Beachhead markets, Stakeholders, Customer journey & factory**

  **1. Beachhead Concept**

    * Borrowed from WWII: land where you can **dominate first**, then expand.
    * In business: choose a focused entry point with the highest success probability.
    * Expansion can be by customers, channels, geography, or portfolio.
    * Use GenAI prompting to brainstorm possible beachheads, then apply human gut + strategy to choose.

  **2. Hero customer & personas**

    * Beachhead ≠ “everyone.” Identify your **hero customer** (the one who buys first, feels the pain most).
    * Personas contain both early adopters and later majority; you need to know who loves you enough at the start to help you cross the chasm.

  **3. Target Solution Profile (TSP)**

    * A collaborative framework outlining: intended use, users, utility, features, competitive edge, regulation, quality, cost, risks.
    * Aligns stakeholders, defines requirements, and avoids mismatched expectations.
    * Same technology can yield different products/businesses depending on its TSP.

  **4. Stakeholder Mapping**

    * Matrix: **Influence vs. Interest** → Manage closely / Keep satisfied / Keep informed / Monitor.
    * Communication must be tailored (don’t overshare, answer what’s asked).
    * AI can help brainstorm stakeholder lists, but humans refine.

  **5. Customer Journey Mapping**

    * Trace all touchpoints: Awareness → Consideration → Purchase → Retention → Advocacy.
    * Examples: Starbucks (experience design), McDonald’s vs Chipotle (efficiency vs personalization), Amazon Fresh vs FairPrice (packaging as customer experience).
    * Even B2B journeys (with tenders, regulators, pilots) must be mapped.

  **6. Customer Factory Blueprint (AARRR + R)**

    * Awareness, Acquisition, Activation, Retention, Revenue, Referral.
    * Think like a production line: leaks = drop-offs between stages.
    * Don’t “scale a broken machine” — fix leaks first, then accelerate growth.
    * Startup failures: \~42% fail due to “no market need”; scaling too early amplifies leaks.
## Study Matrix (Weeks 3 & 4)

  | Framework                         | Definition                                                                                                               | Classic Example                                                                                               | How it maps to **RAG Chatbot**                                                                                                                                                                              | How it maps to **AI Personal Trainer**                                                                                                                                                              |
  | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | **Levels of Innovativeness**      | Classify how bold the innovation is: Incremental → Sustaining → Disruptive → Radical                                     | Netflix vs. Blockbuster = Disruptive. Gillette adding razor blades = Incremental.                             | **Incremental/Sustaining**: Faster retrieval, better UI, smoother integration with docs. **Disruptive**: Turning scattered FAQ/documents into a 24/7 “knowledge agent” that reduces need for human support. | **Sustaining**: Adding features like nutrition logging, wearable syncing. **Disruptive**: AI-driven adaptive workout planning that changes in real-time without human trainers.                     |
  | **Beachhead Market**              | The focused entry point where you dominate first before expanding. Inspired by WWII “Omaha Beach.”                       | Starbucks targeted “third place” café-goers; Amazon Fresh targeted eco-conscious online shoppers.             | Start with **ML engineers inside one company** who constantly struggle with deployment issues → dominate internally before external customers.                                                              | Start with **NUS gym-goers** (clear data, strong need for scheduling/optimization) → then expand to broader Singapore fitness market.                                                               |
  | **Hero Customer**                 | The very first, most motivated buyer/user who feels the pain deeply.                                                     | Early Netflix users who hated late fees.                                                                      | A **junior ML engineer** who always gets stuck in repetitive questions and wastes time digging docs.                                                                                                        | A **student athlete** who trains daily but struggles balancing workload, recovery, and nutrition.                                                                                                   |
  | **TSP – Target Solution Profile** | A “requirements blueprint” for your tech: use, users, utility, features, regulation, risks.                              | Memory tech example in lecture: same chip → can become RAM, flash, or embedded storage, depending on profile. | **Use**: internal knowledge retrieval. **Utility**: reduce debugging time. **Features**: natural language queries, citation links. **Risks**: hallucinations, data leaks.                                   | **Use**: personal training guidance. **Utility**: track & adapt workouts. **Features**: exercise library, integration with Apple/Decathlon wearables. **Risks**: injury liability, data privacy.    |
  | **Stakeholder Mapping**           | Map stakeholders by Influence × Interest → Manage closely / Keep satisfied / Keep informed / Monitor.                    | Instructor’s example: investors vs. small community members.                                                  | **High influence, high interest**: Team leads/CTOs. **High influence, low interest**: HR or finance approving budgets. **Low influence, high interest**: junior engineers.                                  | **High influence, high interest**: Gym managers/fitness centers. **High influence, low interest**: insurers, healthcare partners. **Low influence, high interest**: students/gym-goers.             |
  | **Customer Journey Mapping**      | Trace the whole journey: Awareness → Consideration → Purchase → Retention → Advocacy.                                    | Starbucks journey: Anticipation → Sit & Enjoy → Exit.                                                         | Awareness: “We waste hours searching Slack.” → Consideration: Try chatbot demo. → Activation: Solves first deployment error. → Retention: daily use. → Advocacy: team tells other departments.              | Awareness: “I never know when gym is free.” → Consideration: Try assistant app. → Activation: AI optimizes first week schedule. → Retention: weekly tracking. → Advocacy: user posts results on IG. |
  | **Customer Factory (AARRR + R)**  | A repeatable “factory line”: Awareness, Acquisition, Activation, Retention, Revenue, Referral. Fix leaks before scaling. | Instructor’s biotech startup: pricing misalignment caused leaks at conversion.                                | Leak risk: Engineers **acquire** chatbot but don’t **activate** it (too hard to set up). Fix by simplifying onboarding.                                                                                     | Leak risk: Users try app (acquire) but stop (retention leak) if workouts are boring. Fix by gamifying tracking and progress sharing.                                                                |


  | 框架                             | 定义                                                                                                          | 经典例子                                                       | 映射到 **RAG 知识库聊天机器人**                                                                                     | 映射到 **AI 私人训练助手**                                                                                       |
  | -------------------------------- | ------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
  | **创新程度等级**                 | 创新分四类：**渐进式**（小改进）、**持续式**（功能增强）、**颠覆式**（新商业模式）、**激进式**（全新行业）。  | Netflix 颠覆 Blockbuster = 颠覆式；吉列剃须刀加刀片 = 渐进式。 | **渐进/持续**：提升检索速度、优化界面。**颠覆**：把分散文档转化为 24/7 “知识代理”，减少人工支持。                   | **持续**：增加营养记录、穿戴设备同步。**颠覆**：实时自适应训练计划，取代部分教练功能。                           |
  | **滩头市场（Beachhead Market）** | 第一个集中特攻的市场切入口，要在此处取得压倒性优势，再向外扩张。源自二战诺曼底滩头登陆。                      | 星巴克定位“第三空间”；亚马逊生鲜瞄准环保用户。                 | 先从公司内部 **机器学习工程师** 入手，解决部署痛点 → 内部成功后再扩展外部客户。                                     | 先聚焦 **NUS 健身房用户**（数据清晰，需求强烈） → 再扩展到新加坡健身市场。                                       |
  | **核心客户（Hero Customer）**    | 第一个最有动力的买家/用户，痛点最强烈，愿意尝鲜。                                                             | 讨厌滞纳金的早期 Netflix 用户。                                | **初级 ML 工程师**，经常困在重复问题中，浪费大量查资料时间。                                                        | **学生运动员**，训练频繁，但难以平衡恢复、学业和营养。                                                           |
  | **目标解决方案画像（TSP）**      | 一份“需求蓝图”：包含用途、用户、功能、特性、监管要求、风险。 同一技术根据 TSP 不同 → 可能变成完全不同的产品。 | 记忆芯片案例：同一芯片 → 可做 RAM、闪存、嵌入式存储。          | **用途**：内部知识检索。**功能**：自然语言查询、带引用。**风险**：幻觉、数据泄漏。                                  | **用途**：个性化训练指导。**功能**：动作库、穿戴设备连接。**风险**：运动伤害责任、隐私。                         |
  | **利益相关者映射**               | 按“影响力 × 兴趣”画 2×2 矩阵 → 紧密管理 / 保持满意 / 保持告知 / 低度监控。                                    | 教授举例：投资人 vs 小用户群体。                               | **高影响高兴趣**：技术主管/CTO。**高影响低兴趣**：财务/人事审批。**低影响高兴趣**：一线工程师。                     | **高影响高兴趣**：健身房管理者。**高影响低兴趣**：保险公司/医疗合作方。**低影响高兴趣**：学生健身用户。          |
  | **客户旅程映射**                 | 追踪客户全流程：认知 → 考虑 → 购买 → 保留 → 倡导。每个触点都重要。                                            | 星巴克：预期 → 点单 → 入座 → 享受 → 离开。                     | 认知：“找文档太费时间。” → 考虑：试用机器人。→ 激活：成功解决第一个 bug。→ 保留：每天使用。→ 倡导：推荐给其他部门。 | 认知：“健身房太挤。” → 考虑：尝试助手 App。→ 激活：AI 优化一周计划。→ 保留：持续打卡。→ 倡导：在社交媒体晒成果。 |
  | **客户工厂（AARRR+R）**          | 把获客与留存比作流水线：认知、获取、激活、留存、营收、推荐。要先修复“漏点”，再扩大规模。                      | 教授创业案例：报价错误 → 转化阶段大规模漏损。                  | 漏点风险：安装了机器人（获取），但**没激活**（配置太难）。解决：简化 onboarding。                                   | 漏点风险：用户下载 App（获取），但很快放弃（留存失败）。解决：增加游戏化元素和进度反馈。                         |
***

# Lecture
## Week 2
  **\[2025/8/20 18:05]** And taking the tough innovative decisions. Part of that role is, yes, going after the money — but the backup solution is: what if we don’t have the money?
  Right, not having a startup after November is not really an option. So one of the toughest parts of my job last week was scenario B: What if we fundraise less? What if we fundraise later? What if we don’t fundraise at all? What do we do?

  It comes down to budget reviews — trimming benefits, trimming spending, delaying purchases, and unfortunately starting to look at who we cut. That is part of survival mode.

  **\[2025/8/20 18:06]** Leadership needs to plan backups: who is critical, who is indispensable, who is dispensable. It’s a very dry business mindset — assess the state of affairs, have scenarios ready, and trigger them.

  People often misunderstand big layoffs. When the news says “Company X fires 1,000 people overnight,” it looks brutal. But in a startup context, consider this: I need to raise funds by end of November, my burn rate is \~\$52,000/month. Of this, \$25,000 is salaries, the rest is operations, rent, equipment, purchases.

  If I fire everyone, I save only \~\$20,000 per month. That’s not enough to extend my runway meaningfully. To extend by one month, I need to save \$50,000.

  So small cuts don’t help — at the end of the month you’re still out of cash. To extend 2–3 months, you need to start big cuts early. If you wait too late, your only choice is dramatic layoffs.

  That’s why tracking spending, planning scenarios, and fundraising efficiently is essential.

  **\[2025/8/20 18:08]** When you guys do projects or in your future entrepreneurial life, this is the hard reality of being a CEO: prevent these situations from happening.

  Now, let’s look at the questionnaires you filled last week.

  * Almost everyone has company experience.
  * Company interests are very broad — not narrow. That’s good; I’m excited to put diverse groups together in Week 7.
  * Diversity matters: some of you come from technical backgrounds, others from legal, finance, governance. That’s a strength.

  **\[2025/8/20 18:10]** Many of you worked in MNCs; more of you than before have startup experience. After COVID, this number used to be \~10%; now it’s much higher. Encouraging.

  About 23% have been co-founders — small but positive. Many see themselves in startups or entrepreneurship in the future. That’s a healthy trend for Singapore.

  **\[2025/8/20 18:11]** Most frequent tool you’re using? ChatGPT — no surprise. But it’s not a magic tool for everything. By the end of 13 weeks, I hope you’ll see you need more than just one “knife in the kitchen.”

  AI helps you in: specialization of skills, problem-solving, learning, research, programming, brainstorming. But remember: ChatGPT is not the best fact-finder. It’s creative, good for generating context and insights, not always precise facts.

  **\[2025/8/20 18:12–13]** Concerns with AI:

  * Job displacement, privacy/security (biggest group)
  * Reliability/quality issues (GPT-5 sometimes buggy — I’ve seen it myself)
  * Human development: risk of making people “dumber” if they over-rely on AI
  * Governance, ethics, adoption risks

  If you use AI to “make life easy” without challenging yourself, Harvard research already shows your productivity drops. Use AI to accelerate, not replace your thinking.

  **\[2025/8/20 18:15–20]** Questionnaire results: many did not answer precisely.

  * When asked for “technology,” some gave industries (healthcare, aviation, education). These are sectors, not technologies.
  * Some gave products/brands (iPhone, Starlink). That’s branding, not core tech.
  * Correct answers: discrete technologies (e.g., quantum computing, blockchain protocols, LIDAR, lithium battery chemistry, AR sensors).

  Why this matters: if you’re vague, business peers, investors, or even AI filters may not understand you. Online banking AIs or HR filters might reject vague responses.

  So: be precise, intentional, structured. Even AI models will misinterpret you if your input is imprecise.

  **\[2025/8/20 18:24–28]** Continuing from last week: using AI tools in practice.

  * Example: Otter.ai + ChatGPT. Record stakeholder interviews → transcribe → standardize summaries → cluster insights. AI makes repetitive summarization consistent, and lets you “double-click” later (“who said this?” “in what context?”).
  * Example: Leonardo.ai (visuals). You can instantly storyboard consumer journeys, create pitch images, even comic-like narratives. Prompting with context (camera angle, lighting, style) makes outputs professional.

  **\[2025/8/20 18:30–35]** Comparing tools:

  * ChatGPT: creative, good for writing, coding, brainstorming. But can hallucinate.
  * Claude: stronger in reasoning and structured research.
  * Perplexity: better for fact-finding with citations.
  * Gemini: fast, multimodal, integrated with Google ecosystem.

  Key trade-offs: creativity vs. reasoning vs. speed vs. cost. Pick the right tool for the right purpose.

  **\[2025/8/20 18:42–45]** Open-source models (like Ollama) are emerging. They run locally, offline, private, and free — but require hardware and manual updates. Benefit: you can fine-tune with your company’s own confidential data.

  **\[2025/8/20 18:49–53]** For research:

  * Use Consensus (peer-reviewed literature search) to find real papers.
  * Use NotebookLM (Google) to upload PDFs and interrogate documents.
  * Use ChatGPT/Claude for analysis and report writing.
  * Use Perplexity for fact-checking and source hunting.

  This workflow collapses what used to take weeks into hours.

  **\[2025/8/20 18:55–57]** Question: are AI models generic or domain-specific?
  Answer: depends on training. ChatGPT is generic, but for expert use (e.g., radiology) you need domain-specific models trained on mammography data. Cheaper path: fine-tune open-source locally.

  **\[2025/8/20 18:58–59]** GPT-5 still has different “modes” (thinker vs. turbo). You need to judge: do you want deep reasoning, or faster structured analysis? Similarly with Claude’s new memory features — powerful, but raises privacy/security questions.

  **\[2025/8/20 19:00]** A few weeks ago, in another project, I saw AI “cross-contaminating” results. It compared with older chats when it shouldn’t. Sometimes you need to tell it explicitly: *“Don’t compare to anything else, this is a new project.”* Otherwise it tries to please you, and that’s not always the right thing.

  This isn’t about prompting anymore — assume you already have a perfect prompt. The question is: who do you give it to? The “mathematician” model, or the more “creative” model? The same problem will return very different outputs. You, with human judgment, must decide: do I want a well-written creative answer, or a rules-based structured assessment?

  **\[2025/8/20 19:01]** Deep research modes will go online, pull more sources, and generate long reports. In GPT-4 Omni or GPT-5 “Thinking” mode, it can give you 20–30 pages. Very elaborate, but if you want short, you often need to take that long text, feed it back into a faster GPT-5 mode, and prompt: *“Condense this.”*

  So I often use “Thinking” mode to explore deeply, then switch to GPT-5 Fast for summarization. If you stay in Thinking mode and ask for a summary, you still get a long essay. That’s why I always ask: which model, which setting, what sequence did you use? It makes a huge difference.

  **\[2025/8/20 19:03]** Also, remember prompts are influenced by your behavior. If you copy a friend’s prompt, you may not get the same output — your past chat behavior shapes it. Older versions of ChatGPT sometimes gave totally different answers to the same prompt in different chats. It’s more consistent now, but not perfect.

  **\[2025/8/20 19:04]** Speed matters too. Deep research is 5× slower than fast conversational mode. Sometimes you don’t want to wait 20 minutes; even 10 minutes feels long in a work setting. So choose the right model depending on task urgency.

  **\[2025/8/20 19:05]** In Perplexity:

  * *Search* works like a better Google — quick summaries with references.
  * *Research* pulls from larger datasets, writes longer reports.
  * *Labs* is multimodal: it can generate visuals, graphs, infographics, even website mockups. Great for structuring a business model into a package.

  I recently used it with a Singaporean beauty care brand — combining cultural contexts (Malay, Indian, Chinese) with J-Beauty and K-Beauty. Labs gave me diverse, inspirational visuals that would’ve taken hours to research.

  **\[2025/8/20 19:07]** These tools are advancing fast: reasoning is better, latency lower, costs compressing. GPT-5 is already cheaper and faster per token than GPT-4. This efficiency means R\&D cycles that once took 12 months can shrink to 1 month. Innovation cycles compress — your competitive edge is not just having the best idea, but getting there faster.

  **\[2025/8/20 19:10]** That speed also changes roles in meetings. In the past, you’d sit quietly as a spectator because you didn’t have time to prepare. Now, with AI copilots, you can fact-check, draft, and contribute in real time — even lead discussions. The excuse “I don’t know” is becoming unacceptable when you can know in minutes.

  **\[2025/8/20 19:15]** So — innovation isn’t only about tools like ChatGPT. Let’s talk about invention vs. innovation.

  Example: PepsiCo asked us to make “unpoppable popcorn.” Sorghum and rice don’t naturally pop. We engineered a flexible shell that trapped pressure and popped them. That was an **invention**.

  Another project: designing mesh filters with hydrophobic coatings to capture oil droplets from frying exhausts. Again, invention. But PepsiCo didn’t adopt it — too costly. Invention without adoption doesn’t equal innovation.

  **\[2025/8/20 19:20–27]** More cases:

  * **Periorbital edema relief device**: adapted from a patent into a product concept. But P\&G and L’Oréal rejected it — they didn’t want just one device, they needed a whole line. Lesson: invention ≠ innovation.
  * **Carbon fiber aerogel**: breakthrough material for absorbing oil. Invention was the material; innovation was scaling manufacturing to \$80/kg and building an inline filtration product that oil & gas firms would pay for.
  * **Microfluidics chip for skin grafts**: invention was growing skin in a device; innovation was pivoting to animal-free skin testing services.
  * **Engineered bacteriophage proteins**: invention was modifying viral proteins; innovation was consumer products like odor-control sprays.

  **\[2025/8/20 19:28]** Key point: invention = new idea; innovation = adoption that makes money. If you can’t explain how it generates revenue, it’s not innovation.

  **\[2025/8/20 19:50]** I sometimes describe this with two “characters”:

  * **Aiden** (AI team member): strengths = rapid analysis, pattern synthesis, scaling research.
  * **Harper** (human team member): strengths = judgment, ethics, creativity, foresight.

  Together, they complement each other. One person plus an AI teammate can now do the work that used to need a full team. That’s the “human-AI dance.”

  **\[2025/8/20 20:00]** But humans have 180+ cognitive biases. Our brains cut corners, distort, forget. AI has biases too, but we already build processes in teams (cross-checking, stress-testing) to balance human bias. Treat AI as another teammate whose biases you must compensate for.

  **\[2025/8/20 20:03]** Innovation requires both “left brain” (data, analysis) and “right brain” (creativity, judgment). Humans lean one way, AI leans the other. Together, you get balance.

  One of the key behaviors is **association** — like the Medici family in Renaissance Florence, who brought together artists, scientists, and merchants, sparking innovation by mixing domains. Innovators expose themselves widely, then connect patterns others can’t see.

  Other behaviors:

  * **Questioning**: the child-like “why, why, why” that challenges assumptions.
  * **Observing**: watching how people actually behave, including compensating behaviors.
  * **Experimenting**: running small tests.
  * **Networking**: exposing yourself to diverse people, disciplines, and ideas.

  **\[2025/8/20 20:15]** Case study: Ferrero/Nutella.

  * Started with scarcity (cacao blockades in Napoleonic era). Innovated by mixing hazelnuts, creating a new luxury experience.
  * Spreadability (Nutella) made the product last longer but preserved the feeling of indulgence.
  * Gamification (Disney collectible cards) created loyalty.
  * Even the **gold foil seal** is deliberate — evokes the feeling of opening a gift.

  Innovation here wasn’t invention, but continuous re-framing, emotional design, and behavioral reinforcement.

  **\[2025/8/20 20:25–30]** In modern times, Changi Airport uses AI for dynamic retail: stocking duty-free shops based on real-time passport demographics. Tesla’s hidden asset is not cars but driver data, enabling insurance or infrastructure insights. Uber’s value is not rides but secondary economies from mobility data.

  So innovation isn’t always the product itself — sometimes the hidden secondary value is bigger.

  **\[2025/8/20 20:33–42]** AI can also accelerate legal review, scanning contracts for risks against regulations, or suggesting alternative wording. I use both Perplexity (for factual references) and ChatGPT (for creative loophole-finding). That combination is powerful.

  **\[2025/8/20 20:53–21:12]** Finally, your homework:

  * Pick a **specific technology** (not “drones,” but “fast-charging lithium-polymer battery”).
  * Write a **500-word description** of the technology.
  * Build a **Problem Tree**: focal problem in the middle, effects above, root causes below.
  * Formulate a **problem statement** (clear, negative, urgent, emotionally resonant).
  * Turn it into a **challenge statement**: “How might we…?” (forward-looking, action-oriented).
  * Reflect on the **level of innovativeness**: incremental, sustaining, radical, or disruptive.

  Use AI tools to help, but your human judgment decides what really matters. Submissions open Aug 22 on Canvas.
***

# Prompt
## 🧩 1. Brainstorming Prompt (original)
  > “Generate five creative and professional company names and taglines for a startup offering RAG-based intelligent chatbot technical support solutions.”

  **→ 8-block version**

  * **Intent:** Create naming options that communicate intelligence, trust, and efficiency.
  * **Audience:** Potential investors and early customers — professional tone, not gimmicky.
  * **Context:** Startup in the enterprise AI support space using Retrieval-Augmented Generation (RAG).
  * **Inputs:** Description of product, target users (tech leads, IT managers), desired brand traits.
  * **Task / Process:** Generate ≥ 5 names + matching taglines; briefly explain the rationale behind each.
  * **Output Format:** Bulleted list with one-line explanation per option.
  * **Quality Bar:** Must sound credible in B2B context and pass the “could this be a SaaS brand?” test.
  * **Next Move:** I’ll shortlist 2 names, then re-prompt AI for logo and narrative tone ideas.
## 🧩 2. Research Prompt (original)
  > “Search for current RAG market size, business trends, key competitors, and future outlook.”

  **→ 8-block version**

  * **Intent:** Collect current market evidence to support Business Plan Section 2 (Market Opportunity).
  * **Audience:** Course assessor / investor-style reader who expects referenced data.
  * **Context:** 2025 RAG market, focusing on APAC adoption.
  * **Inputs:** Keywords = “Retrieval-Augmented Generation market 2025 CAGR competitors enterprise chatbot.”
  * **Task / Process:** Retrieve latest figures + sources; summarise key growth drivers & top 5 competitors.
  * **Output Format:** 150-word analytic summary + citation list.
  * **Quality Bar:** Data newer than 2024 and from verifiable tech/market sites.
  * **Next Move:** I’ll cross-check with Gemini and use figures in Financial Framework section.
## 🧩 3. Revenue Model Prompt (original)
  > “Explain QueryFlow’s revenue model in detail, including pricing tiers, usage components, and rationale.”

  **→ 8-block version**

  * **Intent:** Build a clear, defensible revenue structure for inclusion in Executive Summary and Financial Plan.
  * **Audience:** Investors or instructors evaluating business logic.
  * **Context:** QueryFlow AI — B2B SaaS model (Starter, Pro, Enterprise).
  * **Inputs:** Existing tier names + rough pricing assumptions ($500–$5 000 / month).
  * **Task / Process:** Describe each tier, its customer type, value proposition, and usage-based extension.
  * **Output Format:** Table + 150-word justification paragraph.
  * **Quality Bar:** Pricing must align with realistic SaaS benchmarks (> 70 % gross margin).
  * **Next Move:** Feed output into the financial spreadsheet for break-even analysis.
## 🧩 4. Roadmap Prompt (original)
  > “Create a 12–24 month implementation roadmap showing product development, pilot phases, and expected KPIs.”

  **→ 8-block version**

  * **Intent:** Outline phased development milestones to demonstrate execution realism.
  * **Audience:** Professors / investors seeking feasibility evidence.
  * **Context:** QueryFlow AI 2025–2027 timeline (alpha, beta, launch, scale).
  * **Inputs:** Known pilot data + resource estimates + planned funding round.
  * **Task / Process:** Generate a quarter-by-quarter table with key deliverables + success metrics.
  * **Output Format:** Markdown table + 100-word commentary.
  * **Quality Bar:** Each milestone must be measurable (e.g., “≥ 5 pilots,” “ARR > 1 M”).
  * **Next Move:** Validate feasibility with my team and insert into Implementation Roadmap section.
## 🧩 5. Risk Mitigation Prompt (original)
  > “List 3–5 risks for implementing QueryFlow AI and propose mitigation strategies.”

  **→ 8-block version**

  * **Intent:** Identify realistic operational and market risks for investor readiness.
  * **Audience:** Professors evaluating risk awareness in business planning.
  * **Context:** Early-stage AI SaaS startup using external LLM APIs.
  * **Inputs:** Categories = technical, market, organizational, financial.
  * **Task / Process:** For each risk, describe impact + probability + mitigation.
  * **Output Format:** Table with 3 columns (Risk, Impact, Mitigation).
  * **Quality Bar:** Risks must be specific, not generic; mitigation should be actionable.
  * **Next Move:** Incorporate top 3 into Business Plan risk section and validate with peer feedback.
## 🧭 How to Use This in Future Coursework
  * Keep an **8-block skeleton** as a mini-template; fill only the 3–4 blocks most relevant to that task.
  * Treat it as *pre-prompt reflection* — it forces clarity before typing anything into ChatGPT.
  * End each AI session by asking yourself:
    *“Did the model’s output meet my quality bar, and what’s my next move as a human?”*
***

# Presentation
## Prompt for script
  We have a group project for class MSI5003, help create a **script** for our pitch, which should be ~5mins.
  - `MSI5002 Group 4 ACE.md` is our business plan
  - `ACE Pitch Deck.pptx` is the related pitch deck
  - Other files are in-class materials, including methodology and guidance, like `Designing & Delivering a Compelling Innovation Pitch_ A Self-Study Guide.pdf` is for pitch deck instructions.

  An overall instruction is it should be a **clarity and refine story flow based on BP (Problem → Solution → Impact → Financial → Team → CTA).** This flow ensures the pitch tells a **coherent, investor-ready story**, instead of jumping around randomly, and it matches typical venture capital logic.

  - **1. Problem** State the pain point sharply and quantitatively.
    * Who experiences it?
    * How severe is it?
    * Why does it matter now?
  - **2. Solution** Show what you built and why it is uniquely capable of solving the problem.
    * Product demo / 1–2 screenshots
    * Key features
    * Why it works
  - **3. Impact** Demonstrate **what changes** and **why it matters**.
    * Measurable improvement (e.g., 25% comprehension lift)
    * Cost/time saving
    * Student/teacher outcomes
    * Market validation
  - **4. Financial** Summarize your business logic.
    * Revenue model (80/20 Base + Success Fee)
    * ACV
    * TAM/SAM/SOM
    * 3-year projections or unit economics
    * Break-even
  - **5. Team** Show why *you* can execute the idea.
    * Short intros
    * Relevant skills (AI, education, product, GTM)
    * Optional headshots
  - **6. CTA (Call to Action)** The final “ask.”
  * What do you want from the judges?
  * What should they do after hearing your pitch?

  Examples:

  * “We are seeking 1.5M USD Seed funding.”
  * “Looking for 3 pilot universities.”
  * “Join us in improving international STEM retention.”
***

# storytelling
## 🎤 The Story of ACE: A Narrative Pitch Script (5-Minute)
  **(Start: The Persona & The Problem)**

  Let me introduce you to Mei. She’s a brilliant graduate student who just moved here to study AI&I. She’s ambitious, she’s dedicated, but in her lecture on *Introduction to AI*, she is struggling.

  Not because the content is too hard, but because she’s spending 40% of her mental energy just translating—trying to connect complex, domain-specific English terms to her native language. Even the language is not the barrier, some concept are also not so easy. She’s falling behind, and her confidence is dropping.

  Now, meet Mei’s university. They see a different side of the problem. They see an international student attrition rate as high as 25%. They are losing millions in tuition, their reputation is at risk, and their faculty have no way to even *see* that Mei is struggling until she fails the midterm.

  Mei’s comprehension gap has become the university’s multi-million dollar retention crisis. This is the problem we solve.

  **(The Solution: Introducing the Hero - RAG + NMT)**

  Our solution is ACE, the Adaptive Academic Comprehension Engine. We are an AI-powered learning engine that integrates directly into the classroom.

  Imagine Mei in that same lecture. Now, she has ACE.

  As the professor speaks, she sees real-time, domain-accurate translations on her screen. When a complex term like "Monte Carlo boundary conditions" comes up, ACE provides a bilingual explanation, tailored *specifically* to her course. She can ask questions, get instant summaries, and for the first time, she’s not just translating—she’s *comprehending*.

  But the real magic isn't just what Mei sees. It's what ACE learns.

  **(The Solution: Introducing the Hero - PSP)**

  Our "secret sauce" is the **Persistent Student Profile**, or PSP.

  Unlike any other tool, ACE builds a dynamic "digital twin" of Mei’s mind. It maps her unique knowledge graph, her language mastery, and her learning behaviors.

  This creates a powerful **data flywheel**. Every interaction from every student fine-tunes our AI models, that gets smarter every single day.

  **(The Impact & "Secret Sauce")**

  And this creates a powerful **Impact**.

  For Mei, the impact is immediate: comprehension-related risk alerts drop.

  For the faculty, the impact is transformative. They get a real-time analytics dashboard. They can finally *see* the comprehension gaps across their entire cohort. They're alerted to at-risk students like Mei in Week 2, not Week 8, and given the data-driven insights they need to intervene.

  This isn't just a better student experience. This is a direct, measurable improvement on the university's number one problem: **retention**.

  **(The Business: How We Make it Viable)**

  And we’ve built a business model that directly aligns our success with theirs.

  We use a hybrid SaaS model. **80%** is a stable, institutional base subscription. The other **20%** is a **capped success fee**.

  This is our "skin in the game." We only get that 20% bonus when we achieve the verifiable KPIs that matter to the university, like a 25% lift in platform-measured comprehension or a 35% increase in engagement for at-risk students.

  **(The Business: Market Strategy)**

  This model is our entry into a massive opportunity. The total market for global higher-ed software is **$45 billion**. We are targeting the **$8 Billion** student success segment, and our obtainable beachhead niche is a very realistic **$100 million**, which is focusing on APAC/NA/UK/EU 800 priority institutions.

  Our go-to-market plan is not a broad spray-and-pray. We are executing a focused "Lighthouse" strategy.

  Our entire goal for Year 1 is to land **5 "Lighthouse" customers**. We will acquire them through targeted outreach at key industry conferences like EDUCAUSE and QS Asia, and by leveraging policy partnerships.

  **(The Team: Why We're the Ones to Build It)**

  So, why us?

  We are a team of engineers, educators, and strategists who have lived this problem, and we have the direct technical and academic experience to solve this problem. We aren't just building a product; we are solving a problem we deeply understand from the inside out.

  **(The Ask: Our Call to Action)**

  To get this tool into the hands of students like Mei, we are seeking **$1.5 million in seed funding**.

  This capital will fund our "Pilot-to-Scale" strategy, allowing us to secure our first 5 Lighthouse university partners, and achieve $450,000 in Annual Recurring Revenue by the end of Year 1.

  Comprehension drives retention.

  Join us, and let's transform the future of student success. Thank you.
  ***

  Team 4: Adaptive Academic Engine (AI for Student Retention)
  Pitch Summary: Team 4 tackled the issue of high dropout rates among international STEM students in
  universities, proposing an AI-powered retention tool (referred to as an “adaptive academic
  comprehensive engine” in the pitch). They highlighted that 15–25% of international STEM students are
  dropping out, a costly problem for universities (millions in lost tuition). The core issue: these students
  spend up to 40% of study time translating and catching up due to language barriers, rather than truly
  learning. This lack of support turns into attrition. The solution: a real-time adaptive support AI that
  understands nuanced differences in terminology across subjects (e.g., “seal” in biology vs. engineering)
  and acts as a 24/7 personal tutor for students, while also providing faculty with actionable insights to
  catch struggling students early. Essentially, it’s an AI engine built to “stop attrition” by bridging
  comprehension gaps. Their “secret sauce” is a Persistent Student Profile (PSP) – a digital twin of each
  student’s knowledge state and language proficiency that continuously updates. Every interaction
  fine-tunes the AI for that institution, creating an institution-specific data model that gets smarter each
  semester (so the system learns the particular curriculum and common trouble spots). For faculty, the
  tool isn’t just another dashboard; it proactively flags who is struggling with what, enabling timely
  interventions before a student fails or drops out. The market is sizable: a global EdTech software market
  of $45B, with their target niche about $8B (the serviceable segment focused on priority international
  STEM programs) and an initial obtainable market of ~$100M (around 800 priority programs). Their
  business model aligns with outcomes: an “80/20 hybrid” revenue model where 80% is traditional SaaS
  subscription and 20% is a performance-based bonus. This means universities pay more only if retention
  metrics improve – for example, if weekly engagement exceeds 80% or comprehension scores rise 25%,
  the vendor gets a bonus. This model underscores commitment to actually improving retention, not just
  selling software. They contrasted themselves with generic tools: competitors are either basic translation
  apps or simple student trackers, which reside in the “bottom quadrants” of value. By contrast, their
  solution is both highly domain-specific (tailored to STEM context and language nuances) and deeply
  insightful (modeling cognitive understanding, not just surface metrics) – placing them in the top-right
  of the competitive matrix (high specialization, high impact). The go-to-market is a focused “lighthouse”
  strategy: land 5 high-profile university partners in year one as proof points. They plan to reach these
  through targeted outreach, education conferences (like EDUCAUSE), and by leveraging partnerships
  such as a Singapore EdTech group’s co-innovation program. The team reported they already have a
  working prototype nearly complete, validated by 5 faculty interviews, and a roadmap to $4M revenue by
  year 3. Financially, they project narrowing losses and a clear path to break-even by year 4. The funding
  ask was US$1.5M in seed capital, split 50/50 between R&D (to refine the platform’s AI capabilities) and
  sales/marketing (to execute the lighthouse customer acquisition). This investment would carry them
  through key milestones, such as achieving ~$150k–450k in ARR in the first year and proving the
  retention improvements at initial clients. The pitch ended with a strong statement of belief:
  “comprehension drives retention”, underlining the academic philosophy behind the product.
  Instructor’s Feedback: Dr. Bert started by noting the pitch length: 6½ minutes – about 90 seconds
  over. He immediately pointed out some structural adjustments. “Start with the problem and the
  solution, maybe not with the team,” he advised. In the current draft, the team introduced themselves
  and roles first, which might confuse an audience that doesn’t yet know what the project is about.
  Instead, he suggested jumping straight into the pain point (student dropouts) and your solution, then
  later linking the team’s expertise to it. When mentioning team members, Dr. Bert also noted the way
  they were described was not very distinctive: saying everyone is an MTI (Master of Technology
  Innovation) student at NUS “makes every one of you look the same”. Instead, personalize the team intro
  by emphasizing what specific role or skill each person brings (especially relevant if someone has, say, AI
  expertise, or edtech background, etc.), rather than just titles like CEO/CTO which, in a student project
  context, don’t carry much weight. In terms of content, he encouraged them to stay focused on the
  core message throughout and ensure each part of the pitch ties back to that message. He found the
  topic very interesting and even personally relevant (as a professor, he quipped he’d love an AI to alert
  him which of his students are struggling). This is a good sign – it means the problem resonates. So the
  advice was to make sure the investors clearly understand how the AI works and how it delivers value to
  both students and faculty. Specifically, highlight the metrics and technology in a way that connects to
  outcomes: for example, show how improved comprehension (the metric) leads to better grades or
  retention (the outcome). Finally, he reiterated that they’re on the right track but need to sharpen and
  trim the message. Cut any excess and make sure each segment of the pitch – problem, solution,
  market, etc. – is tightly aligned to the main value proposition (improving student retention via improved
  comprehension). In short: reduce the time, refocus the intro, and keep the pitch laser-focused on
  what matters.
  Additional Coaching (Content & Delivery): The idea is compelling, but you can strengthen the pitch by
  reordering and refining a bit. Definitely open with the problem: paint a quick picture of an
  international STEM student struggling (maybe a one-sentence persona: “Meet Ana, a first-year
  international student who spends nights translating lecture notes...”). Then immediately state how your
  solution changes that story. This will hook the audience. Introduce the team later, perhaps right before
  or after the ask, with a one-liner on why you’re the ones to do this (e.g., “Our team combines AI expertise
  and personal experience with this problem – and yes, we’re all pet owners... oh wait, wrong team – but
  something akin to all being intimately familiar with the education challenge”). On content, I suggest
  simplifying the explanation of the Persistent Student Profile. It’s a great concept, but terms like “digital
  twin” and “knowledge graph” might confuse listeners. Instead, say “we create a continuous learning
  profile for each student that adapts as they learn – like a personalized AI tutor that remembers where
  you get stuck”. That conveys PSP without jargon. Also, when you mention the AI will identify differences
  like “seal” (biology vs engineering), that example is good – it shows domain specificity – but ensure it’s
  clear how that helps retention (e.g., understanding context so students don’t get lost in translation).
  Delivery-wise, the speakers need to smooth out the transitions. There were some technical hiccups
  (one speaker said “Sorry, I’ve got to open that” when likely a slide didn’t load). Practice as if it’s live: if
  slides lag, keep talking rather than pausing. Also, avoid repeating information – I noticed the phrase “to
  solve this retention crisis” was used a couple of times; once is enough, then move on to how you solve it.
  Each speaker should jump straight into content when it’s their turn (Dr. Bert noted an example of
  someone re-introducing themselves – that’s not needed mid-pitch). Maintain energy: the enthusiasm
  was there, just keep it consistent and don’t let your voice drop in the more technical parts. Since your
  model has a unique bonus component (20% pay-for-performance), highlight that as a strength – it
  shows confidence in your solution. Something like, “If our tool doesn’t work, we don’t get paid that full
  amount – that’s how much we believe in our impact,” can leave an impression. Lastly, tighten the ask:
  make sure to clearly articulate what $1.5M will achieve (e.g., “This funds 18 months, 5 pilot universities,
  and gets us to $X revenue and Y% improved retention”). Be specific and confident in the request.
  Time Management: Over time (6:30 of 5:00 minutes). You need to trim about 1.5 minutes. Start by
  following the instructor’s tip: drop the lengthy team intro at the start. That could easily save ~20-30
  seconds. Investors care about the problem/solution first; the team’s credibility can be established in one
  succinct slide later. Next, look at your “four problems” list – you can likely compress this. Perhaps
  combine or eliminate one; for example, the point about vets not knowing pets as well as owners (from
  Team 10’s pitch – ignore that, it sneaked in from another context) – focusing back on your pitch, you
  listed multiple facets (students spending time translating, tuition loss for universities, etc.). Pick the top
  two problems (student perspective and university perspective) and state those clearly and briefly. Also,
  you may not need to spell out market size in three tiers (TAM, SAM, niche) during the pitch – it’s good
  you did the research, but mentioning just the $8B segment and $100M initial niche is enough. That
  could cut another 15-20 seconds. Furthermore, ensure no time is wasted on slide transitions or
  apologies. In the transcript, there was a moment of slide opening trouble – practice your tech setup to
  avoid this, or be ready to speak without the slide if needed. Dr. Bert’s feedback specifically said a minute
  and a half can be shaved with smoother delivery and cutting minor technical delays. If each of your 5-6
  speakers cuts just 15 seconds of fluff (like not repeating what the last person said, or not saying “I will
  talk about X” and instead just talking about X), you’ll recover a lot of time. Also consider reducing the
  number of speakers if trimming transitions is hard – perhaps 3 main speakers instead of all 5 speaking.
  Ultimately, aim for a crisp 5-minute run in practice. After trimming, do a timed rehearsal and see if the
  key points still come across – if not, iterate. The instructor was clear that focusing on core elements
  and shaving the rest will get you in shape. By pitch day, you should be able to confidently finish within
  5 minutes, with no rushing at the end.
  ***

  Here is the revised pitch script.

  I've incorporated the detailed feedback from your draft run, focusing on tightening the narrative, simplifying jargon, and strengthening the business case as requested.

  ---

  ### 🎤 The Story of ACE: A Narrative Pitch Script (5-Minute)

  - **(Slide 1: Title Slide)**

  Hello professor and everyone. We are ACE, the Adaptive Academic Comprehension Engine.
  Our belief is simple: **Comprehension Drives Retention**.

  - **(Slide 2: The Problem - Student Persona)**

  Meet Mei, a first-year international STEM student.

  She’s motivated and prepared, yet when a lecturer says *Monte Carlo boundary conditions*, she’s still stuck two sentences back trying to decode the English.

  She spends **40% of her study time translating**, not learning.
  She falls behind, her confidence drops, and becomes an **attrition risk**.

  - **(Slide 3: The Problem - The University's Retention Crisis)**

  Now, meet Mei’s university.

  International student attrition can reach **25%**, causing **over $10B in lost tuition** globally each year.

  Faculty want to help students like Mei, but they **cannot detect comprehension gaps early**. They only see the problem after she fails the midterms-too late to intervene.

  - **(Slide 4: The Problem)**

  Mei’s comprehension gap has become the university’s retention crisis.

  - **(Slide 5: The Solution - From Translating to Comprehension)**

  ACE changes this.
  Imagine Mei in the same lecture. Now, with ACE.

  She receives:
  - real-time, domain-accurate translations
  - Multilingual explanations of complex terms
  - She can also ask questions, get instant summaries

  Her mental energy shifts from decoding English to actually *comprehending*.

  - **(Slide 6: The Solution: A Personalized AI Tutor)**

  The engine behind ACE is **Persistent Student Profile** — a continuously updating learning model that tracks:
  - her specific language gaps
  - what concepts she's mastered
  - and where she gets stuck.
  It acts like a **personalized AI tutor** that truly understands Mei.

  As more students use ACE, the system learns the **curriculum and common pain points** - strengthening retention semester after semester.

  - **(Slide 7: The Impact - From Comprehension to Retention)**

  The impact is immediate.
  - Students understand better and regain confidence.
  - Faculty finally see who is struggling and why—**in Week 2 instead of Week 8**.
  - Universities see measurable improvement on their top priority metric: **retention**.

  - **(Slide 8: The Business - Aligned for Success)**

  Our business model aligns our success with theirs.
  - **80%** stable SaaS subscription
  - **20%** capped success fee

  We get the bonus only when we achieve clear gains, like a 25% lift in comprehension.
  This lowers adoption risk and demonstrates our confidence.

  - **(Slide 9: The Business - A Massive, Focused Opportunity)**

  We are targeting the **$8 Billion** student success market, starting with an obtainable **$100M niche** - about 800 priority STEM institutions across key regions.

  - **(Slide 10: The Business - Growth Projections)**

  Our go-to-market plan is a focused **"Lighthouse" strategy**.

  We aim to land **5 Lighthouse Customers in Year 1**, scaling to **35 by Year 3**.

  This drives revenue from **$450,000 in Year 1** to over **$4 million by Year 3**, with strong unit economics: **75% Gross Margin and a 3.5-to-1 LTV to CAC ratio**.

  - **(slide 11: The Team - Why We're the Ones to Build It**

  We are a team of **AI expertise**, **School operations**, and **Educational designers**.

  We understand the students, the institutions, and the technology - we’ve lived this problem.

  - **(slide 12: The Ask - Our Call to Action)**

  To bring ACE to students like Mei, we are seeking **$1.5 million in seed funding**.

  This will be allocated to R&D and Marketing, funding our lighthouse strategy and helping us reach our first milestone: **$450,000 in ARR** by the end of Year 1.

  - **(Slide 13: Q&A)**

  We are ACE.
  We believe — and demonstrate — that Comprehension Drives Retention.
  Let's transform the future of student success.

  Thank you.
***

## Comments on pitch deck
  - **page 2 "The Problem: The Student Comprehension Gap"**:
    and can just delete “meet”，don't need to show it，focus on persona info. I copied the page and revised for comparison

    problem in this page mainly is on student side. problem in next page mainly is on faculty&university side. problem from both side develop a final problem:Student’s comprehension gap leads to University’s retention crisis
    So problem part can more clearly show the logic, also aligned with the 3 impacted objects(student/faculty/university)
    remember use core quantified metrics in different parts

    shorten the words, highlight pain point

  - **page 3 "The Problem: The University's Retention Crisis"**:
    common tips for revision：
    1. use shorter and precise expression
    2. highlight key words not less meaning words

    add problem directly related to faculty

    whole problem sequence follows the reality logic: study hard👉no alert&efficient tools👉dropout&tuition loss

    page 5:
    all the words and picture this page can mainly focus on "What's ACE"
    all the words and picture in next page  can mainly focus on "How ACE works"
    then, all the words and picture in 9th page can mainly focus on "How ACE impacts"

  - **page 6 "The Solution: A Personalized AI Tutor"**:
    light colours are hard to see clearly in screen

    again，picture can be more strongly and clearly further explain the functions in this slide. may can use 1 student(Mei) profile to show more details of right-side contents,  instead of 6 roughly profiles

    even can delete bullet point contents cos picture below is the same meanings

    Simplify into PSP in the center,
    can add a virtual avatar to represent a personalized AI tutor

    I copied and revised the page for comparision

  - **page 9 The Business: A Massive, Focused Opportunity**

    can see comparison in next page(incompletely revision for reference).shorter and key expression. small black words can be further expand for topic, better don't include repeatedly content

    keep all the icons & headlines in the same color & font for emphasis

    list specific partners: gov, education association and foundation, etc

    list the methods for getting 5 lighthouse customers
