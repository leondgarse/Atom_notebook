# __vLLM__
***

# vLLM 架构
## 参考资料
  - [vLLM Meetups](https://docs.vllm.ai/en/latest/community/meetups.html)
  - [vLLM V1 - The sixth vLLM meetup](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?slide=id.g2fbe9f464f9_0_0#slide=id.g2fbe9f464f9_0_0)
  - [vLLM V1 - The seventh vLLM meetup](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?slide=id.g31455c8bc1e_1_11#slide=id.g31455c8bc1e_1_11)
  - [vLLM V1 源码阅读](https://zhuanlan.zhihu.com/p/32045324831)
  - [图解Vllm V1系列1：整体流程](https://zhuanlan.zhihu.com/p/1900126076279160869)
  - [vLLM V1 整体流程](https://github.com/shen-shanshan/cs-self-learning/blob/master/Open_Source/Projects/vLLM/Notes/%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B/V1%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B.md)
## 安装
  ```sh
  GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Qwen/Qwen3-0.6B
  GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Mostafa8Mehrabi/qwen3-30m-fp16

  GIT_LFS_SKIP_SMUDGE=1 git clone https://modelers.cn/Modelers_Park/Qwen3-0.6B.git
  cd Qwen3-0.6B && git lfs pull && cd -
  ```
  使用 vllm v1：export VLLM_USE_V1=1
  - [Installing vllm CPU](https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html). **Should better br root or a virtualenv**
    ```sh
    sudo apt-get install -y libnuma-dev
    git clone https://github.com/vllm-project/vllm.git
    cd vllm
    pip install "setuptools-scm>=8" ninja
    pip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu

    pip install --upgrade sphinx sphinx-rtd-theme  # or error: ImportError while trying to load entry-point build_sphinx: cannot import name 'Union' from 'types'
    sudo chmod 777 /usr/local/lib/python3.10/dist-packages  # or error: Permission denied: '/usr/local/lib/python3.10/dist-packages/test-easy-install-1064.write-test'
    sudo chmod 777 /usr/local/bin/  # or error: Permission denied: '/usr/local/bin/vllm'
    VLLM_TARGET_DEVICE=cpu python setup.py install

    sudo chmod 755 /usr/local/lib/python3.10/dist-packages
    sudo chmod 755 /usr/local/bin/
    ```
    **编译过程中需要注意是否使用了当前环境的路径，可能会使用 `~/.local/lib/python3.10/site-packages` 或 `/usr/local/lib/python3.10/dist-packages` 下的**
  - **测试执行，不要在 vllm 的路径下** `VLLM_CPU_KVCACHE_SPACE=12 ipy` 指定 workspace 大小，**不能大于内存，默认 4G**
    ```py
    import os
    # Note: There should be no Warning like: No module named 'vllm._C'
    from vllm import LLM, SamplingParams
    # INFO 07-11 09:33:16 [__init__.py:253] Automatically detected platform cpu.

    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
    llm = LLM(model=os.path.expanduser("~/workspace/Qwen3-0.6B/"))
    prompts = ["Hello, my name is"]
    outputs = llm.generate(prompts, sampling_params)
    print(f"Prompt: {outputs[0].prompt}")
    print(f"Output: {outputs[0].outputs[0].text}")
    ```
## EngineCore 架构
  - **`LLM` 类** 这是面向用户的高级抽象接口，直接提供给开发者调用。它封装了底层引擎的复杂性，提供简单的 `generate()` 或 `generate_async()` 等方法，用于文本生成。类似于 PyTorch 的 `nn.Module`，是对外暴露的易用层。
    - 接收用户输入的提示（prompt）和生成参数（如 `max_tokens`、`temperature`）。
    - 与底层引擎（如 `LLMEngine`）交互，处理请求的发送和结果的返回。
    - 可能包含一些高级功能（如批量请求管理、流式输出支持）。
  - **`LLMEngine` 类** 推理引擎的核心实现，负责管理推理的全生命周期，包括请求调度、KV Cache 管理、GPU 资源分配等。相当于推理的“操作系统内核”，是性能优化的核心。
    - **请求调度**：处理并发请求，支持连续批处理（Continuous Batching）。
    - **内存管理**：优化 KV Cache 的显存使用（如 PagedAttention）。
    - **执行控制**：调用底层模型（如 HuggingFace 的 Transformer）执行推理。
    - **状态维护**：跟踪每个请求的生成状态（如已生成的 tokens）。
  - **`EngineCoreClient` 类** 客户端接口，负责与 `EngineCoreProc` 通信，将用户的请求转发给工作进程并返回结果。
    - 封装通信协议（如 gRPC、Ray、PyTorch RPC）。
    - 处理序列化/反序列化（将输入文本转为 Tensor，输出 Tensor 转为文本）。
    - 可能实现重试机制或负载均衡。
  - **`EngineCoreProc` 类** 处理引擎的实际计算任务，通常是运行在独立进程（甚至远程机器）中的工作进程。类似抽象类，定义引擎的通用行为。
    - 加载模型权重并执行 GPU 计算（如矩阵乘法、Attention 计算）。
    - 通过 IPC 或 RPC 与 `EngineCoreClient` 通信（如接收输入 tokens，返回输出 logits）。
    - 可能负责多 GPU 的并行计算（Tensor/Pipeline Parallelism）。
  - **`EngineCore` 类** 可能是引擎的底层抽象基类，定义核心接口（如调度策略、内存管理钩子）。`LLMEngine` 会继承或组合它。
    - 提供可扩展的接口，允许自定义调度算法或内存管理策略。
    - 可能包含跨 `EngineCoreClient` 和 `EngineCoreProc` 的共享逻辑。
  - vllm v1的版本采用了多进程的架构，将API server和LLM的核心功能分开在两个进程上，进程之间使用ZeroMQ进行IPC
  - API server负责接收用户请求，对请求进行预处理、分词，将推理的结果进行de-tokenize并且返回给用户
  - 而另一个进程EngineCore负责LLM的核心功能，主要是在一个循环中对请求进行调度和在GPU上进行推理
  ```py
  vllm/entrypoints/llm.py LLM
  def __init__():
      self.llm_engine = LLMEngine.from_engine_args(engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)
  def generate():
      self._validate_and_add_requests()
      outputs = self._run_engine(use_tqdm=use_tqdm)
      return self.engine_class.validate_outputs(outputs, RequestOutput)
  def _run_engine():
      while self.llm_engine.has_unfinished_requests():
          step_outputs = self.llm_engine.step()
          for output in step_outputs:
              if output.finished:
                  outputs.append(output)
  def _validate_and_add_requests():
      for i, prompt in enumerate(it):
          self._add_request()
  def _add_request():
      request_id = str(next(self.request_counter))
      self.llm_engine.add_request(...)

  vllm/v1/engine/llm_engine.py LLMEngine
  def __init__(vllm_config, executor_class):
      # Processor (convert Inputs --> EngineCoreRequests)
      self.processor = Processor(vllm_config=vllm_config, tokenizer=self.tokenizer, mm_registry=mm_registry)
      # OutputProcessor (convert EngineCoreOutputs --> RequestOutput).
      self.output_processor = OutputProcessor(self.tokenizer, log_stats=self.log_stats)
      # EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)
      self.engine_core = EngineCoreClient.make_client(vllm_config, executor_class)
      if not multiprocess_mode:
          # for v0 compatibility
          self.model_executor = self.engine_core.engine_core.model_executor  # type: ignore
    def from_engine_args(cls, engine_args: EngineArgs, ...) -> "LLMEngine":
        """Creates an LLM engine from the engine arguments."""
        # Create the engine configs.
        vllm_config = engine_args.create_engine_config(usage_context)
        executor_class = Executor.get_class(vllm_config)  # -> Executor
        # Create the LLMEngine.
        return cls(vllm_config=vllm_config, executor_class=executor_class, ...)
  def validate_outputs(cls, outputs, output_type):
      return outputs
  def add_request(self, request_id: str, prompt: PromptType, ) -> None:
      # Process raw inputs into the request.
      prompt_str, request = self.processor.process_inputs(...)
      n = params.n if isinstance(params, SamplingParams) else 1
      # Make a new RequestState and queue.
      self.output_processor.add_request(request, prompt_str, None, 0)
      # Add the request to EngineCore.
      self.engine_core.add_request(request)
      return
  def step(self) -> Union[list[RequestOutput], list[PoolingRequestOutput]]:
      # 1) Get EngineCoreOutput from the EngineCore.
      outputs = self.engine_core.get_output()
      # 2) Process EngineCoreOutputs.
      iteration_stats = IterationStats() if self.log_stats else None
      processed_outputs = self.output_processor.process_outputs(outputs.outputs)
      # 3) Abort any reqs that finished due to stop strings.
      self.engine_core.abort_requests(processed_outputs.reqs_to_abort)
      # 4) Record stats
      if self.stat_logger is not None:
          assert outputs.scheduler_stats is not None
          self.stat_logger.record(scheduler_stats=outputs.scheduler_stats, iteration_stats=iteration_stats)
      return processed_outputs.request_outputs
  ```
  - 构造EngineCore的时候，如果没有选择多进程的模式（multiprocess_mode），那么会在当前进程中直接构造一个EngineCore对象，这样就是v0的模式，可以直接在同一个进程中对EngineCore对象调用add_request和step方法
  - 如果选择了多进程的模式，那么就会只会构造一个MPClient，具体来说，根据参数选择的是异步还是同步（asyncio_mode）会构造不同的MPClient，但是二者的基类都是MPClient。所谓MPClient就是将EngineCore构造在另一个进程上，然后自己就是EngineCore在当前进程上的一个client
  - 具体来说：假设是同步的模式（asyncio_mode=False），那么会构造一个SyncMPClient，在构造它之前，会先构造它的基类MPClient
  - 在MPClient构造时，通过BackgroundProcHandle对象创建了一个后台进程，名为EngineCore，该进程的主线程调用的函数为EngineCoreProc类中的静态成员函数run_engine_core
  ```py
  vllm/v1/engine/core_client.py EngineCoreClient
  def __init__(self, vllm_config, executor_class):
      if multiprocess_mode and asyncio_mode:
          return EngineCoreClient.make_async_mp_client(vllm_config, executor_class, log_stats)
      if multiprocess_mode and not asyncio_mode:
          return SyncMPClient(vllm_config, executor_class, log_stats)
      return InprocClient(vllm_config, executor_class, log_stats)  # for v0 compatibility

  # for v0 compatibility
  vllm/v1/engine/core_client.py InprocClient
  def __init__(self, *args, **kwargs):
      self.engine_core = EngineCore(*args, **kwargs)
  def get_output(self) -> EngineCoreOutputs:
      outputs, _ = self.engine_core.step()
      return outputs.get(0) or EngineCoreOutputs()
  def add_request(self, request: EngineCoreRequest) -> None:
      self.engine_core.add_request(request)
  def profile(self, is_start: bool = True) -> None:
      self.engine_core.profile(is_start)

  vllm/v1/engine/core_client.py MPClient(EngineCoreClient)
  def __init__(vllm_config, executor_class):
      self.encoder = MsgpackEncoder()
      self.decoder = MsgpackDecoder(EngineCoreOutputs)

      sync_ctx = zmq.Context(io_threads=2)
      self.resources = BackgroundResources(ctx=sync_ctx)
      with launch_core_engines(vllm_config, executor_class, log_stats) as (engine_manager, coordinator, addresses):
          self.resources.coordinator = coordinator
          self.resources.engine_manager = engine_manager
          # Create input and output sockets.
          self.input_socket = self.resources.input_socket = make_zmq_socket(self.ctx, input_address, zmq.ROUTER, bind=True)
          self.resources.output_socket = make_zmq_socket(self.ctx, output_address, zmq.PULL)
          # ZMQ identity of each engine that this client will talk to. Then wait for ready messages from each engine on the input socket.
          self.core_engines: list[EngineIdentity] = [index.to_bytes(2, "little") for index in engine_ranks]
          self.core_engine: EngineIdentity = self.core_engines[0]

  vllm/v1/engine/core_client.py SyncMPClient(MPClient)
  def __init__(vllm_config, executor_class, ...):
      super().__init__(vllm_config, executor_class)
      self.outputs_queue = queue.Queue[Union[EngineCoreOutputs, Exception]]()
      def process_outputs_socket():
          outputs: EngineCoreOutputs = decoder.decode(frames)
      self.output_queue_thread = Thread(target=process_outputs_socket, name="EngineCoreOutputQueueThread", daemon=True)
  def get_output(self) -> EngineCoreOutputs:
      outputs = self.outputs_queue.get()
      return outputs
  def _send_input(self, request_type: EngineCoreRequestType, request: Any):
      tracker = self.input_socket.send_multipart(msg, copy=False, track=True)
      self.add_pending_message(tracker, request)
  def add_request(self, request: EngineCoreRequest) -> None:
      self._send_input(EngineCoreRequestType.ADD, request)

  vllm/v1/engine/utils.py launch_core_engines
  with zmq_socket_ctx(local_handshake_address, zmq.ROUTER, bind=True) as handshake_socket:
      from vllm.v1.engine.core import EngineCoreProc
      # Start local engines.
      if local_engine_count:
          local_engine_manager =(EngineCoreProc.run_engine_core, executor_class=executor_class, ...)
      yield local_engine_manager, coordinator, addresses
      # Now wait for engines to start.
      wait_for_engine_startup(.., local_engine_manager, ...)
  ```
  - EngineCoreProc类是EngineCore类的子类，为了实现在独立的进程上运行EngineCore而对EngineCore进行了一层包装，主要用来处理EngineCore与主进程的通信。具体来说，EngineCoreProc对象中有两个队列input_queue和output_queue，一个用来接收LLMEngine发送的请求，一个用来存放自己处理完的结果，并且还创建了两个线程来分别处理这两个队列
  - 如果EngineCore的Scheduler的waiting队列和running队列中没有任何req，那么它会poll input_queue，直到input_queue中出现client发来的新的req。然后会调用_handle_client_request方法处理这个新来的req，会判断这个请求的类型是什么，如果是add的话，那么就调用EngineCore的add_request方法将它加入到Scheduler中，此时才是真的把request加入到Scheduler的waiting队列中，并且将它的id和req对象的映射加入到Scheduler的requests表中。
  - 如果Scheduler中有可以调度的req，那么就不需要对input_queue进行忙等了，而是顺便检查一下input_queue中有没有新到来的req，有的话就顺便把它加到Scheduler的waiting队列中
  - 然后run_busy_loop会调用一次step，其中会从waiting和running队列中调度req并执行一次前向。执行完之后将推理的结果保存在EngineCoreProc的output_queue中，然后重复上面的过程，继续调度Scheduler中的req或者将input_queue中新来的req加入到Scheduler中
  - EngineCore的主线程做的事情，总的来说就是从input_queue中获取请求，然后将它添加到Scheduler的队列中，然后调用step将执行的结果保存在output_queue中；
  - 所以新 token 生成后，一方面交给 scheduler 进行下一个迭代，同时也返回给client 进程进行解码，Scheduler的下一次迭代不用等待解码的结果，而不用像V0那样，每一次step产生的新 token 都会先通过 detokenizer，得到对应的字符串，再将 token 和字符串更新每一个 request 的状态，然后再进行下一次前向
  ```py
  vllm/v1/engine/core.py EngineCoreProc(EngineCore)
  def __init__(self):
      self.input_queue = queue.Queue[tuple[EngineCoreRequestType, Any]]()
      self.output_queue = queue.Queue[Union[tuple[int, EngineCoreOutputs], bytes]]()
      self.step_fn = (self.step if self.batch_queue is None else self.step_with_batch_queue)
  def run_engine_core():
      engine_core = EngineCoreProc(*args, **kwargs)
      engine_core.run_busy_loop()
  def run_busy_loop(self):
      while True:
          self._process_input_queue()  # 1) Poll the input queue until there is work to do.
          self._process_engine_step()  # 2) Step the engine core and return the outputs.
  def _process_input_queue(self):
    while not self.engines_running and not self.scheduler.has_requests():
        req = self.input_queue.get()
      # Handle any more client requests.
      while not self.input_queue.empty():
          req = self.input_queue.get_nowait()
          self._handle_client_request(*req)
  def _process_engine_step(self) -> bool:
      # Step the engine core.
      outputs, model_executed = self.step_fn()
      # Put EngineCoreOutputs into the output queue.
      for output in (outputs.items() if outputs else ()):
          self.output_queue.put_nowait(output)
      return model_executed
  def _handle_client_request(self, request_type: EngineCoreRequestType, request: Any) -> None:
      """Dispatch request from client."""
      if request_type == EngineCoreRequestType.ADD:
          self.add_request(request)
  def process_input_sockets(self, input_addresses: list[str], coord_input_address: Optional[str], identity: bytes):
      """Input socket IO thread."""
      with ExitStack() as stack, zmq.Context() as ctx:
          while True:
              for input_socket, _ in poller.poll():
                  # Deserialize the request data.
                  decoder = add_request_decoder if (request_type == EngineCoreRequestType.ADD) else generic_decoder
                  request = decoder.decode(data_frames)
                  # Push to input queue for core busy loop.
                  self.input_queue.put_nowait((request_type, request))

  def process_output_sockets(self, output_paths: list[str], coord_output_path: Optional[str], engine_index: int):
      """Output socket IO thread."""
      with ExitStack() as stack, zmq.Context() as ctx:
          while True:
              output = self.output_queue.get()
              buffer = reuse_buffers.pop() if reuse_buffers else bytearray()
              buffers = encoder.encode_into(outputs, buffer)
              tracker = sockets[client_index].send_multipart(buffers, copy=False, track=True)

  vllm/v1/engine/core.py EngineCore
  def __init__(vllm_config, executor_class, ...):
      self.model_executor = executor_class(vllm_config)  # -> executor
      num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(vllm_config)
      self.scheduler: SchedulerInterface = Scheduler(...)
      self.batch_queue = queue.Queue(self.batch_queue_size)
  def add_request(self, request: EngineCoreRequest):
      self.scheduler.add_request(req)
  def execute_model(self, scheduler_output: SchedulerOutput):
      return self.model_executor.execute_model(scheduler_output)
  def step(self) -> tuple[dict[int, EngineCoreOutputs], bool]:
      """Schedule, execute, and make output. Returns tuple of outputs and a flag indicating whether the model was executed"""
      # Check for any requests remaining in the scheduler - unfinished, or finished and not yet removed from the batch.
      if not self.scheduler.has_requests():
          return {}, False
      scheduler_output = self.scheduler.schedule()
      model_output = self.execute_model(scheduler_output)
      engine_core_outputs = self.scheduler.update_from_output(scheduler_output, model_output)  # type: ignore
      return (engine_core_outputs, scheduler_output.total_num_scheduled_tokens > 0)
  def profile(self, is_start: bool = True):
      self.model_executor.profile(is_start)
  def _initialize_kv_caches(...):  # -> kv cache
      kv_cache_specs = self.model_executor.get_kv_cache_specs()
      available_gpu_memory = self.model_executor.determine_available_memory()
      self.model_executor.initialize_from_config(kv_cache_configs)
  ```
  - 多进程的EngineCore的数据的流转过程为：
    - client进程的主线程A通过generate方法对prompt挨个调用add_request，调用栈如下所示。最后在llm_engine的add_request方法中，调用LLMEngine的成员变量engine_core的add_request方法
    - 然后实际调用的是SyncMPClient的add_request方法，通过socket将数据发送给EngineCore进程
    - EngineCore进程中的process_input_socket线程负责从socket中读取出来自client进程发送的request数据，并将它反序列化之后保存在input_queue中
    - EngineCore进程的run_busy_loop线程运行负责从input_queue中将数据读取出来，保存到EngineCore对象的Scheduler成员的waiting队列中
    - 然后run_busy_loop线程再调用一次step，从waiting和running队列中调度req并执行一次前向，将计算得到的数据保存在output_queue中，也会将计算得到的token更新req，不需要等待client进程（在Scheduler中有req的情况下），紧接着进行下一次调度
    - EngineCore进程的process_output_socket线程将output_queue中的数据序列化之后通过socket发送出去
    - 然后client进程的EngineCoreOutputQueueThread线程再从socket中将数据反序列化之后保存在SyncMPClient的output_queue中
    - 此时对client进程的主线程A来说，在generate方法中刚刚调用完_validate_and_add_requests方法将prompt发送出去，接着调用_run_engine方法，其中在while循环中反复调用llm_engine的step方法。v1版本的LLMEngine的step方法直接从EngineCore中调用get_output即可（在本例中实际是对MPClient调用get_output），将SyncMPClient的output_queue中的数据获取出来。
    - 对于v1版本的llm_engine来说，step方法很简单，不需要做任何的调度和计算的操作，因为主要的工作都由EngineCore的step方法代劳了，不管是多进程的EngineCore还是本地的EngineCore都是如此；llm_engine的step方法只需要调用engine_core的get_output方法即可：
    - 对于InprocClient，它的get_output方法需要调用本地的EngineCore的step，然后等step执行完毕后将结果返回。这样的话add_request、tokenize、detokenize和调度与GPU执行之间就是串行的，如果模型比较小，在GPU上执行的时间比较少，那么就会导致CPU的overhead很大。
    - 对于MPClient，get_output方法则直接从队列中获取输出即可；因为step由另一个进程的EngineCore执行完了，这样就实现了add_request（包括接受用户请求、对请求进行预处理、分词）和调度与GPU执行的重叠。这就是为什么在调试vllm v1的时候，明明一次性add_request了好几个prompt，但是在调度时却显示waiting队列中只有一个req
## executor 架构
  - 对于EngineCore来说，当它想要worker执行什么方法时，就要调用self.model_executor的方法，比如检测worker所在的device最多可以给kv cache分配多少空间，或者根据device的空间初始化kv cache，或者执行模型
  - 对于这些方法，在EngineCore进程中实际上并没有真正执行，而是调用了Executor基类的对应方法，调用collective_rpc将这些方法名广播给其他进程上的worker，然后等待worker的执行结果，再将结果返回。
  - 而collective_rpc所做的事情就是将方法名加入到rpc_broadcast_mq中，然后遍历所有的worker的worker_reponse_mq，将worker的执行结果返回
  - V1与V0的MultiprocExecutor最主要的区别在于：V0中的EngineCore和worker0在同一个进程，这样做可以在向各个工作进程广播输入数据时减少进程间的通信开销，但这种设计导致了不对称架构，增加了系统复杂性。
  ```py
  vllm/executor/executor_base.py ExecutorBase(ExecutorBase)
  def __init__(self, vllm_config: VllmConfig) -> None:
      self._init_executor()

  vllm/v1/executor/abstract.py Executor(ExecutorBase)
  def get_class(vllm_config: VllmConfig) -> type["Executor"]:
      elif distributed_executor_backend == "mp":
          from vllm.v1.executor.multiproc_executor import MultiprocExecutor
          executor_class = MultiprocExecutor
      elif distributed_executor_backend == "uni":
          executor_class = UniProcExecutor
      return executor_class

  vllm/v1/executor/multiproc_executor MultiprocExecutor
  def _init_executor(self) -> None:
      distributed_init_method = get_distributed_init_method("127.0.0.1", get_open_port())
      # Initialize worker and set up message queues for SchedulerOutputs and ModelRunnerOutputs
      max_chunk_bytes = envs.VLLM_MQ_MAX_CHUNK_BYTES_MB * 1024 * 1024
      self.rpc_broadcast_mq = MessageQueue(self.world_size, self.world_size, max_chunk_bytes=max_chunk_bytes)
      scheduler_output_handle = self.rpc_broadcast_mq.export_handle()

      # Create workers
      for rank in range(self.world_size):
          WorkerProc.make_worker_process(vllm_config=self.vllm_config, local_rank=rank, input_shm_handle=scheduler_output_handle, ...))
          self.workers = WorkerProc.wait_for_ready(unready_workers)
          self.start_worker_monitor()
  ```
  - make_worker_process方法主要的任务就是创建另一个进程并在其中构造worker，并且还要创建一个worker_response_mq消息队列，用来接收worker的执行结果。然后将worker进程和消息队列打包在一起作为worker进程在EngineCore进程本地的一个Handle返回
  - 在WorkerProc对象构造时，会创建两个消息队列，一个是rpc_broadcast_mq，用来从EngineCore接收调度的结果；一个是worker_response_mq，用来将模型的执行结果发送给EngineCore；然后发送信号给EngineCore进程，告诉他Worker的消息队列已经准备好了，EngineCore可以开始进行调度了
  - WorkerProc对象构造完毕后，开始执行worker_busy_loop方法，此方法中就是在一个死循环中不断尝试从rpc_broadcast_mq中获取EngineCore进程广播来的方法名，获取到了之后，就通过self.worker对象执行该方法，然后将执行的结果加入到worker进程的worker_response_mq队列中
  ```py
  vllm/v1/executor/multiproc_executor WorkerProc
  def make_worker_process() -> UnreadyWorkerProcHandle:
      # Run EngineCore busy loop in background process.
      proc = context.Process(target=WorkerProc.worker_main, kwargs=process_kwargs, name=f"VllmWorker-{rank}", daemon=True)
      proc.start()
  def worker_main(*args, **kwargs):
      worker = WorkerProc(*args, **kwargs)
      # Send READY once we know everything is loaded
      ready_writer.send({"status": WorkerProc.READY_STR, "handle": worker.worker_response_mq.export_handle()})
      worker.rpc_broadcast_mq.wait_until_ready()
      worker.worker_response_mq.wait_until_ready()
      worker.worker_busy_loop()
  def __init__():
      # Initialize MessageQueue for receiving SchedulerOutput
      self.rpc_broadcast_mq = MessageQueue.create_from_handle(input_shm_handle, self.worker.rank)
      # Initializes a message queue for sending the model output
      self.worker_response_mq = MessageQueue(1, 1)

      wrapper = WorkerWrapperBase(vllm_config=vllm_config, rpc_rank=rank)
      wrapper.init_worker(all_kwargs)
      self.worker = wrapper
      self.worker.init_device()
      self.worker.load_model()
  def worker_busy_loop(self):
      """Main busy loop for Multiprocessing Workers"""
      while True:
          if isinstance(method, str):
              func = getattr(self.worker, method)
          elif isinstance(method, bytes):
              func = partial(cloudpickle.loads(method), self.worker)
          output = func(*args, **kwargs)
          if output_rank is None or self.rank == output_rank:
              self.worker_response_mq.enqueue((WorkerProc.ResponseStatus.SUCCESS, output))
  ```
  ```py
  vllm/v1/worker/gpu_worker Worker(WorkerBase)
  def init_device(self):
      # Construct the model runner
      self.model_runner: GPUModelRunner = GPUModelRunner(self.vllm_config, self.device)
  def load_model(self) -> None:
      self.model_runner.load_model()

  vllm_ascend/worker/model_runner_v1.py NPUModelRunner(LoRAModelRunnerMixin)
  def load_model(self) -> None:
    with DeviceMemoryProfiler() as m:
      self.model = get_model(vllm_config=self.vllm_config)
  def execute_model():
      attn_metadata, hidden_states, ... = self._process_reqs(scheduler_output, intermediate_tensors)
  def _process_reqs():
      with set_forward_context(...):
          hidden_states = self.model(...)

  vllm/v1/worker/cpu_model_runner class CPUModelRunner(GPUModelRunner)
  vllm/v1/worker/gpu_model_runner GPUModelRunner(LoRAModelRunnerMixin)
  def load_model(self) -> None:
      with DeviceMemoryProfiler() as m:
          model_loader = get_model_loader(self.load_config)
          self.model = model_loader.load_model(vllm_config=self.vllm_config, model_config=self.model_config)
  def execute_model(...):
      with set_forward_context(...):
          self.maybe_setup_kv_connector(scheduler_output)
          model_output = self.model(
              input_ids=input_ids, positions=positions, intermediate_tensors=intermediate_tensors, inputs_embeds=inputs_embeds,
          )
          self.maybe_wait_for_kv_save()
  def _allocate_kv_cache_tensors(self, kv_cache_config: KVCacheConfig) -> dict[str, torch.Tensor]:
      kv_cache_raw_tensors: dict[str, torch.Tensor] = {}
      for kv_cache_tensor in kv_cache_config.kv_cache_tensors:
          tensor = torch.zeros(kv_cache_tensor.size, dtype=torch.int8, device=self.device)
          for layer_name in kv_cache_tensor.shared_by:
              kv_cache_raw_tensors[layer_name] = tensor
      return kv_cache_raw_tensors
  def initialize_kv_cache_tensors(self, kv_cache_config: KVCacheConfig) -> dict[str, torch.Tensor]:
      # Initialize the memory buffer for KV cache
      kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
      # Change the memory buffer to the desired shape
      kv_caches = self._reshape_kv_cache_tensors(kv_cache_config, kv_cache_raw_tensors)
      bind_kv_cache(kv_caches, self.compilation_config.static_forward_context, self.kv_caches)
      return kv_caches
  def initialize_kv_cache(self, kv_cache_config: KVCacheConfig) -> None:
      self.initialize_attn_backend(kv_cache_config)
      kv_caches = self.initialize_kv_cache_tensors(kv_cache_config)
  def _dummy_run():
      pass
  ```
## kv Cache
  - 在engine/core.py中，EngineCore类在初始化完Executor后，就紧接着对KV cache进行初始化，以及对Scheduler进行初始化，将Scheduler对象维护在EngineCore对象中
    - 首先会调用determine_available_memory函数，获取每个worker所在的device的最多可以给kv cache分配多少显存，这个的计算方式是：先使用dummy输入在device上进行一次前向，得到模型峰值的显存使用量，然后将device的总显存量乘以gpu_memory_utilization再减去峰值显存使用量，就能得到该device上最多可以给kv cache分配多少空间。
    - 然后再调用get_kv_cache_config获取kv cache的各种配置信息
  - Scheduler 中会对 KVCacheManager 初始化
    - KVCacheManager 中使用req_to_blocks对象替换了原来的block_tables对象，直接维护了每个request的id到它的kv cache block之间的映射，删除了PhysicalTokenBlock，变成了这里的KVCacheBlock，并且原来的block_tables是以seq为单位，而这里是以request为单位映射（V1疑似不支持beam search？）
    - free_block_queue对象：将空闲的kv cache block串成双向链表，用来做LRU替换；在kv cache block对象内部有两个指针指向相邻的kv cache block；此列表包括未被使用过的KVCacheBlock（没有hash值）和曾经被使用过现在空闲的KVCacheBlock（有hash值）；当一个KVCacheBlock被释放时会push到此列表的最后，当给用户分配新的KVCacheBlock时会从此列表的开头pop一个block；被pop的block有可能是从未被使用过的，也有可能是曾经被用过但现在空闲的block，如果是后者那么需要将它从cached_block表中驱逐
    - cached_block_hash_to_block：kv cache block的hash值到KVCacheBlock之间的映射，此字典相当于是原来的cached_blocks表和evictor中的表的合集，也就是包括了所有拥有hash id的KVCacheBlock（包括正在被running req使用的block和曾经被使用但是现在空闲的block）；用户请求block时直接从此表中查找即可，如果此表中不存在，那么就去上面的free_block_queue查找
    - 所以使用了prefix cache之后，系统中kv cache的整个运转流程是：对于正在运行的req来说，它的kv cache全部保留，不在free_block_queue中，不允许被驱逐；如果该req被抢占，那么它的kv cache被放入free_block_queue中，可以被驱逐，但是不会在被抢占的瞬间被驱逐；然后用户对block_pool进行申请或释放时，会对free_block_queue按照LRU策略进行分配或释放，释放block时放到free_block_queue的最后，分配时从free_block_queue的头部分配，被分配的块有可能是被抢占的req的block，那么此时该req的kv cache才会被驱逐
    - 所以V1这里对prefix cache的管理更加地高效，KVCacheBlock的分配的复杂度是O(1)，启用prefix cache后即使命中率为0也不会导致性能的下降，所以V1默认启用了prefix cache
  ```py
  vllm/v1/core/sched/scheduler.py Scheduler(SchedulerInterface)
  def __init__(vllm_config, kv_cache_config, ...):
      ...
      self.caching_hash_fn = (
            sha256_cbor_64bit if caching_hash_algo == "sha256_cbor_64bit" else
            sha256 if caching_hash_algo == "sha256" else hash)
      self.kv_cache_manager = KVCacheManager(kv_cache_config=kv_cache_config, ...)

  vllm/v1/core/kv_cache_manager.py KVCacheManager
  def __init__(kv_cache_config, enable_caching: bool = True, ...):
      self.coordinator = get_kv_cache_coordinator(kv_cache_config, enable_caching, ...)
      self.block_pool = self.coordinator.block_pool
      # Mapping from request ID to kv block hashes.
      # This is to avoid recomputing the block hashes for each call of `get_computed_blocks` or `allocate_slots`.
      self.req_to_block_hashes: defaultdict[str, list[BlockHash]] = defaultdict(list)
  def get_computed_blocks(self, request: Request) -> tuple[KVCacheBlocks, int]:
      # The block hashes for the request may already be computed if the scheduler has tried to schedule the request before.
      block_hashes = self.req_to_block_hashes[request.request_id]
      if not block_hashes:
          block_hashes = hash_request_tokens(self.caching_hash_fn, self.block_size, request)
          self.req_to_block_hashes[request.request_id] = block_hashes

      max_cache_hit_length = request.num_tokens - 1
      computed_blocks, num_new_computed_tokens = self.coordinator.find_longest_cache_hit(block_hashes, max_cache_hit_length)
      return KVCacheBlocks(computed_blocks), num_new_computed_tokens

  vllm/v1/core/kv_cache_coordinator.py
  def get_kv_cache_coordinator():
      return HybridKVCacheCoordinator(kv_cache_config, ...)

  vllm/v1/core/kv_cache_coordinator.py HybridKVCacheCoordinator(KVCacheCoordinator)
  vllm/v1/core/kv_cache_coordinator.py KVCacheCoordinator
  def __init__(...):
      self.block_pool = BlockPool(kv_cache_config.num_blocks, enable_caching, enable_kv_cache_events)
      self.single_type_managers = tuple(
          get_manager_for_kv_cache_spec(block_pool=self.block_pool, ...) for i, kv_cache_group in enumerate(self.kv_cache_config.kv_cache_groups)
      )
      self.full_attention_manager_cls = FullAttentionManager
  def find_longest_cache_hit(self, block_hashes: list[BlockHash], max_cache_hit_length: int) -> tuple[tuple[list[KVCacheBlock], ...], int]:
      # First, find the longest cache hit for full attention.
      hit_blocks_full_attn = self.full_attention_manager_cls.find_longest_cache_hit(...)
      hit_length = len(hit_blocks_full_attn[0]) * self.full_attention_block_size
      # Next, find the cache hit for the other attention WITHIN the cache hit of full attention.
      hit_blocks_other_attn = self.other_attention_cls.find_longest_cache_hit()
      hit_length = len(hit_blocks_other_attn[0]) * self.other_block_size
      # Truncate the full attention cache hit to the length of the cache hit of the other attention.
      for group_hit_blocks in hit_blocks_full_attn:
          del group_hit_blocks[hit_length // self.full_attention_block_size:]
      # Merge the hit blocks of full attention and other attention.
      if self.full_attn_first:
          hit_blocks = hit_blocks_full_attn + hit_blocks_other_attn
      else:
          hit_blocks = hit_blocks_other_attn + hit_blocks_full_attn
      return hit_blocks, hit_length

  vllm/v1/core/single_type_kv_cache_manager.py FullAttentionManager(SingleTypeKVCacheManager)
  def find_longest_cache_hit(block_hashes, ...):
      computed_blocks: tuple[list[KVCacheBlock], ...] = tuple([] for _ in range(len(kv_cache_group_ids)))
      max_num_blocks = max_length // kv_cache_spec.block_size
      for i, block_hash in zip(range(max_num_blocks), block_hashes):
          # block_hashes is a chain of block hashes. If a block hash is not in the cached_block_hash_to_id,
          # the following block hashes are not computed yet for sure.
          if cached_block := block_pool.get_cached_block(block_hash, kv_cache_group_ids):
              for computed, cached in zip(computed_blocks, cached_block):
                  computed.append(cached)
          else:
              break
      if use_eagle and computed_blocks[0]:
          for computed in computed_blocks:
              computed.pop()
      return computed_blocks

  vllm/v1/core/kv_cache_utils.py hash_request_tokens
  def hash_request_tokens(hash_function: Any, block_size: int, request: Request) -> list[BlockHash]:
      """Computes hash values of a chain of blocks given a sequence of token IDs. The hash value is used for prefix caching."""
      token_ids = request.all_token_ids
      for start in range(0, len(token_ids), block_size):
          end = start + block_size
          block_token_ids = token_ids[start:end]
          # Do not hash the block if it is not full.
          if len(block_token_ids) < block_size:
              break
          block_hash = hash_block_tokens(hash_function, parent_block_hash_value, block_token_ids, req_extra_keys)
          ret.append(block_hash)
          parent_block_hash_value = block_hash.hash_value
      return ret
  def hash_block_tokens(hash_function: Callable, parent_block_hash: Optional[int], curr_block_token_ids: Sequence[int], ...) -> BlockHash:
      """Computes a hash value corresponding to the contents of a block and the contents of the preceding block(s).
      The hash value is used for prefix caching.
      We use LRU cache for this function to avoid recomputing hash values for the same block contents.
      """
      if not parent_block_hash:
          parent_block_hash = NONE_HASH
      curr_block_token_ids_tuple = tuple(curr_block_token_ids)
      return BlockHash(hash_function((parent_block_hash, curr_block_token_ids_tuple, extra_keys)), curr_block_token_ids_tuple, extra_keys)

  vllm/v1/core/block_pool.py BlockPool
  def __init__(num_gpu_blocks: int, enable_caching: bool, enable_kv_cache_events: bool = False):
      # All kv-cache blocks.
      self.blocks: list[KVCacheBlock] = [KVCacheBlock(idx) for idx in range(num_gpu_blocks)]
      # Free block queue that constructs and manipulates a doubly linked list of free blocks (including eviction candidates when caching is enabled).
      self.free_block_queue = FreeKVCacheBlockQueue(self.blocks)
      # {block_hash: {block ID: block}}. A cached block is a full block with a block hash that can be used for prefix caching.
      self.cached_block_hash_to_block: dict[BlockHashWithGroupId, dict[int, KVCacheBlock]] = defaultdict(dict)
  def get_cached_block(...):
      pass
  def cache_full_blocks(...):
      pass
  def get_new_blocks(self, num_blocks: int) -> list[KVCacheBlock]:
      pass
  def free_blocks(self, ordered_blocks: Iterable[KVCacheBlock]) -> None:
      pass
  def get_num_free_blocks(self) -> int:
      pass
  def get_usage(self) -> float:
      pass
  ```
## Scheduler的初始化
  - 在engine/core.py中，EngineCore类init时完成对kv cache的初始化后，会对Scheduler进行初始化，将Scheduler对象维护在EngineCore对象中
  - v1的调度器不区分prefill和decode阶段，调度决策以一个简单的字典形式表示，例如{请求ID: token数量}，调度的过程中，每个请求只记录已处理的token数量和需要处理的token数量，调度器尝试在每次调度时让已处理token数量追上需要处理的token数量。
  - 请求到来调度器需要把它加入到waiting队列中等待调度，同时加入到全局的请求记录Map中
  - 每次调度时，先调度running队列中的请求，如无抢占行为再调度waiting队列中的请求。
    - 对running队列中的每个请求尝试在kv_cache_manager中分配token需要的slots，如果不足失败则开启抢占模式，释放低优先级请求占用的空间。
    - 对waiting队列中的每个请求首先尝试在kv_cache_manager中分配token需要的slots，如果不足则继续在waiting队列等待。
  - waiting队列的调度流程：
    - 对waiting队列的每一个req，首先在kv_cache_manager中调用get_computed_blocks函数获取已经被计算过的KVCacheBlock
  ```py
  vllm/v1/core/sched/scheduler.py Scheduler(SchedulerInterface)
  def __init__(vllm_config, kv_cache_config, ...):
      # req_id -> Request
      self.requests: dict[str, Request] = {}
      # Scheduling policy
      if self.scheduler_config.policy == "priority":
          self.policy = SchedulingPolicy.PRIORITY
      elif self.scheduler_config.policy == "fcfs":
          self.policy = SchedulingPolicy.FCFS
      # Priority queues for requests.
      self.waiting = create_request_queue(self.policy)
      self.running: list[Request] = []

      # The request IDs that are finished in between the previous and the current steps.
      # This is used to notify the workers about the finished requests so that they can free the cached states for those requests.
      # This is flushed at the end of each scheduling step.
      self.finished_req_ids: set[str] = set()
      # KV Connector: requests in process of async KV loading or recving
      self.finished_recving_kv_req_ids: set[str] = set()
      self.kv_cache_manager = KVCacheManager(kv_cache_config=kv_cache_config, ...)
  def add_request(self, request: Request) -> None:
      self.waiting.add_request(request)
      self.requests[request.request_id] = request
  def finish_requests(self, request_ids: Union[str, Iterable[str]], finished_status: RequestStatus) -> None:
      self._free_request(request)
  def _free_request(self, request: Request) -> Optional[dict[str, Any]]:
      delay_free_blocks, kv_xfer_params = self._connector_finished(request)
      self.encoder_cache_manager.free(request)
      request_id = request.request_id
      self.finished_req_ids.add(request_id)
      self.finished_req_ids_dict[request.client_index].add(request_id)
      self._free_blocks(request)
      return kv_xfer_params
  def _free_blocks(self, request: Request):
      self.kv_cache_manager.free(request)
      self.kv_cache_manager.free_block_hashes(request)
      del self.requests[request.request_id]
  def schedule(self) -> SchedulerOutput:
      # First, schedule the RUNNING requests.
      while req_index < len(self.running) and token_budget > 0:
          request = self.running[req_index]
          while True:
              num_new_tokens = min(num_new_tokens, token_budget)
              new_blocks = self.kv_cache_manager.allocate_slots(request, num_new_tokens, num_lookahead_tokens=self.num_lookahead_tokens)
              if new_blocks is None:  # The request cannot be scheduled. Preempt the lowest-priority
                  if self.policy == SchedulingPolicy.PRIORITY:
                      request.preempted_req = max(self.running, key=lambda r: (r.priority, r.arrival_time))
                      self.running.remove(preempted_req)
                  else:
                      preempted_req = self.running.pop()
                  self.kv_cache_manager.free(preempted_req)
                  self.waiting.prepend_request(preempted_req)
                  preempted_reqs.append(preempted_req)
              # Schedule the request.
              scheduled_running_reqs.append(request)
              req_to_new_block_ids[request.request_id] = new_blocks.get_block_ids()
              num_scheduled_tokens[request.request_id] = num_new_tokens
              token_budget -= num_new_tokens
              req_index += 1

      # Next, schedule the WAITING requests.
      while self.waiting and token_budget > 0:
          if len(self.running) == self.max_num_running_reqs:
              break
          request = self.waiting.peek_request()
          # Get already-cached tokens.
          if request.num_computed_tokens == 0:
              # Get locally-cached tokens.
              new_computed_blocks, num_new_local_computed_tokens = self.kv_cache_manager.get_computed_blocks(request)
              # Total computed tokens (local + external).
              num_computed_tokens = (num_new_local_computed_tokens + num_external_computed_tokens)
          new_blocks = self.kv_cache_manager.allocate_slots(request, ...)
          if new_blocks is None:
              # The request cannot be scheduled.
              break
          request = self.waiting.pop_request()
          req_index += 1
          self.running.append(request)

          scheduler_output = SchedulerOutput(...)
          self._update_after_schedule(scheduler_output)
          return scheduler_output
  ```
## Prefix caching
  - [vLLM的prefix cache为何零开销](https://zhuanlan.zhihu.com/p/1896927732027335111)
  - [图解Vllm V1系列6：KVCacheManager与PrefixCaching](https://zhuanlan.zhihu.com/p/1916181593229334390)
  - block和free queue作为独立元素定义在kv_cache_utils里面。block_pool负责block的开辟、释放、cache等操作，还承载free queue；kv_cache_manager 则是对外的接口类（无继承关系），串联所有模块，同时兼容了非prefix的场景。
***

# VLLM Ascend
  - `vllm/__init__.py`
    ```py
    from msserviceprofiler.vllm_profiler.vllm_v0 import request_hookers, model_hookers, batch_hookers, kvcache_hookers
    from msserviceprofiler.vllm_profiler import apply_hooks
    apply_hooks()
    ```
  - `ms_service_profiler.py`
    ```py
    from collections import namedtuple
    Level = namedtuple("Level", ["INFO"])("INFO")

    class FakeProfiler:
        def __init__(self, *args):
            logger.debug(f"__init__ {args = }")

        def domain(self, *args):
            logger.debug(f"domain {args = }")
            return self

        def res(self, *args):
            logger.debug(f"res {args = }")
            return self

        def event(self, *args):
            logger.debug(f"event {args = }")
            return self

        def metric(self, *args):
            logger.debug(f"metric {args = }")
            return self

        def metric_inc(self, *args):
            logger.debug(f"metric {args = }")
            return self

        def span_start(self, *args):
            logger.debug(f"span_start {args = }")
            return self

        def span_end(self, *args):
            logger.debug(f"span_end {args = }")
            return self

    ```
***

# Offline inference
## Basic Offline inference
  ```py
  import os
  # Note: There should be no Warning like: No module named 'vllm._C'
  from vllm import LLM, SamplingParams
  # INFO 07-11 09:33:16 [__init__.py:253] Automatically detected platform cpu.

  sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
  llm = LLM(model=os.path.expanduser("~/workspace/Qwen3-0.6B/"))
  prompts = ["Hello, my name is"]
  outputs = llm.generate(prompts, sampling_params, stream=True)
  print(f"Prompt: {outputs[0].prompt}")
  print(f"Output: {outputs[0].outputs[0].text}")
  ```
## LLM Engine Example
  - [LLM Engine Example](https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example.html)
  ```py
  import argparse

  from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams
  from vllm.utils import FlexibleArgumentParser


  def create_test_prompts() -> list[tuple[str, SamplingParams]]:
      """Create a list of test prompts with their sampling parameters."""
      return [
          ("A robot may not injure a human being", SamplingParams(temperature=0.0, logprobs=1, prompt_logprobs=1)),
          ("To be or not to be,", SamplingParams(temperature=0.8, top_k=5, presence_penalty=0.2)),
          ("What is the meaning of life?", SamplingParams(n=2, temperature=0.8, top_p=0.95, frequency_penalty=0.1)),
      ]


  def process_requests(engine: LLMEngine, test_prompts: list[tuple[str, SamplingParams]]):
      """Continuously process a list of prompts and handle the outputs."""
      request_id = 0

      print("-" * 50)
      while test_prompts or engine.has_unfinished_requests():
          if test_prompts:
              prompt, sampling_params = test_prompts.pop(0)
              engine.add_request(str(request_id), prompt, sampling_params)
              request_id += 1

          request_outputs: list[RequestOutput] = engine.step()

          for request_output in request_outputs:
              print(f"{request_output = }")
              if request_output.finished:
                  print(request_output)
                  print("-" * 50)


  def initialize_engine(args: argparse.Namespace) -> LLMEngine:
      """Initialize the LLMEngine from the command line arguments."""
      engine_args = EngineArgs.from_cli_args(args)
      return LLMEngine.from_engine_args(engine_args)


  def parse_args():
      parser = FlexibleArgumentParser(description="Demo on using the LLMEngine class directly")
      parser = EngineArgs.add_cli_args(parser)
      return parser.parse_args()


  def main(args: argparse.Namespace):
      """Main function that sets up and runs the prompt processing."""
      engine = initialize_engine(args)
      test_prompts = create_test_prompts()
      process_requests(engine, test_prompts)

  if __name__ == "__main__":
      args = parse_args()
      main(args)
  ```
## Async LLM Streaming
  - [Async LLM Streaming](https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming.html)
  ```py
  import asyncio

  from vllm import SamplingParams
  from vllm.engine.arg_utils import AsyncEngineArgs
  from vllm.sampling_params import RequestOutputKind
  from vllm.v1.engine.async_llm import AsyncLLM


  async def stream_response(engine: AsyncLLM, prompt: str, request_id: str) -> None:
      print(f"\n🚀 Prompt: {prompt!r}")
      print("💬 Response: ", end="", flush=True)

      # Configure sampling parameters for streaming. `output_kind` for getting only new tokens each iteration
      sampling_params = SamplingParams(max_tokens=10, temperature=0.8, top_p=0.95, seed=42, output_kind=RequestOutputKind.DELTA)

      # Stream tokens from AsyncLLM
      async for output in engine.generate(request_id=request_id, prompt=prompt, sampling_params=sampling_params):
          # Process each completion in the output
          for completion in output.outputs:
              # In DELTA mode, we get only new tokens generated since last iteration
              new_text = completion.text
              if new_text:
                  print(new_text, end="|", flush=True)
              # Check if generation is finished
              if output.finished:
                  print("\n✅ Generation complete!")
                  break

  async def main():
      print("🔧 Initializing AsyncLLM...")

      # Create AsyncLLM engine with simple configuration. Using eager for faster startup for examples
      engine_args = AsyncEngineArgs(model="workspace/qwen3-30m-fp16/", enforce_eager=True)
      engine = AsyncLLM.from_engine_args(engine_args)
      # Example prompts to demonstrate streaming
      prompts = ["The future of artificial intelligence is", "In a galaxy far, far away", "The key to happiness is"]
      print(f"🎯 Running {len(prompts)} streaming examples...")

      # Process each prompt
      for i, prompt in enumerate(prompts, 1):
          print(f"\n{'=' * 60}")
          print(f"Example {i}/{len(prompts)}")
          print(f"{'=' * 60}")

          request_id = f"stream-example-{i}"
          await stream_response(engine, prompt, request_id)

          # Brief pause between examples
          if i < len(prompts):
              await asyncio.sleep(0.5)
              print("\n🎉 All streaming examples completed!")

  asyncio.run(main())
  ```
  ```py
  import asyncio
  from openai import AsyncOpenAI

  client = AsyncOpenAI(
      base_url="http://localhost:8000/v1",  # 你的 vLLM OpenAI 接口
      api_key="EMPTY",                      # vLLM 默认不校验
      timeout=60.0                          # 可调：整体超时
  )

  async def chat_once(i: int):
      # 发起一个流式请求
      stream = await client.chat.completions.create(
          model="workspace/qwen3-30m-fp16/",      # 换成你加载的模型名
          messages=[
              {"role": "system", "content": "You are concise."},
              {"role": "user", "content": f"任务 #{i}: 用一句话解释什么是流式SSE？"}
          ],
          stream=True
      )

      print(f"[#{i}] >>> streaming start")
      try:
          async for chunk in stream:
              delta = chunk.choices[0].delta
              if delta and delta.content:
                  print(delta.content, end="", flush=True)
      finally:
          # 显式关闭底层连接（可选，便于提早中止）
          await stream.aclose()

      print(f"\n[#{i}] <<< done\n")

  async def main():
      # 并发跑多个 streaming，不会阻塞 event loop
      await asyncio.gather(*(chat_once(i) for i in range(3)))

  if __name__ == "__main__":
      asyncio.run(main())
  ```
